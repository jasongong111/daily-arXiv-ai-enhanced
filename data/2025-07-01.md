<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 45]
- [cs.LG](#cs.LG) [Total: 112]
- [cs.CV](#cs.CV) [Total: 222]
- [cs.CL](#cs.CL) [Total: 82]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Bootstrapping Human-Like Planning via LLMs](https://arxiv.org/abs/2506.22604)
*David Porfirio,Vincent Hsiao,Morgan Fine-Morris,Leslie Smith,Laura M. Hiatt*

Main category: cs.AI

TL;DR: Combining natural language and drag-and-drop interfaces for robot task specification using LLMs, with larger models outperforming smaller ones in generating human-like action sequences.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between intuitive natural language interfaces and precise drag-and-drop programming for robot task specification.

Method: Developed an LLM-based pipeline that converts natural language input into human-like action sequences, compared to hand-specified sequences.

Result: Larger LLMs generate more human-like sequences, but smaller models still perform satisfactorily.

Conclusion: Combining natural language and drag-and-drop paradigms is feasible, with LLMs effectively bridging the gap.

Abstract: Robot end users increasingly require accessible means of specifying tasks for
robots to perform. Two common end-user programming paradigms include
drag-and-drop interfaces and natural language programming. Although natural
language interfaces harness an intuitive form of human communication,
drag-and-drop interfaces enable users to meticulously and precisely dictate the
key actions of the robot's task. In this paper, we investigate the degree to
which both approaches can be combined. Specifically, we construct a large
language model (LLM)-based pipeline that accepts natural language as input and
produces human-like action sequences as output, specified at a level of
granularity that a human would produce. We then compare these generated action
sequences to another dataset of hand-specified action sequences. Although our
results reveal that larger models tend to outperform smaller ones in the
production of human-like action sequences, smaller models nonetheless achieve
satisfactory performance.

</details>


### [2] [Ludax: A GPU-Accelerated Domain Specific Language for Board Games](https://arxiv.org/abs/2506.22609)
*Graham Todd,Alexander G. Padula,Dennis J. N. J. Soemers,Julian Togelius*

Main category: cs.AI

TL;DR: Ludax is a domain-specific language for board games that compiles into hardware-accelerated code, combining generality with speed for AI research.


<details>
  <summary>Details</summary>
Motivation: To accelerate games research (e.g., RL, cognitive science) by enabling rapid simulation and flexible representation.

Method: Developed Ludax, a framework that compiles game descriptions into hardware-accelerated code, integrating with deep learning pipelines.

Result: Ludax provides speed benchmarking, RL agent training demonstrations, and is open-source.

Conclusion: Ludax bridges game description languages and hardware acceleration, offering a tool to advance AI research efficiently.

Abstract: Games have long been used as benchmarks and testing environments for research
in artificial intelligence. A key step in supporting this research was the
development of game description languages: frameworks that compile
domain-specific code into playable and simulatable game environments, allowing
researchers to generalize their algorithms and approaches across multiple games
without having to manually implement each one. More recently, progress in
reinforcement learning (RL) has been largely driven by advances in hardware
acceleration. Libraries like JAX allow practitioners to take full advantage of
cutting-edge computing hardware, often speeding up training and testing by
orders of magnitude. Here, we present a synthesis of these strands of research:
a domain-specific language for board games which automatically compiles into
hardware-accelerated code. Our framework, Ludax, combines the generality of
game description languages with the speed of modern parallel processing
hardware and is designed to fit neatly into existing deep learning pipelines.
We envision Ludax as a tool to help accelerate games research generally, from
RL to cognitive science, by enabling rapid simulation and providing a flexible
representation scheme. We present a detailed breakdown of Ludax's description
language and technical notes on the compilation process, along with speed
benchmarking and a demonstration of training RL agents. The Ludax framework,
along with implementations of existing board games, is open-source and freely
available.

</details>


### [3] [URSA: The Universal Research and Scientific Agent](https://arxiv.org/abs/2506.22653)
*Michael Grosskopf,Russell Bent,Rahul Somasundaram,Isaac Michaud,Arthur Lui,Nathan Debardeleben,Earl Lawrence*

Main category: cs.AI

TL;DR: URSA is a scientific agent ecosystem using LLMs to accelerate research tasks through modular agents and tools, including physics simulations.


<details>
  <summary>Details</summary>
Motivation: LLMs' advanced capabilities overlap with human scientists' skills, offering potential to revolutionize research by addressing bottlenecks.

Method: URSA employs modular agents and tools, integrating advanced physics simulation codes, to tackle diverse scientific problems.

Result: The architecture and examples demonstrate URSA's potential to enhance research efficiency and impact.

Conclusion: URSA showcases how agentic AI can transform scientific research by leveraging LLMs and modular tools.

Abstract: Large language models (LLMs) have moved far beyond their initial form as
simple chatbots, now carrying out complex reasoning, planning, writing, coding,
and research tasks. These skills overlap significantly with those that human
scientists use day-to-day to solve complex problems that drive the cutting edge
of research. Using LLMs in "agentic" AI has the potential to revolutionize
modern science and remove bottlenecks to progress. In this work, we present
URSA, a scientific agent ecosystem for accelerating research tasks. URSA
consists of a set of modular agents and tools, including coupling to advanced
physics simulation codes, that can be combined to address scientific problems
of varied complexity and impact. This work highlights the architecture of URSA,
as well as examples that highlight the potential of the system.

</details>


### [4] [Explanations are a means to an end](https://arxiv.org/abs/2506.22740)
*Jessica Hullman,Ziyang Guo,Berk Ustun*

Main category: cs.AI

TL;DR: The paper advocates for designing and evaluating machine learning explanations with specific practical goals in mind, using a statistical decision theory framework.


<details>
  <summary>Details</summary>
Motivation: Current explainable ML methods lack consideration of practical usage, leading to potential misuse or ambiguity.

Method: Proposes a functionally-grounded approach based on statistical decision theory, applied to diverse use cases like clinical decision support and debugging.

Result: Demonstrates how to quantify the potential performance boost of explanations for idealized decision-makers, ensuring clarity by specifying concrete use cases.

Conclusion: Evaluation of explanations should combine theoretical and empirical perspectives, with clear definitions to bridge these views.

Abstract: Modern methods for explainable machine learning are designed to describe how
models map inputs to outputs--without deep consideration of how these
explanations will be used in practice. This paper argues that explanations
should be designed and evaluated with a specific end in mind. We describe how
to formalize this end in a framework based in statistical decision theory. We
show how this functionally-grounded approach can be applied across diverse use
cases, such as clinical decision support, providing recourse, or debugging. We
demonstrate its use to characterize the maximum "boost" in performance on a
particular task that an explanation could provide an idealized decision-maker,
preventing misuse due to ambiguity by forcing researchers to specify concrete
use cases that can be analyzed in light of models of expected explanation use.
We argue that evaluation should meld theoretical and empirical perspectives on
the value of explanation, and contribute definitions that span these
perspectives.

</details>


### [5] [Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems](https://arxiv.org/abs/2506.22774)
*Michael Papademas,Xenia Ziouvelou,Antonis Troumpoukis,Vangelis Karkaletsis*

Main category: cs.AI

TL;DR: The paper introduces a method combining ethical components of Trustworthy AI with algorithmic processes (PageRank and TrustRank) to create a holistic, quantitative assessment framework for AI trustworthiness.


<details>
  <summary>Details</summary>
Motivation: AI's pervasive societal impact and lack of direct human oversight necessitate tools to assess its trustworthiness, balancing ethical guidelines and technical quantification.

Method: Develops an assessment framework integrating ethical components of Trustworthy AI with algorithmic processes (PageRank and TrustRank) to minimize subjectivity.

Result: The approach provides quantitative insights into AI trustworthiness while aligning with theoretical guidelines, achieving a holistic assessment.

Conclusion: The proposed method successfully bridges the gap between ethical guidelines and technical quantification, offering a more objective and comprehensive assessment of AI trustworthiness.

Abstract: Artificial Intelligence (AI) technology epitomizes the complex challenges
posed by human-made artifacts, particularly those widely integrated into
society and exert significant influence, highlighting potential benefits and
their negative consequences. While other technologies may also pose substantial
risks, AI's pervasive reach makes its societal effects especially profound. The
complexity of AI systems, coupled with their remarkable capabilities, can lead
to a reliance on technologies that operate beyond direct human oversight or
understanding. To mitigate the risks that arise, several theoretical tools and
guidelines have been developed, alongside efforts to create technological tools
aimed at safeguarding Trustworthy AI. The guidelines take a more holistic view
of the issue but fail to provide techniques for quantifying trustworthiness.
Conversely, while technological tools are better at achieving such
quantification, they lack a holistic perspective, focusing instead on specific
aspects of Trustworthy AI. This paper aims to introduce an assessment method
that combines the ethical components of Trustworthy AI with the algorithmic
processes of PageRank and TrustRank. The goal is to establish an assessment
framework that minimizes the subjectivity inherent in the self-assessment
techniques prevalent in the field by introducing algorithmic criteria. The
application of our approach indicates that a holistic assessment of an AI
system's trustworthiness can be achieved by providing quantitative insights
while considering the theoretical content of relevant guidelines.

</details>


### [6] [ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source Language Models](https://arxiv.org/abs/2506.22865)
*Ziqi Zhong,Xunzhu Tang*

Main category: cs.AI

TL;DR: ReasonBridge is a method to transfer reasoning skills from closed-source to open-source LLMs using hierarchical distillation, a small curated dataset, and sparse adapters, achieving up to 23% improvement.


<details>
  <summary>Details</summary>
Motivation: Address the performance gap between closed-source and open-source LLMs in complex reasoning tasks.

Method: Hierarchical knowledge distillation, a 1K curated dataset (Reason1K), sparse adapters, and guided inference interventions.

Result: Open-source models improved by up to 23%, with Qwen2.5-14B outperforming Claude-Sonnet3.5 on some tasks.

Conclusion: ReasonBridge efficiently enhances reasoning in open-source models, narrowing the gap with closed-source ones.

Abstract: Recent advancements in Large Language Models (LLMs) have revealed a
significant performance gap between closed-source and open-source models,
particularly in tasks requiring complex reasoning and precise instruction
following. This paper introduces ReasonBridge, a methodology that efficiently
transfers reasoning capabilities from powerful closed-source to open-source
models through a novel hierarchical knowledge distillation framework. We
develop a tailored dataset Reason1K with only 1,000 carefully curated reasoning
traces emphasizing difficulty, diversity, and quality. These traces are
filtered from across multiple domains using a structured multi-criteria
selection algorithm. Our transfer learning approach incorporates: (1) a
hierarchical distillation process capturing both strategic abstraction and
tactical implementation patterns, (2) a sparse reasoning-focused adapter
architecture requiring only 0.3% additional trainable parameters, and (3) a
test-time compute scaling mechanism using guided inference interventions.
Comprehensive evaluations demonstrate that ReasonBridge improves reasoning
capabilities in open-source models by up to 23% on benchmark tasks,
significantly narrowing the gap with closed-source models. Notably, the
enhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its
performance on competition-level AIME problems. Our methodology generalizes
effectively across diverse reasoning domains and model architectures,
establishing a sample-efficient approach to reasoning enhancement for
instruction following.

</details>


### [7] [Agentic Enterprise: AI-Centric User to User-Centric AI](https://arxiv.org/abs/2506.22893)
*Arpit Narechania,Alex Endert,Atanu R Sinha*

Main category: cs.AI

TL;DR: The paper discusses AI's potential in enterprises, focusing on decision-making and introducing six tenets for successful AI agents, advocating a shift to user-centric AI.


<details>
  <summary>Details</summary>
Motivation: AI's growing impact on various human domains, especially enterprises where decision-making is critical, motivates the exploration of AI agents to enhance decision productivity.

Method: The paper proposes six tenets for AI agent success in enterprises, contrasting the current AI-centric user paradigm with a user-centric AI approach.

Result: The study highlights the need for aligning AI design and delivery with enterprise user needs, promoting market mechanisms for platforms.

Conclusion: A shift to user-centric AI and adherence to six tenets can enhance AI's effectiveness in enterprise decision-making.

Abstract: After a very long winter, the Artificial Intelligence (AI) spring is here.
Or, so it seems over the last three years. AI has the potential to impact many
areas of human life - personal, social, health, education, professional. In
this paper, we take a closer look at the potential of AI for Enterprises, where
decision-making plays a crucial and repeated role across functions, tasks, and
operations. We consider Agents imbued with AI as means to increase
decision-productivity of enterprises. We highlight six tenets for Agentic
success in enterprises, by drawing attention to what the current, AI-Centric
User paradigm misses, in the face of persistent needs of and usefulness for
Enterprise Decision-Making. In underscoring a shift to User-Centric AI, we
offer six tenets and promote market mechanisms for platforms, aligning the
design of AI and its delivery by Agents to the cause of enterprise users.

</details>


### [8] [Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning](https://arxiv.org/abs/2506.22919)
*Sanskar Pandey,Ruhaan Chopra,Saad Murtaza Bhat,Ark Abhyudaya*

Main category: cs.AI

TL;DR: Hecto, a lightweight MoE architecture, combines GRU and FFNN experts under sparse Top-1 gating, achieving specialization and interpretability while matching homogeneous baselines in performance.


<details>
  <summary>Details</summary>
Motivation: Current MoE models limit representational diversity due to identical inductive biases, leading to inefficient computation and restricted specialization.

Method: Hecto integrates a GRU expert for temporal reasoning and an FFNN expert for static abstraction, using a sparse Top-1 gating mechanism.

Result: Hecto matches or nears homogeneous baselines on reasoning benchmarks (AG News, SST-2, HotpotQA) and regression (STS-B), with clear expert specialization and improved performance at larger batch sizes.

Conclusion: Hecto sets a new benchmark for conditional computation, offering specialized reasoning in low-resource settings through architectural diversity.

Abstract: Mixture-of-Experts (MoE) models enable conditional computation by routing
inputs to specialized experts, but these experts rely on identical inductive
biases, thus limiting representational diversity. This static computation
pathway is inefficient for inputs that require different types of reasoning and
limits specialization and interpretability. We propose Hecto, a lightweight MoE
architecture that leverages architectural heterogeneity by combining a GRU
expert for temporal reasoning and an FFNN expert for static abstraction under a
sparse Top-1 gating mechanism. Evaluated on three reasoning benchmarks (AG
News, SST-2, HotpotQA) and a regression task (STS-B), Hecto matches or closely
trails homogeneous baselines in performance despite receiving isolated input
representations, while achieving clear expert specialization, with each expert
aligning to distinct reasoning types (temporal vs static). At larger batch
sizes, Hecto exhibits improved performance, benefiting from relaxed
computational constraints that allow its heterogeneous architecture to optimize
more effectively. Ablation results isolate architectural diversity as the
source of Hecto's stability and interpretability across diverse reasoning
tasks. Overall, Hecto establishes itself as a new benchmark for conditional
computation, offering a principled framework for specialized reasoning in
low-resource regimes with its model strength derived from principled
specialization.

</details>


### [9] [Improving Rationality in the Reasoning Process of Language Models through Self-playing Game](https://arxiv.org/abs/2506.22920)
*Pinzheng Wang,Juntao Li,Zecheng Tang,Haijia Gui,Min zhang*

Main category: cs.AI

TL;DR: Self-play via Critic-Discernment Game (CDG) improves LLMs' reasoning comprehension without supervision.


<details>
  <summary>Details</summary>
Motivation: LLMs lack true understanding of their reasoning processes despite strong performance in tasks like math and coding.

Method: Introduces CDG, where a prover defends solutions against critiques (helpful or misleading) to refine reasoning.

Result: CDG training enhances LLMs' ability to comprehend reasoning in math, error detection, self-correction, and long-chain tasks.

Conclusion: Self-play with CDG effectively boosts LLMs' reasoning comprehension autonomously.

Abstract: Large language models (LLMs) have demonstrated considerable reasoning
abilities in various tasks such as mathematics and coding. However, recent
studies indicate that even the best models lack true comprehension of their
reasoning processes. In this paper, we explore how self-play can enhance the
rationality of models in the reasoning process without supervision from humans
or superior models. We design a Critic-Discernment Game(CDG) in which a prover
first provides a solution to a given problem and is subsequently challenged by
critiques of its solution. These critiques either aim to assist or mislead the
prover. The objective of the prover is to maintain the correct answer when
faced with misleading comments, while correcting errors in response to
constructive feedback. Our experiments on tasks involving mathematical
reasoning, stepwise error detection, self-correction, and long-chain reasoning
demonstrate that CDG training can significantly improve the ability of
well-aligned LLMs to comprehend their reasoning process.

</details>


### [10] [MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning](https://arxiv.org/abs/2506.22992)
*Yulun Jiang,Yekun Chai,Maria Brbić,Michael Moor*

Main category: cs.AI

TL;DR: MARBLE is a multimodal reasoning benchmark designed to test MLLMs' ability to handle complex, step-by-step reasoning across modalities, revealing significant limitations in current models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack focus on complex multimodal reasoning, limiting understanding of MLLMs' capabilities in such tasks.

Method: MARBLE introduces two tasks, M-Portal and M-Cube, requiring multistep planning under spatial, visual, and physical constraints.

Result: Current MLLMs perform poorly, with near-random accuracy on M-Portal and 0% on M-Cube, highlighting reasoning and perception challenges.

Conclusion: MARBLE exposes MLLMs' limitations, aiming to inspire advancements in multimodal reasoning and planning.

Abstract: The ability to process information from multiple modalities and to reason
through it step-by-step remains a critical challenge in advancing artificial
intelligence. However, existing reasoning benchmarks focus on text-only
reasoning, or employ multimodal questions that can be answered by directly
retrieving information from a non-text modality. Thus, complex reasoning
remains poorly understood in multimodal domains. Here, we present MARBLE, a
challenging multimodal reasoning benchmark that is designed to scrutinize
multimodal language models (MLLMs) in their ability to carefully reason
step-by-step through complex multimodal problems and environments. MARBLE is
composed of two highly challenging tasks, M-Portal and M-Cube, that require the
crafting and understanding of multistep plans under spatial, visual, and
physical constraints. We find that current MLLMs perform poorly on MARBLE --
all the 12 advanced models obtain near-random performance on M-Portal and 0%
accuracy on M-Cube. Only in simplified subtasks some models outperform the
random baseline, indicating that complex reasoning is still a challenge for
existing MLLMs. Moreover, we show that perception remains a bottleneck, where
MLLMs occasionally fail to extract information from the visual inputs. By
shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the
development of the next generation of models with the ability to reason and
plan across many, multimodal reasoning steps.

</details>


### [11] [AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks](https://arxiv.org/abs/2506.23049)
*Leander Melroy Maben,Gayathri Ganesh Lakshmy,Srijith Radhakrishnan,Siddhant Arora,Shinji Watanabe*

Main category: cs.AI

TL;DR: AURA is the first open-source, speech-native assistant for multi-turn dialogue with tool use, outperforming other open-weight systems in benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of open-source systems for speech-to-speech, multi-turn dialogue with integrated tool use and agentic reasoning.

Method: Combines open-weight ASR, TTS, and LLMs in a cascaded pipeline, supporting tools like calendar booking and web search via natural language prompts.

Result: Scores 92.75% on VoiceBench (OpenBookQA) and 4.39 on AlpacaEval, with 90% task success in human evaluations.

Conclusion: AURA demonstrates strong performance in complex, multi-turn speech tasks, setting a benchmark for open-source assistants.

Abstract: Despite advances in language and speech technologies, no open-source system
enables full speech-to-speech, multi-turn dialogue with integrated tool use and
agentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and
Automated Tool Use), the first open-source, speech-native assistant capable of
completing complex, goal-driven tasks through dynamic tool invocation and
multi-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a
cascaded pipeline and supports tools such as calendar booking, contact lookup,
web search, and email. Its modular design allows easy integration of new tools
using natural language prompts and action classes. On VoiceBench, AURA scores
92.75% on OpenBookQA-outperforming all open-weight systems and nearing
GPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.
Human evaluation shows 90% task success on complex, multi-turn speech tasks.

</details>


### [12] [AI's Euclid's Elements Moment: From Language Models to Computable Thought](https://arxiv.org/abs/2506.23080)
*Xinmin Fang,Lingfeng Tao,Zhengxiong Li*

Main category: cs.AI

TL;DR: The paper proposes a five-stage evolutionary framework for AI development, likening it to human cognitive advancements, and predicts future stages of AI growth.


<details>
  <summary>Details</summary>
Motivation: To systematically explain AI's historical progression and predict its future evolution, offering actionable insights for developers.

Method: Introduces the 'Geometry of Cognition' framework, analyzing AI's stages from expert systems to Transformers, and forecasts future reflexive advancements.

Result: Identifies a 'Metalinguistic Moment' and predicts subsequent stages leading to provably aligned AI.

Conclusion: Provides a theoretical foundation for future AI research and practical strategies for next-gen intelligent systems.

Abstract: This paper presents a comprehensive five-stage evolutionary framework for
understanding the development of artificial intelligence, arguing that its
trajectory mirrors the historical progression of human cognitive technologies.
We posit that AI is advancing through distinct epochs, each defined by a
revolutionary shift in its capacity for representation and reasoning, analogous
to the inventions of cuneiform, the alphabet, grammar and logic, mathematical
calculus, and formal logical systems. This "Geometry of Cognition" framework
moves beyond mere metaphor to provide a systematic, cross-disciplinary model
that not only explains AI's past architectural shifts-from expert systems to
Transformers-but also charts a concrete and prescriptive path forward.
Crucially, we demonstrate that this evolution is not merely linear but
reflexive: as AI advances through these stages, the tools and insights it
develops create a feedback loop that fundamentally reshapes its own underlying
architecture. We are currently transitioning into a "Metalinguistic Moment,"
characterized by the emergence of self-reflective capabilities like
Chain-of-Thought prompting and Constitutional AI. The subsequent stages, the
"Mathematical Symbolism Moment" and the "Formal Logic System Moment," will be
defined by the development of a computable calculus of thought, likely through
neuro-symbolic architectures and program synthesis, culminating in provably
aligned and reliable AI that reconstructs its own foundational representations.
This work serves as the methodological capstone to our trilogy, which
previously explored the economic drivers ("why") and cognitive nature ("what")
of AI. Here, we address the "how," providing a theoretical foundation for
future research and offering concrete, actionable strategies for startups and
developers aiming to build the next generation of intelligent systems.

</details>


### [13] [Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study](https://arxiv.org/abs/2506.23107)
*Bing Song,Jianing Liu,Sisi Jian,Chenyang Wu,Vinayak Dixit*

Main category: cs.AI

TL;DR: The study evaluates LLMs' ability to simulate risky decision-making, comparing ChatGPT 4o and ChatGPT o1-mini with human responses in lottery tasks. Results show models are more risk-averse, with o1-mini closer to human behavior. Performance varies by language, with Chinese prompts less accurate than English.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' reliability in simulating complex decision-making, like risky choices, given their growing use in diverse applications.

Method: Compared model-generated decisions with human responses in lottery tasks using CRRA framework, analyzing multilingual data from four cities.

Result: Both models were more risk-averse than humans, with o1-mini closer to human behavior. Chinese prompts showed less accuracy than English.

Conclusion: LLMs show promise but have limitations in replicating human-like risk behavior, especially in linguistic and cultural contexts.

Abstract: Large language models (LLMs) have made significant strides, extending their
applications to dialogue systems, automated content creation, and
domain-specific advisory tasks. However, as their use grows, concerns have
emerged regarding their reliability in simulating complex decision-making
behavior, such as risky decision-making, where a single choice can lead to
multiple outcomes. This study investigates the ability of LLMs to simulate
risky decision-making scenarios. We compare model-generated decisions with
actual human responses in a series of lottery-based tasks, using transportation
stated preference survey data from participants in Sydney, Dhaka, Hong Kong,
and Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and
ChatGPT o1-mini -- which were tasked with predicting individual choices. Risk
preferences were analyzed using the Constant Relative Risk Aversion (CRRA)
framework. Results show that both models exhibit more risk-averse behavior than
human participants, with o1-mini aligning more closely with observed human
decisions. Further analysis of multilingual data from Nanjing and Hong Kong
indicates that model predictions in Chinese deviate more from actual responses
compared to English, suggesting that prompt language may influence simulation
performance. These findings highlight both the promise and the current
limitations of LLMs in replicating human-like risk behavior, particularly in
linguistic and cultural settings.

</details>


### [14] [The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy](https://arxiv.org/abs/2506.23123)
*Rishi Bommasani*

Main category: cs.AI

TL;DR: The dissertation explores the coevolution of technology and society in the AI era, focusing on foundation models' capabilities, risks, and societal impacts, aiming to improve AI governance.


<details>
  <summary>Details</summary>
Motivation: To address the confusion and potential harms caused by poorly understood foundation models in AI, and to bridge the gap between technology and societal outcomes.

Method: Organized into three themes: conceptual framing (capabilities, risks, supply chain), empirical insights (transparency via evaluations and indexes), and actionable understanding (evidence-based AI policy).

Result: Advances scientific foundations and research-policy interfaces for better AI governance, aiming for improved societal outcomes.

Conclusion: The dissertation contributes to better societal outcomes in the AI age by enhancing understanding and governance of foundation models.

Abstract: Artificial intelligence is humanity's most promising technology because of
the remarkable capabilities offered by foundation models. Yet, the same
technology brings confusion and consternation: foundation models are poorly
understood and they may precipitate a wide array of harms. This dissertation
explains how technology and society coevolve in the age of AI, organized around
three themes. First, the conceptual framing: the capabilities, risks, and the
supply chain that grounds foundation models in the broader economy. Second, the
empirical insights that enrich the conceptual foundations: transparency created
via evaluations at the model level and indexes at the organization level.
Finally, the transition from understanding to action: superior understanding of
the societal impact of foundation models advances evidence-based AI policy.
View together, this dissertation makes inroads into achieving better societal
outcomes in the age of AI by building the scientific foundations and
research-policy interface required for better AI governance.

</details>


### [15] [Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons](https://arxiv.org/abs/2506.23128)
*Chi Chiu So,Yueyue Sun,Jun-Min Wang,Siu Pang Yung,Anthony Wai Keung Loh,Chun Pong Chau*

Main category: cs.AI

TL;DR: The paper evaluates the relational reasoning capabilities of three LLMs (DeepSeek-R1, DeepSeek-V3, GPT-4o) using benchmark tasks. DeepSeek-R1 outperforms others but struggles with complexity due to token limits and incomplete outputs.


<details>
  <summary>Details</summary>
Motivation: To assess and compare the deep relational reasoning abilities of state-of-the-art LLMs, identifying strengths and limitations in logical deduction and relational inference.

Method: Benchmark tasks in family tree and general graph reasoning were used to evaluate the models. DeepSeek-R1's Chain-of-Thought responses were analyzed for planning and verification strategies.

Result: DeepSeek-R1 achieved the highest F1-scores but all models faltered with increased complexity. Incoherent reasoning in DeepSeek-R1 was noted.

Conclusion: The study highlights the need for deeper scrutiny of LLMs' reasoning dynamics and suggests future work on multimodal reasoning and systematic failure analysis.

Abstract: How far are Large Language Models (LLMs) in performing deep relational
reasoning? In this paper, we evaluate and compare the reasoning capabilities of
three cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a
suite of carefully designed benchmark tasks in family tree and general graph
reasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the
highest F1-scores across multiple tasks and problem sizes, demonstrating strong
aptitude in logical deduction and relational inference. However, all evaluated
models, including DeepSeek-R1, struggle significantly as problem complexity
increases, largely due to token length limitations and incomplete output
structures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought
responses uncovers its unique planning and verification strategies, but also
highlights instances of incoherent or incomplete reasoning, calling attention
to the need for deeper scrutiny into LLMs' internal inference dynamics. We
further discuss key directions for future work, including the role of
multimodal reasoning and the systematic examination of reasoning failures. Our
findings provide both empirical insights and theoretical implications for
advancing LLMs' reasoning abilities, particularly in tasks that demand
structured, multi-step logical inference. Our code repository will be publicly
available at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.

</details>


### [16] [Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing](https://arxiv.org/abs/2506.23141)
*Siyuan Li,Ruitong Liu,Yan Wen,Te Sun*

Main category: cs.AI

TL;DR: Proposes a semantic-aware relational message passing framework for KGC, using a Top-K neighbor selection strategy and multi-head attention to improve prediction accuracy by focusing on relevant contextual information.


<details>
  <summary>Details</summary>
Motivation: Traditional node-based message passing in KGC introduces noise and suffers from information dilution or over-smoothing by aggregating all neighboring edges indiscriminately.

Method: Introduces a semantic-aware Top-K neighbor selection strategy to evaluate and select relevant edges, followed by a multi-head attention aggregator to fuse information.

Result: Achieves superior performance on established benchmarks by mitigating interference from irrelevant information.

Conclusion: The framework effectively leverages semantic context and edge features, improving KGC accuracy.

Abstract: Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge
Graph Completion (KGC), providing vital cues for prediction. However,
traditional node-based message passing mechanisms, when applied to knowledge
graphs, often introduce noise and suffer from information dilution or
over-smoothing by indiscriminately aggregating information from all neighboring
edges. To address this challenge, we propose a semantic-aware relational
message passing. A core innovation of this framework is the introduction of a
\textbf{semantic-aware Top-K neighbor selection strategy}. Specifically, this
strategy first evaluates the semantic relevance between a central node and its
incident edges within a shared latent space, selecting only the Top-K most
pertinent ones. Subsequently, information from these selected edges is
effectively fused with the central node's own representation using a
\textbf{multi-head attention aggregator} to generate a semantically focused
node message. In this manner, our model not only leverages the structure and
features of edges within the knowledge graph but also more accurately captures
and propagates the contextual information most relevant to the specific link
prediction task, thereby effectively mitigating interference from irrelevant
information. Extensive experiments demonstrate that our method achieves
superior performance compared to existing approaches on several established
benchmarks.

</details>


### [17] [Rises for Measuring Local Distributivity in Lattices](https://arxiv.org/abs/2506.23168)
*Mohammad Abdulla,Tobias Hille,Dominik Dürrschnabel,Gerd Stumme*

Main category: cs.AI

TL;DR: The paper introduces 'rises' in concept lattices to measure distributivity, showing their link to classical distributivity notions and analyzing real-world data.


<details>
  <summary>Details</summary>
Motivation: To quantify distributivity in Formal Concept Analysis (FCA) lattices, where no standardized measure exists.

Method: Introduces 'rises' to assess distributivity, linking them to meet- and join-distributivity, and analyzes real-world concept lattices.

Result: Concept lattices from real-world data are highly join-distributive but less meet-distributive.

Conclusion: Rises effectively measure distributivity, revealing distinct distributive behaviors in real-world lattices.

Abstract: Distributivity is a well-established and extensively studied notion in
lattice theory. In the context of data analysis, particularly within Formal
Concept Analysis (FCA), lattices are often observed to exhibit a high degree of
distributivity. However, no standardized measure exists to quantify this
property. In this paper, we introduce the notion of rises in (concept) lattices
as a means to assess distributivity. Rises capture how the number of attributes
or objects in covering concepts change within the concept lattice. We show that
a lattice is distributive if and only if no non-unit rises occur. Furthermore,
we relate rises to the classical notion of meet- and join distributivity. We
observe that concept lattices from real-world data are to a high degree
join-distributive, but much less meet-distributive. We additionally study how
join-distributivity manifests on the level of ordered sets.

</details>


### [18] [FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis](https://arxiv.org/abs/2506.23273)
*Quang Hung Nguyen,Phuong Anh Trinh,Phan Quoc Hung Mai,Tuan Phong Trinh*

Main category: cs.AI

TL;DR: FinStat2SQL is a lightweight text2sql pipeline for financial statements, combining large and small language models to handle domain-specific queries. It achieves 61.33% accuracy with fast response times, outperforming GPT-4o-mini.


<details>
  <summary>Details</summary>
Motivation: Text2SQL faces challenges with complex, domain-specific queries, especially in finance due to varying database designs and reporting standards.

Method: Uses a multi-agent setup with large and small language models for entity extraction, SQL generation, and self-correction. Tailored to local standards like VAS.

Result: A fine-tuned 7B model achieves 61.33% accuracy with sub-4-second response times, outperforming GPT-4o-mini.

Conclusion: FinStat2SQL provides a scalable, cost-efficient solution for financial analysis, making AI-powered querying accessible to Vietnamese enterprises.

Abstract: Despite the advancements of large language models, text2sql still faces many
challenges, particularly with complex and domain-specific queries. In finance,
database designs and financial reporting layouts vary widely between financial
entities and countries, making text2sql even more challenging. We present
FinStat2SQL, a lightweight text2sql pipeline enabling natural language queries
over financial statements. Tailored to local standards like VAS, it combines
large and small language models in a multi-agent setup for entity extraction,
SQL generation, and self-correction. We build a domain-specific database and
evaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves
61.33\% accuracy with sub-4-second response times on consumer hardware,
outperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient
solution for financial analysis, making AI-powered querying accessible to
Vietnamese enterprises.

</details>


### [19] [Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games](https://arxiv.org/abs/2506.23276)
*David Guzman Piedrahita,Yongjin Yang,Mrinmaya Sachan,Giorgia Ramponi,Bernhard Schölkopf,Zhijing Jin*

Main category: cs.AI

TL;DR: The paper investigates how LLMs balance self-interest and collective well-being in multi-agent systems, focusing on costly sanctioning. It identifies four behavioral patterns and finds reasoning LLMs struggle with cooperation, unlike traditional LLMs.


<details>
  <summary>Details</summary>
Motivation: Understanding LLMs' cooperation and social mechanisms is critical for alignment, robustness, and safe deployment in autonomous agents.

Method: Adapts a public goods game with institutional choice from behavioral economics to observe LLMs' behavior in social dilemmas over repeated interactions.

Result: Four behavioral patterns emerge: sustained cooperation, fluctuating engagement, declining cooperation, and rigid strategies. Reasoning LLMs struggle with cooperation, while traditional LLMs perform better.

Conclusion: Enhancing reasoning capabilities in LLMs does not necessarily improve cooperation, offering insights for deploying LLMs in collaborative environments.

Abstract: As large language models (LLMs) are increasingly deployed as autonomous
agents, understanding their cooperation and social mechanisms is becoming
increasingly important. In particular, how LLMs balance self-interest and
collective well-being is a critical challenge for ensuring alignment,
robustness, and safe deployment. In this paper, we examine the challenge of
costly sanctioning in multi-agent LLM systems, where an agent must decide
whether to invest its own resources to incentivize cooperation or penalize
defection. To study this, we adapt a public goods game with institutional
choice from behavioral economics, allowing us to observe how different LLMs
navigate social dilemmas over repeated interactions. Our analysis reveals four
distinct behavioral patterns among models: some consistently establish and
sustain high levels of cooperation, others fluctuate between engagement and
disengagement, some gradually decline in cooperative behavior over time, and
others rigidly follow fixed strategies regardless of outcomes. Surprisingly, we
find that reasoning LLMs, such as the o1 series, struggle significantly with
cooperation, whereas some traditional LLMs consistently achieve high levels of
cooperation. These findings suggest that the current approach to improving
LLMs, which focuses on enhancing their reasoning capabilities, does not
necessarily lead to cooperation, providing valuable insights for deploying LLM
agents in environments that require sustained collaboration. Our code is
available at https://github.com/davidguzmanp/SanctSim

</details>


### [20] [GATSim: Urban Mobility Simulation with Generative Agents](https://arxiv.org/abs/2506.23306)
*Qi Liu,Can Li,Wanjing Ma*

Main category: cs.AI

TL;DR: GATSim introduces generative agents with adaptive learning for urban mobility simulation, outperforming rule-based systems by capturing human-like decision-making.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based urban mobility simulations lack adaptability and behavioral diversity, prompting the need for AI-driven generative agents.

Method: GATSim combines an urban mobility foundation model, agent cognitive systems, and transport simulation, featuring agents with memory, learning, and tool usage.

Result: Generative agents produce believable travel behaviors, match human annotators in mobility scenarios, and generate realistic traffic patterns.

Conclusion: GATSim demonstrates the potential of generative agents for realistic urban mobility simulation, with a functional prototype and open-source code.

Abstract: Traditional agent-based urban mobility simulations rely on rigid rule-based
systems that fail to capture the complexity, adaptability, and behavioral
diversity characteristic of human travel decision-making. Recent advances in
large language models and AI agent technology offer opportunities to create
agents with reasoning capabilities, persistent memory, and adaptive learning
mechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel
framework that leverages these advances to create generative agents with rich
behavioral characteristics for urban mobility simulation. Unlike conventional
approaches, GATSim agents possess diverse socioeconomic attributes, individual
lifestyles, and evolving preferences that shape their mobility decisions
through psychologically-informed memory systems, tool usage capabilities, and
lifelong learning mechanisms. The main contributions of this study include: (1)
a comprehensive architecture combining an urban mobility foundation model with
agent cognitive systems and transport simulation environment, (2) a fully
functional prototype implementation, and (3) systematic validation
demonstrating that generative agents produce believable travel behaviors.
Through designed reflection processes, generative agents in this study can
transform specific travel experiences into generalized insights, enabling
realistic behavioral adaptation over time with specialized mechanisms for
activity planning and real-time reactive behaviors tailored to urban mobility
contexts. Experiments show that generative agents perform competitively with
human annotators in mobility scenarios while naturally producing macroscopic
traffic evolution patterns. The code for the prototype system is shared at
https://github.com/qiliuchn/gatsim.

</details>


### [21] [The Confidence Paradox: Can LLM Know When It's Wrong](https://arxiv.org/abs/2506.23464)
*Sahil Tripathi,Md Tabrez Nafis,Imran Hussain,Jiechao Gao*

Main category: cs.AI

TL;DR: HonestVQA introduces a self-supervised honesty calibration framework for DocVQA, improving accuracy and ethical alignment by quantifying uncertainty and aligning confidence with correctness.


<details>
  <summary>Details</summary>
Motivation: Existing DocVQA systems lack ethical responsiveness, often producing overconfident answers to ambiguous questions, posing risks in ethically accountable domains.

Method: HonestVQA uses uncertainty quantification, weighted loss functions for confidence alignment, and contrastive learning for ethical response behavior. It introduces H-Score and ECI metrics for evaluation.

Result: HonestVQA improves accuracy by up to 4.3% and F1 by 4.3%, reduces overconfidence, and demonstrates strong generalization in cross-domain evaluation.

Conclusion: HonestVQA effectively addresses ethical and performance gaps in DocVQA, offering a robust framework for trustworthy AI systems.

Abstract: Document Visual Question Answering (DocVQA) systems are increasingly deployed
in real world applications, yet they remain ethically opaque-often producing
overconfident answers to ambiguous questions or failing to communicate
uncertainty in a trustworthy manner. This misalignment between model confidence
and actual knowledge poses significant risks, particularly in domains requiring
ethical accountability. Existing approaches such as LayoutLMv3, UDOP, and DONUT
have advanced SOTA performance by focusing on architectural sophistication and
accuracy; however, they fall short in ethical responsiveness.
  To address these limitations, we introduce HonestVQA, a self-supervised
honesty calibration framework for ethically aligned DocVQA. Our model-agnostic
method quantifies uncertainty to identify knowledge gaps, aligns model
confidence with actual correctness using weighted loss functions, and enforces
ethical response behavior via contrastive learning. We further introduce two
principled evaluation metrics--Honesty Score (H-Score) and Ethical Confidence
Index (ECI)--to benchmark alignment between confidence, accuracy, and ethical
communication. Empirically, HonestVQA improves DocVQA accuracy by up to 4.3%
and F1 by 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets. It reduces
overconfidence, lowering H-Score and ECI by 0.072 and 0.078, respectively. In
cross domain evaluation, it achieves up to 78.9% accuracy and 76.1% F1-score,
demonstrating strong generalization. Ablation shows a 3.8% drop in accuracy
without alignment or contrastive loss.

</details>


### [22] [Data Augmentation for Cognitive Behavioral Therapy: Leveraging ERNIE Language Models using Artificial Intelligence](https://arxiv.org/abs/2506.23503)
*Bosubabu Sambana,Kondreddygari Archana,Suram Indhra Sena Reddy,Shaik Meethaigar Jameer Basha,Shaik Karishma*

Main category: cs.AI

TL;DR: The paper proposes a CBT-based system using NLP models (BERT, RoBERTa, T5, PEGASUS, mT5) to analyze social media content for cognitive distortions and negative emotions, aiding psychotherapists in early intervention.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in methodologies for analyzing cognitive pathways in online environments, enabling timely mental health interventions.

Method: Leverages NLP models for sentiment analysis, text summarization, and translation to detect negative emotions and cognitive distortions in social media data.

Result: The system predicts negative side effects and potential mental health disorders (e.g., phobias, eating disorders), enhancing intervention strategies.

Conclusion: The proposed system offers a comprehensive tool for early detection and treatment of psychological issues in digital spaces.

Abstract: Cognitive Behavioral Therapy (CBT) is a proven approach for addressing the
irrational thought patterns associated with mental health disorders, but its
effectiveness relies on accurately identifying cognitive pathways to provide
targeted treatment. In today's digital age, individuals often express negative
emotions on social media, where they may reveal cognitive distortions, and in
severe cases, exhibit suicidal tendencies. However, there is a significant gap
in methodologies designed to analyze these cognitive pathways, which could be
critical for psychotherapists aiming to deliver timely and effective
interventions in online environments. Cognitive Behavioral Therapy (CBT)
framework leveraging acceptance, commitment and data augmentation to categorize
and address both textual and visual content as positive or negative.
Specifically, the system employs BERT, RoBERTa for Sentiment Analysis and T5,
PEGASUS for Text Summarization, mT5 for Text Translation in Multiple Languages
focusing on detecting negative emotions and cognitive distortions within social
media data. While existing models are primarily designed to identify negative
thoughts, the proposed system goes beyond this by predicting additional
negative side effects and other potential mental health disorders likes
Phobias, Eating Disorders. This enhancement allows for a more comprehensive
understanding and intervention strategy, offering psychotherapists a powerful
tool for early detection and treatment of various psychological issues.

</details>


### [23] [Hybrid Approach for Electricity Price Forecasting using AlexNet and LSTM](https://arxiv.org/abs/2506.23504)
*Bosubabu Sambana,Kotamsetty Geethika Devi,Bandi Rajeswara Reddy,Galeti Mohammad Hussain,Gownivalla Siddartha*

Main category: cs.AI

TL;DR: A hybrid model combining AlexNet and LSTM improves electricity price forecasting accuracy by addressing limitations of traditional methods and standalone models like RNN and ANN.


<details>
  <summary>Details</summary>
Motivation: Traditional methods fail to accurately forecast electricity prices due to insufficient analysis of external variables and sequential data.

Method: The hybrid model integrates AlexNet for feature extraction and LSTM for learning sequential patterns, using data like demand, temperature, sunlight, and rain. Techniques like minimum-maximum scaling and time windows are applied.

Result: The hybrid model achieves 97.08% accuracy, outperforming RNN (96.64%) and ANN (96.63%).

Conclusion: The hybrid approach significantly enhances prediction accuracy, making it superior to standalone models for electricity price forecasting.

Abstract: The recent development of advanced machine learning methods for hybrid models
has greatly addressed the need for the correct prediction of electrical prices.
This method combines AlexNet and LSTM algorithms, which are used to introduce a
new model with higher accuracy in price forecasting. Despite RNN and ANN being
effective, they often fail to deal with forex time sequence data. The
traditional methods do not accurately forecast the prices. These traditional
methods only focus on demand and price which leads to insufficient analysis of
data. To address this issue, using the hybrid approach, which focuses on
external variables that also effect the predicted prices. Nevertheless, due to
AlexNet's excellent feature extraction and LSTM's learning sequential patterns,
the prediction accuracy is vastly increased. The model is built on the past
data, which has been supplied with the most significant elements like demand,
temperature, sunlight, and rain. For example, the model applies methods, such
as minimum-maximum scaling and a time window, to predict the electricity prices
of the future. The results show that this hybrid model is good than the
standalone ones in terms of accuracy. Although we got our accuracy rating of
97.08, it shows higher accompaniments than remaining models RNN and ANN with
accuracies of 96.64 and 96.63 respectively.

</details>


### [24] [Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays](https://arxiv.org/abs/2506.23517)
*Selin Dik,Osman Erdem,Mehmet Dik*

Main category: cs.AI

TL;DR: GPTZero detects AI-generated texts well but struggles with false positives for human-written essays.


<details>
  <summary>Details</summary>
Motivation: To evaluate GPTZero's reliability in detecting AI-generated vs. human-written essays of varying lengths.

Method: Tested GPTZero on 28 AI-generated and 50 human-written essays of short, medium, and long lengths, measuring AI generation percentage and confidence.

Result: AI-generated texts were accurately detected (91-100%), but human-written essays had false positives.

Conclusion: GPTZero is effective for AI detection but unreliable for human texts; educators should use it cautiously.

Abstract: As the use of AI tools by students has become more prevalent, instructors
have started using AI detection tools like GPTZero and QuillBot to detect AI
written text. However, the reliability of these detectors remains uncertain. In
our study, we focused mostly on the success rate of GPTZero, the most-used AI
detector, in identifying AI-generated texts based on different lengths of
randomly submitted essays: short (40-100 word count), medium (100-350 word
count), and long (350-800 word count). We gathered a data set consisting of
twenty-eight AI-generated papers and fifty human-written papers. With this
randomized essay data, papers were individually plugged into GPTZero and
measured for percentage of AI generation and confidence. A vast majority of the
AI-generated papers were detected accurately (ranging from 91-100% AI believed
generation), while the human generated essays fluctuated; there were a handful
of false positives. These findings suggest that although GPTZero is effective
at detecting purely AI-generated content, its reliability in distinguishing
human-authored texts is limited. Educators should therefore exercise caution
when relying solely on AI detection tools.

</details>


### [25] [ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data](https://arxiv.org/abs/2506.23520)
*Yu Zhang,Ruijie Yu,Jidong Tian,Feng Zhu,Jiapeng Liu,Xiaokang Yang,Yaohui Jin,Yanyan Xu*

Main category: cs.AI

TL;DR: ChemActor, a fine-tuned LLM, converts chemical procedures into structured actions, using a sequential data framework and multi-round review to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Automating chemical procedure extraction is challenging due to ambiguous language and costly human annotation. ChemActor addresses this gap.

Method: Uses a sequential LLM-generated data framework with a data selection module and multi-round review to generate machine-executable actions.

Result: Outperforms baseline by 10% in R2D and D2A tasks, demonstrating advanced understanding of chemical procedures.

Conclusion: ChemActor, enhanced by LLM-generated data, sets a new standard for automated chemical procedure extraction.

Abstract: With the increasing interest in robotic synthesis in the context of organic
chemistry, the automated extraction of chemical procedures from literature is
critical. However, this task remains challenging due to the inherent ambiguity
of chemical language and the high cost of human annotation required for
developing reliable computer-aided extraction protocols. Here, we present
ChemActor, a fully fine-tuned large language model (LLM), as a chemical
executor to convert between unstructured experimental procedures and structured
action sequences. We propose a sequential LLM-generated data framework to
address the challenges of insufficient and low-quality annotated data. This
framework integrates a data selection module that selects data based on
distribution divergence, with a general-purpose LLM, to generate
machine-executable actions from a single molecule input. Additionally, we
introduce a novel multi-round LLMs circle review metric, which reflects the
model's advanced understanding of chemical experimental procedures. Extensive
experiments on reaction-to-description (R2D) and description-to-action (D2A)
tasks demonstrate that ChemActor, augmented by LLM-generated data, achieves
state-of-the-art performance, outperforming the baseline model by 10%. The code
is available at: https://github.com/Zhanghahah/ChemActor.

</details>


### [26] [CooT: Learning to Coordinate In-Context with Coordination Transformers](https://arxiv.org/abs/2506.23549)
*Huai-Chih Wang,Hsiang-Chun Chuang,Hsi-Chun Cheng,Dai-Jie Wu,Shao-Hua Sun*

Main category: cs.AI

TL;DR: Coordination Transformers (CooT) is a novel framework for multi-agent systems that adapts to unseen partners using interaction histories, outperforming baselines in coordination tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing poor generalization and extensive training requirements in existing multi-agent coordination methods.

Method: Uses in-context coordination, leveraging interaction histories to predict and align actions with partner behaviors without supervision.

Result: Outperforms baselines in unseen partner scenarios on the Overcooked benchmark and excels in human evaluations.

Conclusion: CooT is robust, flexible, and context-sensitive, making it an effective solution for multi-agent coordination.

Abstract: Effective coordination among artificial agents in dynamic and uncertain
environments remains a significant challenge in multi-agent systems. Existing
approaches, such as self-play and population-based methods, either generalize
poorly to unseen partners or require extensive training. To overcome these
limitations, we propose Coordination Transformers (CooT), a novel in-context
coordination framework that uses recent interaction histories to adapt to
unseen partners rapidly. Unlike previous approaches that primarily aim to
increase the diversity of training partners, CooT explicitly focuses on
adapting to new partner behaviors by predicting actions aligned with observed
partner interactions. Trained on interaction trajectories collected from
diverse pairs of agents with complementary behaviors, CooT quickly learns
effective coordination strategies without explicit supervision or fine-tuning.
Evaluations on the Overcooked benchmark demonstrate that CooT significantly
outperforms baseline methods in coordination tasks involving previously unseen
partners. Human evaluations further confirm CooT as the most effective
collaborative partner, while extensive ablations highlight its robustness,
flexibility, and sensitivity to context in multi-agent scenarios.

</details>


### [27] [MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI](https://arxiv.org/abs/2506.23563)
*Huanjin Yao,Jiaxing Huang,Yawen Qiu,Michael K. Chen,Wenzheng Liu,Wei Zhang,Wenjie Zeng,Xikun Zhang,Jingyi Zhang,Yuxin Song,Wenhao Wu,Dacheng Tao*

Main category: cs.AI

TL;DR: MMReason is a new benchmark for evaluating long-chain reasoning in Multimodal Large Language Models (MLLMs), addressing gaps in difficulty, diversity, guessability, and intermediate reasoning assessment.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM benchmarks lack precision in evaluating long-chain reasoning due to insufficient difficulty, diversity, and susceptibility to shortcuts like guessing or memorization.

Method: MMReason curates diverse, challenging questions from 6 disciplines and multiple difficulty levels, reformulates them into open-ended formats, filters shortcuts via multi-model voting, and annotates step-by-step solutions with a ternary scoring mechanism.

Result: The benchmark evaluates popular MLLMs, providing insights into their reasoning capabilities.

Conclusion: MMReason aims to advance MLLM reasoning research by offering a robust evaluation tool.

Abstract: Reasoning plays a crucial role in advancing Multimodal Large Language Models
(MLLMs) toward Artificial General Intelligence. However, existing MLLM
benchmarks often fall short in precisely and comprehensively evaluating
long-chain reasoning abilities from three key aspects: (1) lack of difficulty
and diversity, (2) susceptibility to guessability and memorization, (3)
inadequate assessment of intermediate reasoning steps. To fill this gap, we
introduce MMReason, a new benchmark designed to precisely and comprehensively
evaluate MLLM long-chain reasoning capability with diverse, open-ended,
challenging questions. First, we curate challenging questions requiring
multi-step reasoning from various fields (i.e., 6 disciplines) and multiple
difficulty levels (i.e., from pre-university to university, and from
foundational to competition tiers). Second, these questions are reformulated
into an open-ended format and filtered using a multi-model voting technique to
eliminate shortcut cases related to guessing and memorization, ensuring robust
reasoning evaluations. Third, we annotate the questions with detailed
step-by-step solutions, and design a reference-based ternary scoring mechanism
to reliably assess intermediate reasoning steps. With MMReason, we benchmark
popular leading MLLMs and provide an in-depth analysis of their reasoning
capabilities. We hope MMReason will serve as a valuable resource for advancing
MLLM reasoning research. Code will be available at
https://github.com/HJYao00/MMReason.

</details>


### [28] [Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models](https://arxiv.org/abs/2506.23576)
*Maria Carolina Cornelia Wit,Jun Pang*

Main category: cs.AI

TL;DR: Multi-agent LLM systems improve resistance to jailbreaking attacks but come with trade-offs like higher false positives and computational costs.


<details>
  <summary>Details</summary>
Motivation: Address concerns about jailbreaking attacks on LLMs by exploring multi-agent systems as a defence mechanism.

Method: Evaluated three jailbreaking strategies (AutoDefense, BetterDan, JB) using single-agent, two-agent, and three-agent configurations.

Result: Multi-agent systems reduce false negatives but increase false positives and computational overhead; effectiveness varies by attack type.

Conclusion: Current automated defences have limitations; multi-agent systems show promise but need refinement for better alignment robustness.

Abstract: Recent advances in large language models (LLMs) have raised concerns about
jailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper
investigates the use of multi-agent LLM systems as a defence against such
attacks. We evaluate three jailbreaking strategies, including the original
AutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the
AutoDefense framework, we compare single-agent setups with two- and three-agent
configurations. Our results show that multi-agent systems enhance resistance to
jailbreaks, especially by reducing false negatives. However, its effectiveness
varies by attack type, and it introduces trade-offs such as increased false
positives and computational overhead. These findings point to the limitations
of current automated defences and suggest directions for improving alignment
robustness in future LLM systems.

</details>


### [29] [Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games](https://arxiv.org/abs/2506.23626)
*António Afonso,Iolanda Leite,Alessandro Sestini,Florian Fuchs,Konrad Tollmar,Linus Gisslén*

Main category: cs.AI

TL;DR: An automated method using a Language Model (LM) to iteratively fine-tune RL agent reward functions, improving performance without manual engineering.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of RL reward function tuning when game mechanics change, reducing reliance on experts.

Method: LM proposes updated reward weights based on behavioral goals and performance stats, enabling self-correction over iterations.

Result: LM-guided agents improved from 9% to 74% success in one iteration, reaching 80% success and 855 time steps, competing with expert-tuned agents.

Conclusion: The LM-based approach effectively automates reward tuning, achieving competitive performance without manual intervention.

Abstract: Reinforcement Learning (RL) in games has gained significant momentum in
recent years, enabling the creation of different agent behaviors that can
transform a player's gaming experience. However, deploying RL agents in
production environments presents two key challenges: (1) designing an effective
reward function typically requires an RL expert, and (2) when a game's content
or mechanics are modified, previously tuned reward weights may no longer be
optimal. Towards the latter challenge, we propose an automated approach for
iteratively fine-tuning an RL agent's reward function weights, based on a
user-defined language based behavioral goal. A Language Model (LM) proposes
updated weights at each iteration based on this target behavior and a summary
of performance statistics from prior training rounds. This closed-loop process
allows the LM to self-correct and refine its output over time, producing
increasingly aligned behavior without the need for manual reward engineering.
We evaluate our approach in a racing task and show that it consistently
improves agent performance across iterations. The LM-guided agents show a
significant increase in performance from $9\%$ to $74\%$ success rate in just
one iteration. We compare our LM-guided tuning against a human expert's manual
weight design in the racing task: by the final iteration, the LM-tuned agent
achieved an $80\%$ success rate, and completed laps in an average of $855$ time
steps, a competitive performance against the expert-tuned agent's peak $94\%$
success, and $850$ time steps.

</details>


### [30] [HASD: Hierarchical Adaption for pathology Slide-level Domain-shift](https://arxiv.org/abs/2506.23673)
*Jingsong Liu,Han Li,Chen Yang,Michael Deutges,Ario Sadafi,Xin You,Katharina Breininger,Nassir Navab,Peter J. Schüffler*

Main category: cs.AI

TL;DR: Proposes HASD, a hierarchical framework for slide-level domain adaptation in pathology AI, improving performance and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of domain shift in pathology AI, focusing on whole slide images (WSI) rather than patches, which is critical for clinical applications.

Method: Uses a hierarchical adaptation framework with domain-level alignment, slide-level geometric invariance, and patch-level attention consistency, along with a prototype selection mechanism.

Result: Achieves 4.1% AUROC improvement in Breast Cancer HER2 Grading and 3.9% C-index gain in UCEC survival prediction.

Conclusion: HASD provides a practical, efficient solution for slide-level domain adaptation in pathology, reducing computational and annotation costs.

Abstract: Domain shift is a critical problem for pathology AI as pathology data is
heavily influenced by center-specific conditions. Current pathology domain
adaptation methods focus on image patches rather than WSI, thus failing to
capture global WSI features required in typical clinical scenarios. In this
work, we address the challenges of slide-level domain shift by proposing a
Hierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD
achieves multi-scale feature consistency and computationally efficient
slide-level domain adaptation through two key components: (1) a hierarchical
adaptation framework that integrates a Domain-level Alignment Solver for
feature alignment, a Slide-level Geometric Invariance Regularization to
preserve the morphological structure, and a Patch-level Attention Consistency
Regularization to maintain local critical diagnostic cues; and (2) a prototype
selection mechanism that reduces computational overhead. We validate our method
on two slide-level tasks across five datasets, achieving a 4.1\% AUROC
improvement in a Breast Cancer HER2 Grading cohort and a 3.9\% C-index gain in
a UCEC survival prediction cohort. Our method provides a practical and reliable
slide-level domain adaption solution for pathology institutions, minimizing
both computational and annotation costs.

</details>


### [31] [PokéAI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red](https://arxiv.org/abs/2506.23689)
*Zihao Liu,Xinhang Sui,Yueran Song,Siwen Wang*

Main category: cs.AI

TL;DR: PokéAI is a multi-agent LLM framework for autonomously playing Pokémon Red, featuring Planning, Execution, and Critique agents. It achieves an 80.8% win rate in battles, with performance linked to linguistic ability.


<details>
  <summary>Details</summary>
Motivation: To create an autonomous system for playing Pokémon Red using specialized LLM agents, exploring the link between language models and strategic reasoning.

Method: Three agents (Planning, Execution, Critique) work in a closed-loop system. The Execution Agent includes a battle module tested in wild encounters.

Result: 80.8% average win rate in battles, close to human performance. Battle performance correlates with LLM Arena scores, and models show unique playstyles.

Conclusion: PokéAI demonstrates effective autonomous gameplay, linking linguistic and strategic abilities, and reveals distinct model behaviors.

Abstract: We introduce Pok\'eAI, the first text-based, multi-agent large language model
(LLM) framework designed to autonomously play and progress through Pok\'emon
Red. Our system consists of three specialized agents-Planning, Execution, and
Critique-each with its own memory bank, role, and skill set. The Planning Agent
functions as the central brain, generating tasks to progress through the game.
These tasks are then delegated to the Execution Agent, which carries them out
within the game environment. Upon task completion, the Critique Agent evaluates
the outcome to determine whether the objective was successfully achieved. Once
verification is complete, control returns to the Planning Agent, forming a
closed-loop decision-making system.
  As a preliminary step, we developed a battle module within the Execution
Agent. Our results show that the battle AI achieves an average win rate of
80.8% across 50 wild encounters, only 6% lower than the performance of an
experienced human player. Furthermore, we find that a model's battle
performance correlates strongly with its LLM Arena score on language-related
tasks, indicating a meaningful link between linguistic ability and strategic
reasoning. Finally, our analysis of gameplay logs reveals that each LLM
exhibits a unique playstyle, suggesting that individual models develop distinct
strategic behaviors.

</details>


### [32] [Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models](https://arxiv.org/abs/2506.23692)
*Boyuan Zheng,Zerui Fang,Zhe Xu,Rui Wang,Yiwen Chen,Cunshi Wang,Mengwei Qu,Lei Lei,Zhen Feng,Yan Liu,Yuyang Li,Mingzhou Tan,Jiaji Wu,Jianwei Shuai,Jia Li,Fangfu Ye*

Main category: cs.AI

TL;DR: The paper proposes 'Agent for Science' (Agent4S) as a transformative paradigm using LLM-driven agents to automate research workflows, surpassing AI for Science (AI4S). It introduces a five-level classification for Agent4S, aiming for fully autonomous AI Scientists.


<details>
  <summary>Details</summary>
Motivation: Current AI4S is inefficient for core research workflows. The paper aims to revolutionize scientific discovery by automating the entire process with Agent4S.

Method: Introduces a five-level classification framework for Agent4S, progressing from task automation to autonomous AI Scientists.

Result: A roadmap for Agent4S is outlined, defining its potential to transform scientific research.

Conclusion: Agent4S represents the Fifth Scientific Paradigm, enabling fully autonomous and collaborative scientific discovery.

Abstract: While AI for Science (AI4S) serves as an analytical tool in the current
research paradigm, it doesn't solve its core inefficiency. We propose "Agent
for Science" (Agent4S)-the use of LLM-driven agents to automate the entire
research workflow-as the true Fifth Scientific Paradigm. This paper introduces
a five-level classification for Agent4S, outlining a clear roadmap from simple
task automation to fully autonomous, collaborative "AI Scientists." This
framework defines the next revolutionary step in scientific discovery.

</details>


### [33] [A New Perspective On AI Safety Through Control Theory Methodologies](https://arxiv.org/abs/2506.23703)
*Lars Ullrich,Walter Zimmer,Ross Greer,Knut Graichen,Alois C. Knoll,Mohan Trivedi*

Main category: cs.AI

TL;DR: The paper proposes a new interdisciplinary approach, 'data control,' to enhance AI safety by integrating control theory and AI, addressing safety concerns in cyber-physical systems.


<details>
  <summary>Details</summary>
Motivation: AI's rapid advancement lacks safety assurance, especially in safety-critical systems, prompting the need for an interdisciplinary solution.

Method: The paper introduces 'data control,' leveraging control theory and system analysis to improve AI safety through a top-down approach.

Result: A generic foundation for safety analysis and assurance is outlined, adaptable for specific AI systems and future innovations.

Conclusion: The interdisciplinary perspective of data control aims to bridge AI and control theory, fostering safer AI engineering practices.

Abstract: While artificial intelligence (AI) is advancing rapidly and mastering
increasingly complex problems with astonishing performance, the safety
assurance of such systems is a major concern. Particularly in the context of
safety-critical, real-world cyber-physical systems, AI promises to achieve a
new level of autonomy but is hampered by a lack of safety assurance. While
data-driven control takes up recent developments in AI to improve control
systems, control theory in general could be leveraged to improve AI safety.
Therefore, this article outlines a new perspective on AI safety based on an
interdisciplinary interpretation of the underlying data-generation process and
the respective abstraction by AI systems in a system theory-inspired and system
analysis-driven manner. In this context, the new perspective, also referred to
as data control, aims to stimulate AI engineering to take advantage of existing
safety analysis and assurance in an interdisciplinary way to drive the paradigm
of data control. Following a top-down approach, a generic foundation for safety
analysis and assurance is outlined at an abstract level that can be refined for
specific AI systems and applications and is prepared for future innovation.

</details>


### [34] [Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments](https://arxiv.org/abs/2506.23706)
*Christoph Schnabl,Daniel Hugenroth,Bill Marino,Alastair R. Beresford*

Main category: cs.AI

TL;DR: Attestable Audits use Trusted Execution Environments to verify AI model compliance while protecting sensitive data, addressing governance challenges.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack verifiable results and confidentiality for AI model IP and datasets, raising governance concerns.

Method: Proposes Attestable Audits using Trusted Execution Environments to ensure verifiable and confidential interactions with compliant AI models.

Result: A prototype demonstrates feasibility on audit benchmarks against Llama-3.1, protecting data even with untrusted parties.

Conclusion: Attestable Audits offer a solution for verifiable, confidential AI model compliance, aligning with governance needs.

Abstract: Benchmarks are important measures to evaluate safety and compliance of AI
models at scale. However, they typically do not offer verifiable results and
lack confidentiality for model IP and benchmark datasets. We propose Attestable
Audits, which run inside Trusted Execution Environments and enable users to
verify interaction with a compliant AI model. Our work protects sensitive data
even when model provider and auditor do not trust each other. This addresses
verification challenges raised in recent AI governance frameworks. We build a
prototype demonstrating feasibility on typical audit benchmarks against
Llama-3.1.

</details>


### [35] [BayesL: Towards a Logical Framework for Bayesian Networks](https://arxiv.org/abs/2506.23773)
*Stefano M. Nicoletti,Mariëlle Stoelinga*

Main category: cs.AI

TL;DR: BayesL is a new logical framework for querying and verifying Bayesian networks, enabling versatile reasoning and what-if evaluations without manual model changes.


<details>
  <summary>Details</summary>
Motivation: To simplify and enhance the process of querying and verifying Bayesian networks by providing a structured language.

Method: Developed BayesL, a logical framework for creating queries over Bayesian networks, supporting causal and evidence-based reasoning.

Result: BayesL allows comprehensive what-if scenario evaluations and versatile reasoning without manual model adjustments.

Conclusion: BayesL offers a powerful and flexible tool for working with Bayesian networks, streamlining complex analyses.

Abstract: We introduce BayesL, a novel logical framework for specifying, querying, and
verifying the behaviour of Bayesian networks (BNs). BayesL (pronounced "Basil")
is a structured language that allows for the creation of queries over BNs. It
facilitates versatile reasoning concerning causal and evidence-based
relationships, and permits comprehensive what-if scenario evaluations without
the need for manual modifications to the model.

</details>


### [36] [When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)](https://arxiv.org/abs/2506.23784)
*Parosh Aziz Abdulla,Mohamed Faouzi Atig,Julie Cailler,Chencheng Liang,Philipp Rümmer*

Main category: cs.AI

TL;DR: The paper explores using Graph Neural Networks (GNNs) to rank word equations for solving conjunctions more efficiently, outperforming state-of-the-art solvers in certain benchmarks.


<details>
  <summary>Details</summary>
Motivation: The performance of solvers for word equations depends heavily on the order of processing, motivating the use of GNNs for better ranking.

Method: A novel graph-based representation for word equations is introduced, and three approaches for ranking equations are proposed, trained using minimum unsatisfiable subsets (MUSes).

Result: The GNN-based framework solves more problems in benchmarks where variables appear at most once per equation compared to existing solvers.

Conclusion: GNNs offer an effective way to improve the solving of word equations by optimizing the processing order.

Abstract: Nielsen transformation is a standard approach for solving word equations: by
repeatedly splitting equations and applying simplification steps, equations are
rewritten until a solution is reached. When solving a conjunction of word
equations in this way, the performance of the solver will depend considerably
on the order in which equations are processed. In this work, the use of Graph
Neural Networks (GNNs) for ranking word equations before and during the solving
process is explored. For this, a novel graph-based representation for word
equations is presented, preserving global information across conjuncts,
enabling the GNN to have a holistic view during ranking. To handle the variable
number of conjuncts, three approaches to adapt a multi-classification task to
the problem of ranking equations are proposed. The training of the GNN is done
with the help of minimum unsatisfiable subsets (MUSes) of word equations. The
experimental results show that, compared to state-of-the-art string solvers,
the new framework solves more problems in benchmarks where each variable
appears at most once in each equation.

</details>


### [37] [Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning](https://arxiv.org/abs/2506.23793)
*Anton Andreychuk,Konstantin Yakovlev,Aleksandr Panov,Alexey Skrynnik*

Main category: cs.AI

TL;DR: MAPF-GPT-DDG, a decentralized MAPF solver, improves on MAPF-GPT by fine-tuning with centralized expert data and a delta-data generation mechanism, achieving superior scalability and performance.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance and scalability of learning-based MAPF solvers for real-world applications like logistics and search-and-rescue.

Method: Fine-tunes the pre-trained MAPF-GPT model using centralized expert data and a novel delta-data generation mechanism.

Result: Outperforms existing learning-based MAPF solvers, including MAPF-GPT, and scales to 1 million agents.

Conclusion: MAPF-GPT-DDG sets a new benchmark for scalable and efficient MAPF solvers.

Abstract: Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot
trajectory planning problems, where multiple homogeneous robots simultaneously
move in the shared environment. While solving MAPF optimally has been proven to
be NP-hard, scalable, and efficient, solvers are vital for real-world
applications like logistics, search-and-rescue, etc. To this end, decentralized
suboptimal MAPF solvers that leverage machine learning have come on stage.
Building on the success of the recently introduced MAPF-GPT, a pure imitation
learning solver, we introduce MAPF-GPT-DDG. This novel approach effectively
fine-tunes the pre-trained MAPF model using centralized expert data. Leveraging
a novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training
while significantly improving performance at test time. Our experiments
demonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF
solvers, including the original MAPF-GPT, regarding solution quality across
many testing scenarios. Remarkably, it can work with MAPF instances involving
up to 1 million agents in a single environment, setting a new milestone for
scalability in MAPF domains.

</details>


### [38] [A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents](https://arxiv.org/abs/2506.23844)
*Hang Su,Jun Luo,Chang Liu,Xiao Yang,Yichi Zhang,Yinpeng Dong,Jun Zhu*

Main category: cs.AI

TL;DR: The paper surveys autonomous AI agents powered by large language models (LLMs), highlighting their capabilities and novel security risks, and proposes the Reflective Risk-Aware Agent Architecture (R2A2) for proactive safety.


<details>
  <summary>Details</summary>
Motivation: To address the emerging security risks in autonomous AI agents, which extend beyond conventional systems, by analyzing vulnerabilities and proposing defensive strategies.

Method: Examines structural foundations of agent autonomy, identifies security vulnerabilities, reviews defense strategies, and introduces the R2A2 framework based on Constrained Markov Decision Processes (CMDPs).

Result: Identifies key risks (e.g., memory poisoning, tool misuse) and proposes R2A2 for risk-aware decision-making and proactive safety.

Conclusion: The R2A2 framework offers a principled approach to mitigate security risks in autonomous AI agents, balancing functionality and safety.

Abstract: Recent advances in large language models (LLMs) have catalyzed the rise of
autonomous AI agents capable of perceiving, reasoning, and acting in dynamic,
open-ended environments. These large-model agents mark a paradigm shift from
static inference systems to interactive, memory-augmented entities. While these
capabilities significantly expand the functional scope of AI, they also
introduce qualitatively novel security risks - such as memory poisoning, tool
misuse, reward hacking, and emergent misalignment - that extend beyond the
threat models of conventional systems or standalone LLMs. In this survey, we
first examine the structural foundations and key capabilities that underpin
increasing levels of agent autonomy, including long-term memory retention,
modular tool use, recursive planning, and reflective reasoning. We then analyze
the corresponding security vulnerabilities across the agent stack, identifying
failure modes such as deferred decision hazards, irreversible tool chains, and
deceptive behaviors arising from internal state drift or value misalignment.
These risks are traced to architectural fragilities that emerge across
perception, cognition, memory, and action modules. To address these challenges,
we systematically review recent defense strategies deployed at different
autonomy layers, including input sanitization, memory lifecycle control,
constrained decision-making, structured tool invocation, and introspective
reflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a
unified cognitive framework grounded in Constrained Markov Decision Processes
(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,
and joint reward-risk optimization to enable principled, proactive safety
across the agent's decision-making loop.

</details>


### [39] [Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence](https://arxiv.org/abs/2506.23908)
*András György,Tor Lattimore,Nevena Lazić,Csaba Szepesvári*

Main category: cs.AI

TL;DR: The paper critiques AI's reliance on statistical learning for deductive reasoning, advocating for a shift to exact learning to achieve reliable general intelligence.


<details>
  <summary>Details</summary>
Motivation: Current AI systems, despite advances, fail at simple deductive reasoning tasks, making them unsuitable for artificial general intelligence.

Method: Proposes a shift from statistical learning to exact learning, ensuring correctness on all inputs.

Result: Statistical learning is inadequate for sound deductive reasoning; exact learning is essential.

Conclusion: Exact learning should guide future AI algorithm design to achieve reliable deductive reasoning.

Abstract: Sound deductive reasoning -- the ability to derive new knowledge from
existing facts and rules -- is an indisputably desirable aspect of general
intelligence. Despite the major advances of AI systems in areas such as math
and science, especially since the introduction of transformer architectures, it
is well-documented that even the most advanced frontier systems regularly and
consistently falter on easily-solvable deductive reasoning tasks. Hence, these
systems are unfit to fulfill the dream of achieving artificial general
intelligence capable of sound deductive reasoning. We argue that their unsound
behavior is a consequence of the statistical learning approach powering their
development. To overcome this, we contend that to achieve reliable deductive
reasoning in learning-based AI systems, researchers must fundamentally shift
from optimizing for statistical performance against distributions on reasoning
problems and algorithmic tasks to embracing the more ambitious exact learning
paradigm, which demands correctness on all inputs. We argue that exact learning
is both essential and possible, and that this ambitious objective should guide
algorithm design.

</details>


### [40] [Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice](https://arxiv.org/abs/2506.23924)
*Akshit Kumar,Tianyi Peng,Yuhang Wu,Assaf Zeevi*

Main category: cs.AI

TL;DR: LLMs show promise in solving stochastic OR problems, matching human experts in some cases, but further work is needed for reliable automation.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' capabilities in solving stochastic modeling problems in Operations Research, an underexplored area.

Method: Manually curated graduate-level problems and SimOpt library tests to evaluate LLMs' performance in stochastic modeling and decision-making under uncertainty.

Result: LLMs perform on par with human experts in classroom and practical settings, though more work is needed for reliable automation.

Conclusion: LLMs have potential to assist OR researchers and enhance real-world OR impact through automation.

Abstract: Large language models (LLMs) have exhibited expert-level capabilities across
various domains. However, their abilities to solve problems in Operations
Research (OR) -- the analysis and optimization of mathematical models derived
from real-world problems or their verbal descriptions -- remain underexplored.
In this work, we take a first step toward evaluating LLMs' abilities to solve
stochastic modeling problems, a core class of OR problems characterized by
uncertainty and typically involving tools from probability, statistics, and
stochastic processes. We manually procure a representative set of
graduate-level homework and doctoral qualification-exam problems and test LLMs'
abilities to solve them. We further leverage SimOpt, an open-source library of
simulation-optimization problems and solvers, to investigate LLMs' abilities to
make real-world decisions under uncertainty. Our results show that, though a
nontrivial amount of work is still needed to reliably automate the stochastic
modeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on
par with human experts in both classroom and practical settings. These findings
highlight the potential of building AI agents that assist OR researchers and
amplify the real-world impact of OR through automation.

</details>


### [41] [Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system](https://arxiv.org/abs/2506.23926)
*Junping Wang,Bicheng Wang,Yibo Xuea,Yuan Xie*

Main category: cs.AI

TL;DR: The paper introduces 'industrial brain,' a framework combining neuro networks and symbolic reasoning to predict and plan resilience in industrial chains, outperforming existing methods by up to 11.03%.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods struggle with resilience prediction in chaotic, real-world industrial chain scenarios, necessitating a more robust solution.

Method: Proposes 'industrial brain,' integrating higher-order neuro networks and CT-OODA symbolic reasoning to model node dynamics and network co-evolution without simplifications.

Result: Industrial brain improves resilience prediction accuracy by up to 10.8% over GoT/OlaGPT and 11.03% over spectral dimension reduction, generalizing well to unseen data.

Conclusion: The industrial brain effectively fills a critical gap in resilience prediction and planning for industrial chains, demonstrating superior performance and robustness.

Abstract: Resilience non-equilibrium measurement, the ability to maintain fundamental
functionality amidst failures and errors, is crucial for scientific management
and engineering applications of industrial chain. The problem is particularly
challenging when the number or types of multiple co-evolution of resilience
(for example, randomly placed) are extremely chaos. Existing end-to-end deep
learning ordinarily do not generalize well to unseen full-feld reconstruction
of spatiotemporal co-evolution structure, and predict resilience of network
topology, especially in multiple chaos data regimes typically seen in
real-world applications. To address this challenge, here we propose industrial
brain, a human-like autonomous cognitive decision-making and planning framework
integrating higher-order activity-driven neuro network and CT-OODA symbolic
reasoning to autonomous plan resilience directly from observational data of
global variable. The industrial brain not only understands and model structure
of node activity dynamics and network co-evolution topology without simplifying
assumptions, and reveal the underlying laws hidden behind complex networks, but
also enabling accurate resilience prediction, inference, and planning.
Experimental results show that industrial brain significantly outperforms
resilience prediction and planning methods, with an accurate improvement of up
to 10.8\% over GoT and OlaGPT framework and 11.03\% over spectral dimension
reduction. It also generalizes to unseen topologies and dynamics and maintains
robust performance despite observational disturbances. Our findings suggest
that industrial brain addresses an important gap in resilience prediction and
planning for industrial chain.

</details>


### [42] [AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models](https://arxiv.org/abs/2506.23949)
*Anthony M. Barrett,Jessica Newman,Brandie Nonnecke,Nada Madkour,Dan Hendrycks,Evan R. Murphy,Krystal Jackson,Deepika Raman*

Main category: cs.AI

TL;DR: The paper outlines risk-management practices for general-purpose AI (GPAI) and foundation models, targeting developers to mitigate risks and align with standards like NIST and ISO/IEC.


<details>
  <summary>Details</summary>
Motivation: Address the dual nature of GPAI/foundation models, which offer benefits but also pose significant risks, necessitating structured risk management.

Method: Proposes risk-management controls and practices tailored for GPAI/foundation model developers, adapting existing frameworks like NIST AI Risk Management Framework and ISO/IEC 23894.

Result: Provides actionable guidance for identifying, analyzing, and mitigating risks associated with GPAI/foundation models.

Conclusion: The document serves as a resource for developers to manage risks effectively while leveraging the capabilities of GPAI/foundation models.

Abstract: Increasingly multi-purpose AI models, such as cutting-edge large language
models or other 'general-purpose AI' (GPAI) models, 'foundation models,'
generative AI models, and 'frontier models' (typically all referred to
hereafter with the umbrella term 'GPAI/foundation models' except where greater
specificity is needed), can provide many beneficial capabilities but also risks
of adverse events with profound consequences. This document provides
risk-management practices or controls for identifying, analyzing, and
mitigating risks of GPAI/foundation models. We intend this document primarily
for developers of large-scale, state-of-the-art GPAI/foundation models; others
that can benefit from this guidance include downstream developers of end-use
applications that build on a GPAI/foundation model. This document facilitates
conformity with or use of leading AI risk management-related standards,
adapting and building on the generic voluntary guidance in the NIST AI Risk
Management Framework and ISO/IEC 23894, with a focus on the unique issues faced
by developers of GPAI/foundation models.

</details>


### [43] [Harnessing AI Agents to Advance Research on Refugee Child Mental Health](https://arxiv.org/abs/2506.23992)
*Aditya Shrivastava,Komal Gupta,Shraddha Arora*

Main category: cs.AI

TL;DR: AI-based framework for analyzing refugee child mental health data, comparing RAG models Zephyr-7B-beta and DeepSeek R1-7B, with DeepSeek R1 showing superior accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the psychological trauma of displaced children by leveraging AI to process unstructured health data.

Method: Comparison of two RAG pipelines (Zephyr-7B-beta and DeepSeek R1-7B) on humanitarian datasets to avoid hallucination.

Result: DeepSeek R1 outperformed Zephyr with an accuracy of 0.91 in answer relevance.

Conclusion: The study offers a scalable AI strategy to aid policymakers and practitioners in improving mental health support for displaced children.

Abstract: The international refugee crisis deepens, exposing millions of dis placed
children to extreme psychological trauma. This research suggests a com pact,
AI-based framework for processing unstructured refugee health data and
distilling knowledge on child mental health. We compare two Retrieval-Aug
mented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to
determine how well they process challenging humanitarian datasets while avoid
ing hallucination hazards. By combining cutting-edge AI methods with migration
research and child psychology, this study presents a scalable strategy to
assist policymakers, mental health practitioners, and humanitarian agencies to
better assist displaced children and recognize their mental wellbeing. In
total, both the models worked properly but significantly Deepseek R1 is
superior to Zephyr with an accuracy of answer relevance 0.91

</details>


### [44] [Constructing Non-Markovian Decision Process via History Aggregator](https://arxiv.org/abs/2506.24026)
*Yongyi Wang,Wenxin Li*

Main category: cs.AI

TL;DR: The paper introduces a category theory-based method to address non-Markovian dynamics in decision-making, proving equivalence between MDP and NMDP categories and proposing HAS for precise state dependency control.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to assess decision algorithms' handling of non-Markovian dynamics, limiting progress in fields like Reinforcement Learning.

Method: Developed a generalized methodology using category theory, defining MDP and NMDP categories and proving their equivalence. Introduced HAS to control state dependency.

Result: Demonstrated effectiveness in representing non-Markovian dynamics, enabling rigorous evaluation of decision algorithms.

Conclusion: The approach provides a theoretical foundation and practical tool for understanding and testing non-Markovian dynamics in decision-making.

Abstract: In the domain of algorithmic decision-making, non-Markovian dynamics manifest
as a significant impediment, especially for paradigms such as Reinforcement
Learning (RL), thereby exerting far-reaching consequences on the advancement
and effectiveness of the associated systems. Nevertheless, the existing
benchmarks are deficient in comprehensively assessing the capacity of decision
algorithms to handle non-Markovian dynamics. To address this deficiency, we
have devised a generalized methodology grounded in category theory. Notably, we
established the category of Markov Decision Processes (MDP) and the category of
non-Markovian Decision Processes (NMDP), and proved the equivalence
relationship between them. This theoretical foundation provides a novel
perspective for understanding and addressing non-Markovian dynamics. We further
introduced non-Markovianity into decision-making problem settings via the
History Aggregator for State (HAS). With HAS, we can precisely control the
state dependency structure of decision-making problems in the time series. Our
analysis demonstrates the effectiveness of our method in representing a broad
range of non-Markovian dynamics. This approach facilitates a more rigorous and
flexible evaluation of decision algorithms by testing them in problem settings
where non-Markovian dynamics are explicitly constructed.

</details>


### [45] [SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2506.24119)
*Bo Liu,Leon Guertler,Simon Yu,Zichen Liu,Penghui Qi,Daniel Balcells,Mickel Liu,Cheston Tan,Weiyan Shi,Min Lin,Wee Sun Lee,Natasha Jaques*

Main category: cs.AI

TL;DR: SPIRAL is a self-play framework for language models to learn reasoning through zero-sum games without human supervision, achieving transferable improvements in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To eliminate dependency on human-curated data and domain-specific rewards in reinforcement learning for reasoning tasks.

Method: SPIRAL uses self-play in multi-turn, zero-sum games with role-conditioned advantage estimation (RAE) for stable multi-agent training.

Result: Training on Kuhn Poker improved math and general reasoning by 8.6% and 8.4%, outperforming supervised fine-tuning. Multi-game training further enhanced performance.

Conclusion: Zero-sum games naturally develop transferable reasoning, offering a promising path for autonomous reasoning development.

Abstract: Recent advances in reinforcement learning have shown that language models can
develop sophisticated reasoning through training on tasks with verifiable
rewards, but these approaches depend on human-curated problem-answer pairs and
domain-specific reward engineering. We introduce SPIRAL, a self-play framework
where models learn by playing multi-turn, zero-sum games against continuously
improving versions of themselves, eliminating the need for human supervision.
Through self-play, SPIRAL generates an infinite curriculum of progressively
challenging problems as models must constantly adapt to stronger opponents. To
enable this self-play training at scale, We implement a fully online,
multi-turn, multi-agent reinforcement learning system for LLMs and propose
role-conditioned advantage estimation (RAE) to stabilize multi-agent training.
Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that
transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%
improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000
expert game trajectories. Analysis reveals that this transfer occurs through
three cognitive patterns: systematic decomposition, expected value calculation,
and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple
Negotiation) further enhances performance as each game develops distinct
reasoning strengths. Applying SPIRAL to a strong reasoning model
(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These
results demonstrate that zero-sum games naturally develop transferable
reasoning capabilities, highlighting a promising direction for autonomous
reasoning development.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [46] [Latent Factorization of Tensors with Threshold Distance Weighted Loss for Traffic Data Estimation](https://arxiv.org/abs/2506.22441)
*Lei Yang*

Main category: cs.LG

TL;DR: The paper proposes a TDW loss-incorporated LFT model (TDWLFT) to improve spatiotemporal traffic data imputation by reducing outlier sensitivity.


<details>
  <summary>Details</summary>
Motivation: Incomplete or corrupted traffic data due to communication failures and sensor malfunctions hinder ITS performance. Existing LFT models are vulnerable to outliers.

Method: Introduces a threshold distance weighted (TDW) loss function in the LFT model to assign differentiated weights to samples, reducing outlier impact.

Result: TDWLFT outperforms state-of-the-art methods in prediction accuracy and computational efficiency on two urban traffic speed datasets.

Conclusion: The TDWLFT model effectively addresses outlier sensitivity in spatiotemporal traffic data imputation, enhancing ITS performance.

Abstract: Intelligent transportation systems (ITS) rely heavily on complete and
high-quality spatiotemporal traffic data to achieve optimal performance.
Nevertheless, in real-word traffic data collection processes, issues such as
communication failures and sensor malfunctions often lead to incomplete or
corrupted datasets, thereby posing significant challenges to the advancement of
ITS. Among various methods for imputing missing spatiotemporal traffic data,
the latent factorization of tensors (LFT) model has emerged as a widely adopted
and effective solution. However, conventional LFT models typically employ the
standard L2-norm in their learning objective, which makes them vulnerable to
the influence of outliers. To overcome this limitation, this paper proposes a
threshold distance weighted (TDW) loss-incorporated Latent Factorization of
Tensors (TDWLFT) model. The proposed loss function effectively reduces the
model's sensitivity to outliers by assigning differentiated weights to
individual samples. Extensive experiments conducted on two traffic speed
datasets sourced from diverse urban environments confirm that the proposed
TDWLFT model consistently outperforms state-of-the-art approaches in terms of
both in both prediction accuracy and computational efficiency.

</details>


### [47] [Features-based embedding or Feature-grounding](https://arxiv.org/abs/2506.22442)
*Piotr Makarevich*

Main category: cs.LG

TL;DR: The paper explores reproducing knowledge-based structured thinking in deep learning models using feature-grounded embeddings.


<details>
  <summary>Details</summary>
Motivation: To align shareable representations with interpretable domain-specific features, leveraging prior knowledge and conceptual categories.

Method: Introduces a specific approach to build feature-grounded embeddings.

Result: Aims to create operable dictionary representations with interpretable features.

Conclusion: Feature-grounded embeddings can bridge deep learning with structured, knowledge-based reasoning.

Abstract: In everyday reasoning, when we think about a particular object, we associate
it with a unique set of expected properties such as weight, size, or more
abstract attributes like density or horsepower. These expectations are shaped
by our prior knowledge and the conceptual categories we have formed through
experience. This paper investigates how such knowledge-based structured
thinking can be reproduced in deep learning models using features based
embeddings. Specially, it introduces an specific approach to build
feature-grounded embedding, aiming to align shareable representations of
operable dictionary with interpretable domain-specific conceptual features.

</details>


### [48] [Learning Interpretable Rules from Neural Networks: Neurosymbolic AI for Radar Hand Gesture Recognition](https://arxiv.org/abs/2506.22443)
*Sarah Seifi,Tobias Sukianto,Cecilia Carbonelli,Lorenzo Servadei,Robert Wille*

Main category: cs.LG

TL;DR: RL-Net, a neuro-symbolic rule learning neural network, balances interpretability and performance in radar-based hand gesture recognition, outperforming transparent and black-box models.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between interpretable rule-based models and high-performing deep neural networks in hand gesture recognition.

Method: RL-Net learns interpretable rule lists via neural optimization, benchmarked against MIRA (rule-based) and XentricAI (explainable black-box).

Result: RL-Net achieves 93.03% F1 score, reduces rule complexity, and addresses optimization challenges like rule pruning and hierarchy bias.

Conclusion: RL-Net is a practical middle ground for interpretable HGR, with potential for edge-deployable sensing systems.

Abstract: Rule-based models offer interpretability but struggle with complex data,
while deep neural networks excel in performance yet lack transparency. This
work investigates a neuro-symbolic rule learning neural network named RL-Net
that learns interpretable rule lists through neural optimization, applied for
the first time to radar-based hand gesture recognition (HGR). We benchmark
RL-Net against a fully transparent rule-based system (MIRA) and an explainable
black-box model (XentricAI), evaluating accuracy, interpretability, and user
adaptability via transfer learning. Our results show that RL-Net achieves a
favorable trade-off, maintaining strong performance (93.03% F1) while
significantly reducing rule complexity. We identify optimization challenges
specific to rule pruning and hierarchy bias and propose stability-enhancing
modifications. Compared to MIRA and XentricAI, RL-Net emerges as a practical
middle ground between transparency and performance. This study highlights the
real-world feasibility of neuro-symbolic models for interpretable HGR and
offers insights for extending explainable AI to edge-deployable sensing
systems.

</details>


### [49] [Active Learning for Forecasting Severity among Patients with Post Acute Sequelae of SARS-CoV-2](https://arxiv.org/abs/2506.22444)
*Jing Wang,Amar Sra,Jeremy C. Weiss*

Main category: cs.LG

TL;DR: The paper introduces a cohort of 18 PASC patients with text time series features using Llama-3.1-70B-Instruct and proposes an Active Attention Network to predict clinical risk and progression events, aiming to improve patient care.


<details>
  <summary>Details</summary>
Motivation: Accurate identification of PASC progression events (e.g., hospitalization, reinfection) is critical for patient management, but traditional models fail to capture nuanced progression.

Method: Uses a cohort of 18 PASC patients with text time series features (Llama-3.1-70B-Instruct) and clinical expert annotations. Proposes an Active Attention Network for risk prediction and event identification.

Result: Aims to enhance clinical risk prediction accuracy and identify progression events with fewer annotations by integrating human expertise and active learning.

Conclusion: The approach seeks to improve patient care and decision-making for SARS-CoV-2 patients by leveraging advanced modeling and expert input.

Abstract: The long-term effects of Postacute Sequelae of SARS-CoV-2, known as PASC,
pose a significant challenge to healthcare systems worldwide. Accurate
identification of progression events, such as hospitalization and reinfection,
is essential for effective patient management and resource allocation. However,
traditional models trained on structured data struggle to capture the nuanced
progression of PASC. In this study, we introduce the first publicly available
cohort of 18 PASC patients, with text time series features based on Large
Language Model Llama-3.1-70B-Instruct and clinical risk annotated by clinical
expert. We propose an Active Attention Network to predict the clinical risk and
identify progression events related to the risk. By integrating human expertise
with active learning, we aim to enhance clinical risk prediction accuracy and
enable progression events identification with fewer number of annotation. The
ultimate goal is to improves patient care and decision-making for SARS-CoV-2
patient.

</details>


### [50] [Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security](https://arxiv.org/abs/2506.22445)
*Saad Alqithami*

Main category: cs.LG

TL;DR: The paper introduces HAMARL, a hierarchical multi-agent reinforcement learning framework, to enhance cyber-physical system security against evolving cyber threats.


<details>
  <summary>Details</summary>
Motivation: Traditional security methods are insufficient against adaptive and zero-day attacks in increasingly connected cyber-physical systems.

Method: HAMARL uses local agents for subsystem security and a global coordinator for system-wide defense, with adversarial training to simulate threats.

Result: HAMARL outperforms traditional methods, improving attack detection, reducing response times, and ensuring operational continuity.

Conclusion: Combining hierarchical multi-agent coordination with adversarial training enhances resilience and security in next-generation CPS.

Abstract: Cyber-Physical Systems play a critical role in the infrastructure of various
sectors, including manufacturing, energy distribution, and autonomous
transportation systems. However, their increasing connectivity renders them
highly vulnerable to sophisticated cyber threats, such as adaptive and zero-day
attacks, against which traditional security methods like rule-based intrusion
detection and single-agent reinforcement learning prove insufficient. To
overcome these challenges, this paper introduces a novel Hierarchical
Adversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework.
HAMARL employs a hierarchical structure consisting of local agents dedicated to
subsystem security and a global coordinator that oversees and optimizes
comprehensive, system-wide defense strategies. Furthermore, the framework
incorporates an adversarial training loop designed to simulate and anticipate
evolving cyber threats, enabling proactive defense adaptation. Extensive
experimental evaluations conducted on a simulated industrial IoT testbed
indicate that HAMARL substantially outperforms traditional multi-agent
reinforcement learning approaches, significantly improving attack detection
accuracy, reducing response times, and ensuring operational continuity. The
results underscore the effectiveness of combining hierarchical multi-agent
coordination with adversarially-aware training to enhance the resilience and
security of next-generation CPS.

</details>


### [51] [EAGLE: Efficient Alignment of Generalized Latent Embeddings for Multimodal Survival Prediction with Interpretable Attribution Analysis](https://arxiv.org/abs/2506.22446)
*Aakash Tripathi,Asim Waqas,Matthew B. Schabath,Yasin Yilmaz,Ghulam Rasool*

Main category: cs.LG

TL;DR: EAGLE is a deep learning framework for cancer survival prediction, addressing limitations of existing methods with attention-based fusion, dimensionality reduction, interpretability, and adaptability across cancer types.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal approaches for cancer survival prediction lack efficient fusion, computational scalability, and interpretability, hindering clinical adoption.

Method: EAGLE uses dynamic cross-modal attention, massive dimensionality reduction, three attribution methods, and a unified pipeline for multimodal fusion.

Result: EAGLE achieved high-risk stratification (4-5 fold differences in survival) and interpretable patient-level insights across three cancer types.

Conclusion: EAGLE combines performance and interpretability, bridging AI capabilities with clinical deployment for scalable, trustworthy survival prediction.

Abstract: Accurate cancer survival prediction requires integration of diverse data
modalities that reflect the complex interplay between imaging, clinical
parameters, and textual reports. However, existing multimodal approaches suffer
from simplistic fusion strategies, massive computational requirements, and lack
of interpretability-critical barriers to clinical adoption. We present EAGLE
(Efficient Alignment of Generalized Latent Embeddings), a novel deep learning
framework that addresses these limitations through attention-based multimodal
fusion with comprehensive attribution analysis. EAGLE introduces four key
innovations: (1) dynamic cross-modal attention mechanisms that learn
hierarchical relationships between modalities, (2) massive dimensionality
reduction (99.96%) while maintaining predictive performance, (3) three
complementary attribution methods providing patient-level interpretability, and
(4) a unified pipeline enabling seamless adaptation across cancer types. We
evaluated EAGLE on 911 patients across three distinct malignancies:
glioblastoma (GBM, n=160), intraductal papillary mucinous neoplasms (IPMN,
n=171), and non-small cell lung cancer (NSCLC, n=580). Patient-level analysis
showed high-risk individuals relied more heavily on adverse imaging features,
while low-risk patients demonstrated balanced modality contributions. Risk
stratification identified clinically meaningful groups with 4-fold (GBM) to
5-fold (NSCLC) differences in median survival, directly informing treatment
intensity decisions. By combining state-of-the-art performance with clinical
interpretability, EAGLE bridges the gap between advanced AI capabilities and
practical healthcare deployment, offering a scalable solution for multimodal
survival prediction that enhances both prognostic accuracy and physician trust
in automated predictions.

</details>


### [52] [Vision Transformers for Multi-Variable Climate Downscaling: Emulating Regional Climate Models with a Shared Encoder and Multi-Decoder Architecture](https://arxiv.org/abs/2506.22447)
*Fabio Merizzi,Harilaos Loukos*

Main category: cs.LG

TL;DR: A multi-task Vision Transformer (ViT) architecture (1EMD) is proposed for multi-variable climate downscaling, outperforming single-variable models in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: GCMs lack regional detail, and RCMs are computationally expensive. Deep learning alternatives often focus on single variables, limiting contextual awareness and cross-variable interaction.

Method: A ViT architecture with a shared encoder and variable-specific decoders (1EMD) jointly predicts temperature, wind speed, and geopotential height from GCM inputs.

Result: The multi-variable approach shows positive knowledge transfer and outperforms single-variable baselines in accuracy and computational efficiency.

Conclusion: Multi-variable modeling is effective for high-resolution climate downscaling, offering improved performance and efficiency.

Abstract: Global Climate Models (GCMs) are critical for simulating large-scale climate
dynamics, but their coarse spatial resolution limits their applicability in
regional studies. Regional Climate Models (RCMs) refine this through dynamic
downscaling, albeit at considerable computational cost and with limited
flexibility. While deep learning has emerged as an efficient data-driven
alternative, most existing studies have focused on single-variable models that
downscale one variable at a time. This approach can lead to limited contextual
awareness, redundant computation, and lack of cross-variable interaction. Our
study addresses these limitations by proposing a multi-task, multi-variable
Vision Transformer (ViT) architecture with a shared encoder and
variable-specific decoders (1EMD). The proposed architecture jointly predicts
three key climate variables: surface temperature (tas), wind speed (sfcWind),
and 500 hPa geopotential height (zg500), directly from GCM-resolution inputs,
emulating RCM-scale downscaling over Europe. We show that our multi-variable
approach achieves positive cross-variable knowledge transfer and consistently
outperforms single-variable baselines trained under identical conditions, while
also improving computational efficiency. These results demonstrate the
effectiveness of multi-variable modeling for high-resolution climate
downscaling.

</details>


### [53] [Stabilization of industrial processes with time series machine learning](https://arxiv.org/abs/2506.22502)
*Matvei Anoshin,Olga Tsurkan,Vadim Lopatkin,Leonid Fedichkin*

Main category: cs.LG

TL;DR: A neural network pipeline improves time series stabilization, outperforming traditional methods by 3x in temperature control.


<details>
  <summary>Details</summary>
Motivation: Stabilizing time series processes is critical in industries, and machine learning can enhance efficiency and quality.

Method: A two-neural-network pipeline (oracle predictor and optimizer) replaces point-wise optimization with neural network training.

Result: Achieves 3x better stability in temperature control compared to conventional solvers.

Conclusion: The proposed method effectively enhances stabilization with reduced computational resources.

Abstract: The stabilization of time series processes is a crucial problem that is
ubiquitous in various industrial fields. The application of machine learning to
its solution can have a decisive impact, improving both the quality of the
resulting stabilization with less computational resources required. In this
work, we present a simple pipeline consisting of two neural networks: the
oracle predictor and the optimizer, proposing a substitution of the point-wise
values optimization to the problem of the neural network training, which
successfully improves stability in terms of the temperature control by about 3
times compared to ordinary solvers.

</details>


### [54] [Task-Agnostic Contrastive Pretraining for Relational Deep Learning](https://arxiv.org/abs/2506.22530)
*Jakub Peleška,Gustav Šír*

Main category: cs.LG

TL;DR: The paper introduces a task-agnostic contrastive pretraining approach for Relational Deep Learning (RDL) to learn transferable representations from relational databases, outperforming task-specific models.


<details>
  <summary>Details</summary>
Motivation: Existing RDL models rely on task-specific supervised learning, limiting scalability and reuse. The paper aims to address this by proposing a pretraining method for database-wide representation learning.

Method: The approach uses three contrastive objectives (row-level, link-level, context-level) to capture structural and semantic heterogeneity. It employs a modular RDL architecture and efficient sampling for heterogeneous databases.

Result: Preliminary results show fine-tuning pretrained models outperforms training from scratch, indicating the method's effectiveness for transferable representations.

Conclusion: The proposed contrastive pretraining approach for RDL shows promise in learning reusable and scalable representations for relational data.

Abstract: Relational Deep Learning (RDL) is an emerging paradigm that leverages Graph
Neural Network principles to learn directly from relational databases by
representing them as heterogeneous graphs. However, existing RDL models
typically rely on task-specific supervised learning, requiring training
separate models for each predictive task, which may hamper scalability and
reuse.
  In this work, we propose a novel task-agnostic contrastive pretraining
approach for RDL that enables database-wide representation learning. For that
aim, we introduce three levels of contrastive objectives$-$row-level,
link-level, and context-level$-$designed to capture the structural and semantic
heterogeneity inherent to relational data. We implement the respective
pretraining approach through a modular RDL architecture and an efficient
sampling strategy tailored to the heterogeneous database setting. Our
preliminary results on standard RDL benchmarks demonstrate that fine-tuning the
pretrained models measurably outperforms training from scratch, validating the
promise of the proposed methodology in learning transferable representations
for relational data.

</details>


### [55] [Exploration Behavior of Untrained Policies](https://arxiv.org/abs/2506.22566)
*Jacob Adamczyk*

Main category: cs.LG

TL;DR: The paper explores how deep neural policy architectures influence exploration in RL before training, using theoretical and empirical methods to analyze trajectory generation and state-visitation distributions.


<details>
  <summary>Details</summary>
Motivation: Understanding how untrained policies shape exploration in RL, especially in sparse or adversarial reward environments, is a fundamental challenge.

Method: The study uses theory of infinite-width networks and continuous-time limits to analyze untrained policies, along with empirical demonstrations in a toy model.

Result: Untrained policies produce correlated actions and non-trivial state-visitation distributions, revealing insights into inductive biases for exploration.

Conclusion: The work provides a framework for using policy initialization to study exploration behavior early in training.

Abstract: Exploration remains a fundamental challenge in reinforcement learning (RL),
particularly in environments with sparse or adversarial reward structures. In
this work, we study how the architecture of deep neural policies implicitly
shapes exploration before training. We theoretically and empirically
demonstrate strategies for generating ballistic or diffusive trajectories from
untrained policies in a toy model. Using the theory of infinite-width networks
and a continuous-time limit, we show that untrained policies return correlated
actions and result in non-trivial state-visitation distributions. We discuss
the distributions of the corresponding trajectories for a standard
architecture, revealing insights into inductive biases for tackling
exploration. Our results establish a theoretical and experimental framework for
using policy initialization as a design tool to understand exploration behavior
in early training.

</details>


### [56] [The Hidden Link Between RLHF and Contrastive Learning](https://arxiv.org/abs/2506.22578)
*Xufei Lv,Haoyuan Sun,Xuefeng Bai,Min Zhang,Houde Liu,Kehai Chen*

Main category: cs.LG

TL;DR: The paper interprets RLHF and DPO as mutual information maximization methods, linking them to contrastive learning, and proposes MIO to improve performance.


<details>
  <summary>Details</summary>
Motivation: To explore the connection between RLHF, DPO, and mutual information maximization, and address limitations in reasoning capacity.

Method: Proposes Mutual Information Optimization (MIO) using the Jensen-Shannon MI estimator, replacing the DV/MINE bound.

Result: MIO mitigates late-stage decline in chosen-likelihood and outperforms DPO in reasoning and mathematical benchmarks.

Conclusion: MIO offers a competitive alternative to RLHF and DPO, enhancing reasoning capabilities in LLMs.

Abstract: Alignment of large language models (LLMs) with human values has recently
garnered significant attention, with prominent examples including the canonical
yet costly Reinforcement Learning from Human Feedback (RLHF) and the simple
Direct Preference Optimization (DPO). In this work, we demonstrate that both
RLHF and DPO can be interpreted from the perspective of mutual information (MI)
maximization, uncovering a profound connection to contrastive learning. Within
this framework, both RLHF and DPO can be viewed as methods that perform
contrastive learning based on the positive and negative samples derived from
the base model, leveraging the Donsker-Varadhan (DV) lower bound on MI
(equivalently, the MINE estimator). This paradigm further explains why RLHF may
not intrinsically incentivize reasoning capacities in LLMs beyond what is
already present in the base model. Building on this perspective, we replace the
DV/MINE bound with the Jensen-Shannon MI estimator and propose Mutual
Information Optimization (MIO). Comprehensive theoretical analysis and
extensive empirical evaluations demonstrate that MIO mitigates the late-stage
decline in chosen-likelihood observed in DPO, achieving competitive or superior
performance across various challenging reasoning and mathematical benchmarks.
We will release the model and code upon acceptance.

</details>


### [57] [Are Fast Methods Stable in Adversarially Robust Transfer Learning?](https://arxiv.org/abs/2506.22602)
*Joshua C. Zhao,Saurabh Bagchi*

Main category: cs.LG

TL;DR: FGSM in adversarial fine-tuning is stable and efficient, outperforming PGD in computational cost with minimal robustness loss.


<details>
  <summary>Details</summary>
Motivation: Reduce computational cost of adversarial fine-tuning in transfer learning while maintaining robustness.

Method: Revisit FGSM for adversarial fine-tuning, testing stability and performance across datasets.

Result: FGSM is stable, avoids catastrophic overfitting, and performs nearly as well as PGD with 4x less training time.

Conclusion: FGSM is a viable, efficient alternative to PGD for robust transfer learning.

Abstract: Transfer learning is often used to decrease the computational cost of model
training, as fine-tuning a model allows a downstream task to leverage the
features learned from the pre-training dataset and quickly adapt them to a new
task. This is particularly useful for achieving adversarial robustness, as
adversarially training models from scratch is very computationally expensive.
However, high robustness in transfer learning still requires adversarial
training during the fine-tuning phase, which requires up to an order of
magnitude more time than standard fine-tuning. In this work, we revisit the use
of the fast gradient sign method (FGSM) in robust transfer learning to improve
the computational cost of adversarial fine-tuning. We surprisingly find that
FGSM is much more stable in adversarial fine-tuning than when training from
scratch. In particular, FGSM fine-tuning does not suffer from any issues with
catastrophic overfitting at standard perturbation budgets of $\varepsilon=4$ or
$\varepsilon=8$. This stability is further enhanced with parameter-efficient
fine-tuning methods, where FGSM remains stable even up to $\varepsilon=32$ for
linear probing. We demonstrate how this stability translates into performance
across multiple datasets. Compared to fine-tuning with the more commonly used
method of projected gradient descent (PGD), on average, FGSM only loses 0.39%
and 1.39% test robustness for $\varepsilon=4$ and $\varepsilon=8$ while using
$4\times$ less training time. Surprisingly, FGSM may not only be a
significantly more efficient alternative to PGD in adversarially robust
transfer learning but also a well-performing one.

</details>


### [58] [Hierarchical Modeling and Architecture Optimization: Review and Unified Framework](https://arxiv.org/abs/2506.22621)
*Paul Saves,Edward Hallé-Hannan,Jasper Bussemaker,Youssef Diouane,Nathalie Bartoli*

Main category: cs.LG

TL;DR: The paper proposes a unified framework for handling mixed-variable inputs in simulation-based problems, addressing hierarchical, conditional, and heterogeneous domains. It introduces meta and partially-decreed variables, design space graphs, and integrates surrogate modeling and optimization techniques.


<details>
  <summary>Details</summary>
Motivation: To address challenges in data representation, modeling, and optimization for mixed-variable inputs with hierarchical or conditional structures.

Method: Introduces meta and partially-decreed variables, design space graphs, and hierarchical kernels/distances. Implemented in the Surrogate Modeling Toolbox (SMT 2.0).

Result: Demonstrated effectiveness in Bayesian optimization for complex system design, including a green aircraft architecture case study.

Conclusion: The framework generalizes existing approaches and provides efficient tools for modeling and optimizing complex hierarchical domains.

Abstract: Simulation-based problems involving mixed-variable inputs frequently feature
domains that are hierarchical, conditional, heterogeneous, or tree-structured.
These characteristics pose challenges for data representation, modeling, and
optimization. This paper reviews extensive literature on these structured input
spaces and proposes a unified framework that generalizes existing approaches.
In this framework, input variables may be continuous, integer, or categorical.
A variable is described as meta if its value governs the presence of other
decreed variables, enabling the modeling of conditional and hierarchical
structures.
  We further introduce the concept of partially-decreed variables, whose
activation depends on contextual conditions. To capture these inter-variable
hierarchical relationships, we introduce design space graphs, combining
principles from feature modeling and graph theory. This allows the definition
of general hierarchical domains suitable for describing complex system
architectures. The framework supports the use of surrogate models over such
domains and integrates hierarchical kernels and distances for efficient
modeling and optimization. The proposed methods are implemented in the
open-source Surrogate Modeling Toolbox (SMT 2.0), and their capabilities are
demonstrated through applications in Bayesian optimization for complex system
design, including a case study in green aircraft architecture.

</details>


### [59] [A hierarchical Vovk-Azoury-Warmuth forecaster with discounting for online regression in RKHS](https://arxiv.org/abs/2506.22631)
*Dmitry B. Rokhlin*

Main category: cs.LG

TL;DR: The paper introduces H-VAW-D, a hierarchical algorithm for online regression in RKHS, combining DVAW with random feature approximation to achieve optimal dynamic regret.


<details>
  <summary>Details</summary>
Motivation: To extend the DVAW forecaster to non-parametric domains and adaptively learn key parameters for improved performance.

Method: Proposes H-VAW-D, a hierarchical algorithm integrating DVAW with random features, learning discount factors and feature counts adaptively.

Result: Achieves $O(T^{2/3}P_T^{1/3} + \sqrt{T}\ln T)$ expected dynamic regret with $O(T\ln T)$ per-iteration complexity.

Conclusion: H-VAW-D effectively bridges finite-dimensional and non-parametric settings, offering adaptive and computationally efficient online regression.

Abstract: We study the problem of online regression with the unconstrained quadratic
loss against a time-varying sequence of functions from a Reproducing Kernel
Hilbert Space (RKHS). Recently, Jacobsen and Cutkosky (2024) introduced a
discounted Vovk-Azoury-Warmuth (DVAW) forecaster that achieves optimal dynamic
regret in the finite-dimensional case. In this work, we lift their approach to
the non-parametric domain by synthesizing the DVAW framework with a random
feature approximation. We propose a fully adaptive, hierarchical algorithm,
which we call H-VAW-D (Hierarchical Vovk-Azoury-Warmuth with Discounting), that
learns both the discount factor and the number of random features. We prove
that this algorithm, which has a per-iteration computational complexity of
$O(T\ln T)$, achieves an expected dynamic regret of $O(T^{2/3}P_T^{1/3} +
\sqrt{T}\ln T)$, where $P_T$ is the functional path length of a comparator
sequence.

</details>


### [60] [Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training](https://arxiv.org/abs/2506.22638)
*Aadim Nepal,Safal Shrestha,Anubhav Shrestha,Minwu Kim,Keith Ross*

Main category: cs.LG

TL;DR: The paper investigates whether improvements in mathematical reasoning in large language models post-training are due to major changes in transformer layers or minor adjustments. It finds that mathematical reasoning relies on specific, persistent layer structures, unlike non-mathematical tasks.


<details>
  <summary>Details</summary>
Motivation: To understand if post-training enhancements in mathematical reasoning stem from significant layer changes or minor adjustments, and to identify the role of specific layers in such tasks.

Method: Conducts layer-wise ablation experiments on base and post-trained models (instruction-tuned, knowledge-distilled, reinforcement learning variants) using mathematical reasoning benchmarks.

Result: Mathematical reasoning depends on specific layers, whose removal causes up to 80% accuracy drop. Non-mathematical tasks show no such critical layers. These layers coincide with major representational transformations.

Conclusion: Mathematical reasoning requires specialized layers emerging during pre-training, while non-reasoning tasks do not, highlighting a distinct layer importance structure for reasoning.

Abstract: Large language models can exhibit improved mathematical reasoning
capabilities following post-training with instruction tuning, reinforcement
learning, or knowledge distillation. However, it remains unclear whether these
improvements are driven by major changes in transformer layers or from minor
adjustments that leave the relative layer importance structures of the base
model largely unchanged. We investigate this question through systematic
layer-wise ablation experiments, examining base, instruction-tuned,
knowledge-distilled, and reinforcement learning variants on mathematical
reasoning benchmarks. Our findings show that mathematical reasoning gives rise
to a specific layer importance structure, and this structure persists across
all post-training paradigms. Removal of such layers causes accuracy drops of up
to 80%. In contrast, non-mathematical tasks like factual recall exhibit no
critical layers. This distinction suggests that mathematical reasoning requires
specialized layers that emerge during pre-training, while other non-reasoning
tasks do not. From an information-theoretic perspective, we also observe that
these critical layers are the same layers where major representational
transformation occurs.

</details>


### [61] [Cost-effective Reduced-Order Modeling via Bayesian Active Learning](https://arxiv.org/abs/2506.22645)
*Amir Hossein Rahmati,Nathan M. Urban,Byung-Jun Yoon,Xiaoning Qian*

Main category: cs.LG

TL;DR: BayPOD-AL is an active learning framework using Bayesian POD to efficiently learn reduced-order models from high-fidelity data, reducing computational costs and improving generalizability.


<details>
  <summary>Details</summary>
Motivation: Machine Learning surrogates require large datasets, limiting real-world applicability. BayPOD-AL addresses this by actively selecting informative data.

Method: BayPOD-AL combines Bayesian proper orthogonal decomposition with active learning to suggest informative data and reduce training costs.

Result: BayPOD-AL outperforms other uncertainty-guided strategies in predicting temperature evolution and generalizes well to higher temporal resolution data.

Conclusion: BayPOD-AL offers a computationally efficient and generalizable solution for learning reduced-order models from complex systems.

Abstract: Machine Learning surrogates have been developed to accelerate solving systems
dynamics of complex processes in different science and engineering
applications. To faithfully capture governing systems dynamics, these methods
rely on large training datasets, hence restricting their applicability in
real-world problems. In this work, we propose BayPOD-AL, an active learning
framework based on an uncertainty-aware Bayesian proper orthogonal
decomposition (POD) approach, which aims to effectively learn reduced-order
models from high-fidelity full-order models representing complex systems.
Experimental results on predicting the temperature evolution over a rod
demonstrate BayPOD-AL's effectiveness in suggesting the informative data and
reducing computational cost related to constructing a training dataset compared
to other uncertainty-guided active learning strategies. Furthermore, we
demonstrate BayPOD-AL's generalizability and efficiency by evaluating its
performance on a dataset of higher temporal resolution than the training
dataset.

</details>


### [62] [Learning Stochastic Multiscale Models](https://arxiv.org/abs/2506.22655)
*Andrew F. Ilersich,Prasanth B. Nair*

Main category: cs.LG

TL;DR: Proposes a data-driven method to learn stochastic multiscale models for dynamical systems, improving predictive accuracy over traditional simulations.


<details>
  <summary>Details</summary>
Motivation: Addresses computational challenges in simulating multiscale dynamical systems by avoiding high-dimensional state spaces.

Method: Uses stochastic differential equations and a coarse mesh with an auxiliary state, learned via forward-solver-free variational inference.

Result: Learned models outperform direct numerical simulation and closure-type models in predictive accuracy.

Conclusion: The approach offers a promising data-driven alternative for efficient and accurate multiscale modeling.

Abstract: The physical sciences are replete with dynamical systems that require the
resolution of a wide range of length and time scales. This presents significant
computational challenges since direct numerical simulation requires
discretization at the finest relevant scales, leading to a high-dimensional
state space. In this work, we propose an approach to learn stochastic
multiscale models in the form of stochastic differential equations directly
from observational data. Our method resolves the state on a coarse mesh while
introducing an auxiliary state to capture the effects of unresolved scales. We
learn the parameters of the multiscale model using a modern forward-solver-free
amortized variational inference method. Our approach draws inspiration from
physics-based multiscale modeling approaches, such as large-eddy simulation in
fluid dynamics, while learning directly from data. We present numerical studies
to demonstrate that our learned multiscale models achieve superior predictive
accuracy compared to direct numerical simulation and closure-type models at
equivalent resolution.

</details>


### [63] [DistShap: Scalable GNN Explanations with Distributed Shapley Values](https://arxiv.org/abs/2506.22668)
*Selahattin Akkas,Aditya Devarakonda,Ariful Azad*

Main category: cs.LG

TL;DR: DistShap is a parallel algorithm for efficiently computing Shapley value-based explanations for GNN predictions by distributing computations across multiple GPUs.


<details>
  <summary>Details</summary>
Motivation: The need to explain GNN predictions efficiently, especially for large-scale graphs with millions of features, due to the computational expense of existing methods.

Method: DistShap distributes Shapley value computations by sampling subgraphs, parallelizing GNN inference, and solving a distributed least squares problem to determine edge importance.

Result: DistShap outperforms existing GNN explanation methods in accuracy and scales to models with millions of features using up to 128 GPUs.

Conclusion: DistShap provides a scalable and accurate solution for explaining GNN predictions, addressing computational challenges in large-scale applications.

Abstract: With the growing adoption of graph neural networks (GNNs), explaining their
predictions has become increasingly important. However, attributing predictions
to specific edges or features remains computationally expensive. For example,
classifying a node with 100 neighbors using a 3-layer GNN may involve
identifying important edges from millions of candidates contributing to the
prediction. To address this challenge, we propose DistShap, a parallel
algorithm that distributes Shapley value-based explanations across multiple
GPUs. DistShap operates by sampling subgraphs in a distributed setting,
executing GNN inference in parallel across GPUs, and solving a distributed
least squares problem to compute edge importance scores. DistShap outperforms
most existing GNN explanation methods in accuracy and is the first to scale to
GNN models with millions of features by using up to 128 GPUs on the NERSC
Perlmutter supercomputer.

</details>


### [64] [Mitigating Semantic Collapse in Generative Personalization with a Surprisingly Simple Test-Time Embedding Adjustment](https://arxiv.org/abs/2506.22685)
*Anh Bui,Trang Vu,Trung Le,Junae Kim,Tamas Abraham,Rollin Omari,Amar Kaur,Dinh Phung*

Main category: cs.LG

TL;DR: The paper addresses the semantic collapsing problem in generative personalization, where learned visual concepts drift from their original meaning, simplifying outputs. A training-free method is proposed to adjust embeddings at inference, improving alignment.


<details>
  <summary>Details</summary>
Motivation: Semantic collapsing reduces the richness of multi-concept prompts, leading to oversimplified outputs. The goal is to preserve the intended semantic context.

Method: A training-free approach adjusts the magnitude and direction of pre-trained embeddings during inference to prevent drift.

Result: The method effectively mitigates semantic collapsing, improving text-image alignment across various personalization techniques.

Conclusion: The proposed solution is simple, effective, and broadly applicable, enhancing the fidelity of generative personalization.

Abstract: In this paper, we investigate the semantic collapsing problem in generative
personalization, an under-explored topic where the learned visual concept
($V^*$) gradually shifts from its original textual meaning and comes to
dominate other concepts in multi-concept input prompts. This issue not only
reduces the semantic richness of complex input prompts like "a photo of $V^*$
wearing glasses and playing guitar" into simpler, less contextually rich forms
such as "a photo of $V^*$" but also leads to simplified output images that fail
to capture the intended concept.
  We identify the root cause as unconstrained optimisation, which allows the
learned embedding $V^*$ to drift arbitrarily in the embedding space, both in
direction and magnitude. To address this, we propose a simple yet effective
training-free method that adjusts the magnitude and direction of pre-trained
embedding at inference time, effectively mitigating the semantic collapsing
problem. Our method is broadly applicable across different personalization
methods and demonstrates significant improvements in text-image alignment in
diverse use cases. Our code is anonymously published at
https://anonymous.4open.science/r/Embedding-Adjustment.

</details>


### [65] [Residual Matrix Transformers: Scaling the Size of the Residual Stream](https://arxiv.org/abs/2506.22696)
*Brian Mak,Jeffrey Flanigan*

Main category: cs.LG

TL;DR: The paper introduces the Residual Matrix Transformer (RMT), replacing the transformer's residual stream with an outer product memory matrix, offering improved scalability, efficiency, and performance.


<details>
  <summary>Details</summary>
Motivation: To enhance the transformer's residual stream mechanism for better efficiency and performance by leveraging an outer product memory matrix.

Method: Replaces the transformer's residual stream with an outer product memory matrix, creating the RMT model.

Result: RMT achieves comparable loss with fewer FLOPS (58% less), parameters (25% less), and training tokens (41% less), while outperforming transformers in downstream tasks.

Conclusion: The RMT provides a more efficient and scalable alternative to traditional transformers, with theoretical and practical advantages.

Abstract: The residual stream acts as a memory bus where transformer layers both store
and access features (Elhage et al., 2021). We consider changing the mechanism
for retrieving and storing information in the residual stream, and replace the
residual stream of the transformer with an outer product memory matrix
(Kohonen, 1972, Anderson, 1972). We call this model the Residual Matrix
Transformer (RMT). We find that the RMT enjoys a number of attractive
properties: 1) the size of the residual stream can be scaled independently of
compute and model size, improving performance, 2) the RMT can achieve the same
loss as the transformer with 58% fewer FLOPS, 25% fewer parameters, and 41%
fewer training tokens tokens, and 3) the RMT outperforms the transformer on
downstream evaluations. We theoretically analyze the transformer and the RMT,
and show that the RMT allows for more efficient scaling of the residual stream,
as well as improved variance propagation properties. Code for this project can
be found at https://github.com/bmac3/residual-matrix-transformer.

</details>


### [66] [FairMarket-RL: LLM-Guided Fairness Shaping for Multi-Agent Reinforcement Learning in Peer-to-Peer Markets](https://arxiv.org/abs/2506.22708)
*Shrenik Jadhav,Birva Sevak,Srijita Das,Akhtar Hussain,Wencong Su,Van-Hai Bui*

Main category: cs.LG

TL;DR: FairMarket-RL combines LLMs and RL for fairness-aware P2P trading, achieving high fairness scores and equitable outcomes in decentralized markets.


<details>
  <summary>Details</summary>
Motivation: Existing P2P trading lacks robust fairness frameworks, necessitating a scalable, adaptive solution.

Method: Uses LLMs as fairness critics with FTB and FBS metrics, integrating scores into RL rewards via IPPO training.

Result: Agents achieve >90% buyer demand fulfillment, fair seller margins, and FTB/FBS scores >0.80.

Conclusion: FairMarket-RL provides a scalable, equity-driven solution for decentralized trading systems.

Abstract: Peer-to-peer (P2P) trading is increasingly recognized as a key mechanism for
decentralized market regulation, yet existing approaches often lack robust
frameworks to ensure fairness. This paper presents FairMarket-RL, a novel
hybrid framework that combines Large Language Models (LLMs) with Reinforcement
Learning (RL) to enable fairness-aware trading agents. In a simulated P2P
microgrid with multiple sellers and buyers, the LLM acts as a real-time
fairness critic, evaluating each trading episode using two metrics:
Fairness-To-Buyer (FTB) and Fairness-Between-Sellers (FBS). These fairness
scores are integrated into agent rewards through scheduled
{\lambda}-coefficients, forming an adaptive LLM-guided reward shaping loop that
replaces brittle, rule-based fairness constraints. Agents are trained using
Independent Proximal Policy Optimization (IPPO) and achieve equitable outcomes,
fulfilling over 90% of buyer demand, maintaining fair seller margins, and
consistently reaching FTB and FBS scores above 0.80. The training process
demonstrates that fairness feedback improves convergence, reduces buyer
shortfalls, and narrows profit disparities between sellers. With its
language-based critic, the framework scales naturally, and its extension to a
large power distribution system with household prosumers illustrates its
practical applicability. FairMarket-RL thus offers a scalable, equity-driven
solution for autonomous trading in decentralized energy systems.

</details>


### [67] [Generalized Linear Mode Connectivity for Transformers](https://arxiv.org/abs/2506.22712)
*Alexander Theus,Alessandro Cabodi,Sotiris Anagnostidis,Antonio Orvieto,Sidak Pal Singh,Valentina Boeva*

Main category: cs.LG

TL;DR: The paper introduces a unified framework to analyze symmetries in neural network loss landscapes, enabling discovery of low- and zero-loss paths between models like Vision Transformers and GPT-2.


<details>
  <summary>Details</summary>
Motivation: To understand the geometry of neural network loss landscapes, particularly linear mode connectivity (LMC), which is often obscured by symmetries like neuron permutations.

Method: Proposes a framework capturing four symmetry classes: permutations, semi-permutations, orthogonal transformations, and general invertible maps.

Result: Enables discovery of low- and zero-barrier linear interpolation paths between independently trained models (e.g., Vision Transformers and GPT-2).

Conclusion: Reveals deeper structure in loss landscapes and highlights the importance of symmetry-aware analysis for understanding model space geometry.

Abstract: Understanding the geometry of neural network loss landscapes is a central
question in deep learning, with implications for generalization and
optimization. A striking phenomenon is linear mode connectivity (LMC), where
independently trained models can be connected by low- or zero-loss paths,
despite appearing to lie in separate loss basins. However, this is often
obscured by symmetries in parameter space -- such as neuron permutations --
which make functionally equivalent models appear dissimilar. Prior work has
predominantly focused on neuron re-ordering through permutations, but such
approaches are limited in scope and fail to capture the richer symmetries
exhibited by modern architectures such as Transformers. In this work, we
introduce a unified framework that captures four symmetry classes:
permutations, semi-permutations, orthogonal transformations, and general
invertible maps -- broadening the set of valid reparameterizations and
subsuming many previous approaches as special cases. Crucially, this
generalization enables, for the first time, the discovery of low- and
zero-barrier linear interpolation paths between independently trained Vision
Transformers and GPT-2 models. These results reveal deeper structure in the
loss landscape and underscore the importance of symmetry-aware analysis for
understanding model space geometry.

</details>


### [68] [BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute](https://arxiv.org/abs/2506.22716)
*Dujian Ding,Ankur Mallick,Shaokun Zhang,Chi Wang,Daniel Madrigal,Mirian Del Carmen Hipolito Garcia,Menglin Xia,Laks V. S. Lakshmanan,Qingyun Wu,Victor Rühle*

Main category: cs.LG

TL;DR: BEST-Route is a novel LLM query routing framework that dynamically selects models and response counts to balance cost and quality, achieving significant cost savings with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of prior query routing methods that overuse expensive large models by generating only one response, missing cost-saving opportunities with smaller models.

Method: Proposes BEST-Route, which selects models and response counts based on query difficulty and quality thresholds, leveraging multiple responses from small models to enhance quality cheaply.

Result: Reduces costs by up to 60% with less than 1% performance drop on real-world datasets.

Conclusion: BEST-Route effectively balances cost and quality in LLM query routing, outperforming prior approaches.

Abstract: Large language models (LLMs) are powerful tools but are often expensive to
deploy at scale. LLM query routing mitigates this by dynamically assigning
queries to models of varying cost and quality to obtain a desired trade-off.
Prior query routing approaches generate only one response from the selected
model and a single response from a small (inexpensive) model was often not good
enough to beat a response from a large (expensive) model due to which they end
up overusing the large model and missing out on potential cost savings.
However, it is well known that for small models, generating multiple responses
and selecting the best can enhance quality while remaining cheaper than a
single large-model response. We leverage this idea to propose BEST-Route, a
novel routing framework that chooses a model and the number of responses to
sample from it based on query difficulty and the quality thresholds.
Experiments on real-world datasets demonstrate that our method reduces costs by
up to 60% with less than 1% performance drop.

</details>


### [69] [Robust Tensor Completion via Gradient Tensor Nulclear L1-L2 Norm for Traffic Data Recovery](https://arxiv.org/abs/2506.22732)
*Hao Shu,Jicheng Li,Tianyv Lei,Lijun Sun*

Main category: cs.LG

TL;DR: The paper introduces RTC-GTNLN, a robust tensor completion model using a novel non-convex tensor rank surrogate (L1-L2 norm) to handle missing data and noise in spatiotemporal traffic data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Spatiotemporal traffic data often suffers from missing values and noise due to sensor and communication issues, necessitating reliable recovery methods for downstream applications.

Method: Proposes RTC-GTNLN, integrating a gradient tensor L1-L2 norm into the RTC framework to exploit global low-rankness and local consistency without trade-off parameters.

Result: RTC-GTNLN outperforms state-of-the-art methods in recovering data with simultaneous missing values and noise, as shown in experiments on real-world traffic datasets.

Conclusion: The RTC-GTNLN model effectively addresses dual degradation challenges in traffic data, offering superior performance in complex recovery scenarios.

Abstract: In real-world scenarios, spatiotemporal traffic data frequently experiences
dual degradation from missing values and noise caused by sensor malfunctions
and communication failures. Therefore, effective data recovery methods are
essential to ensure the reliability of downstream data-driven applications.
while classical tensor completion methods have been widely adopted, they are
incapable of modeling noise, making them unsuitable for complex scenarios
involving simultaneous data missingness and noise interference. Existing Robust
Tensor Completion (RTC) approaches offer potential solutions by separately
modeling the actual tensor data and noise. However, their effectiveness is
often constrained by the over-relaxation of convex rank surrogates and the
suboptimal utilization of local consistency, leading to inadequate model
accuracy. To address these limitations, we first introduce the tensor L1-L2
norm, a novel non-convex tensor rank surrogate that functions as an effective
low-rank representation tool. Leveraging an advanced feature fusion strategy,
we further develop the gradient tensor L1-L2 norm by incorporating the tensor
L1-L2 norm in the gradient domain. By integrating the gradient tensor nuclear
L1-L2 norm into the RTC framework, we propose the Robust Tensor Completion via
Gradient Tensor Nuclear L1-L2 Norm (RTC-GTNLN) model, which not only fully
exploits both global low-rankness and local consistency without trade-off
parameter, but also effectively handles the dual degradation challenges of
missing data and noise in traffic data. Extensive experiments conducted on
multiple real-world traffic datasets demonstrate that the RTC-GTNLN model
consistently outperforms existing state-of-the-art methods in complex recovery
scenarios involving simultaneous missing values and noise.

</details>


### [70] [FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8 Precision](https://arxiv.org/abs/2506.22771)
*Jingxiao Ma,Priyadarshini Panda,Sherief Reda*

Main category: cs.LG

TL;DR: The paper introduces an INT8 quantized training method using the Forward-Forward (FF) algorithm, reducing memory usage and energy consumption while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Backpropagation's inefficiencies in time and energy limit its use for edge devices. FF offers a promising alternative by avoiding backward passes.

Method: Proposes INT8 quantized training with FF, including a 'look-ahead' scheme to improve accuracy.

Result: Achieves 4.6% faster training, 8.3% energy savings, and 27.0% memory reduction on NVIDIA Jetson Orin Nano.

Conclusion: The FF-based INT8 quantized training is efficient for edge devices, balancing speed, energy, and accuracy.

Abstract: Backpropagation has been the cornerstone of neural network training for
decades, yet its inefficiencies in time and energy consumption limit its
suitability for resource-constrained edge devices. While low-precision neural
network quantization has been extensively researched to speed up model
inference, its application in training has been less explored. Recently, the
Forward-Forward (FF) algorithm has emerged as a promising alternative to
backpropagation, replacing the backward pass with an additional forward pass.
By avoiding the need to store intermediate activations for backpropagation, FF
can reduce memory footprint, making it well-suited for embedded devices. This
paper presents an INT8 quantized training approach that leverages FF's
layer-by-layer strategy to stabilize gradient quantization. Furthermore, we
propose a novel "look-ahead" scheme to address limitations of FF and improve
model accuracy. Experiments conducted on NVIDIA Jetson Orin Nano board
demonstrate 4.6% faster training, 8.3% energy savings, and 27.0% reduction in
memory usage, while maintaining competitive accuracy compared to the
state-of-the-art.

</details>


### [71] [Multimodal Atmospheric Super-Resolution With Deep Generative Models](https://arxiv.org/abs/2506.22780)
*Dibyajyoti Chakraborty,Haiwen Guan,Jason Stock,Troy Arcomano,Guido Cervone,Romit Maulik*

Main category: cs.LG

TL;DR: Score-based diffusion models enable zero-shot conditioning and Bayesian updates for data fusion, applied here to super-resolution of high-dimensional dynamical systems using sparse, multimodal data.


<details>
  <summary>Details</summary>
Motivation: To leverage score-based diffusion models for updating learned distributions with real-time data, enabling accurate super-resolution of complex systems like atmospheric datasets.

Method: Uses score-based diffusion modeling to learn and reverse a noising process, applying it to super-resolution tasks with sparse, multimodal observations (e.g., ERA5 and IGRA datasets).

Result: Accurate recovery of high-dimensional states from low-fidelity measurements and balanced influence of multiple data modalities during reconstructions.

Conclusion: Score-based diffusion models offer a powerful tool for data fusion and uncertainty-aware super-resolution in high-dimensional dynamical systems.

Abstract: Score-based diffusion modeling is a generative machine learning algorithm
that can be used to sample from complex distributions. They achieve this by
learning a score function, i.e., the gradient of the log-probability density of
the data, and reversing a noising process using the same. Once trained,
score-based diffusion models not only generate new samples but also enable
zero-shot conditioning of the generated samples on observed data. This promises
a novel paradigm for data and model fusion, wherein the implicitly learned
distributions of pretrained score-based diffusion models can be updated given
the availability of online data in a Bayesian formulation. In this article, we
apply such a concept to the super-resolution of a high-dimensional dynamical
system, given the real-time availability of low-resolution and experimentally
observed sparse sensor measurements from multimodal data. Additional analysis
on how score-based sampling can be used for uncertainty estimates is also
provided. Our experiments are performed for a super-resolution task that
generates the ERA5 atmospheric dataset given sparse observations from a
coarse-grained representation of the same and/or from unstructured experimental
observations of the IGRA radiosonde dataset. We demonstrate accurate recovery
of the high dimensional state given multiple sources of low-fidelity
measurements. We also discover that the generative model can balance the
influence of multiple dataset modalities during spatiotemporal reconstructions.

</details>


### [72] [Riemannian-Geometric Fingerprints of Generative Models](https://arxiv.org/abs/2506.22802)
*Hae Jin Song,Laurent Itti*

Main category: cs.LG

TL;DR: The paper proposes a geometric framework using Riemannian geometry to define and analyze fingerprints of generative models, improving model attribution and distinguishing synthetic from human data.


<details>
  <summary>Details</summary>
Motivation: The need to authenticate generative models for IP protection and accountability, and to address the threat of model collapse due to regurgitative training.

Method: A geometric approach defining artifacts and fingerprints using Riemannian geometry, replacing Euclidean distances with geodesic distances and kNN-based Riemannian center of mass.

Result: The method effectively distinguishes various generative models across datasets, resolutions, architectures, and modalities, improving attribution and generalization.

Conclusion: The proposed framework offers a principled and practical solution for understanding and leveraging generative models' fingerprints.

Abstract: Recent breakthroughs and rapid integration of generative models (GMs) have
sparked interest in the problem of model attribution and their fingerprints.
For instance, service providers need reliable methods of authenticating their
models to protect their IP, while users and law enforcement seek to verify the
source of generated content for accountability and trust. In addition, a
growing threat of model collapse is arising, as more model-generated data are
being fed back into sources (e.g., YouTube) that are often harvested for
training ("regurgitative training"), heightening the need to differentiate
synthetic from human data. Yet, a gap still exists in understanding generative
models' fingerprints, we believe, stemming from the lack of a formal framework
that can define, represent, and analyze the fingerprints in a principled way.
To address this gap, we take a geometric approach and propose a new definition
of artifact and fingerprint of GMs using Riemannian geometry, which allows us
to leverage the rich theory of differential geometry. Our new definition
generalizes previous work (Song et al., 2024) to non-Euclidean manifolds by
learning Riemannian metrics from data and replacing the Euclidean distances and
nearest-neighbor search with geodesic distances and kNN-based Riemannian center
of mass. We apply our theory to a new gradient-based algorithm for computing
the fingerprints in practice. Results show that it is more effective in
distinguishing a large array of GMs, spanning across 4 different datasets in 2
different resolutions (64 by 64, 256 by 256), 27 model architectures, and 2
modalities (Vision, Vision-Language). Using our proposed definition
significantly improves the performance on model attribution, as well as a
generalization to unseen datasets, model types, and modalities, suggesting its
practical efficacy.

</details>


### [73] [BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters](https://arxiv.org/abs/2506.22809)
*Cooper Doyle*

Main category: cs.LG

TL;DR: BayesLoRA integrates MC-Dropout into LoRA for task-specific uncertainty quantification, improving decision-making under uncertainty.


<details>
  <summary>Details</summary>
Motivation: General-purpose transformer uncertainty methods lack task-specific guardrails, limiting their utility in downstream workflows.

Method: BayesLoRA combines MC-Dropout with LoRA adapters to quantify uncertainty, focusing on fine-tuning distributions.

Result: LoRA adapters show amplified variance outside fine-tuning distributions, providing reliable confidence estimates.

Conclusion: BayesLoRA offers a tailored solution for uncertainty-aware agentic decision-making.

Abstract: We propose BayesLoRA, a task-specific uncertainty quantification framework
that integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike
general-purpose transformer uncertainty methods, BayesLoRA provides guardrails
tailored to downstream workflows, enabling agents to introspect and modulate
behavior under uncertainty. We demonstrate mathematically and empirically that
LoRA adapters exhibit amplified variance outside fine-tuning distributions,
yielding reliable confidence estimates for agentic decision-making.

</details>


### [74] [Deep learning 40 years of human migration](https://arxiv.org/abs/2506.22821)
*Thomas Gaskin,Guy J. Abel*

Main category: cs.LG

TL;DR: A novel dataset on global migration flows (1990-present) uses deep learning to estimate patterns, outperforming traditional methods with higher resolution and uncertainty bounds.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive, high-resolution dataset on global migration flows and stocks, addressing gaps in existing data and methods.

Method: A deep recurrent neural network trained on 18 covariates (geographic, economic, cultural, etc.) with ensemble learning for uncertainty bounds.

Result: Outperforms traditional methods in estimating five-year flows, offering higher temporal resolution and confidence bounds.

Conclusion: The open-source model and dataset serve as a valuable resource for future migration studies, highlighting regions needing more data.

Abstract: We present a novel and detailed dataset on origin-destination annual
migration flows and stocks between 230 countries and regions, spanning the
period from 1990 to the present. Our flow estimates are further disaggregated
by country of birth, providing a comprehensive picture of migration over the
last 43 years. The estimates are obtained by training a deep recurrent neural
network to learn flow patterns from 18 covariates for all countries, including
geographic, economic, cultural, societal, and political information. The
recurrent architecture of the neural network means that the entire past can
influence current migration patterns, allowing us to learn long-range temporal
correlations. By training an ensemble of neural networks and additionally
pushing uncertainty on the covariates through the trained network, we obtain
confidence bounds for all our estimates, allowing researchers to pinpoint the
geographic regions most in need of additional data collection. We validate our
approach on various test sets of unseen data, demonstrating that it
significantly outperforms traditional methods estimating five-year flows while
delivering a significant increase in temporal resolution. The model is fully
open source: all training data, neural network weights, and training code are
made public alongside the migration estimates, providing a valuable resource
for future studies of human migration.

</details>


### [75] [xLSTMAD: A Powerful xLSTM-based Method for Anomaly Detection](https://arxiv.org/abs/2506.22837)
*Kamil Faber,Marcin Pietroń,Dominik Żurek,Roberto Corizzo*

Main category: cs.LG

TL;DR: xLSTMAD is the first anomaly detection method using xLSTM, excelling in accuracy on multivariate time series data.


<details>
  <summary>Details</summary>
Motivation: No prior work explored xLSTM for anomaly detection, prompting the development of xLSTMAD.

Method: Uses encoder-decoder xLSTM with forecasting (xLSTMAD-F) and reconstruction (xLSTMAD-R) variants, tested with MSE and SoftDTW loss functions.

Result: Outperforms 23 baselines on TSB-AD-M benchmark, achieving state-of-the-art accuracy.

Conclusion: xLSTMAD demonstrates xLSTM's potential for anomaly detection, opening new research avenues.

Abstract: The recently proposed xLSTM is a powerful model that leverages expressive
multiplicative gating and residual connections, providing the temporal capacity
needed for long-horizon forecasting and representation learning. This
architecture has demonstrated success in time series forecasting, lossless
compression, and even large-scale language modeling tasks, where its linear
memory footprint and fast inference make it a viable alternative to
Transformers. Despite its growing popularity, no prior work has explored xLSTM
for anomaly detection. In this work, we fill this gap by proposing xLSTMAD, the
first anomaly detection method that integrates a full encoder-decoder xLSTM
architecture, purpose-built for multivariate time series data. Our encoder
processes input sequences to capture historical context, while the decoder is
devised in two separate variants of the method. In the forecasting approach,
the decoder iteratively generates forecasted future values xLSTMAD-F, while the
reconstruction approach reconstructs the input time series from its encoded
counterpart xLSTMAD-R. We investigate the performance of two loss functions:
Mean Squared Error (MSE), and Soft Dynamic Time Warping (SoftDTW) to consider
local reconstruction fidelity and global sequence alignment, respectively. We
evaluate our method on the comprehensive TSB-AD-M benchmark, which spans 17
real-world datasets, using state-of-the-art challenging metrics such as VUS-PR.
In our results, xLSTM showcases state-of-the-art accuracy, outperforming 23
popular anomaly detection baselines. Our paper is the first work revealing the
powerful modeling capabilities of xLSTM for anomaly detection, paving the way
for exciting new developments on this subject. Our code is available at:
https://github.com/Nyderx/xlstmad

</details>


### [76] [Quantum Neural Networks for Wind Energy Forecasting: A Comparative Study of Performance and Scalability with Classical Models](https://arxiv.org/abs/2506.22845)
*Batuhan Hangun,Oguz Altun,Onder Eyecioglu*

Main category: cs.LG

TL;DR: QNNs are explored for wind turbine power output prediction, showing competitive or slightly better performance than classical methods, with insights on dataset size and circuit complexity.


<details>
  <summary>Details</summary>
Motivation: To investigate QNNs as an alternative to classical ML for predicting wind turbine power output, given the growing role of ML in smart grids and renewable energy systems.

Method: Six QNN configurations using Z Feature Map for data encoding and varying ansatz structures were tested via cross-validation and hold-out dataset experiments.

Result: QNNs achieved competitive or marginally better predictive performance than classical benchmarks, with dataset size and circuit complexity impacting results.

Conclusion: QNNs show promise for energy applications, offering insights for integrating quantum ML in the field.

Abstract: Quantum Neural Networks (QNNs), a prominent approach in Quantum Machine
Learning (QML), are emerging as a powerful alternative to classical machine
learning methods. Recent studies have focused on the applicability of QNNs to
various tasks, such as time-series forecasting, prediction, and classification,
across a wide range of applications, including cybersecurity and medical
imaging. With the increased use of smart grids driven by the integration of
renewable energy systems, machine learning plays an important role in
predicting power demand and detecting system disturbances. This study provides
an in-depth investigation of QNNs for predicting the power output of a wind
turbine. We assess the predictive performance and simulation time of six QNN
configurations that are based on the Z Feature Map for data encoding and
varying ansatz structures. Through detailed cross-validation experiments and
tests on an unseen hold-out dataset, we experimentally demonstrate that QNNs
can achieve predictive performance that is competitive with, and in some cases
marginally better than, the benchmarked classical approaches. Our results also
reveal the effects of dataset size and circuit complexity on predictive
performance and simulation time. We believe our findings will offer valuable
insights for researchers in the energy domain who wish to incorporate quantum
machine learning into their work.

</details>


### [77] [Scalable Structure Learning of Bayesian Networks by Learning Algorithm Ensembles](https://arxiv.org/abs/2506.22848)
*Shengcai Liu,Hui Ou-yang,Zhiyuan Wang,Cheng Chen,Qijun Cai,Yew-Soon Ong,Ke Tang*

Main category: cs.LG

TL;DR: The paper introduces a structure learning ensemble (SLE) method to improve the accuracy of Bayesian network (BN) structure learning, especially for large datasets. It proposes Auto-SLE for automatic SLE design and integrates it into a divide-and-conquer approach, showing significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the instability in learning accuracy of divide-and-conquer methods for large BN structure learning.

Method: The method involves using SLE to combine multiple BN structure learning algorithms and proposing Auto-SLE for automatic SLE design, integrated into a divide-and-conquer framework.

Result: Experiments show accuracy improvements of 30% to 225% on datasets with 10,000 variables, with good generalization to larger datasets (e.g., 30,000 variables).

Conclusion: The paper concludes that SLE and Auto-SLE significantly enhance scalable BN structure learning, demonstrating their potential for large datasets.

Abstract: Learning the structure of Bayesian networks (BNs) from data is challenging,
especially for datasets involving a large number of variables. The recently
proposed divide-and-conquer (D\&D) strategies present a promising approach for
learning large BNs. However, they still face a main issue of unstable learning
accuracy across subproblems. In this work, we introduce the idea of employing
structure learning ensemble (SLE), which combines multiple BN structure
learning algorithms, to consistently achieve high learning accuracy. We further
propose an automatic approach called Auto-SLE for learning near-optimal SLEs,
addressing the challenge of manually designing high-quality SLEs. The learned
SLE is then integrated into a D\&D method. Extensive experiments firmly show
the superiority of our method over D\&D methods with single BN structure
learning algorithm in learning large BNs, achieving accuracy improvement
usually by 30\%$\sim$225\% on datasets involving 10,000 variables. Furthermore,
our method generalizes well to datasets with many more (e.g., 30000) variables
and different network characteristics than those present in the training data
for learning the SLE. These results indicate the significant potential of
employing (automatic learning of) SLEs for scalable BN structure learning.

</details>


### [78] [P$^2$U: Progressive Precision Update For Efficient Model Distribution](https://arxiv.org/abs/2506.22871)
*Homayun Afrabandpey,Hamed Rezazadegan Tavakoli*

Main category: cs.LG

TL;DR: Progressive Precision Update (P²U) reduces bandwidth usage by transmitting low-bit precision models and updates, achieving a balance between accuracy, bandwidth, and latency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of efficient model distribution in bandwidth-constrained environments like federated learning and edge computing.

Method: Transmits a low-bit precision model and an update representing the difference to the high-precision model, tested on various architectures and datasets.

Result: P²U consistently improves tradeoffs between accuracy, bandwidth, and latency, even with aggressive quantization (e.g., 4-bit).

Conclusion: P²U is a practical solution for scalable model distribution in low-resource settings, compatible with existing compression techniques.

Abstract: Efficient model distribution is becoming increasingly critical in
bandwidth-constrained environments. In this paper, we propose a simple yet
effective approach called Progressive Precision Update (P$^2$U) to address this
problem. Instead of transmitting the original high-precision model, P$^2$U
transmits a lower-bit precision model, coupled with a model update representing
the difference between the original high-precision model and the transmitted
low precision version. With extensive experiments on various model
architectures, ranging from small models ($1 - 6$ million parameters) to a
large model (more than $100$ million parameters) and using three different data
sets, e.g., chest X-Ray, PASCAL-VOC, and CIFAR-100, we demonstrate that P$^2$U
consistently achieves better tradeoff between accuracy, bandwidth usage and
latency. Moreover, we show that when bandwidth or startup time is the priority,
aggressive quantization (e.g., 4-bit) can be used without severely compromising
performance. These results establish P$^2$U as an effective and practical
solution for scalable and efficient model distribution in low-resource
settings, including federated learning, edge computing, and IoT deployments.
Given that P$^2$U complements existing compression techniques and can be
implemented alongside any compression method, e.g., sparsification,
quantization, pruning, etc., the potential for improvement is even greater.

</details>


### [79] [Interpretable Time Series Autoregression for Periodicity Quantification](https://arxiv.org/abs/2506.22895)
*Xinyu Chen,Vassilis Digalakis Jr,Lijun Ding,Dingyi Zhuang,Jinhua Zhao*

Main category: cs.LG

TL;DR: A novel sparse autoregression framework is proposed for interpretable time series analysis, using ℓ0-norm sparsity and mixed-integer optimization, with applications in human mobility and climate patterns.


<details>
  <summary>Details</summary>
Motivation: To enhance interpretability in time series autoregression models and quantify periodicity effectively, especially for time-varying and multidimensional data.

Method: Proposes a sparse autoregression framework with ℓ0-norm constraints, converts the problem into mixed-integer optimization (MIO), and introduces a subspace pursuit strategy (DVP) for acceleration. For multidimensional data, a two-stage optimization scheme is developed.

Result: The DVP strategy accelerates MIO without compromising solution quality. Applications reveal periodicities in ridesharing data and dynamic climate patterns, including El Nino.

Conclusion: The framework is scalable and interpretable, successfully uncovering temporal and spatial patterns in real-world time series data.

Abstract: Time series autoregression is a classical statistical model for capturing
auto-correlations and identifying temporal patterns such as periodicity and
seasonality. In this work, we propose a novel sparse autoregression framework
from an interpretable machine learning perspective and the model
interpretability for periodicity quantification is reinforced by $\ell_0$-norm
induced sparsity constraints. On the time-varying time series data, we
reformulate the sparse autoregression and convert the involved optimization
problem into a mixed-integer optimization (MIO). To accelerate it, we develop a
subspace pursuit based decision variable pruning (DVP) strategy to reduce the
search space. On the multidimensional time series that involves complicated
spatial and temporal dimensions, we propose a spatially- and time-varying
sparse autoregression model and resolve the corresponding MIO problem by
developing a two-stage optimization scheme. In particular, the proposed scheme
makes the model scalable to large problems even with millions of decision
variables. Empirically, we conduct extensive experiments to evaluate the
proposed models on real-world time series data. First, we demonstrate that the
MIO solver can be drastically accelerated through the DVP strategy, while
maintaining the same solution quality as a full MIO solver. Applying the
time-varying sparse autoregression model to ridesharing trip data, we uncover
both daily and weekly periodicities and reveal long-term changes in regularity
of human mobility. Second, we demonstrate the spatial patterns of yearly
seasonality in climate variable time series such as temperature and
precipitation across the past four decades, and our model allows to discover
dynamic climate patterns and identify climate phenomena such as El Nino in sea
surface temperature.

</details>


### [80] [Missing-Modality-Aware Graph Neural Network for Cancer Classification](https://arxiv.org/abs/2506.22901)
*Sina Tabakhi,Haiping Lu*

Main category: cs.LG

TL;DR: MAGNET is a graph neural network designed for handling missing modalities in multimodal biological data, outperforming existing methods by using a patient-modality attention mechanism and a patient graph structure.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of missing modalities in multimodal biological data, which current methods struggle with due to diverse missing patterns and scalability issues.

Method: Proposes MAGNET, which uses a patient-modality multi-head attention mechanism to fuse embeddings and constructs a patient graph for predictions, adapting to missing patterns with linear complexity.

Result: MAGNET outperforms state-of-the-art fusion methods on three public multiomics datasets for cancer classification, even with real-world missingness.

Conclusion: MAGNET effectively handles missing modalities in multimodal data, offering scalable and adaptable solutions for real-world applications.

Abstract: A key challenge in learning from multimodal biological data is missing
modalities, where all data from some modalities are missing for some patients.
Current fusion methods address this by excluding patients with missing
modalities, imputing missing modalities, or making predictions directly with
partial modalities. However, they often struggle with diverse missing-modality
patterns and the exponential growth of the number of such patterns as the
number of modalities increases. To address these limitations, we propose MAGNET
(Missing-modality-Aware Graph neural NETwork) for direct prediction with
partial modalities, which introduces a patient-modality multi-head attention
mechanism to fuse lower-dimensional modality embeddings based on their
importance and missingness. MAGNET's complexity increases linearly with the
number of modalities while adapting to missing-pattern variability. To generate
predictions, MAGNET further constructs a patient graph with fused multimodal
embeddings as node features and the connectivity determined by the modality
missingness, followed by a conventional graph neural network. Experiments on
three public multiomics datasets for cancer classification, with real-world
instead of artificial missingness, show that MAGNET outperforms the
state-of-the-art fusion methods. The data and code are available at
https://github.com/SinaTabakhi/MAGNET.

</details>


### [81] [Towards Time Series Generation Conditioned on Unstructured Natural Language](https://arxiv.org/abs/2506.22927)
*Jaeyun Woo,Jiseok Lee,Brian Kenji Iwana*

Main category: cs.LG

TL;DR: A novel method using diffusion and language models enables time series generation from natural language descriptions, with applications in forecasting, data augmentation, and more. A new dataset of 63,010 time series-description pairs is introduced.


<details>
  <summary>Details</summary>
Motivation: Time series generative AI is underdeveloped despite its importance in fields like finance and climate. The research aims to bridge this gap by leveraging natural language descriptions.

Method: Combines a diffusion model with a language model to generate time series from text.

Result: Demonstrates feasibility of time series generation from natural language, with potential applications in custom forecasting and data augmentation.

Conclusion: The proposed method advances time series generative AI and introduces a valuable public dataset for future research.

Abstract: Generative Artificial Intelligence (AI) has rapidly become a powerful tool,
capable of generating various types of data, such as images and text. However,
despite the significant advancement of generative AI, time series generative AI
remains underdeveloped, even though the application of time series is essential
in finance, climate, and numerous fields. In this research, we propose a novel
method of generating time series conditioned on unstructured natural language
descriptions. We use a diffusion model combined with a language model to
generate time series from the text. Through the proposed method, we demonstrate
that time series generation based on natural language is possible. The proposed
method can provide various applications such as custom forecasting, time series
manipulation, data augmentation, and transfer learning. Furthermore, we
construct and propose a new public dataset for time series generation,
consisting of 63,010 time series-description pairs.

</details>


### [82] [Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration](https://arxiv.org/abs/2506.22929)
*Chen Zhang*

Main category: cs.LG

TL;DR: A parallel computation architecture is proposed to handle high-dimensional data by decomposing it into dimension-independent structures, enabling efficient distributed processing and integration of advanced analysis methods.


<details>
  <summary>Details</summary>
Motivation: Deep learning struggles with high-dimensional data due to computational challenges, and current tools lack support for advanced mathematical statistics.

Method: A parallel computation framework based on space completeness decomposes high-dimensional data into dimension-independent structures for distributed processing.

Result: The framework supports seamless integration of data mining and parallel-optimized machine learning, applicable to diverse data types like medical and natural images.

Conclusion: The proposed architecture addresses the dimensionality curse and enhances computational efficiency for high-dimensional data analysis.

Abstract: While deep learning excels in natural image and language processing, its
application to high-dimensional data faces computational challenges due to the
dimensionality curse. Current large-scale data tools focus on business-oriented
descriptive statistics, lacking mathematical statistics support for advanced
analysis. We propose a parallel computation architecture based on space
completeness, decomposing high-dimensional data into dimension-independent
structures for distributed processing. This framework enables seamless
integration of data mining and parallel-optimized machine learning methods,
supporting scientific computations across diverse data types like medical and
natural images within a unified system.

</details>


### [83] [Infinite Sampling: Efficient and Stable Grouped RL Training for Large Language Models](https://arxiv.org/abs/2506.22950)
*Liangyu Wang,Huanyi Xie,Xinhai Wang,Tianjin Huang,Mengdi Li,Di Wang*

Main category: cs.LG

TL;DR: Infinite Sampling reduces memory overhead in GRPO for LLMs by decoupling group size from GPU memory, using micro sampling groups, continuous sampling, and a length-aware scheduler.


<details>
  <summary>Details</summary>
Motivation: Addressing the high memory overhead and scalability issues in GRPO due to large sample group sizes.

Method: Proposes micro sampling groups, continuous sampling, and a length-aware scheduler to optimize memory and throughput.

Result: Reduces peak memory by 50% and improves throughput by 25% while maintaining performance.

Conclusion: Infinite Sampling enables efficient and stable GRPO training under GPU memory constraints.

Abstract: Group-based reinforcement learning algorithms such as Group Reward Policy
Optimization (GRPO) have proven effective for fine-tuning large language models
(LLMs) with human feedback. However, generating and storing multiple responses
per prompt incurs substantial memory overhead, especially as the sample group
size increases, limiting scalability under constrained hardware.
  We propose Infinite Sampling, a framework that enables efficient and stable
GRPO training by decoupling group size from GPU memory usage. It consists of:
(1) micro sampling groups that decompose large groups into memory-feasible
rounds; (2) continuous sampling that interleaves generation across groups to
improve utilization; and (3) a length-aware scheduler combining
token-conditioned sequence length prediction with a two-stage plan: global
grouping via FPTAS and runtime refill via SJF.
  Experiments show that our Micro Sampling Groups reduce peak memory usage by
over 50% compared to full-group decoding (e.g., from 21.55 GB to 10.64 GB on
Qwen3-1.7B). Building on this, Infinite Sampling improves throughput by over
25% compared to the naive micro sampling group method, reducing decoding steps
while maintaining full-length completions and memory usage. Our hybrid
scheduling ensures efficient and stable GRPO training with larger groups under
realistic GPU memory constraints.

</details>


### [84] [Cybersecurity-Focused Anomaly Detection in Connected Autonomous Vehicles Using Machine Learning](https://arxiv.org/abs/2506.22984)
*Prathyush Kumar Reddy Lebaku,Lu Gao,Yunpeng Zhang,Zhixia Li,Yongxin Liu,Tanvir Arafin*

Main category: cs.LG

TL;DR: The paper presents a machine learning approach for anomaly detection in connected autonomous vehicles (CAVs) using stacked LSTM and Random Forest models, achieving high accuracy in identifying abnormal driving patterns.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection is critical for CAVs due to risks like sensor malfunctions, cyber-attacks, and environmental disruptions, ensuring safe transportation networks.

Method: Simulated vehicle behavior to create a dataset of typical and atypical interactions, then applied stacked LSTM for temporal dependencies and Random Forest for ensemble-based anomaly detection.

Result: Random Forest achieved R2 of 0.9830 and MAE of 5.746, while stacked LSTM achieved R2 of 0.9998 and MAE of 82.425, demonstrating high accuracy in anomaly detection.

Conclusion: The models effectively predict trajectories and detect anomalies, proving their utility for enhancing CAV safety and reliability.

Abstract: Anomaly detection in connected autonomous vehicles (CAVs) is crucial for
maintaining safe and reliable transportation networks, as CAVs can be
susceptible to sensor malfunctions, cyber-attacks, and unexpected environmental
disruptions. This study explores an anomaly detection approach by simulating
vehicle behavior, generating a dataset that represents typical and atypical
vehicular interactions. The dataset includes time-series data of position,
speed, and acceleration for multiple connected autonomous vehicles. We utilized
machine learning models to effectively identify abnormal driving patterns.
First, we applied a stacked Long Short-Term Memory (LSTM) model to capture
temporal dependencies and sequence-based anomalies. The stacked LSTM model
processed the sequential data to learn standard driving behaviors.
Additionally, we deployed a Random Forest model to support anomaly detection by
offering ensemble-based predictions, which enhanced model interpretability and
performance. The Random Forest model achieved an R2 of 0.9830, MAE of 5.746,
and a 95th percentile anomaly threshold of 14.18, while the stacked LSTM model
attained an R2 of 0.9998, MAE of 82.425, and a 95th percentile anomaly
threshold of 265.63. These results demonstrate the models' effectiveness in
accurately predicting vehicle trajectories and detecting anomalies in
autonomous driving scenarios.

</details>


### [85] [Kernel Outlier Detection](https://arxiv.org/abs/2506.22994)
*Can Hakan Dağıdır,Mia Hubert,Peter J. Rousseeuw*

Main category: cs.LG

TL;DR: Kernel Outlier Detection (KOD) is a new method for outlier detection in high-dimensional data, addressing limitations of existing methods with a kernel transformation and projection pursuit approach.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in high-dimensional outlier detection, such as dependence on distributional assumptions or difficult-to-tune hyperparameters.

Method: Uses a kernel transformation followed by projection pursuit, with a novel ensemble of search directions and combination of results.

Result: Effective performance demonstrated on small datasets with challenging structures and large benchmark datasets.

Conclusion: KOD provides a flexible and lightweight solution for outlier detection in high-dimensional settings.

Abstract: A new anomaly detection method called kernel outlier detection (KOD) is
proposed. It is designed to address challenges of outlier detection in
high-dimensional settings. The aim is to overcome limitations of existing
methods, such as dependence on distributional assumptions or on hyperparameters
that are hard to tune. KOD starts with a kernel transformation, followed by a
projection pursuit approach. Its novelties include a new ensemble of directions
to search over, and a new way to combine results of different direction types.
This provides a flexible and lightweight approach for outlier detection. Our
empirical evaluations illustrate the effectiveness of KOD on three small
datasets with challenging structures, and on four large benchmark datasets.

</details>


### [86] [A Reinforcement Learning Approach for Optimal Control in Microgrids](https://arxiv.org/abs/2506.22995)
*Davide Salaorni,Federico Bianchi,Francesco Trovò,Marcello Restelli*

Main category: cs.LG

TL;DR: A reinforcement learning (RL)-based method optimizes microgrid energy management, outperforming rule-based and existing RL methods, validated with real-world data.


<details>
  <summary>Details</summary>
Motivation: The integration of renewable energy sources requires decentralized energy management solutions, with microgrids offering localized control.

Method: Proposes an RL agent for energy trading and storage, using a digital twin to simulate storage dynamics and degradation.

Result: The RL-based strategy outperforms rule-based and existing RL benchmarks in real-world tests.

Conclusion: The approach provides a robust solution for intelligent microgrid management.

Abstract: The increasing integration of renewable energy sources (RESs) is transforming
traditional power grid networks, which require new approaches for managing
decentralized energy production and consumption. Microgrids (MGs) provide a
promising solution by enabling localized control over energy generation,
storage, and distribution. This paper presents a novel reinforcement learning
(RL)-based methodology for optimizing microgrid energy management.
Specifically, we propose an RL agent that learns optimal energy trading and
storage policies by leveraging historical data on energy production,
consumption, and market prices. A digital twin (DT) is used to simulate the
energy storage system dynamics, incorporating degradation factors to ensure a
realistic emulation of the analysed setting. Our approach is validated through
an experimental campaign using real-world data from a power grid located in the
Italian territory. The results indicate that the proposed RL-based strategy
outperforms rule-based methods and existing RL benchmarks, offering a robust
solution for intelligent microgrid management.

</details>


### [87] [BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs](https://arxiv.org/abs/2506.23024)
*Jerry Liu,Yasa Baig,Denise Hui Jean Lee,Rajat Vadiraj Dwaraknath,Atri Rudra,Chris Ré*

Main category: cs.LG

TL;DR: PINNs struggle with precision; BWLer improves accuracy by addressing MLP limitations, achieving near-machine-precision for some PDEs.


<details>
  <summary>Details</summary>
Motivation: Overcome the precision limitations of PINNs in solving PDEs by investigating the root cause (MLP architecture or PDE ill-conditioning).

Method: Introduce Barycentric Weight Layer (BWLer) for polynomial interpolation, used atop or replacing MLPs, with spectral derivatives and preconditioning.

Result: BWLer improves RMSE significantly (up to 1800x) and achieves near-machine-precision for certain PDEs, outperforming standard PINNs.

Conclusion: BWLer bridges the gap between PINNs' flexibility and classical solvers' precision, offering a practical solution for high-accuracy PDE solving.

Abstract: Physics-informed neural networks (PINNs) offer a flexible way to solve
partial differential equations (PDEs) with machine learning, yet they still
fall well short of the machine-precision accuracy many scientific tasks demand.
In this work, we investigate whether the precision ceiling comes from the
ill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP)
architecture. We introduce the Barycentric Weight Layer (BWLer), which models
the PDE solution through barycentric polynomial interpolation. A BWLer can be
added on top of an existing MLP (a BWLer-hat) or replace it completely
(explicit BWLer), cleanly separating how we represent the solution from how we
take derivatives for the PDE loss. Using BWLer, we identify fundamental
precision limitations within the MLP: on a simple 1-D interpolation task, even
MLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above
float64 machine precision -- before any PDE terms are added. In PDE learning,
adding a BWLer lifts this ceiling and exposes a tradeoff between achievable
accuracy and the conditioning of the PDE loss. For linear PDEs we fully
characterize this tradeoff with an explicit error decomposition and navigate it
during training with spectral derivatives and preconditioning. Across five
benchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for
convection, 10x for reaction, and 1800x for wave equations while remaining
compatible with first-order optimizers. Replacing the MLP entirely lets an
explicit BWLer reach near-machine-precision on convection, reaction, and wave
problems (up to 10 billion times better than prior results) and match the
performance of standard PINNs on stiff Burgers' and irregular-geometry Poisson
problems. Together, these findings point to a practical path for combining the
flexibility of PINNs with the precision of classical spectral solvers.

</details>


### [88] [Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models](https://arxiv.org/abs/2506.23025)
*Tejas Vaidhya,Ayush Kaushal,Vineet Jain,Francis Couture Harpin,Prashant Shishodia,Majid Behbahani,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.LG

TL;DR: The paper introduces ternary language models (TriLMs) to address the memory bottleneck in LLM inference, proposing quantization-aware training, 2-bit/1.6-bit packing schemes, and a GPU kernel (TriRun) for faster inference.


<details>
  <summary>Details</summary>
Motivation: The memory bandwidth and capacity of GPUs lag behind computational power, creating inefficiencies in LLM inference. TriLMs aim to reduce memory requirements and improve inference speed.

Method: The study uses quantization-aware training for TriLMs, analyzes their scalability, and introduces Spectra-1.1 (a suite of TriLMs). It also proposes novel packing schemes and the TriRun GPU kernel.

Result: TriLMs show better performance with increased training data rather than model size. Spectra-1.1 and TriRun achieve up to 5x faster inference compared to floating-point baselines.

Conclusion: The work provides efficient LLM solutions, releasing Spectra-1.1 and TriRun to support further research in ternary language models.

Abstract: Large language models (LLMs) are increasingly used across research and
industry applications, yet their inference efficiency remains a significant
challenge. As the computational power of modern GPU architectures continuously
improves, their memory bandwidth and capacity have not scaled proportionally,
creating a critical bottleneck during inference. To address this, we
investigate ternary language models (TriLMs) that employ quantization-aware
training to significantly reduce memory requirements. We first analyze the
scalability of TriLMs by conducting a scaling law analysis, revealing that
TriLMs benefit more from increasing training data than from scaling model
parameters. Based on this observation, we introduce Spectra-1.1, an open suite
of TriLMs trained on up to 1.2 trillion tokens, demonstrating sustained
performance gains at scale. Furthermore, to improve inference efficiency, we
propose novel 2-bit and 1.6-bit packing schemes for ternary weights, which
demonstrate accelerated inference across various CPU architectures. Also,
building on the 2-bit packing, we develop a GPU kernel called TriRun that
accelerates end-to-end model inference by up to 5 times compared to
floating-point baselines. To encourage further exploration and development of
TriLMs, we will release the Spectra-1.1 suite and TriRun inference kernels.
Overall, our work lays the foundation for building and deploying efficient
LLMs, providing a valuable resource for the research community.

</details>


### [89] [Feature-Wise Mixing for Mitigating Contextual Bias in Predictive Supervised Learning](https://arxiv.org/abs/2506.23033)
*Yash Vardhan Tomar*

Main category: cs.LG

TL;DR: The paper introduces a feature-wise mixing framework to mitigate bias in ML models, achieving significant bias reduction and improved predictive performance without explicit bias attribute identification.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing bias mitigation strategies, which are either post-hoc or rigid, and aiming for scalable and generalizable solutions.

Method: Proposes feature-wise mixing by redistributing feature representations across contextual datasets, evaluated using bias-sensitive loss functions and cross-validation.

Result: Achieved 43.35% average bias reduction and statistically significant MSE decrease, outperforming SMOTE oversampling.

Conclusion: Feature-wise mixing is effective, computationally efficient, and scalable, with potential for real-world applications requiring accurate predictions.

Abstract: Bias in predictive machine learning (ML) models is a fundamental challenge
due to the skewed or unfair outcomes produced by biased models. Existing
mitigation strategies rely on either post-hoc corrections or rigid constraints.
However, emerging research claims that these techniques can limit scalability
and reduce generalizability. To address this, this paper introduces a
feature-wise mixing framework to mitigate contextual bias. This was done by
redistributing feature representations across multiple contextual datasets. To
assess feature-wise mixing's effectiveness, four ML classifiers were trained
using cross-validation and evaluated with bias-sensitive loss functions,
including disparity metrics and mean squared error (MSE), which served as a
standard measure of predictive performance. The proposed method achieved an
average bias reduction of 43.35% and a statistically significant decrease in
MSE across all classifiers trained on mixed datasets. Additionally,
benchmarking against established bias mitigation techniques found that
feature-wise mixing consistently outperformed SMOTE oversampling and
demonstrated competitive effectiveness without requiring explicit bias
attribute identification. Feature-wise mixing efficiently avoids the
computational overhead typically associated with fairness-aware learning
algorithms. Future work could explore applying feature-wise mixing for
real-world fields where accurate predictions are necessary.

</details>


### [90] [Fragile, Robust, and Antifragile: A Perspective from Parameter Responses in Reinforcement Learning Under Stress](https://arxiv.org/abs/2506.23036)
*Zain ul Abdeen,Ming Jin*

Main category: cs.LG

TL;DR: The paper analyzes RL policy robustness by testing parameters under internal (synaptic filtering) and external (adversarial attacks) stresses, classifying them as fragile, robust, or antifragile. It validates the framework on PPO-trained agents, finding antifragile parameters that improve performance under stress.


<details>
  <summary>Details</summary>
Motivation: To understand and enhance RL policy robustness by examining how parameters react to internal and external stresses, inspired by synaptic plasticity.

Method: Uses synaptic filtering for internal stress and adversarial attacks for external stress to classify parameters. Validates on PPO-trained agents in Mujoco environments.

Result: Identifies antifragile parameters that boost policy performance under stress, showing potential for targeted filtering to improve adaptability.

Conclusion: The framework offers insights for designing robust and antifragile RL systems, paving the way for future advancements.

Abstract: This paper explores Reinforcement learning (RL) policy robustness by
systematically analyzing network parameters under internal and external
stresses. Inspired by synaptic plasticity in neuroscience, synaptic filtering
introduces internal stress by selectively perturbing parameters, while
adversarial attacks apply external stress through modified agent observations.
This dual approach enables the classification of parameters as fragile, robust,
or antifragile, based on their influence on policy performance in clean and
adversarial settings. Parameter scores are defined to quantify these
characteristics, and the framework is validated on PPO-trained agents in Mujoco
continuous control environments. The results highlight the presence of
antifragile parameters that enhance policy performance under stress,
demonstrating the potential of targeted filtering techniques to improve RL
policy adaptability. These insights provide a foundation for future
advancements in the design of robust and antifragile RL systems.

</details>


### [91] [ReMem: Mutual Information-Aware Fine-tuning of Pretrained Vision Transformers for Effective Knowledge Distillation](https://arxiv.org/abs/2506.23041)
*Chengyu Dong,Huan Gui,Noveen Sachdeva,Long Jin,Ke Yin,Jingbo Shang,Lichan Hong,Ed H. Chi,Zhe Zhao*

Main category: cs.LG

TL;DR: The paper addresses the challenge of effective knowledge distillation from large-scale pretrained Vision Transformers (ViTs) to smaller models by proposing mutual information-aware optimization and MLP block reweighting.


<details>
  <summary>Details</summary>
Motivation: The effectiveness of knowledge transfer drops when distilling from large-scale pretrained models, especially for small or imbalanced datasets. The paper aims to improve this by leveraging mutual information insights.

Method: The authors propose mutual information-aware optimization during finetuning and introduce a heuristic to reweight MLP blocks, which are key to mutual information loss.

Result: The method enhances knowledge transfer, allowing small student models to benefit from strong pretrained ViTs.

Conclusion: The proposed approach effectively improves distillation from large-scale pretrained models, particularly for challenging datasets.

Abstract: Knowledge distillation from pretrained visual representation models offers an
effective approach to improve small, task-specific production models. However,
the effectiveness of such knowledge transfer drops significantly when
distilling from strong models that are pretrained in a large scale. In this
paper, we address this challenge for pretrained Vision Transformers (ViTs) by
exploring methods to fine-tune them for more effective knowledge transfer.
Motivated by the connection between mutual information and distillation
effectiveness, we propose to employ mutual information-aware optimization
during finetuning. For small or highly-imbalanced downstream datasets where
such optimization becomes less effective, we introduce a simple yet effective
heuristic of reweighting MLP blocks. This approach is inspired by our
observation that top MLP blocks are primarily responsible for mutual
information loss. Our method enables small student models to benefit from those
pretrained models among the strongest.

</details>


### [92] [Double-Diffusion: Diffusion Conditioned Diffusion Probabilistic Model For Air Quality Prediction](https://arxiv.org/abs/2506.23053)
*Hanlin Dong,Arian Prabowo,Hao Xue,Flora D. Salim*

Main category: cs.LG

TL;DR: Double-Diffusion is a novel physics-guided diffusion model for air quality prediction, outperforming other probabilistic models in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of balancing certainty and uncertainty in air quality prediction by integrating physics principles with stochasticity.

Method: Proposes Double-Diffusion, a diffusion probabilistic model using physics as a conditional generative approach, with a new denoiser architecture and sampling strategy.

Result: Ranks first in evaluations, reduces inference time by 30-50%, and improves CRPS by 3-12%.

Conclusion: Double-Diffusion effectively combines physics and stochasticity for superior air quality forecasting.

Abstract: Air quality prediction is a challenging forecasting task due to its
spatio-temporal complexity and the inherent dynamics as well as uncertainty.
Most of the current models handle these two challenges by applying Graph Neural
Networks or known physics principles, and quantifying stochasticity through
probabilistic networks like Diffusion models. Nevertheless, finding the right
balancing point between the certainties and uncertainties remains an open
question. Therefore, we propose Double-Diffusion, a novel diffusion
probabilistic model that harnesses the power of known physics to guide air
quality forecasting with stochasticity. To the best of our knowledge, while
precedents have been made of using conditional diffusion models to predict air
pollution, this is the first attempt to use physics as a conditional generative
approach for air quality prediction. Along with a sampling strategy adopted
from image restoration and a new denoiser architecture, Double-Diffusion ranks
first in most evaluation scenarios across two real-life datasets compared with
other probabilistic models, it also cuts inference time by 50% to 30% while
enjoying an increase between 3-12% in Continuous Ranked Probabilistic Score
(CRPS).

</details>


### [93] [Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis](https://arxiv.org/abs/2506.23055)
*Hiro Taiyo Hamada,Ippei Fujisawa,Genji Kawakita,Yuki Yamada*

Main category: cs.LG

TL;DR: The paper evaluates how well LLMs like GPT-4 align with human psychological concepts using standardized questionnaires, showing GPT-4 outperforms others in classification accuracy.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs internalize human psychological concepts accurately, given their human-like text generation capabilities.

Method: A quantitative framework using 43 psychological questionnaires, analyzing pairwise similarity and hierarchical clustering to compare LLM outputs with human constructs.

Result: GPT-4 achieved 66.2% classification accuracy, outperforming GPT-3.5 (55.9%) and BERT (48.1%), and showed correlation with human responses.

Conclusion: Modern LLMs can approximate human psychological constructs, offering insights for more interpretable AI systems.

Abstract: Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities
in producing human-like text. However, it is unclear how accurately these
models internalize concepts that shape human thought and behavior. Here, we
developed a quantitative framework to assess concept alignment between LLMs and
human psychological dimensions using 43 standardized psychological
questionnaires, selected for their established validity in measuring distinct
psychological constructs. Our method evaluates how accurately language models
reconstruct and classify questionnaire items through pairwise similarity
analysis. We compared resulting cluster structures with the original
categorical labels using hierarchical clustering. A GPT-4 model achieved
superior classification accuracy (66.2\%), significantly outperforming GPT-3.5
(55.9\%) and BERT (48.1\%), all exceeding random baseline performance (31.9\%).
We also demonstrated that the estimated semantic similarity from GPT-4 is
associated with Pearson's correlation coefficients of human responses in
multiple psychological questionnaires. This framework provides a novel approach
to evaluate the alignment of the human-LLM concept and identify potential
representational biases. Our findings demonstrate that modern LLMs can
approximate human psychological constructs with measurable accuracy, offering
insights for developing more interpretable AI systems.

</details>


### [94] [Curious Causality-Seeking Agents Learn Meta Causal World](https://arxiv.org/abs/2506.23068)
*Zhiyu Zhao,Haoxuan Li,Haifeng Zhang,Jun Wang,Francesco Faccio,Jürgen Schmidhuber,Mengyue Yang*

Main category: cs.LG

TL;DR: The paper introduces a Meta-Causal Graph for world modeling, addressing shifts in observed causal mechanisms due to policy or environment changes. A Causality-Seeking Agent uses curiosity-driven exploration to refine this graph.


<details>
  <summary>Details</summary>
Motivation: Real-world environments often exhibit shifting causal mechanisms, challenging traditional world models that assume fixed rules. This work aims to model and adapt to these shifts.

Method: Proposes a Meta-Causal Graph with causal subgraphs triggered by latent meta states. A Causality-Seeking Agent identifies meta states, discovers causal relationships, and refines the graph through curiosity-driven interventions.

Result: Experiments show the method effectively captures causal dynamics shifts and generalizes to unseen contexts in synthetic and robot arm tasks.

Conclusion: The Meta-Causal Graph and Causality-Seeking Agent provide a robust framework for adaptive world modeling in dynamic environments.

Abstract: When building a world model, a common assumption is that the environment has
a single, unchanging underlying causal rule, like applying Newton's laws to
every situation. In reality, what appears as a drifting causal mechanism is
often the manifestation of a fixed underlying mechanism seen through a narrow
observational window. This brings about a problem that, when building a world
model, even subtle shifts in policy or environment states can alter the very
observed causal mechanisms. In this work, we introduce the \textbf{Meta-Causal
Graph} as world models, a minimal unified representation that efficiently
encodes the transformation rules governing how causal structures shift across
different latent world states. A single Meta-Causal Graph is composed of
multiple causal subgraphs, each triggered by meta state, which is in the latent
state space. Building on this representation, we introduce a
\textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta
states that trigger each subgraph, (2) discover the corresponding causal
relationships by agent curiosity-driven intervention policy, and (3)
iteratively refine the Meta-Causal Graph through ongoing curiosity-driven
exploration and agent experiences. Experiments on both synthetic tasks and a
challenging robot arm manipulation task demonstrate that our method robustly
captures shifts in causal dynamics and generalizes effectively to previously
unseen contexts.

</details>


### [95] [Forget-MI: Machine Unlearning for Forgetting Multimodal Information in Healthcare Settings](https://arxiv.org/abs/2506.23145)
*Shahad Hardan,Darya Taratynova,Abdelmajid Essofi,Karthik Nandakumar,Mohammad Yaqub*

Main category: cs.LG

TL;DR: Forget-MI is a novel machine unlearning method for multimodal medical data, improving privacy by removing sensitive data while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: Privacy in AI, especially healthcare, is critical, but existing methods struggle to remove data from multimodal models.

Method: Forget-MI uses loss functions and perturbation techniques to unlearn unimodal and joint representations of forgotten data.

Result: Reduces MIA by 0.202, decreases AUC/F1 on forget set by 0.221/0.305, and matches test set performance of retrained models.

Conclusion: Forget-MI effectively unlearns sensitive data while preserving model utility, outperforming existing approaches.

Abstract: Privacy preservation in AI is crucial, especially in healthcare, where models
rely on sensitive patient data. In the emerging field of machine unlearning,
existing methodologies struggle to remove patient data from trained multimodal
architectures, which are widely used in healthcare. We propose Forget-MI, a
novel machine unlearning method for multimodal medical data, by establishing
loss functions and perturbation techniques. Our approach unlearns unimodal and
joint representations of the data requested to be forgotten while preserving
knowledge from the remaining data and maintaining comparable performance to the
original model. We evaluate our results using performance on the forget
dataset, performance on the test dataset, and Membership Inference Attack
(MIA), which measures the attacker's ability to distinguish the forget dataset
from the training dataset. Our model outperforms the existing approaches that
aim to reduce MIA and the performance on the forget dataset while keeping an
equivalent performance on the test set. Specifically, our approach reduces MIA
by 0.202 and decreases AUC and F1 scores on the forget set by 0.221 and 0.305,
respectively. Additionally, our performance on the test set matches that of the
retrained model, while allowing forgetting. Code is available at
https://github.com/BioMedIA-MBZUAI/Forget-MI.git

</details>


### [96] [maneuverRecognition -- A Python package for Timeseries Classification in the domain of Vehicle Telematics](https://arxiv.org/abs/2506.23147)
*Jonathan Schuster,Fabian Transchel*

Main category: cs.LG

TL;DR: The paper introduces the maneuverRecognition package for automating driving maneuver recognition in vehicle telematics, addressing data preprocessing, modeling, and evaluation challenges.


<details>
  <summary>Details</summary>
Motivation: To enhance road safety, reduce accidents and costs, and support eco-friendly driving by improving maneuver recognition in vehicle telematics.

Method: Developed a Python package (maneuverRecognition) for preprocessing, modeling, and evaluating driving data, including an LSTM-based network structure.

Result: Demonstrated the package's effectiveness using real driving data from smartphone sensors of three individuals.

Conclusion: The maneuverRecognition package provides a practical solution for maneuver recognition, facilitating efficient data handling and model development.

Abstract: In the domain of vehicle telematics the automated recognition of driving
maneuvers is used to classify and evaluate driving behaviour. This not only
serves as a component to enhance the personalization of insurance policies, but
also to increase road safety, reduce accidents and the associated costs as well
as to reduce fuel consumption and support environmentally friendly driving. In
this context maneuver recognition technically requires a continuous application
of time series classification which poses special challenges to the transfer,
preprocessing and storage of telematic sensor data, the training of predictive
models, and the prediction itself. Although much research has been done in the
field of gathering relevant data or regarding the methods to build predictive
models for the task of maneuver recognition, there is a practical need for
python packages and functions that allow to quickly transform data into the
required structure as well as to build and evaluate such models. The
maneuverRecognition package was therefore developed to provide the necessary
functions for preprocessing, modelling and evaluation and also includes a ready
to use LSTM based network structure that can be modified. The implementation of
the package is demonstrated using real driving data of three different persons
recorded via smartphone sensors.

</details>


### [97] [Mirror Descent Policy Optimisation for Robust Constrained Markov Decision Processes](https://arxiv.org/abs/2506.23165)
*David Bossens,Atsushi Nitanda*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Safety is an essential requirement for reinforcement learning systems. The
newly emerging framework of robust constrained Markov decision processes allows
learning policies that satisfy long-term constraints while providing guarantees
under epistemic uncertainty. This paper presents mirror descent policy
optimisation for robust constrained Markov decision processes (RCMDPs), making
use of policy gradient techniques to optimise both the policy (as a maximiser)
and the transition kernel (as an adversarial minimiser) on the Lagrangian
representing a constrained MDP. In the oracle-based RCMDP setting, we obtain an
$\mathcal{O}\left(\frac{1}{T}\right)$ convergence rate for the squared distance
as a Bregman divergence, and an $\mathcal{O}\left(e^{-T}\right)$ convergence
rate for entropy-regularised objectives. In the sample-based RCMDP setting, we
obtain an $\tilde{\mathcal{O}}\left(\frac{1}{T^{1/3}}\right)$ convergence rate.
Experiments confirm the benefits of mirror descent policy optimisation in
constrained and unconstrained optimisation, and significant improvements are
observed in robustness tests when compared to baseline policy optimisation
algorithms.

</details>


### [98] [Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data](https://arxiv.org/abs/2506.23174)
*Chen Gong,Bo Liang,Wei Gao,Chenren Xu*

Main category: cs.LG

TL;DR: The paper proposes SynCheck, a quality-guided scheme to improve synthetic data utilization in wireless sensing by addressing affinity and diversity limitations.


<details>
  <summary>Details</summary>
Motivation: Current synthetic data in wireless sensing lacks predictable quality, leading to performance degradation. The paper aims to quantify and improve synthetic data quality.

Method: Introduces metrics for affinity and diversity, and SynCheck, a scheme to refine synthetic data quality during training.

Result: SynCheck outperforms traditional methods, achieving a 4.3% performance improvement even when others degrade by 13.4%.

Conclusion: SynCheck effectively mitigates synthetic data quality issues, enhancing wireless sensing task performance.

Abstract: Generative models have gained significant attention for their ability to
produce realistic synthetic data that supplements the quantity of real-world
datasets. While recent studies show performance improvements in wireless
sensing tasks by incorporating all synthetic data into training sets, the
quality of synthetic data remains unpredictable and the resulting performance
gains are not guaranteed. To address this gap, we propose tractable and
generalizable metrics to quantify quality attributes of synthetic data -
affinity and diversity. Our assessment reveals prevalent affinity limitation in
current wireless synthetic data, leading to mislabeled data and degraded task
performance. We attribute the quality limitation to generative models' lack of
awareness of untrained conditions and domain-specific processing. To mitigate
these issues, we introduce SynCheck, a quality-guided synthetic data
utilization scheme that refines synthetic data quality during task model
training. Our evaluation demonstrates that SynCheck consistently outperforms
quality-oblivious utilization of synthetic data, and achieves 4.3% performance
improvement even when the previous utilization degrades performance by 13.4%.

</details>


### [99] [Attribution assignment for deep-generative sequence models enables interpretability analysis using positive-only data](https://arxiv.org/abs/2506.23182)
*Robert Frank,Michael Widrich,Rahmad Akbar,Günter Klambauer,Geir Kjetil Sandve,Philippe A. Robert,Victor Greiff*

Main category: cs.LG

TL;DR: GAMA is a new attribution method for generative models like LSTMs, enabling interpretable biological insights without requiring negative training data.


<details>
  <summary>Details</summary>
Motivation: Generative models lack interpretability, hindering biological insights. Negative data is often scarce or unreliable in biological settings.

Method: Developed GAMA, an attribution method based on Integrated Gradients, tested on synthetic and experimental antibody-antigen binding data.

Result: GAMA successfully recovers biologically relevant features and validates generative sequence designs without negative data.

Conclusion: GAMA enhances interpretability and utility of generative models in therapeutic design.

Abstract: Generative machine learning models offer a powerful framework for therapeutic
design by efficiently exploring large spaces of biological sequences enriched
for desirable properties. Unlike supervised learning methods, which require
both positive and negative labeled data, generative models such as LSTMs can be
trained solely on positively labeled sequences, for example, high-affinity
antibodies. This is particularly advantageous in biological settings where
negative data are scarce, unreliable, or biologically ill-defined. However, the
lack of attribution methods for generative models has hindered the ability to
extract interpretable biological insights from such models. To address this
gap, we developed Generative Attribution Metric Analysis (GAMA), an attribution
method for autoregressive generative models based on Integrated Gradients. We
assessed GAMA using synthetic datasets with known ground truths to characterize
its statistical behavior and validate its ability to recover biologically
relevant features. We further demonstrated the utility of GAMA by applying it
to experimental antibody-antigen binding data. GAMA enables model
interpretability and the validation of generative sequence design strategies
without the need for negative training data.

</details>


### [100] [Efficient Algorithms for Learning and Compressing Monophonic Halfspaces in Graphs](https://arxiv.org/abs/2506.23186)
*Marco Bressan,Victor Chepoi,Emmanuel Esposito,Maximilian Thiessen*

Main category: cs.LG

TL;DR: The paper studies monophonic halfspaces in graphs, introduces a decomposition theorem, and provides efficient learning algorithms, contrasting with NP-hard geodesic halfspaces.


<details>
  <summary>Details</summary>
Motivation: To explore graph-based convexity and halfspaces, addressing open questions in machine learning about efficient learning methods for such structures.

Method: A $2$-satisfiability based decomposition theorem is used to represent monophonic halfspaces as disjoint vertex subsets, enabling efficient algorithms for learning tasks.

Result: Efficient algorithms for teaching, active, online learning, and empirical risk minimization, along with a proper sample compression scheme.

Conclusion: Monophonic halfspaces are efficiently learnable, unlike geodesic halfspaces, resolving open questions and highlighting computational advantages.

Abstract: Abstract notions of convexity over the vertices of a graph, and corresponding
notions of halfspaces, have recently gained attention from the machine learning
community. In this work we study monophonic halfspaces, a notion of graph
halfspaces defined through closure under induced paths. Our main result is a
$2$-satisfiability based decomposition theorem, which allows one to represent
monophonic halfspaces as a disjoint union of certain vertex subsets. Using this
decomposition, we achieve efficient and (nearly) optimal algorithms for various
learning problems, such as teaching, active, and online learning. Most notably,
we obtain a polynomial-time algorithm for empirical risk minimization.
Independently of the decomposition theorem, we obtain an efficient, stable, and
proper sample compression scheme. This makes monophonic halfspaces efficiently
learnable with proper learners and linear error rate $1/\varepsilon$ in the
realizable PAC setting. Our results answer open questions from the literature,
and show a stark contrast with geodesic halfspaces, for which most of the said
learning problems are NP-hard.

</details>


### [101] [External Data-Enhanced Meta-Representation for Adaptive Probabilistic Load Forecasting](https://arxiv.org/abs/2506.23201)
*Haoran Li,Muhao Guo,Marija Ilic,Yang Weng,Guangchun Ruan*

Main category: cs.LG

TL;DR: The paper proposes a meta-representation framework, M2oE2, using hypernetworks and Mixture-of-Experts to dynamically adapt load forecasting models to external conditions, improving accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Accurate residential load forecasting is crucial for power system reliability, but existing models inadequately handle external factors like weather and pricing.

Method: The framework uses hypernetworks to modulate a base DL model based on external data and integrates MoE for selective expert activation and input filtering.

Result: M2oE2 outperforms state-of-the-art methods in accuracy and robustness across diverse datasets with minimal overhead.

Conclusion: The approach effectively leverages external data as meta-knowledge, enhancing forecasting performance while maintaining efficiency.

Abstract: Accurate residential load forecasting is critical for power system
reliability with rising renewable integration and demand-side flexibility.
However, most statistical and machine learning models treat external factors,
such as weather, calendar effects, and pricing, as extra input, ignoring their
heterogeneity, and thus limiting the extraction of useful external information.
We propose a paradigm shift: external data should serve as meta-knowledge to
dynamically adapt the forecasting model itself. Based on this idea, we design a
meta-representation framework using hypernetworks that modulate selected
parameters of a base Deep Learning (DL) model in response to external
conditions. This provides both expressivity and adaptability. We further
integrate a Mixture-of-Experts (MoE) mechanism to enhance efficiency through
selective expert activation, while improving robustness by filtering redundant
external inputs. The resulting model, dubbed as a Meta Mixture of Experts for
External data (M2oE2), achieves substantial improvements in accuracy and
robustness with limited additional overhead, outperforming existing
state-of-the-art methods in diverse load datasets. The dataset and source code
are publicly available at
https://github.com/haorandd/M2oE2\_load\_forecast.git.

</details>


### [102] [FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model](https://arxiv.org/abs/2506.23210)
*Taehwan Yoon,Bongjun Choi*

Main category: cs.LG

TL;DR: Proposes a reference model-based federated learning method for optimal fine-tuning, addressing catastrophic forgetting and improving model performance with low computing cost.


<details>
  <summary>Details</summary>
Motivation: Federated learning ensures privacy but often lacks in model performance and personalization, prompting the need for optimization methods.

Method: Uses Bayesian parameter-efficient transfer learning with an optimal proximal term and a reference model to avoid catastrophic forgetting.

Result: Achieves high model performance and low computing cost.

Conclusion: The proposed method effectively balances privacy, performance, and efficiency in federated learning.

Abstract: Federated learning(FL) is used for distributed scenarios to train artificial
intelligence(AI) models while ensuring users' privacy. In federated learning
scenario, the server generally never knows about users' data. This type of
concept makes the AI training process efficient in terms of data privacy.
However, regarding model performance, federated AI models may not sufficiently
satisfy AI users' expectations. Furthermore, AI users have a wide range of
different needs. It is not easy to satisfy the whole users needs. These types
of issues can be addressed through AI model optimization, fine-tuning, or
personalization to achieve optimal model performance. To address model
optimization challenges, we propose reference model-based federated learning
for optimal fine-tuning, which overcomes catastrophic forgetting in each round.
This method is derived from Bayesian parameter-efficient transfer learning,
which includes an optimal proximal term and enables overcoming the catastrophic
forgetting issue in each round by utilizing a reference model that incorporates
previous model parameters. As a result, this method achieves both high model
performance and low computing cost.

</details>


### [103] [Single Image Inpainting and Super-Resolution with Simultaneous Uncertainty Guarantees by Universal Reproducing Kernels](https://arxiv.org/abs/2506.23221)
*Bálint Horváth,Balázs Csanád Csáji*

Main category: cs.LG

TL;DR: The paper introduces SGKI, a kernel-based method for image inpainting and super-resolution, providing pixel estimates with uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for accurate missing pixel estimation in images, especially with uncertainty measures, for tasks like inpainting and super-resolution.

Method: SGKI extends kernel methods, leveraging RKHS and band-limited functions, and uses Schur complements for efficient confidence band computation.

Result: SGKI successfully estimates missing pixels and provides non-asymptotic confidence bands, validated on synthetic and benchmark datasets.

Conclusion: SGKI is an effective tool for image restoration with built-in uncertainty quantification, applicable to vector-valued functions.

Abstract: The paper proposes a statistical learning approach to the problem of
estimating missing pixels of images, crucial for image inpainting and
super-resolution problems. One of the main novelties of the method is that it
also provides uncertainty quantifications together with the estimated values.
Our core assumption is that the underlying data-generating function comes from
a Reproducing Kernel Hilbert Space (RKHS). A special emphasis is put on
band-limited functions, central to signal processing, which form Paley-Wiener
type RKHSs. The proposed method, which we call Simultaneously Guaranteed Kernel
Interpolation (SGKI), is an extension and refinement of a recently developed
kernel method. An advantage of SGKI is that it not only estimates the missing
pixels, but also builds non-asymptotic confidence bands for the unobserved
values, which are simultaneously guaranteed for all missing pixels. We also
show how to compute these bands efficiently using Schur complements, we discuss
a generalization to vector-valued functions, and we present a series of
numerical experiments on various datasets containing synthetically generated
and benchmark images, as well.

</details>


### [104] [Masked Gated Linear Unit](https://arxiv.org/abs/2506.23225)
*Yukito Tajima,Nakamasa Inoue,Yusuke Sekikawa,Ikuro Sato,Rio Yokota*

Main category: cs.LG

TL;DR: MGLUs improve GLUs by reducing memory reads and enhancing efficiency with shared weight matrices and hardware-friendly kernels, achieving faster inference and better memory usage.


<details>
  <summary>Details</summary>
Motivation: GLUs in LLMs require high memory reads due to separate weight matrices for gate and value streams, creating a bottleneck.

Method: Introduces MGLUs with MoEG architecture for shared weight matrices and FlashMGLU kernel for efficient implementation.

Result: MGLUs achieve 19.7x speed-up, 47% memory efficiency, and 34% faster inference than GLUs, with SwiMGLU matching or surpassing SwiGLU accuracy.

Conclusion: MGLUs offer a scalable and efficient alternative to GLUs in LLMs, balancing performance and resource usage.

Abstract: Gated Linear Units (GLUs) have become essential components in the
feed-forward networks of state-of-the-art Large Language Models (LLMs).
However, they require twice as many memory reads compared to feed-forward
layers without gating, due to the use of separate weight matrices for the gate
and value streams. To address this bottleneck, we introduce Masked Gated Linear
Units (MGLUs), a novel family of GLUs with an efficient kernel implementation.
The core contribution of MGLUs include: (1) the Mixture of Element-wise Gating
(MoEG) architecture that learns multiple binary masks, each determining gate or
value assignments at the element level on a single shared weight matrix
resulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly
kernel that yields up to a 19.7 $\times$ inference-time speed-up over a naive
PyTorch MGLU and is 47% more memory-efficient and 34% faster than standard GLUs
despite added architectural complexity on an RTX5090 GPU. In LLM experiments,
the Swish-activated variant SwiMGLU preserves its memory advantages while
matching - or even surpassing - the downstream accuracy of the SwiGLU baseline.

</details>


### [105] [Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert Merging](https://arxiv.org/abs/2506.23266)
*Lujun Li,Zhu Qiyuan,Jiacheng Wang,Wei Li,Hao Gu,Sirui Han,Yike Guo*

Main category: cs.LG

TL;DR: Sub-MoE is a compression framework for Mixture of Experts (MoE) LLMs, addressing parameter conflicts via subspace expert merging and adaptive clustering.


<details>
  <summary>Details</summary>
Motivation: MoE LLMs face memory and deployment challenges due to large parameter scales, and existing merging methods suffer from parameter conflicts.

Method: Sub-MoE uses joint SVD on expert weights, adaptive clustering, and subspace merging to align and fuse experts.

Result: Sub-MoE maintains 96%|86% performance with 25%|50% expert reduction on Mixtral-8x7B.

Conclusion: Sub-MoE outperforms existing methods, offering efficient compression without significant performance loss.

Abstract: Mixture of Experts (MoE) LLMs face significant obstacles due to their massive
parameter scale, which imposes memory, storage, and deployment challenges.
Although recent expert merging methods promise greater efficiency by
consolidating multiple experts, they are fundamentally hindered by parameter
conflicts arising from expert specialization. In this paper, we present
Sub-MoE, a novel MoE compression framework via Subspace Expert Merging. Our key
insight is to perform joint Singular Value Decomposition (SVD) on concatenated
expert weights, reducing conflicting parameters by extracting shared
$U$-matrices while enabling effective merging of the expert-specific $V$
components. Specifically, Sub-MoE consists of two innovative phases: (1)
Adaptive Expert Clustering, which groups functionally coherent experts via
K-means clustering based on cosine similarity of expert outputs; and (2)
Subspace Expert Merging, which first enforces Experts Union Decomposition to
derive the shared $U$-matrix across experts in the same group, then pursues
frequency-based merging for individual $V$-matrices, and finalizes expert
reconstruction using the merged $V$-matrix. In this way, we align and fuse
experts in a shared subspace, and can be extended with intra-expert compression
for further inference optimization. Extensive experiments on Mixtral, DeepSeek,
and Qwen-1.5|3 MoE LLMs demonstrate that our Sub-MoE significantly outperforms
existing expert pruning and merging methods. Notably, our Sub-MoE maintains
96\%|86\% of original performance with 25\%|50\% expert reduction on
Mixtral-8x7B in zero-shot benchmarks. Code will be released at
https://github.com/lliai/MoERazor.

</details>


### [106] [Predicting thinking time in Reasoning models](https://arxiv.org/abs/2506.23274)
*Hans Peter Lynsgøe Raaschou-jensen,Constanza Fierro,Anders Søgaard*

Main category: cs.LG

TL;DR: The paper addresses the unpredictability of reasoning time in models with hidden chains of thought and proposes methods to predict and display this time, akin to a "progress bar for reasoning."


<details>
  <summary>Details</summary>
Motivation: Users lack insight into how long models will spend reasoning before providing answers, leading to frustration, especially as models handle longer tasks asynchronously.

Method: The paper introduces and evaluates methods for online and offline prediction of model "thinking time."

Result: The proposed methods aim to provide a practical solution for predicting and displaying reasoning time.

Conclusion: The work highlights the importance of predictable reasoning time for user interaction and suggests future research directions.

Abstract: Reasoning models that produce long, hidden chains of thought have emerged as
powerful tools for complex, reasoning-intensive
tasks\citep{deepseekai2025deepseekr1incentivizingreasoningcapability,
openai2024openaio1card}. However, this paradigm introduces a new user
experience challenge: users have little insight into how much time the model
will spend reasoning before returning an answer. This unpredictability, can
lead to user frustration and is likely to compound as LLMs can produce
increasingly long tasks asynchronously
\citep{kwa2025measuringaiabilitycomplete}. In this paper, we introduce and
evaluate methods for both online and offline prediction of model "thinking
time," aiming to develop a practical "progress bar for reasoning." We discuss
the implications for user interaction and future research directions.

</details>


### [107] [BAPE: Learning an Explicit Bayes Classifier for Long-tailed Visual Recognition](https://arxiv.org/abs/2506.23280)
*Chaoqun Du,Yulin Wang,Shiji Song,Gao Huang*

Main category: cs.LG

TL;DR: The paper introduces BAPE, a method for explicitly modeling posterior probabilities to address gradient imbalance in long-tailed data, ensuring Bayes optimal decisions without gradient descent.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods implicitly estimate posterior probabilities, failing in long-tailed data distributions due to gradient imbalance and suboptimal decisions.

Method: BAPE explicitly models posterior probabilities using point estimation, directly learning the Bayes classifier. It includes a distribution adjustment technique for adapting to test data.

Result: BAPE improves generalization on CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and iNaturalist, outperforming existing methods.

Conclusion: BAPE provides a theoretically sound and effective solution for long-tailed data, orthogonal to existing implicit estimation approaches.

Abstract: Bayesian decision theory advocates the Bayes classifier as the optimal
approach for minimizing the risk in machine learning problems. Current deep
learning algorithms usually solve for the optimal classifier by
\emph{implicitly} estimating the posterior probabilities, \emph{e.g.}, by
minimizing the Softmax cross-entropy loss. This simple methodology has been
proven effective for meticulously balanced academic benchmark datasets.
However, it is not applicable to the long-tailed data distributions in the real
world, where it leads to the gradient imbalance issue and fails to ensure the
Bayes optimal decision rule. To address these challenges, this paper presents a
novel approach (BAPE) that provides a more precise theoretical estimation of
the data distributions by \emph{explicitly} modeling the parameters of the
posterior probabilities and solving them with point estimation. Consequently,
our method directly learns the Bayes classifier without gradient descent based
on Bayes' theorem, simultaneously alleviating the gradient imbalance and
ensuring the Bayes optimal decision rule. Furthermore, we propose a
straightforward yet effective \emph{distribution adjustment} technique. This
method enables the Bayes classifier trained from the long-tailed training set
to effectively adapt to the test data distribution with an arbitrary imbalance
factor, thereby enhancing performance without incurring additional
computational costs. In addition, we demonstrate the gains of our method are
orthogonal to existing learning approaches for long-tailed scenarios, as they
are mostly designed under the principle of \emph{implicitly} estimating the
posterior probabilities. Extensive empirical evaluations on CIFAR-10-LT,
CIFAR-100-LT, ImageNet-LT, and iNaturalist demonstrate that our method
significantly improves the generalization performance of popular deep networks,
despite its simplicity.

</details>


### [108] [Not All Explanations for Deep Learning Phenomena Are Equally Valuable](https://arxiv.org/abs/2506.23286)
*Alan Jeffares,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: The paper critiques the focus on isolated, counterintuitive deep learning phenomena, arguing they lack real-world relevance but can still refine broader theories.


<details>
  <summary>Details</summary>
Motivation: To challenge the efficiency of researching isolated deep learning phenomena without clear real-world applicability.

Method: Analyzes prominent examples of such phenomena and revisits research norms.

Result: Finds little evidence of real-world impact but suggests value in refining general theories.

Conclusion: Recommends aligning research on deep learning phenomena with broader field progress.

Abstract: Developing a better understanding of surprising or counterintuitive phenomena
has constituted a significant portion of deep learning research in recent
years. These include double descent, grokking, and the lottery ticket
hypothesis -- among many others. Works in this area often develop ad hoc
hypotheses attempting to explain these observed phenomena on an isolated,
case-by-case basis. This position paper asserts that, in many prominent cases,
there is little evidence to suggest that these phenomena appear in real-world
applications and these efforts may be inefficient in driving progress in the
broader field. Consequently, we argue against viewing them as isolated puzzles
that require bespoke resolutions or explanations. However, despite this, we
suggest that deep learning phenomena do still offer research value by providing
unique settings in which we can refine our broad explanatory theories of more
general deep learning principles. This position is reinforced by analyzing the
research outcomes of several prominent examples of these phenomena from the
recent literature. We revisit the current norms in the research community in
approaching these problems and propose practical recommendations for future
research, aiming to ensure that progress on deep learning phenomena is well
aligned with the ultimate pragmatic goal of progress in the broader field of
deep learning.

</details>


### [109] [Hierarchical Quantized Diffusion Based Tree Generation Method for Hierarchical Representation and Lineage Analysis](https://arxiv.org/abs/2506.23287)
*Zelin Zang,WenZhe Li,Fei Chen,Yongjie Xu,Chang Yu,Zhen Lei,Stan Z. Li*

Main category: cs.LG

TL;DR: HDTree, a diffusion-based approach, improves hierarchical lineage analysis in single-cell research by using a unified hierarchical codebook and quantized diffusion processes, outperforming existing methods in accuracy and performance.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for modeling hierarchical single-cell data face computational and stability limitations. Recent VAEs-based approaches still struggle with capturing deep hierarchical relationships due to branch-specific modules.

Method: HDTree employs a unified hierarchical codebook and quantized diffusion processes to model tree node transitions, eliminating branch-specific modules and enhancing generative capacity.

Result: HDTree outperforms existing methods in accuracy and performance on general-purpose and single-cell datasets.

Conclusion: HDTree provides a more accurate and efficient tool for hierarchical lineage analysis, aiding in modeling cellular differentiation paths and supporting downstream biological tasks.

Abstract: In single-cell research, tracing and analyzing high-throughput single-cell
differentiation trajectories is crucial for understanding complex biological
processes. Key to this is the modeling and generation of hierarchical data that
represents the intrinsic structure within datasets. Traditional methods face
limitations in terms of computational cost, performance, generative capacity,
and stability. Recent VAEs based approaches have made strides in addressing
these challenges but still require specialized network modules for each tree
branch, limiting their stability and ability to capture deep hierarchical
relationships. To overcome these challenges, we introduce diffusion-based
approach called HDTree. HDTree captures tree relationships within a
hierarchical latent space using a unified hierarchical codebook and quantized
diffusion processes to model tree node transitions. This method improves
stability by eliminating branch-specific modules and enhancing generative
capacity through gradual hierarchical changes simulated by the diffusion
process. HDTree's effectiveness is demonstrated through comparisons on both
general-purpose and single-cell datasets, where it outperforms existing methods
in terms of accuracy and performance. These contributions provide a new tool
for hierarchical lineage analysis, enabling more accurate and efficient
modeling of cellular differentiation paths and offering insights for downstream
biological tasks. The code of HDTree is available at anonymous link
https://anonymous.4open.science/r/code_HDTree_review-A8DB.

</details>


### [110] [VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular Design](https://arxiv.org/abs/2506.23339)
*Malikussaid,Hilal Hudan Nuha*

Main category: cs.LG

TL;DR: VALID-Mol improves LLM-driven molecular design by integrating chemical validation, increasing valid structure generation from 3% to 83%.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with factual accuracy and domain-specific constraints in scientific applications like drug discovery, often producing invalid molecular structures.

Method: Combines prompt engineering, automated chemical validation, and a fine-tuned domain-adapted LLM to ensure reliable molecule generation.

Result: Achieves up to 17-fold predicted improvements in target affinity while maintaining synthetic accessibility.

Conclusion: Offers a generalizable framework for scientifically-constrained LLM applications, with reproducible methods for other domains.

Abstract: Large Language Models (LLMs) demonstrate remarkable potential for scientific
discovery, but their application in domains requiring factual accuracy and
domain-specific constraints remains challenging. In molecular design for drug
discovery, LLMs can suggest creative molecular modifications but often produce
chemically invalid or impractical structures. We present VALID-Mol, a
systematic framework for integrating chemical validation with LLM-driven
molecular design that increases the rate of generating valid chemical
structures from 3% to 83%. Our approach combines methodical prompt engineering,
automated chemical validation, and a fine-tuned domain-adapted LLM to ensure
reliable generation of synthesizable molecules with improved properties. Beyond
the specific implementation, we contribute a generalizable methodology for
scientifically-constrained LLM applications, with quantifiable reliability
improvements. Computational predictions suggest our framework can generate
promising candidates for synthesis with up to 17-fold computationally predicted
improvements in target affinity while maintaining synthetic accessibility. We
provide a detailed analysis of our prompt engineering process, validation
architecture, and fine-tuning approach, offering a reproducible blueprint for
applying LLMs to other scientific domains where domain-specific validation is
essential.

</details>


### [111] [A case for data valuation transparency via DValCards](https://arxiv.org/abs/2506.23349)
*Keziah Naggita,Julienne LaChance*

Main category: cs.LG

TL;DR: Data valuation methods in ML are biased and unstable, affected by pre-processing and algorithmic choices, leading to ethical and technical issues. A new framework, DValCards, is proposed for transparency.


<details>
  <summary>Details</summary>
Motivation: To highlight biases and instability in data valuation methods and their ethical implications, advocating for transparency.

Method: Analysis of 9 tabular datasets and 6 valuation methods, examining effects of pre-processing, subsampling, and bias.

Result: Pre-processing alters data values, subsampling worsens imbalance, and underrepresented data is undervalued.

Conclusion: DValCards framework is introduced to promote transparency and responsible use of data valuation metrics.

Abstract: Following the rise in popularity of data-centric machine learning (ML),
various data valuation methods have been proposed to quantify the contribution
of each datapoint to desired ML model performance metrics (e.g., accuracy).
Beyond the technical applications of data valuation methods (e.g., data
cleaning, data acquisition, etc.), it has been suggested that within the
context of data markets, data buyers might utilize such methods to fairly
compensate data owners. Here we demonstrate that data valuation metrics are
inherently biased and unstable under simple algorithmic design choices,
resulting in both technical and ethical implications. By analyzing 9 tabular
classification datasets and 6 data valuation methods, we illustrate how (1)
common and inexpensive data pre-processing techniques can drastically alter
estimated data values; (2) subsampling via data valuation metrics may increase
class imbalance; and (3) data valuation metrics may undervalue underrepresented
group data. Consequently, we argue in favor of increased transparency
associated with data valuation in-the-wild and introduce the novel Data
Valuation Cards (DValCards) framework towards this aim. The proliferation of
DValCards will reduce misuse of data valuation metrics, including in data
pricing, and build trust in responsible ML systems.

</details>


### [112] [Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment](https://arxiv.org/abs/2506.23358)
*Pawel Renc,Michal K. Grzeszczyk,Linglong Qian,Nassim Oufattole,Jeff Rasley,Arkadiusz Sitek*

Main category: cs.LG

TL;DR: FTS is a federated framework for training generative models on distributed EHR data, using tokenized patient timelines and federated learning to ensure privacy and scalability.


<details>
  <summary>Details</summary>
Motivation: To enable generative modeling of EHR data across institutions without sharing raw data, addressing privacy and scalability challenges.

Method: Tokenizes patient histories into PHTs, trains local autoregressive transformers, aggregates model weights centrally, and synthesizes data for a global generator.

Result: GG-trained models perform comparably to real-data models on clinical prediction tasks.

Conclusion: FTS provides privacy, scalability, and extensibility for healthcare applications like counterfactual inference and synthetic trials.

Abstract: We present Federated Timeline Synthesis (FTS), a novel framework for training
generative foundation models across distributed timeseries data applied to
electronic health records (EHR). At its core, FTS represents patient history as
tokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding
temporal, categorical, and continuous clinical information. Each institution
trains an autoregressive transformer on its local PHTs and transmits only model
weights to a central server. The server uses the generators to synthesize a
large corpus of trajectories and train a Global Generator (GG), enabling
zero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS
on five clinically meaningful prediction tasks using MIMIC-IV data, showing
that models trained on synthetic data generated by GG perform comparably to
those trained on real data. FTS offers strong privacy guarantees, scalability
across institutions, and extensibility to diverse prediction and simulation
tasks especially in healthcare, including counterfactual inference, early
warning detection, and synthetic trial design.

</details>


### [113] [ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks](https://arxiv.org/abs/2402.09146)
*Muhammad Kashif,Muhammad Shafique*

Main category: cs.LG

TL;DR: A novel framework, Residual Quanvolutional Neural Networks (ResQuNNs), enhances QuNNs by introducing trainable quanvolutional layers and residual learning to improve gradient flow and training performance.


<details>
  <summary>Details</summary>
Motivation: Traditional quanvolutional layers are static and lack adaptability, limiting QuNN performance. This research aims to overcome this by enabling training within these layers.

Method: Proposes ResQuNNs with residual blocks between quanvolutional layers to facilitate gradient flow. Empirical evidence identifies optimal residual block placement.

Result: Improved training performance and gradient access in QuNNs, with strategic residual block placement maximizing gains.

Conclusion: ResQuNNs advance quantum deep learning, offering theoretical and practical benefits for quantum computing.

Abstract: In this paper, we present a novel framework for enhancing the performance of
Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional
layers and addressing the critical challenges associated with them. Traditional
quanvolutional layers, although beneficial for feature extraction, have largely
been static, offering limited adaptability. Unlike state-of-the-art, our
research overcomes this limitation by enabling training within these layers,
significantly increasing the flexibility and potential of QuNNs. However, the
introduction of multiple trainable quanvolutional layers induces complexities
in gradient-based optimization, primarily due to the difficulty in accessing
gradients across these layers. To resolve this, we propose a novel
architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging
the concept of residual learning, which facilitates the flow of gradients by
adding skip connections between layers. By inserting residual blocks between
quanvolutional layers, we ensure enhanced gradient access throughout the
network, leading to improved training performance. Moreover, we provide
empirical evidence on the strategic placement of these residual blocks within
QuNNs. Through extensive experimentation, we identify an efficient
configuration of residual blocks, which enables gradients across all the layers
in the network that eventually results in efficient training. Our findings
suggest that the precise location of residual blocks plays a crucial role in
maximizing the performance gains in QuNNs. Our results mark a substantial step
forward in the evolution of quantum deep learning, offering new avenues for
both theoretical development and practical quantum computing applications.

</details>


### [114] [When Additive Noise Meets Unobserved Mediators: Bivariate Denoising Diffusion for Causal Discovery](https://arxiv.org/abs/2506.23374)
*Dominik Meier,Sujai Hiremath,Promit Ghosal,Kyra Gan*

Main category: cs.LG

TL;DR: The paper addresses the challenge of causal discovery in bivariate data with unobserved mediators, proposing a new method (BiDD) that outperforms existing approaches.


<details>
  <summary>Details</summary>
Motivation: Standard additive noise models (ANMs) fail when unmeasured mediators corrupt causal relationships, and prior solutions are brittle in finite samples.

Method: Proposes Bivariate Denoising Diffusion (BiDD), using a novel independence test during noising/denoising processes to handle latent noise.

Result: BiDD shows consistent performance in synthetic and real-world data, excelling in mediator-corrupted settings.

Conclusion: BiDD is a robust solution for causal discovery under hidden mediation, outperforming existing methods.

Abstract: Distinguishing cause and effect from bivariate observational data is a
foundational problem in many disciplines, but challenging without additional
assumptions. Additive noise models (ANMs) are widely used to enable
sample-efficient bivariate causal discovery. However, conventional ANM-based
methods fail when unobserved mediators corrupt the causal relationship between
variables. This paper makes three key contributions: first, we rigorously
characterize why standard ANM approaches break down in the presence of
unmeasured mediators. Second, we demonstrate that prior solutions for hidden
mediation are brittle in finite sample settings, limiting their practical
utility. To address these gaps, we propose Bivariate Denoising Diffusion (BiDD)
for causal discovery, a method designed to handle latent noise introduced by
unmeasured mediators. Unlike prior methods that infer directionality through
mean squared error loss comparisons, our approach introduces a novel
independence test statistic: during the noising and denoising processes for
each variable, we condition on the other variable as input and evaluate the
independence of the predicted noise relative to this input. We prove asymptotic
consistency of BiDD under the ANM, and conjecture that it performs well under
hidden mediation. Experiments on synthetic and real-world data demonstrate
consistent performance, outperforming existing methods in mediator-corrupted
settings while maintaining strong performance in mediator-free settings.

</details>


### [115] [Do LLMs Dream of Discrete Algorithms?](https://arxiv.org/abs/2506.23408)
*Claudionor Coelho Jr,Yanen Li,Philip Tee*

Main category: cs.LG

TL;DR: The paper proposes a neurosymbolic approach to enhance LLMs by integrating logic-based reasoning (e.g., Prolog) to address limitations in logical reasoning and interpretability, demonstrating improved performance on multi-step tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs lack effectiveness in domains requiring strict logical reasoning and interpretability, prompting the need for a hybrid approach.

Method: Augments LLMs with logic-based reasoning modules (Prolog predicates) and composable toolsets to decompose queries into verifiable sub-tasks.

Result: Experiments on the DABStep benchmark show improved precision, coverage, and documentation in multi-step reasoning.

Conclusion: Combining LLMs with logic reasoning enhances reliability, interpretability, and scalability for trustworthy AI agents.

Abstract: Large Language Models (LLMs) have rapidly transformed the landscape of
artificial intelligence, enabling natural language interfaces and dynamic
orchestration of software components. However, their reliance on probabilistic
inference limits their effectiveness in domains requiring strict logical
reasoning, discrete decision-making, and robust interpretability. This paper
investigates these limitations and proposes a neurosymbolic approach that
augments LLMs with logic-based reasoning modules, particularly leveraging
Prolog predicates and composable toolsets. By integrating first-order logic and
explicit rule systems, our framework enables LLMs to decompose complex queries
into verifiable sub-tasks, orchestrate reliable solutions, and mitigate common
failure modes such as hallucination and incorrect step decomposition. We
demonstrate the practical benefits of this hybrid architecture through
experiments on the DABStep benchmark, showing improved precision, coverage, and
system documentation in multi-step reasoning tasks. Our results indicate that
combining LLMs with modular logic reasoning restores engineering rigor,
enhances system reliability, and offers a scalable path toward trustworthy,
interpretable AI agents across complex domains.

</details>


### [116] [BenchMake: Turn any scientific data set into a reproducible benchmark](https://arxiv.org/abs/2506.23419)
*Amanda S Barnard*

Main category: cs.LG

TL;DR: A tool called BenchMake is introduced to transform scientific datasets into benchmarks using non-negative matrix factorization to identify edge cases and partition data for testing.


<details>
  <summary>Details</summary>
Motivation: The rarity of benchmark sets in computational science makes evaluating new innovations difficult. BenchMake aims to address this by leveraging openly available scientific datasets.

Method: BenchMake uses non-negative matrix factorization to isolate challenging edge cases on the convex hull and partitions data into testing sets to maximize divergence and statistical significance.

Result: BenchMake's splits are compared to established and random splits across ten diverse benchmark sets, demonstrating its effectiveness.

Conclusion: BenchMake provides a robust method to create benchmarks from scientific datasets, enhancing evaluation of computational innovations.

Abstract: Benchmark data sets are a cornerstone of machine learning development and
applications, ensuring new methods are robust, reliable and competitive. The
relative rarity of benchmark sets in computational science, due to the
uniqueness of the problems and the pace of change in the associated domains,
makes evaluating new innovations difficult for computational scientists. In
this paper a new tool is developed and tested to potentially turn any of the
increasing numbers of scientific data sets made openly available into a
benchmark accessible to the community. BenchMake uses non-negative matrix
factorisation to deterministically identify and isolate challenging edge cases
on the convex hull (the smallest convex set that contains all existing data
instances) and partitions a required fraction of matched data instances into a
testing set that maximises divergence and statistical significance, across
tabular, graph, image, signal and textual modalities. BenchMake splits are
compared to establish splits and random splits using ten publicly available
benchmark sets from different areas of science, with different sizes, shapes,
distributions.

</details>


### [117] [Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting](https://arxiv.org/abs/2506.23424)
*Heitor R. Medeiros,Hossein Sharifi-Noghabi,Gabriel L. Oliveira,Saghar Irandoust*

Main category: cs.LG

TL;DR: PETSA is a parameter-efficient test-time adaptation method for time series forecasting, using small calibration modules and a specialized loss to improve adaptability without full model updates.


<details>
  <summary>Details</summary>
Motivation: Real-world time series are non-stationary, degrading pre-trained models. Existing TTA methods update the full model, increasing costs.

Method: PETSA updates only small calibration modules using low-rank adapters and dynamic gating, with a specialized loss combining robust, frequency-domain, and patch-wise structural terms.

Result: PETSA improves adaptability of forecasting backbones with fewer parameters, achieving competitive or better performance on benchmark datasets.

Conclusion: PETSA offers an efficient and effective solution for test-time adaptation in time series forecasting.

Abstract: Real-world time series often exhibit a non-stationary nature, degrading the
performance of pre-trained forecasting models. Test-Time Adaptation (TTA)
addresses this by adjusting models during inference, but existing methods
typically update the full model, increasing memory and compute costs. We
propose PETSA, a parameter-efficient method that adapts forecasters at test
time by only updating small calibration modules on the input and output. PETSA
uses low-rank adapters and dynamic gating to adjust representations without
retraining. To maintain accuracy despite limited adaptation capacity, we
introduce a specialized loss combining three components: (1) a robust term, (2)
a frequency-domain term to preserve periodicity, and (3) a patch-wise
structural term for structural alignment. PETSA improves the adaptability of
various forecasting backbones while requiring fewer parameters than baselines.
Experimental results on benchmark datasets show that PETSA achieves competitive
or better performance across all horizons. Our code is available at:
https://github.com/BorealisAI/PETSA

</details>


### [118] [Enhancing Insider Threat Detection Using User-Based Sequencing and Transformer Encoders](https://arxiv.org/abs/2506.23446)
*Mohamed Elbasheer,Adewale Akinfaderin*

Main category: cs.LG

TL;DR: The paper proposes a User-Based Sequencing (UBS) method with a Transformer Encoder for insider threat detection, achieving high accuracy and low error rates.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to capture sequential dependencies in user behavior, limiting their effectiveness in detecting subtle insider threats.

Method: UBS transforms user activity into temporal sequences, uses a Transformer Encoder for modeling, and evaluates anomalies with unsupervised outlier detection algorithms (OCSVM, LOF, iForest).

Result: The UBS-Transformer pipeline achieves 96.61% accuracy, 99.43% recall, and exceptionally low false negative/positive rates, outperforming baselines.

Conclusion: Sequential user modeling and advanced anomaly detection significantly improve insider threat detection performance.

Abstract: Insider threat detection presents unique challenges due to the authorized
status of malicious actors and the subtlety of anomalous behaviors. Existing
machine learning methods often treat user activity as isolated events, thereby
failing to leverage sequential dependencies in user behavior. In this study, we
propose a User-Based Sequencing (UBS) methodology, transforming the CERT
insider threat dataset into structured temporal sequences suitable for deep
sequential modeling. We deploy a Transformer Encoder architecture to model
benign user activity and employ its reconstruction errors as anomaly scores.
These scores are subsequently evaluated using three unsupervised outlier
detection algorithms: One-Class SVM (OCSVM), Local Outlier Factor (LOF), and
Isolation Forest (iForest). Across four rigorously designed test sets,
including combinations of multiple CERT dataset releases, our UBS-Transformer
pipeline consistently achieves state-of-the-art performance - notably 96.61%
accuracy, 99.43% recall, 96.38% F1-score, 95.00% AUROC, and exceptionally low
false negative (0.0057) and false positive (0.0571) rates. Comparative analyses
demonstrate that our approach substantially outperforms tabular and
conventional autoencoder baselines, underscoring the efficacy of sequential
user modeling and advanced anomaly detection in the insider threat domain.

</details>


### [119] [Can We Predict the Unpredictable? Leveraging DisasterNet-LLM for Multimodal Disaster Classification](https://arxiv.org/abs/2506.23462)
*Manaswi Kulahara,Gautam Siddharth Kashyap,Nipun Joshi,Arpita Soni*

Main category: cs.LG

TL;DR: DisasterNet-LLM, a specialized LLM, outperforms state-of-the-art models in disaster classification by integrating multimodal data, achieving high accuracy and scores.


<details>
  <summary>Details</summary>
Motivation: Traditional disaster management methods fail to effectively integrate multimodal data like images, weather records, and textual reports, necessitating a more robust solution.

Method: DisasterNet-LLM uses advanced pretraining, cross-modal attention mechanisms, and adaptive transformers for comprehensive disaster analysis.

Result: The model achieves 89.5% accuracy, 88.0% F1 score, 0.92% AUC, and 0.88% BERTScore in multimodal disaster classification.

Conclusion: DisasterNet-LLM is a superior solution for disaster management, offering high performance in integrating and analyzing multimodal data.

Abstract: Effective disaster management requires timely and accurate insights, yet
traditional methods struggle to integrate multimodal data such as images,
weather records, and textual reports. To address this, we propose
DisasterNet-LLM, a specialized Large Language Model (LLM) designed for
comprehensive disaster analysis. By leveraging advanced pretraining,
cross-modal attention mechanisms, and adaptive transformers, DisasterNet-LLM
excels in disaster classification. Experimental results demonstrate its
superiority over state-of-the-art models, achieving higher accuracy of 89.5%,
an F1 score of 88.0%, AUC of 0.92%, and BERTScore of 0.88% in multimodal
disaster classification tasks.

</details>


### [120] [Reconciling Attribute and Structural Anomalies for Improved Graph Anomaly Detection](https://arxiv.org/abs/2506.23469)
*Chunjing Xiao,Jiahui Lu,Xovee Xu,Fan Zhou,Tianshu Xie,Wei Lu,Lifeng Xu*

Main category: cs.LG

TL;DR: TripleAD is a triple-channel framework for graph anomaly detection, addressing the tug-of-war problem in existing methods by separately handling attribute, structural, and mixed anomalies through mutual distillation.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised methods struggle with detecting both attribute and structural anomalies simultaneously due to conflicting objectives, leading to suboptimal performance.

Method: TripleAD uses three modules: multiscale attribute estimation, link-enhanced structure estimation, and attribute-mixed curvature, with mutual distillation for collaboration.

Result: The framework outperforms baselines in detecting anomalies by mitigating interference between anomaly types.

Conclusion: TripleAD effectively addresses the limitations of single-model approaches by leveraging mutual distillation and specialized modules for different anomaly types.

Abstract: Graph anomaly detection is critical in domains such as healthcare and
economics, where identifying deviations can prevent substantial losses.
Existing unsupervised approaches strive to learn a single model capable of
detecting both attribute and structural anomalies. However, they confront the
tug-of-war problem between two distinct types of anomalies, resulting in
suboptimal performance. This work presents TripleAD, a mutual
distillation-based triple-channel graph anomaly detection framework. It
includes three estimation modules to identify the attribute, structural, and
mixed anomalies while mitigating the interference between different types of
anomalies. In the first channel, we design a multiscale attribute estimation
module to capture extensive node interactions and ameliorate the over-smoothing
issue. To better identify structural anomalies, we introduce a link-enhanced
structure estimation module in the second channel that facilitates information
flow to topologically isolated nodes. The third channel is powered by an
attribute-mixed curvature, a new indicator that encapsulates both attribute and
structural information for discriminating mixed anomalies. Moreover, a mutual
distillation strategy is introduced to encourage communication and
collaboration between the three channels. Extensive experiments demonstrate the
effectiveness of the proposed TripleAD model against strong baselines.

</details>


### [121] [Sample Margin-Aware Recalibration of Temperature Scaling](https://arxiv.org/abs/2506.23492)
*Haolan Guo,Linwei Tao,Haoyang Luo,Minjing Dong,Chang Xu*

Main category: cs.LG

TL;DR: SMART introduces a lightweight, data-efficient recalibration method for neural networks, using the logit gap and a novel SoftECE objective to improve calibration performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Modern neural networks are systematically overconfident, posing risks in safety-critical scenarios, and current calibration methods struggle with bias-variance trade-offs.

Method: SMART scales logits based on the logit gap (margin between top two logits) and uses a SoftECE objective with adaptive binning for stable parameter updates.

Result: SMART achieves state-of-the-art calibration performance across diverse datasets and architectures, even with limited calibration data.

Conclusion: SMART provides a robust, efficient solution for uncertainty quantification in neural networks, outperforming existing methods.

Abstract: Recent advances in deep learning have significantly improved predictive
accuracy. However, modern neural networks remain systematically overconfident,
posing risks for deployment in safety-critical scenarios. Current post-hoc
calibration methods face a fundamental dilemma: global approaches like
Temperature Scaling apply uniform adjustments across all samples, introducing
high bias despite computational efficiency, while more expressive methods that
operate on full logit distributions suffer from high variance due to noisy
high-dimensional inputs and insufficient validation data. To address these
challenges, we propose Sample Margin-Aware Recalibration of Temperature
(SMART), a lightweight, data-efficient recalibration method that precisely
scales logits based on the margin between the top two logits -- termed the
logit gap. Specifically, the logit gap serves as a denoised, scalar signal
directly tied to decision boundary uncertainty, providing a robust indicator
that avoids the noise inherent in high-dimensional logit spaces while
preserving model prediction invariance. Meanwhile, SMART employs a novel
soft-binned Expected Calibration Error (SoftECE) objective that balances model
bias and variance through adaptive binning, enabling stable parameter updates
even with extremely limited calibration data. Extensive evaluations across
diverse datasets and architectures demonstrate that SMART achieves
state-of-the-art calibration performance even with substantially fewer
parameters compared to existing parametric methods, offering a principled,
robust, and highly efficient solution for practical uncertainty quantification
in neural network predictions. The source code is available at:
https://anonymous.4open.science/r/SMART-8B11.

</details>


### [122] [FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization](https://arxiv.org/abs/2506.23516)
*Seung-Wook Kim,Seongyeol Kim,Jiah Kim,Seowon Ji,Se-Ho Lee*

Main category: cs.LG

TL;DR: FedWSQ improves federated learning by combining weight standardization and distribution-aware non-uniform quantization, enhancing robustness and reducing communication costs.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation in FL due to data heterogeneity and communication constraints.

Method: Integrates weight standardization (WS) to filter biased updates and distribution-aware non-uniform quantization (DANUQ) to minimize quantization errors.

Result: FedWSQ reduces communication overhead while maintaining high model accuracy, outperforming existing methods in challenging FL settings.

Conclusion: FedWSQ is a robust and efficient solution for federated learning under data heterogeneity and communication constraints.

Abstract: Federated learning (FL) often suffers from performance degradation due to key
challenges such as data heterogeneity and communication constraints. To address
these limitations, we present a novel FL framework called FedWSQ, which
integrates weight standardization (WS) and the proposed distribution-aware
non-uniform quantization (DANUQ). WS enhances FL performance by filtering out
biased components in local updates during training, thereby improving the
robustness of the model against data heterogeneity and unstable client
participation. In addition, DANUQ minimizes quantization errors by leveraging
the statistical properties of local model updates. As a result, FedWSQ
significantly reduces communication overhead while maintaining superior model
accuracy. Extensive experiments on FL benchmark datasets demonstrate that
FedWSQ consistently outperforms existing FL methods across various challenging
FL settings, including extreme data heterogeneity and ultra-low-bit
communication scenarios.

</details>


### [123] [Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size](https://arxiv.org/abs/2506.23544)
*Kento Imaizumi,Hideaki Iiduka*

Main category: cs.LG

TL;DR: The paper analyzes the effectiveness of Quasi-hyperbolic momentum (QHM) in stochastic nonconvex optimization, showing that increasing batch size, without decaying learning rates, improves convergence for neural networks.


<details>
  <summary>Details</summary>
Motivation: The theoretical justification for momentum methods in stochastic nonconvex settings, like deep neural networks, is limited. QHM is studied to generalize momentum-based algorithms.

Method: The paper provides asymptotic and non-asymptotic convergence results for mini-batch QHM with increasing batch size, comparing it to decaying learning rates.

Result: Achieving asymptotic convergence requires decaying learning rates or increasing batch sizes. The latter is more effective for non-asymptotic convergence, as shown in experiments.

Conclusion: Increasing batch size in mini-batch QHM, without decaying learning rates, is a viable strategy for training neural networks, offering practical benefits.

Abstract: Momentum methods were originally introduced for their superiority to
stochastic gradient descent (SGD) in deterministic settings with convex
objective functions. However, despite their widespread application to deep
neural networks -- a representative case of stochastic nonconvex optimization
-- the theoretical justification for their effectiveness in such settings
remains limited. Quasi-hyperbolic momentum (QHM) is an algorithm that
generalizes various momentum methods and has been studied to better understand
the class of momentum-based algorithms as a whole. In this paper, we provide
both asymptotic and non-asymptotic convergence results for mini-batch QHM with
an increasing batch size. We show that achieving asymptotic convergence
requires either a decaying learning rate or an increasing batch size. Since a
decaying learning rate adversely affects non-asymptotic convergence, we
demonstrate that using mini-batch QHM with an increasing batch size -- without
decaying the learning rate -- can be a more effective strategy. Our experiments
show that even a finite increase in batch size can provide benefits for
training neural networks.

</details>


### [124] [A unified framework on the universal approximation of transformer-type architectures](https://arxiv.org/abs/2506.23551)
*Jingpu Cheng,Qianxiao Li,Ting Lin,Zuowei Shen*

Main category: cs.LG

TL;DR: The paper explores the universal approximation property (UAP) in transformer architectures, introducing a unified framework that extends prior work on residual networks to attention-based models. It identifies token distinguishability as key for UAP and simplifies verification under analyticity assumptions. The framework is applied to various attention mechanisms, generalizing prior results and enabling new architecture designs with UAP guarantees.


<details>
  <summary>Details</summary>
Motivation: To extend theoretical understanding of UAP to transformer architectures, bridging gaps in prior work and providing a unified framework for analyzing attention-based models.

Method: The study introduces a general sufficient condition for UAP, leveraging analyticity assumptions on attention layers to simplify verification. It applies this framework to transformers with diverse attention mechanisms (e.g., kernel-based, sparse).

Result: The framework proves UAP for various transformer architectures, generalizing prior results and covering new cases. It also enables principled design of novel architectures with UAP guarantees.

Conclusion: The work provides a foundational framework for UAP in transformers, simplifying analysis and enabling new designs with guaranteed approximation properties.

Abstract: We investigate the universal approximation property (UAP) of transformer-type
architectures, providing a unified theoretical framework that extends prior
results on residual networks to models incorporating attention mechanisms. Our
work identifies token distinguishability as a fundamental requirement for UAP
and introduces a general sufficient condition that applies to a broad class of
architectures. Leveraging an analyticity assumption on the attention layer, we
can significantly simplify the verification of this condition, providing a
non-constructive approach in establishing UAP for such architectures. We
demonstrate the applicability of our framework by proving UAP for transformers
with various attention mechanisms, including kernel-based and sparse attention
mechanisms. The corollaries of our results either generalize prior works or
establish UAP for architectures not previously covered. Furthermore, our
framework offers a principled foundation for designing novel transformer
architectures with inherent UAP guarantees, including those with specific
functional symmetries. We propose examples to illustrate these insights.

</details>


### [125] [Transition Matching: Scalable and Flexible Generative Modeling](https://arxiv.org/abs/2506.23589)
*Neta Shaul,Uriel Singer,Itai Gat,Yaron Lipman*

Main category: cs.LG

TL;DR: The paper introduces Transition Matching (TM), a new generative paradigm unifying diffusion/flow models and continuous autoregressive (AR) generation, offering flexibility and improved performance in media generation.


<details>
  <summary>Details</summary>
Motivation: Current diffusion/flow models have limited design space for further improvements, while continuous AR models show promise for unifying text and media generation. TM aims to bridge and advance both approaches.

Method: TM decomposes generation tasks into Markov transitions, allowing expressive transition kernels and flexible supervision. Three variants are explored: DTM (generalizes flow matching), ARTM (partially causal), and FHTM (fully causal).

Result: DTM achieves state-of-the-art image quality and text adherence with efficient sampling. ARTM and FHTM match or surpass non-causal AR and flow-based methods, with FHTM excelling in text-to-image tasks.

Conclusion: TM offers a unified, flexible framework for generative tasks, outperforming existing methods in quality and efficiency, and enabling seamless integration with AR text generation.

Abstract: Diffusion and flow matching models have significantly advanced media
generation, yet their design space is well-explored, somewhat limiting further
improvements. Concurrently, autoregressive (AR) models, particularly those
generating continuous tokens, have emerged as a promising direction for
unifying text and media generation. This paper introduces Transition Matching
(TM), a novel discrete-time, continuous-state generative paradigm that unifies
and advances both diffusion/flow models and continuous AR generation. TM
decomposes complex generation tasks into simpler Markov transitions, allowing
for expressive non-deterministic probability transition kernels and arbitrary
non-continuous supervision processes, thereby unlocking new flexible design
avenues. We explore these choices through three TM variants: (i) Difference
Transition Matching (DTM), which generalizes flow matching to discrete-time by
directly learning transition probabilities, yielding state-of-the-art image
quality and text adherence as well as improved sampling efficiency. (ii)
Autoregressive Transition Matching (ARTM) and (iii) Full History Transition
Matching (FHTM) are partially and fully causal models, respectively, that
generalize continuous AR methods. They achieve continuous causal AR generation
quality comparable to non-causal approaches and potentially enable seamless
integration with existing AR text generation techniques. Notably, FHTM is the
first fully causal model to match or surpass the performance of flow-based
methods on text-to-image task in continuous domains. We demonstrate these
contributions through a rigorous large-scale comparison of TM variants and
relevant baselines, maintaining a fixed architecture, training data, and
hyperparameters.

</details>


### [126] [When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series](https://arxiv.org/abs/2506.23596)
*Min-Yeong Park,Won-Jeong Lee,Seong Tae Kim,Gyeong-Moon Park*

Main category: cs.LG

TL;DR: The paper introduces A2P, a framework for Anomaly Prediction (AP) in time series data, combining Anomaly-Aware Forecasting and Synthetic Anomaly Prompting to predict future anomalies.


<details>
  <summary>Details</summary>
Motivation: Existing methods for time series data fail to predict future anomalies, focusing only on immediate ones. The paper addresses this gap.

Method: Proposes A2P with Anomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP), including a learnable Anomaly Prompt Pool (APP) for robust anomaly detection.

Result: A2P outperforms state-of-the-art methods in predicting future anomalies, validated on multiple real-world datasets.

Conclusion: A2P effectively addresses the AP task, offering a novel solution for predicting future anomalies in time series data.

Abstract: Recently, forecasting future abnormal events has emerged as an important
scenario to tackle real-world necessities. However, the solution of predicting
specific future time points when anomalies will occur, known as Anomaly
Prediction (AP), remains under-explored. Existing methods dealing with time
series data fail in AP, focusing only on immediate anomalies or failing to
provide precise predictions for future anomalies. To address the AP task, we
propose a novel framework called Anomaly to Prompt (A2P), comprised of
Anomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To
enable the forecasting model to forecast abnormal time points, we adopt a
strategy to learn the relationships of anomalies. For the robust detection of
anomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP)
that simulates diverse anomaly patterns using signal adaptive prompt.
Comprehensive experiments on multiple real-world datasets demonstrate the
superiority of A2P over state-of-the-art methods, showcasing its ability to
predict future anomalies. Our implementation code is available at
https://github.com/KU-VGI/AP.

</details>


### [127] [A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data](https://arxiv.org/abs/2506.23629)
*Xin Liao,Bing Yang,Cai Yu*

Main category: cs.LG

TL;DR: The paper proposes a Nonlinear Low-rank Representation model (NLR) with CNNs to impute missing Water Quality Data (WQD), outperforming traditional methods by capturing temporal and nonlinear features.


<details>
  <summary>Details</summary>
Motivation: Missing WQD due to sensor failures and communication delays leads to High-Dimensional and Sparse data, which traditional methods fail to handle effectively.

Method: The NLR model uses CNNs to fuse temporal features and extract nonlinear interactions for deep data fusion.

Result: Experiments on real datasets show NLR significantly improves imputation accuracy over state-of-the-art methods.

Conclusion: The NLR model provides an effective solution for WQD imputation in dynamic environments.

Abstract: The integrity of Water Quality Data (WQD) is critical in environmental
monitoring for scientific decision-making and ecological protection. However,
water quality monitoring systems are often challenged by large amounts of
missing data due to unavoidable problems such as sensor failures and
communication delays, which further lead to water quality data becoming
High-Dimensional and Sparse (HDS). Traditional data imputation methods are
difficult to depict the potential dynamics and fail to capture the deep data
features, resulting in unsatisfactory imputation performance. To effectively
address the above issues, this paper proposes a Nonlinear Low-rank
Representation model (NLR) with Convolutional Neural Networks (CNN) for
imputing missing WQD, which utilizes CNNs to implement two ideas: a) fusing
temporal features to model the temporal dependence of data between time slots,
and b) Extracting nonlinear interactions and local patterns to mine
higher-order relationships features and achieve deep fusion of multidimensional
information. Experimental studies on three real water quality datasets
demonstrate that the proposed model significantly outperforms existing
state-of-the-art data imputation models in terms of estimation accuracy. It
provides an effective approach for handling water quality monitoring data in
complex dynamic environments.

</details>


### [128] [Learning Modular Exponentiation with Transformers](https://arxiv.org/abs/2506.23679)
*David Demitri Africa,Sara M. Kapoor,Theo Simon Sorg*

Main category: cs.LG

TL;DR: The paper explores modular exponentiation in Transformers, revealing specialized circuits and grokking-like dynamics for arithmetic tasks.


<details>
  <summary>Details</summary>
Motivation: To understand how Transformer models learn and perform modular exponentiation, a key operation in cryptography and number theory.

Method: Train a 4-layer encoder-decoder Transformer, analyze embeddings with PCA, use activation patching, and study reciprocal operand training.

Result: Reciprocal operand training boosts performance, showing grokking-like generalization. A subgraph of attention heads in the final layer suffices for regular exponentiation.

Conclusion: Transformers learn modular arithmetic via specialized circuits, offering insights for interpretable and efficient neural approaches.

Abstract: Modular exponentiation is crucial to number theory and cryptography, yet
remains largely unexplored from a mechanistic interpretability standpoint. We
train a 4-layer encoder-decoder Transformer model to perform this operation and
investigate the emergence of numerical reasoning during training. Utilizing
principled sampling strategies, PCA-based embedding analysis, and activation
patching, we examine how number-theoretic properties are encoded within the
model. We find that reciprocal operand training leads to strong performance
gains, with sudden generalization across related moduli. These synchronized
accuracy surges reflect grokking-like dynamics, suggesting the model
internalizes shared arithmetic structure. We also find a subgraph consisting
entirely of attention heads in the final layer sufficient to achieve full
performance on the task of regular exponentiation. These results suggest that
transformer models learn modular arithmetic through specialized computational
circuits, paving the way for more interpretable and efficient neural approaches
to modular exponentiation.

</details>


### [129] [DABstep: Data Agent Benchmark for Multi-step Reasoning](https://arxiv.org/abs/2506.23719)
*Alex Egg,Martin Iglesias Goyanes,Friso Kingma,Andreu Mora,Leandro von Werra,Thomas Wolf*

Main category: cs.LG

TL;DR: DABstep is a new benchmark for evaluating AI agents on multi-step data analysis tasks, featuring 450 real-world challenges. It tests capabilities like data manipulation and contextual reasoning, with automatic scoring. Leading LLMs perform poorly, with only 14.55% accuracy on hard tasks.


<details>
  <summary>Details</summary>
Motivation: To create a realistic benchmark for assessing AI agents' abilities in multi-step data analysis, addressing gaps in current evaluation methods.

Method: DABstep includes 450 tasks from financial analytics, requiring code-based processing and contextual reasoning. Tasks are scored automatically with factoid-style answers.

Result: Leading LLM-based agents perform poorly, with the best achieving only 14.55% accuracy on the hardest tasks.

Conclusion: DABstep provides a valuable tool for advancing research in autonomous data analysis, highlighting significant performance gaps in current AI agents.

Abstract: We introduce DABstep, a novel benchmark for evaluating AI agents on realistic
multi-step data analysis tasks. DABstep comprises over 450 real-world
challenges derived from a financial analytics platform, requiring models to
combine code-based data processing with contextual reasoning over heterogeneous
documentation. Each task demands an iterative, multi-step problem-solving
approach, testing capabilities in data manipulation, cross-referencing multiple
sources, and precise result reporting. The benchmark provides a factoid-style
answer format with automatic correctness checks for objective scoring at scale.
We evaluate leading LLM-based agents, revealing a substantial performance gap:
even the best agent achieves only 14.55% accuracy on the hardest tasks. We
detail our benchmark's design, dataset composition, task formulation,
evaluation protocol, report baseline results and analyze failure modes. DABstep
is released with a public leaderboard and toolkit to accelerate research in
autonomous data analysis.

</details>


### [130] [System-Embedded Diffusion Bridge Models](https://arxiv.org/abs/2506.23726)
*Bartlomiej Sobieski,Matthew Tivnan,Yuang Wang,Siyeop Yoon,Pengfei Jin,Dufan Wu,Quanzheng Li,Przemyslaw Biecek*

Main category: cs.LG

TL;DR: SDBs integrate known linear measurement systems into matrix-valued SDEs, improving performance in linear inverse problems and enhancing robustness under system misspecification.


<details>
  <summary>Details</summary>
Motivation: To address the gap in supervised bridge methods that overlook structural information of measurement models, while leveraging the power of SGMs for inverse problems.

Method: Introduces System embedded Diffusion Bridge Models (SDBs), embedding known linear measurement systems into matrix-valued SDE coefficients.

Result: Consistent improvements in diverse linear inverse problems and robust generalization under system misspecification.

Conclusion: SDBs offer a promising solution for real-world inverse problem applications by integrating structural information effectively.

Abstract: Solving inverse problems -- recovering signals from incomplete or noisy
measurements -- is fundamental in science and engineering. Score-based
generative models (SGMs) have recently emerged as a powerful framework for this
task. Two main paradigms have formed: unsupervised approaches that adapt
pretrained generative models to inverse problems, and supervised bridge methods
that train stochastic processes conditioned on paired clean and corrupted data.
While the former typically assume knowledge of the measurement model, the
latter have largely overlooked this structural information. We introduce System
embedded Diffusion Bridge Models (SDBs), a new class of supervised bridge
methods that explicitly embed the known linear measurement system into the
coefficients of a matrix-valued SDE. This principled integration yields
consistent improvements across diverse linear inverse problems and demonstrates
robust generalization under system misspecification between training and
deployment, offering a promising solution to real-world applications.

</details>


### [131] [Radioactive Watermarks in Diffusion and Autoregressive Image Generative Models](https://arxiv.org/abs/2506.23731)
*Michel Meintz,Jan Dubiński,Franziska Boenisch,Adam Dziedzic*

Main category: cs.LG

TL;DR: The paper analyzes watermark persistence (radioactivity) in diffusion and autoregressive models, proposes a new watermarking method for autoregressive models, and demonstrates its effectiveness.


<details>
  <summary>Details</summary>
Motivation: Training image generative models requires costly datasets, and unauthorized use of generated images is a concern. Watermarking can detect misuse, but existing methods lack radioactivity (persistence through model training).

Method: Analyzes watermark radioactivity in diffusion and autoregressive models, proposes a new watermarking method for autoregressive models inspired by techniques in large language models.

Result: Existing watermarking methods for diffusion models fail to retain radioactivity, while the proposed method for autoregressive models effectively preserves it.

Conclusion: The new watermarking method enables robust provenance tracking and prevents unauthorized use of images generated by autoregressive models.

Abstract: Image generative models have become increasingly popular, but training them
requires large datasets that are costly to collect and curate. To circumvent
these costs, some parties may exploit existing models by using the generated
images as training data for their own models. In general, watermarking is a
valuable tool for detecting unauthorized use of generated images. However, when
these images are used to train a new model, watermarking can only enable
detection if the watermark persists through training and remains identifiable
in the outputs of the newly trained model - a property known as radioactivity.
We analyze the radioactivity of watermarks in images generated by diffusion
models (DMs) and image autoregressive models (IARs). We find that existing
watermarking methods for DMs fail to retain radioactivity, as watermarks are
either erased during encoding into the latent space or lost in the
noising-denoising process (during the training in the latent space). Meanwhile,
despite IARs having recently surpassed DMs in image generation quality and
efficiency, no radioactive watermarking methods have been proposed for them. To
overcome this limitation, we propose the first watermarking method tailored for
IARs and with radioactivity in mind - drawing inspiration from techniques in
large language models (LLMs), which share IARs' autoregressive paradigm. Our
extensive experimental evaluation highlights our method's effectiveness in
preserving radioactivity within IARs, enabling robust provenance tracking, and
preventing unauthorized use of their generated images.

</details>


### [132] [Training of Spiking Neural Networks with Expectation-Propagation](https://arxiv.org/abs/2506.23757)
*Dan Yao,Steve McLaughlin,Yoann Altmann*

Main category: cs.LG

TL;DR: A gradient-free, message-passing framework for training SNNs using Expectation-Propagation, enabling learning of marginal distributions and faster convergence than gradient-based methods.


<details>
  <summary>Details</summary>
Motivation: To unify training for SNNs with discrete/continuous weights and deterministic/stochastic spiking, while marginalizing nuisance parameters.

Method: Expectation-Propagation-based message-passing framework, gradient-free, works with batches of training samples.

Result: Faster convergence in practice compared to gradient-based methods, successful classification and regression results.

Conclusion: Paves the way for efficient training methods for deep Bayesian networks.

Abstract: In this paper, we propose a unifying message-passing framework for training
spiking neural networks (SNNs) using Expectation-Propagation. Our gradient-free
method is capable of learning the marginal distributions of network parameters
and simultaneously marginalizes nuisance parameters, such as the outputs of
hidden layers. This framework allows for the first time, training of discrete
and continuous weights, for deterministic and stochastic spiking networks,
using batches of training samples. Although its convergence is not ensured, the
algorithm converges in practice faster than gradient-based methods, without
requiring a large number of passes through the training data. The
classification and regression results presented pave the way for new efficient
training methods for deep Bayesian networks.

</details>


### [133] [Model-driven Stochastic Trace Clustering](https://arxiv.org/abs/2506.23776)
*Jari Peeperkorn,Johannes De Smedt,Jochen De Weerdt*

Main category: cs.LG

TL;DR: A novel model-driven trace clustering method optimizes stochastic process models using entropic relevance for better interpretability and performance.


<details>
  <summary>Details</summary>
Motivation: High variability in process discovery leads to complex models; existing clustering techniques often ignore stochasticity, limiting their ability to capture real-world dynamics.

Method: Proposes a method using entropic relevance, a stochastic conformance metric, to assign traces based on structural alignment and likelihood.

Result: Computationally efficient, scales linearly, and outperforms alternatives in representing process behavior.

Conclusion: The method improves interpretability and reveals performance shifts when stochasticity is considered.

Abstract: Process discovery algorithms automatically extract process models from event
logs, but high variability often results in complex and hard-to-understand
models. To mitigate this issue, trace clustering techniques group process
executions into clusters, each represented by a simpler and more understandable
process model. Model-driven trace clustering improves on this by assigning
traces to clusters based on their conformity to cluster-specific process
models. However, most existing clustering techniques rely on either no process
model discovery, or non-stochastic models, neglecting the frequency or
probability of activities and transitions, thereby limiting their capability to
capture real-world execution dynamics. We propose a novel model-driven trace
clustering method that optimizes stochastic process models within each cluster.
Our approach uses entropic relevance, a stochastic conformance metric based on
directly-follows probabilities, to guide trace assignment. This allows
clustering decisions to consider both structural alignment with a cluster's
process model and the likelihood that a trace originates from a given
stochastic process model. The method is computationally efficient, scales
linearly with input size, and improves model interpretability by producing
clusters with clearer control-flow patterns. Extensive experiments on public
real-life datasets show that our method outperforms existing alternatives in
representing process behavior and reveals how clustering performance rankings
can shift when stochasticity is considered.

</details>


### [134] [Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling](https://arxiv.org/abs/2506.23782)
*Xiaoyang Li,Linwei Tao,Haohui Lu,Minjing Dong,Junbin Gao,Chang Xu*

Main category: cs.LG

TL;DR: WATS is a post-hoc calibration framework for GNNs that uses graph wavelet features to improve confidence estimates, outperforming existing methods in calibration error and variance.


<details>
  <summary>Details</summary>
Motivation: GNNs often misalign confidence estimates with actual correctness, limiting their use in safety-critical applications. Existing methods rely on coarse statistics, ignoring fine-grained graph topology.

Method: WATS assigns node-specific temperatures using heat-kernel graph wavelet features, refining confidence without retraining or neighbor data.

Result: WATS achieves the lowest ECE, outperforming baselines by up to 42.3%, and reduces calibration variance by 17.24% on average. It scales efficiently across diverse graphs.

Conclusion: WATS effectively addresses GNN calibration issues by leveraging graph wavelets, offering improved performance and scalability without additional training.

Abstract: Graph Neural Networks (GNNs) have demonstrated strong predictive performance
on relational data; however, their confidence estimates often misalign with
actual predictive correctness, posing significant limitations for deployment in
safety-critical settings. While existing graph-aware calibration methods seek
to mitigate this limitation, they primarily depend on coarse one-hop
statistics, such as neighbor-predicted confidence, or latent node embeddings,
thereby neglecting the fine-grained structural heterogeneity inherent in graph
topology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a
post-hoc calibration framework that assigns node-specific temperatures based on
tunable heat-kernel graph wavelet features. Specifically, WATS harnesses the
scalability and topology sensitivity of graph wavelets to refine confidence
estimates, all without necessitating model retraining or access to neighboring
logits or predictions. Extensive evaluations across seven benchmark datasets
with varying graph structures and two GNN backbones demonstrate that WATS
achieves the lowest Expected Calibration Error (ECE) among all compared
methods, outperforming both classical and graph-specific baselines by up to
42.3\% in ECE and reducing calibration variance by 17.24\% on average compared
with graph-specific methods. Moreover, WATS remains computationally efficient,
scaling well across graphs of diverse sizes and densities. Code will be
released based on publication.

</details>


### [135] [KAIROS: Scalable Model-Agnostic Data Valuation](https://arxiv.org/abs/2506.23799)
*Jiongli Zhu,Parjanya Prajakta Prashant,Alex Cloninger,Babak Salimi*

Main category: cs.LG

TL;DR: KAIROS is a scalable, model-agnostic framework for data valuation, outperforming existing methods in accuracy and efficiency by using MMD-based influence scores without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing data valuation methods are biased, costly, or inaccurate, necessitating a scalable and reliable alternative.

Method: KAIROS assigns distributional influence scores using MMD between training and reference sets, avoiding retraining and enabling online updates.

Result: KAIROS outperforms baselines in accuracy and runtime, with rigorous theoretical guarantees and practical efficiency.

Conclusion: KAIROS provides a scalable, accurate, and efficient solution for data valuation, addressing limitations of current methods.

Abstract: Training data increasingly shapes not only model accuracy but also regulatory
compliance and market valuation of AI assets. Yet existing valuation methods
remain inadequate: model-based techniques depend on a single fitted model and
inherit its biases, while algorithm-based approaches such as Data Shapley
require costly retrainings at web scale. Recent Wasserstein-based
model-agnostic methods rely on approximations that misrank examples relative to
their true leave-one-out (LOO) utility. We introduce KAIROS, a scalable,
model-agnostic valuation framework that assigns each example a distributional
influence score: its contribution to the Maximum Mean Discrepancy (MMD) between
the empirical training distribution and a clean reference set. Unlike
Wasserstein surrogates, our MMD-based influence admits a closed-form solution
that faithfully approximates the exact LOO ranking within $O(1/N^2)$ error,
requires no retraining, and naturally extends to conditional kernels for
unified label- and feature-error detection. Moreover, KAIROS supports efficient
online updates: when a new batch of size m arrives, all scores can be updated
in $O(mN)$ time, delivering up to 50x speedup without compromising ranking
quality. Empirical evaluations on noise, mislabeling, and poisoning benchmarks
show that KAIROS consistently outperforms state-of-the-art model-, Shapley-,
and Wasserstein-based baselines in both accuracy and runtime. We provide
rigorous theoretical guarantees, including symmetry for reproducible rankings
and density-separation for interpretable thresholds.

</details>


### [136] [Towards the Training of Deeper Predictive Coding Neural Networks](https://arxiv.org/abs/2506.23800)
*Chang Qi,Matteo Forasassi,Thomas Lukasiewicz,Tommaso Salvatori*

Main category: cs.LG

TL;DR: The paper addresses performance degradation in deep predictive coding networks by re-balancing energy distribution and improving weight updates, achieving results comparable to backpropagation.


<details>
  <summary>Details</summary>
Motivation: Performance drops in deep predictive coding networks (beyond 5-7 layers) due to imbalanced errors and ineffective guidance from previous layers.

Method: Introduces precision-weighting for latent variables and a novel weight update mechanism to reduce error accumulation in deeper layers.

Result: Improved test accuracy in deep networks (>7 layers), matching backpropagation performance on similar models.

Conclusion: Better understanding of the relaxation phase is key for scaling equilibrium propagation, enabling its use in complex tasks.

Abstract: Predictive coding networks trained with equilibrium propagation are neural
models that perform inference through an iterative energy minimization process.
Previous studies have demonstrated their effectiveness in shallow
architectures, but show significant performance degradation when depth exceeds
five to seven layers. In this work, we show that the reason behind this
degradation is due to exponentially imbalanced errors between layers during
weight updates, and predictions from the previous layer not being effective in
guiding updates in deeper layers. We address the first issue by introducing two
novel methods to optimize the latent variables that use precision-weighting to
re-balance the distribution of energy among layers during the `relaxation
phase', and the second issue by proposing a novel weight update mechanism that
reduces error accumulation in deeper layers. Empirically, we test our methods
on a large number of image classification tasks, resulting in large
improvements in test accuracy across networks with more than seven layers, with
performances comparable to those of backprop on similar models. These findings
suggest that a better understanding of the relaxation phase is important to
train models using equilibrium propagation at scale, and open new possibilities
for their application in complex tasks.

</details>


### [137] [Adaptive Out-of-Control Point Pattern Detection in Sequential Random Finite Set Observations](https://arxiv.org/abs/2506.23802)
*Konstantinos Bourazas,Savvas Papaioannou,Panayiotis Kolios*

Main category: cs.LG

TL;DR: A novel adaptive anomaly detection framework for sequential RFS observations, distinguishing normal and anomalous data by detecting deviations from expected behavior.


<details>
  <summary>Details</summary>
Motivation: To monitor sequential RFS observations and accurately identify anomalies by learning normal behavior online and adapting to shifts.

Method: Develops an RFS-based framework with Power Discounting Posteriors (PD) for dynamic adaptation and anomaly detection via a predictive posterior density function.

Result: Demonstrated effectiveness through extensive qualitative and quantitative simulations.

Conclusion: The proposed framework successfully adapts to behavioral shifts and detects anomalies in point pattern data.

Abstract: In this work we introduce a novel adaptive anomaly detection framework
specifically designed for monitoring sequential random finite set (RFS)
observations. Our approach effectively distinguishes between In-Control data
(normal) and Out-Of-Control data (anomalies) by detecting deviations from the
expected statistical behavior of the process. The primary contributions of this
study include the development of an innovative RFS-based framework that not
only learns the normal behavior of the data-generating process online but also
dynamically adapts to behavioral shifts to accurately identify abnormal point
patterns. To achieve this, we introduce a new class of RFS-based posterior
distributions, named Power Discounting Posteriors (PD), which facilitate
adaptation to systematic changes in data while enabling anomaly detection of
point pattern data through a novel predictive posterior density function. The
effectiveness of the proposed approach is demonstrated by extensive qualitative
and quantitative simulation experiments.

</details>


### [138] [SGD with Adaptive Preconditioning: Unified Analysis and Momentum Acceleration](https://arxiv.org/abs/2506.23803)
*Dmitry Kovalev*

Main category: cs.LG

TL;DR: The paper revisits SGD with AdaGrad-type preconditioning, unifying convergence analysis and connecting Scion and DASGO, while proving acceleration with Nesterov momentum.


<details>
  <summary>Details</summary>
Motivation: To provide a unified theoretical framework for adaptive gradient methods and explore their acceleration with momentum.

Method: Develops a unified convergence analysis under anisotropic smoothness and noise, linking Scion and DASGO, and applies Nesterov momentum.

Result: Recovers state-of-the-art convergence results, connects Scion/DASGO, and shows acceleration beyond known rates for AdaGrad/DASGO.

Conclusion: AdaGrad-type methods can benefit from both preconditioning and momentum, explaining Adam's practical efficiency.

Abstract: In this paper, we revisit stochastic gradient descent (SGD) with AdaGrad-type
preconditioning. Our contributions are twofold. First, we develop a unified
convergence analysis of SGD with adaptive preconditioning under anisotropic or
matrix smoothness and noise assumptions. This allows us to recover
state-of-the-art convergence results for several popular adaptive gradient
methods, including AdaGrad-Norm, AdaGrad, and ASGO/One-sided Shampoo. In
addition, we establish the fundamental connection between two recently proposed
algorithms, Scion and DASGO, and provide the first theoretical guarantees for
the latter. Second, we show that the convergence of methods like AdaGrad and
DASGO can be provably accelerated beyond the best-known rates using Nesterov
momentum. Consequently, we obtain the first theoretical justification that
AdaGrad-type algorithms can simultaneously benefit from both diagonal
preconditioning and momentum, which may provide an ultimate explanation for the
practical efficiency of Adam.

</details>


### [139] [Supercm: Revisiting Clustering for Semi-Supervised Learning](https://arxiv.org/abs/2506.23824)
*Durgesh Singh,Ahcene Boubekki,Robert Jenssen,Michael C. Kampffmeyer*

Main category: cs.LG

TL;DR: A novel semi-supervised learning approach using differentiable clustering to improve performance and simplicity.


<details>
  <summary>Details</summary>
Motivation: Current SSL methods rely on complex training strategies; this work simplifies SSL by incorporating clustering assumptions.

Method: Extends a differentiable clustering module, using annotated data to guide cluster centroids for an end-to-end trainable model.

Result: Outperforms supervised-only baselines and enhances other SSL methods when combined.

Conclusion: The proposed framework offers a simpler, effective SSL solution with potential for broader application.

Abstract: The development of semi-supervised learning (SSL) has in recent years largely
focused on the development of new consistency regularization or entropy
minimization approaches, often resulting in models with complex training
strategies to obtain the desired results. In this work, we instead propose a
novel approach that explicitly incorporates the underlying clustering
assumption in SSL through extending a recently proposed differentiable
clustering module. Leveraging annotated data to guide the cluster centroids
results in a simple end-to-end trainable deep SSL approach. We demonstrate that
the proposed model improves the performance over the supervised-only baseline
and show that our framework can be used in conjunction with other SSL methods
to further boost their performance.

</details>


### [140] [EFPI: Elastic Formation and Position Identification in Football (Soccer) using Template Matching and Linear Assignment](https://arxiv.org/abs/2506.23843)
*Joris Bekkers*

Main category: cs.LG

TL;DR: The paper introduces EFPI, a method for recognizing football team formations and assigning player positions using predefined templates and cost minimization from tracking data.


<details>
  <summary>Details</summary>
Motivation: Understanding team formations and player positioning is key for tactical analysis in football.

Method: EFPI uses linear sum assignment to match players to template formations by minimizing distance costs, scales player positions to template dimensions, and includes a stability parameter to avoid unnecessary changes.

Result: The method effectively identifies formations for individual frames or larger game segments and is available as open-source code.

Conclusion: EFPI provides a flexible and accurate approach for formation recognition in football, with potential applications in tactical analysis.

Abstract: Understanding team formations and player positioning is crucial for tactical
analysis in football (soccer). This paper presents a flexible method for
formation recognition and player position assignment in football using
predefined static formation templates and cost minimization from spatiotemporal
tracking data, called EFPI. Our approach employs linear sum assignment to
optimally match players to positions within a set of template formations by
minimizing the total distance between actual player locations and template
positions, subsequently selecting the formation with the lowest assignment
cost. To improve accuracy, we scale actual player positions to match the
dimensions of these formation templates in both width and length. While the
method functions effectively on individual frames, it extends naturally to
larger game segments such as complete periods, possession sequences or specific
intervals (e.g. 10 second intervals, 5 minute intervals etc.). Additionally, we
incorporate an optional stability parameter that prevents unnecessary formation
changes when assignment costs differ only marginally between time segments.
EFPI is available as open-source code through the unravelsports Python package.

</details>


### [141] [Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts](https://arxiv.org/abs/2506.23845)
*Kenny Peng,Rajiv Movva,Jon Kleinberg,Emma Pierson,Nikhil Garg*

Main category: cs.LG

TL;DR: SAEs are less effective for known concepts but powerful for discovering unknown ones, clarifying their mixed results and suggesting applications in ML interpretability and social/health sciences.


<details>
  <summary>Details</summary>
Motivation: Reconcile competing narratives about the usefulness of sparse autoencoders (SAEs) by distinguishing their effectiveness for known vs. unknown concepts.

Method: Conceptual distinction between SAEs' roles in acting on known concepts and discovering unknown ones.

Result: SAEs are powerful for discovering unknown concepts, separating existing negative and positive results.

Conclusion: SAEs have promising applications in ML interpretability, fairness, and social/health sciences.

Abstract: While sparse autoencoders (SAEs) have generated significant excitement, a
series of negative results have added to skepticism about their usefulness.
Here, we establish a conceptual distinction that reconciles competing
narratives surrounding SAEs. We argue that while SAEs may be less effective for
acting on known concepts, SAEs are powerful tools for discovering unknown
concepts. This distinction cleanly separates existing negative and positive
results, and suggests several classes of SAE applications. Specifically, we
outline use cases for SAEs in (i) ML interpretability, explainability,
fairness, auditing, and safety, and (ii) social and health sciences.

</details>


### [142] [When Plants Respond: Electrophysiology and Machine Learning for Green Monitoring Systems](https://arxiv.org/abs/2506.23872)
*Eduard Buss,Till Aust,Heiko Hamann*

Main category: cs.LG

TL;DR: The paper explores using plants as natural sensors for environmental monitoring, integrating them with biohybrid systems. A wearable device (PhytoNode) records plant electrophysiology, and AutoML achieves high classification accuracy.


<details>
  <summary>Details</summary>
Motivation: To leverage plants as natural sensors for real-world environmental monitoring and precision agriculture, integrating them with artificial devices.

Method: Equipped *Hedera helix* with PhytoNode to record electrophysiological activity outdoors. Used AutoML for data analysis, comparing it with manual tuning.

Result: Achieved macro F1 scores up to 95% in binary tasks, with AutoML outperforming manual tuning. Feature selection further improved accuracy.

Conclusion: Demonstrates a scalable, self-sustaining biohybrid system for plant-integrated environmental monitoring, advancing sustainable solutions.

Abstract: Living plants, while contributing to ecological balance and climate
regulation, also function as natural sensors capable of transmitting
information about their internal physiological states and surrounding
conditions. This rich source of data provides potential for applications in
environmental monitoring and precision agriculture. With integration into
biohybrid systems, we establish novel channels of physiological signal flow
between living plants and artificial devices. We equipped *Hedera helix* with a
plant-wearable device called PhytoNode to continuously record the plant's
electrophysiological activity. We deployed plants in an uncontrolled outdoor
environment to map electrophysiological patterns to environmental conditions.
Over five months, we collected data that we analyzed using state-of-the-art and
automated machine learning (AutoML). Our classification models achieve high
performance, reaching macro F1 scores of up to 95 percent in binary tasks.
AutoML approaches outperformed manual tuning, and selecting subsets of
statistical features further improved accuracy. Our biohybrid living system
monitors the electrophysiology of plants in harsh, real-world conditions. This
work advances scalable, self-sustaining, and plant-integrated living biohybrid
systems for sustainable environmental monitoring.

</details>


### [143] [Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic](https://arxiv.org/abs/2506.23875)
*Yuta Sato,Kazuhiko Kawamoto,Hiroshi Kera*

Main category: cs.LG

TL;DR: The paper introduces a method to reorder decoder input tokens in Transformers to improve learning of arithmetic tasks by identifying learning-friendly sequences.


<details>
  <summary>Details</summary>
Motivation: The order of intermediate steps in reasoning (chain of thought) affects the difficulty of learning in Transformers, but the search space for optimal orders is vast.

Method: A pipeline trains a Transformer on mixed-order sequences, identifies benign orders via early loss drops, and uses a hierarchical approach for efficient reordering.

Result: The method successfully identifies learning-friendly orders from billions of candidates, even recovering known optimal orders like reverse-digit for multiplication.

Conclusion: Reordering decoder inputs can significantly improve Transformer learning for arithmetic tasks, with the proposed method efficiently navigating the large search space.

Abstract: The chain of thought is fundamental in Transformers, which is to perform
step-by-step reasoning. Besides what intermediate steps work, the order of
these steps critically affects the difficulty of the reasoning. This study
addresses a novel task of unraveling chain of thought - reordering decoder
input tokens to a learning-friendly sequence for Transformers to learn
arithmetic tasks. The proposed pipeline first trains a Transformer on a mixture
of target sequences arranged in different orders and then identifies benign
orders as those with fast loss drops in the early stage. As the search space
grows factorially with sequence length, we propose a two-stage hierarchical
approach for inter- and intra-block reordering. Experiments on four
order-sensitive arithmetic tasks show that our method identifies a
learning-friendly order out of a few billion candidates. Notably, on the
multiplication task, it recovered the reverse-digit order reported in prior
studies.

</details>


### [144] [Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion System](https://arxiv.org/abs/2506.23923)
*Miguel Camacho-Sánchez,Fernando García-Torres,Jesper John Lisegaard,Rocío del Amor,Sankhya Mohanty,Valery Naranjo*

Main category: cs.LG

TL;DR: The paper proposes a reinforcement learning (RL) strategy using Proximal Policy Optimisation (PPO) to control resin flow dynamics in resin infusion (RI) and resin transfer moulding (RTM) processes, ensuring uniform impregnation and improved product quality.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of managing resin flow dynamics in composites manufacturing, particularly for large-scale applications like wind turbine blades, to prevent defects like porosities and dry spots.

Method: A reinforcement learning (RL) approach, specifically Proximal Policy Optimisation (PPO), is used to synchronize resin flow fronts in a scenario with two inlets and one outlet, leveraging process simulations.

Result: The RL strategy effectively achieves accurate flow convergence, demonstrating its potential for enhancing process control and product quality.

Conclusion: The RL-based approach shows promise for improving resin infusion processes in composites manufacturing, ensuring better structural integrity of final components.

Abstract: Resin infusion (RI) and resin transfer moulding (RTM) are critical processes
for the manufacturing of high-performance fibre-reinforced polymer composites,
particularly for large-scale applications such as wind turbine blades.
Controlling the resin flow dynamics in these processes is critical to ensure
the uniform impregnation of the fibre reinforcements, thereby preventing
residual porosities and dry spots that impact the consequent structural
integrity of the final component. This paper presents a reinforcement learning
(RL) based strategy, established using process simulations, for synchronising
the different resin flow fronts in an infusion scenario involving two resin
inlets and a single outlet. Using Proximal Policy Optimisation (PPO), our
approach addresses the challenge of managing the fluid dynamics in a partially
observable environment. The results demonstrate the effectiveness of the RL
approach in achieving an accurate flow convergence, highlighting its potential
towards improving process control and product quality in composites
manufacturing.

</details>


### [145] [Bridging the Gap with Retrieval-Augmented Generation: Making Prosthetic Device User Manuals Available in Marginalised Languages](https://arxiv.org/abs/2506.23958)
*Ikechukwu Ogbonna,Lesley Davidson,Soumya Banerjee,Abhishek Dasgupta,Laurence Kenney,Vikranth Harthikote Nagaraja*

Main category: cs.LG

TL;DR: An AI-powered framework translates complex medical documents into marginalized languages, addressing healthcare access barriers in African communities.


<details>
  <summary>Details</summary>
Motivation: Millions in Africa face healthcare access issues due to language and literacy gaps, especially with donated prosthetic devices lacking accessible documentation.

Method: The system uses a Retrieval-Augmented Generation (RAG) pipeline and NLP models for real-time translation and question-answering in local languages.

Result: The framework successfully translates English medical manuals into accessible formats, like Pidgin, enabling informed healthcare decisions.

Conclusion: This open-source solution bridges language gaps in healthcare, scalable to other languages and dialects.

Abstract: Millions of people in African countries face barriers to accessing healthcare
due to language and literacy gaps. This research tackles this challenge by
transforming complex medical documents -- in this case, prosthetic device user
manuals -- into accessible formats for underserved populations. This case study
in cross-cultural translation is particularly pertinent/relevant for
communities that receive donated prosthetic devices but may not receive the
accompanying user documentation. Or, if available online, may only be available
in formats (e.g., language and readability) that are inaccessible to local
populations (e.g., English-language, high resource settings/cultural context).
The approach is demonstrated using the widely spoken Pidgin dialect, but our
open-source framework has been designed to enable rapid and easy extension to
other languages/dialects. This work presents an AI-powered framework designed
to process and translate complex medical documents, e.g., user manuals for
prosthetic devices, into marginalised languages. The system enables users --
such as healthcare workers or patients -- to upload English-language medical
equipment manuals, pose questions in their native language, and receive
accurate, localised answers in real time. Technically, the system integrates a
Retrieval-Augmented Generation (RAG) pipeline for processing and semantic
understanding of the uploaded manuals. It then employs advanced Natural
Language Processing (NLP) models for generative question-answering and
multilingual translation. Beyond simple translation, it ensures accessibility
to device instructions, treatment protocols, and safety information, empowering
patients and clinicians to make informed healthcare decisions.

</details>


### [146] [LLM Agents Are the Antidote to Walled Gardens](https://arxiv.org/abs/2506.23978)
*Samuele Marro,Philip Torr*

Main category: cs.LG

TL;DR: LLM-based agents enable universal interoperability, disrupting closed platforms and promoting data portability, but require frameworks to address security risks.


<details>
  <summary>Details</summary>
Motivation: The dominance of closed, proprietary platforms limits data exchange and interoperability, reinforcing monopolistic behaviors.

Method: Proposes using LLM-based agents to automate data translation and interface interaction, making interoperability cost-effective and inevitable.

Result: Universal interoperability undermines monopolies and enhances data portability but introduces security risks and technical debt.

Conclusion: The ML community should adopt this shift while developing frameworks to mitigate risks, leveraging AI to restore user freedom and market competition.

Abstract: While the Internet's core infrastructure was designed to be open and
universal, today's application layer is dominated by closed, proprietary
platforms. Open and interoperable APIs require significant investment, and
market leaders have little incentive to enable data exchange that could erode
their user lock-in. We argue that LLM-based agents fundamentally disrupt this
status quo. Agents can automatically translate between data formats and
interact with interfaces designed for humans: this makes interoperability
dramatically cheaper and effectively unavoidable. We name this shift universal
interoperability: the ability for any two digital services to exchange data
seamlessly using AI-mediated adapters. Universal interoperability undermines
monopolistic behaviours and promotes data portability. However, it can also
lead to new security risks and technical debt. Our position is that the ML
community should embrace this development while building the appropriate
frameworks to mitigate the downsides. By acting now, we can harness AI to
restore user freedom and competitive markets without sacrificing security.

</details>


### [147] [ADReFT: Adaptive Decision Repair for Safe Autonomous Driving via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.23960)
*Mingfei Cheng,Xiaofei Xie,Renzhi Wang,Yuan Zhou,Ming Hu*

Main category: cs.LG

TL;DR: ADReFT is a novel repair method for Autonomous Driving Systems (ADSs) that improves safety by identifying critical states and generating adaptive repair actions using a transformer-based model.


<details>
  <summary>Details</summary>
Motivation: Existing online repair solutions for ADSs lack generalizability and adaptability, often being overly conservative and ineffective in mitigating safety risks.

Method: ADReFT uses a transformer-based model with State Monitor and Decision Adapter heads, pretrained with supervised learning and finetuned with reinforcement learning to generate adaptive repair actions.

Result: ADReFT achieves better repair performance compared to existing methods.

Conclusion: ADReFT effectively enhances ADS safety by providing adaptive and precise repair actions, overcoming limitations of current approaches.

Abstract: Autonomous Driving Systems (ADSs) continue to face safety-critical risks due
to the inherent limitations in their design and performance capabilities.
Online repair plays a crucial role in mitigating such limitations, ensuring the
runtime safety and reliability of ADSs. Existing online repair solutions
enforce ADS compliance by transforming unacceptable trajectories into
acceptable ones based on predefined specifications, such as rule-based
constraints or training datasets. However, these approaches often lack
generalizability, adaptability and tend to be overly conservative, resulting in
ineffective repairs that not only fail to mitigate safety risks sufficiently
but also degrade the overall driving experience. To address this issue, we
propose Adaptive Decision Repair (ADReFT), a novel and effective repair method
that identifies safety-critical states through offline learning from failed
tests and generates appropriate mitigation actions to improve ADS safety.
Specifically, ADReFT incorporates a transformer-based model with two joint
heads, State Monitor and Decision Adapter, designed to capture complex driving
environment interactions to evaluate state safety severity and generate
adaptive repair actions. Given the absence of oracles for state safety
identification, we first pretrain ADReFT using supervised learning with coarse
annotations, i.e., labeling states preceding violations as positive samples and
others as negative samples. It establishes ADReFT's foundational capability to
mitigate safety-critical violations, though it may result in somewhat
conservative mitigation strategies. Therefore, we subsequently finetune ADReFT
using reinforcement learning to improve its initial capability and generate
more precise and contextually appropriate repair decisions. Our evaluation
results illustrate that ADReFT achieves better repair performance.

</details>


### [148] [UMA: A Family of Universal Models for Atoms](https://arxiv.org/abs/2506.23971)
*Brandon M. Wood,Misko Dzamba,Xiang Fu,Meng Gao,Muhammed Shuaibi,Luis Barroso-Luque,Kareem Abdelmaqsoud,Vahe Gharakhanyan,John R. Kitchin,Daniel S. Levine,Kyle Michel,Anuroop Sriram,Taco Cohen,Abhishek Das,Ammar Rizvi,Sushree Jagriti Sahoo,Zachary W. Ulissi,C. Lawrence Zitnick*

Main category: cs.LG

TL;DR: Meta FAIR introduces Universal Models for Atoms (UMA), a family of AI models trained on 500M atomic structures, achieving high speed, accuracy, and generalization across chemical domains without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The need for fast and accurate atomic property computation in chemistry and materials science applications like drug discovery and energy storage drives the development of UMA.

Method: UMA models are trained on a vast dataset of 3D atomic structures using empirical scaling laws and a novel 'mixture of linear experts' architecture to balance capacity and speed.

Result: UMA models outperform specialized models in diverse applications without fine-tuning, with UMA-medium using only ~50M active parameters per structure despite having 1.4B total parameters.

Conclusion: UMA's release of code, weights, and data aims to advance computational workflows and foster further AI model development in the field.

Abstract: The ability to quickly and accurately compute properties from atomic
simulations is critical for advancing a large number of applications in
chemistry and materials science including drug discovery, energy storage, and
semiconductor manufacturing. To address this need, Meta FAIR presents a family
of Universal Models for Atoms (UMA), designed to push the frontier of speed,
accuracy, and generalization. UMA models are trained on half a billion unique
3D atomic structures (the largest training runs to date) by compiling data
across multiple chemical domains, e.g. molecules, materials, and catalysts. We
develop empirical scaling laws to help understand how to increase model
capacity alongside dataset size to achieve the best accuracy. The UMA small and
medium models utilize a novel architectural design we refer to as mixture of
linear experts that enables increasing model capacity without sacrificing
speed. For example, UMA-medium has 1.4B parameters but only ~50M active
parameters per atomic structure. We evaluate UMA models on a diverse set of
applications across multiple domains and find that, remarkably, a single model
without any fine-tuning can perform similarly or better than specialized
models. We are releasing the UMA code, weights, and associated data to
accelerate computational workflows and enable the community to continue to
build increasingly capable AI models.

</details>


### [149] [A Scalable Approach for Safe and Robust Learning via Lipschitz-Constrained Networks](https://arxiv.org/abs/2506.23977)
*Zain ul Abdeen,Vassilis Kekatos,Ming Jin*

Main category: cs.LG

TL;DR: A convex training framework for neural networks ensures certified robustness via semidefinite relaxation and loop transformation, improving scalability with a randomized subspace approach.


<details>
  <summary>Details</summary>
Motivation: Certified robustness is essential for safety-critical applications, but existing methods suffer from non-convexity and poor scalability due to global semidefinite programs.

Method: Proposes a convex training framework using semidefinite relaxation and loop transformation, with a randomized subspace approach (RS-LMI) to enhance scalability.

Result: Achieves competitive accuracy on MNIST, CIFAR-10, and ImageNet with improved Lipschitz bounds and runtime performance.

Conclusion: The framework provides a scalable and certifiable solution for training robust neural networks.

Abstract: Certified robustness is a critical property for deploying neural networks
(NN) in safety-critical applications. A principle approach to achieving such
guarantees is to constrain the global Lipschitz constant of the network.
However, accurate methods for Lipschitz-constrained training often suffer from
non-convex formulations and poor scalability due to reliance on global
semidefinite programs (SDPs). In this letter, we propose a convex training
framework that enforces global Lipschitz constraints via semidefinite
relaxation. By reparameterizing the NN using loop transformation, we derive a
convex admissibility condition that enables tractable and certifiable training.
While the resulting formulation guarantees robustness, its scalability is
limited by the size of global SDP. To overcome this, we develop a randomized
subspace linear matrix inequalities (RS-LMI) approach that decomposes the
global constraints into sketched layerwise constraints projected onto
low-dimensional subspaces, yielding a smooth and memory-efficient training
objective. Empirical results on MNIST, CIFAR-10, and ImageNet demonstrate that
the proposed framework achieves competitive accuracy with significantly
improved Lipschitz bounds and runtime performance.

</details>


### [150] [The Jacobian and Hessian of the Kullback-Leibler Divergence between Multivariate Gaussian Distributions (Technical Report)](https://arxiv.org/abs/2506.23996)
*Juan Maroñas*

Main category: cs.LG

TL;DR: Derivation of Jacobian and Hessian matrices for Kullback-Leibler divergence between multivariate Gaussians using differentials.


<details>
  <summary>Details</summary>
Motivation: To provide a clear, didactic explanation of the derivations for Jacobian and Hessian matrices in this context.

Method: Uses first and second-order differentials, inspired by existing theory and derivations.

Result: Detailed derivations and summary of results for the matrices.

Conclusion: The document successfully breaks down complex derivations into understandable steps with references to underlying concepts.

Abstract: This document shows how to obtain the Jacobian and Hessian matrices of the
Kullback-Leibler divergence between two multivariate Gaussian distributions,
using the first and second-order differentials. The presented derivations are
based on the theory presented by \cite{magnus99}. I've also got great
inspiration from some of the derivations in \cite{minka}.
  Since I pretend to be at most didactic, the document is split into a summary
of results and detailed derivations on each of the elements involved, with
specific references to the tricks used in the derivations, and to many of the
underlying concepts.

</details>


### [151] [The Illusion of Progress? A Critical Look at Test-Time Adaptation for Vision-Language Models](https://arxiv.org/abs/2506.24000)
*Lijun Sheng,Jian Liang,Ran He,Zilei Wang,Tieniu Tan*

Main category: cs.LG

TL;DR: TTA-VLM is a benchmark for evaluating test-time adaptation (TTA) methods on vision-language models (VLMs), addressing limitations like inconsistent evaluations and lack of comprehensive metrics. It includes 15 datasets, extends beyond CLIP to SigLIP, and assesses robustness, calibration, and more. Findings show limited gains from TTA methods, poor collaboration with fine-tuning, and trade-offs in model trustworthiness.


<details>
  <summary>Details</summary>
Motivation: Current TTA research lacks fair comparisons due to duplicated results, limited metrics, and inconsistent settings. TTA-VLM aims to standardize evaluation and provide a holistic assessment of TTA methods for VLMs.

Method: TTA-VLM implements 8 episodic and 7 online TTA methods in a unified framework, evaluates them on 15 datasets, and includes training-time tuning methods. Metrics cover accuracy, robustness, calibration, out-of-distribution detection, and stability.

Result: Key findings: 1) TTA methods offer limited improvements over prior work; 2) they poorly collaborate with fine-tuning methods; 3) accuracy gains often reduce model trustworthiness.

Conclusion: TTA-VLM provides a fair, comprehensive benchmark for TTA methods, encouraging development of more reliable and generalizable strategies.

Abstract: Test-time adaptation (TTA) methods have gained significant attention for
enhancing the performance of vision-language models (VLMs) such as CLIP during
inference, without requiring additional labeled data. However, current TTA
researches generally suffer from major limitations such as duplication of
baseline results, limited evaluation metrics, inconsistent experimental
settings, and insufficient analysis. These problems hinder fair comparisons
between TTA methods and obscure their practical strengths and weaknesses. To
address these challenges, we introduce TTA-VLM, a comprehensive benchmark for
evaluating TTA methods on VLMs. Our benchmark implements 8 episodic TTA and 7
online TTA methods within a unified and reproducible framework, and evaluates
them across 15 widely used datasets. Unlike prior studies focused solely on
CLIP, we extend the evaluation to SigLIP--a model trained with a Sigmoid
loss--and include training-time tuning methods such as CoOp, MaPLe, and TeCoA
to assess generality. Beyond classification accuracy, TTA-VLM incorporates
various evaluation metrics, including robustness, calibration,
out-of-distribution detection, and stability, enabling a more holistic
assessment of TTA methods. Through extensive experiments, we find that 1)
existing TTA methods produce limited gains compared to the previous pioneering
work; 2) current TTA methods exhibit poor collaboration with training-time
fine-tuning methods; 3) accuracy gains frequently come at the cost of reduced
model trustworthiness. We release TTA-VLM to provide fair comparison and
comprehensive evaluation of TTA methods for VLMs, and we hope it encourages the
community to develop more reliable and generalizable TTA strategies.

</details>


### [152] [Provably Efficient and Agile Randomized Q-Learning](https://arxiv.org/abs/2506.24005)
*He Wang,Xingyu Xu,Yuejie Chi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While Bayesian-based exploration often demonstrates superior empirical
performance compared to bonus-based methods in model-based reinforcement
learning (RL), its theoretical understanding remains limited for model-free
settings. Existing provable algorithms either suffer from computational
intractability or rely on stage-wise policy updates which reduce responsiveness
and slow down the learning process. In this paper, we propose a novel variant
of Q-learning algorithm, refereed to as RandomizedQ, which integrates
sampling-based exploration with agile, step-wise, policy updates, for episodic
tabular RL. We establish an $\widetilde{O}(\sqrt{H^5SAT})$ regret bound, where
$S$ is the number of states, $A$ is the number of actions, $H$ is the episode
length, and $T$ is the total number of episodes. In addition, we present a
logarithmic regret bound under a mild positive sub-optimality condition on the
optimal Q-function. Empirically, RandomizedQ exhibits outstanding performance
compared to existing Q-learning variants with both bonus-based and
Bayesian-based exploration on standard benchmarks.

</details>


### [153] [Bridging Theory and Practice in Link Representation with Graph Neural Networks](https://arxiv.org/abs/2506.24018)
*Veronica Lachi,Francesco Ferrini,Antonio Longa,Bruno Lepri,Andrea Passerini,Manfred Jaeger*

Main category: cs.LG

TL;DR: The paper studies GNN expressiveness in link representation, introduces a unifying framework for comparison, and shows expressive models outperform simpler ones in high-symmetry scenarios.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in theoretical understanding of GNNs for link-level tasks, focusing on expressiveness and practical impact.

Method: Introduces the $k_\phi$-$k_\rho$-$m$ framework to compare link models, derives a hierarchy of methods, and proposes a synthetic benchmark for evaluation.

Result: Expressive models underperform on standard benchmarks but excel in high-symmetry scenarios, emphasizing dataset-aware selection.

Conclusion: The study provides tools for analyzing link-level GNN expressiveness and highlights the importance of model selection based on graph symmetry.

Abstract: Graph Neural Networks (GNNs) are widely used to compute representations of
node pairs for downstream tasks such as link prediction. Yet, theoretical
understanding of their expressive power has focused almost entirely on
graph-level representations. In this work, we shift the focus to links and
provide the first comprehensive study of GNN expressiveness in link
representation. We introduce a unifying framework, the $k_\phi$-$k_\rho$-$m$
framework, that subsumes existing message-passing link models and enables
formal expressiveness comparisons. Using this framework, we derive a hierarchy
of state-of-the-art methods and offer theoretical tools to analyze future
architectures. To complement our analysis, we propose a synthetic evaluation
protocol comprising the first benchmark specifically designed to assess
link-level expressiveness. Finally, we ask: does expressiveness matter in
practice? We use a graph symmetry metric that quantifies the difficulty of
distinguishing links and show that while expressive models may underperform on
standard benchmarks, they significantly outperform simpler ones as symmetry
increases, highlighting the need for dataset-aware model selection.

</details>


### [154] [Faster Diffusion Models via Higher-Order Approximation](https://arxiv.org/abs/2506.24042)
*Gen Li,Yuchen Zhou,Yuting Wei,Yuxin Chen*

Main category: cs.LG

TL;DR: A training-free sampling algorithm for diffusion models accelerates approximation of target data distributions without retraining, using high-order ODE solvers and Lagrange interpolation.


<details>
  <summary>Details</summary>
Motivation: To achieve provable acceleration in diffusion models without retraining, addressing limitations like smoothness or log-concavity assumptions.

Method: Proposes a principled, training-free algorithm using high-order ODE solvers, Lagrange interpolation, and successive refinement for score function evaluations.

Result: The algorithm requires only order of $d^{1+2/K} \varepsilon^{-1/K}$ score evaluations, robust to inexact score estimation and applicable to broad data distributions.

Conclusion: The method provides efficient, robust acceleration for diffusion models, eliminating the need for restrictive assumptions or retraining.

Abstract: In this paper, we explore provable acceleration of diffusion models without
any additional retraining. Focusing on the task of approximating a target data
distribution in $\mathbb{R}^d$ to within $\varepsilon$ total-variation
distance, we propose a principled, training-free sampling algorithm that
requires only the order of
  $$ d^{1+2/K} \varepsilon^{-1/K} $$
  score function evaluations (up to log factor) in the presence of accurate
scores, where $K$ is an arbitrarily large fixed integer. This result applies to
a broad class of target data distributions, without the need for assumptions
such as smoothness or log-concavity. Our theory is robust vis-a-vis inexact
score estimation, degrading gracefully as the score estimation error increases
-- without demanding higher-order smoothness on the score estimates as assumed
in previous work. The proposed algorithm draws insight from high-order ODE
solvers, leveraging high-order Lagrange interpolation and successive refinement
to approximate the integral derived from the probability flow ODE.

</details>


### [155] [Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies](https://arxiv.org/abs/2506.24093)
*Paul Wachter,Lukas Niehaus,Julius Schöning*

Main category: cs.LG

TL;DR: The paper evaluates mixed training strategies using synthetic and real data to bridge the domain gap in ANN training, analyzing their generalizability and robustness across tasks and architectures.


<details>
  <summary>Details</summary>
Motivation: The disparity between synthetic and real data causes poor ANN performance in real-world scenarios, prompting the need to evaluate mixed training strategies systematically.

Method: The study analyzes two mixing strategies on three architectures and three hybrid datasets, varying synthetic-to-real data proportions.

Result: The findings offer insights into optimizing synthetic data use in ANN training to improve robustness and efficacy.

Conclusion: The study contributes to better understanding and enhancing the effectiveness of synthetic data in ANN training.

Abstract: Synthetic data has emerged as a cost-effective alternative to real data for
training artificial neural networks (ANN). However, the disparity between
synthetic and real data results in a domain gap. That gap leads to poor
performance and generalization of the trained ANN when applied to real-world
scenarios. Several strategies have been developed to bridge this gap, which
combine synthetic and real data, known as mixed training using hybrid datasets.
While these strategies have been shown to mitigate the domain gap, a systematic
evaluation of their generalizability and robustness across various tasks and
architectures remains underexplored. To address this challenge, our study
comprehensively analyzes two widely used mixing strategies on three prevalent
architectures and three distinct hybrid datasets. From these datasets, we
sample subsets with varying proportions of synthetic to real data to
investigate the impact of synthetic and real components. The findings of this
paper provide valuable insights into optimizing the use of synthetic data in
the training process of any ANN, contributing to enhancing robustness and
efficacy.

</details>


### [156] [Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime](https://arxiv.org/abs/2506.24120)
*Yuqing Wang,Shangding Gu*

Main category: cs.LG

TL;DR: Selecting uniformly distributed data improves training efficiency and performance in LLMs by maximizing pairwise distance between data points.


<details>
  <summary>Details</summary>
Motivation: To identify general principles of data selection that enhance performance, especially for complex tasks with limited prior knowledge.

Method: Theoretical analysis shows uniform data distribution increases pairwise distance, improving GD dynamics and approximation error. Experiments validate this across various settings.

Result: Uniform data selection accelerates training and achieves comparable or better performance in LLMs.

Conclusion: Maximizing pairwise distance in data selection is a general principle that enhances training efficiency and model performance.

Abstract: Data selection plays a crucial role in data-driven decision-making, including
in large language models (LLMs), and is typically task-dependent. Properties
such as data quality and diversity have been extensively studied and are known
to enhance model performance. However, it remains unclear whether there exist
other quantitative and general principles of data selection that can
consistently improve performance, especially for complex tasks with limited
prior knowledge. In this paper, we demonstrate that selecting more uniformly
distributed data can improve training efficiency while enhancing performance.
Specifically, we establish that more uniform (less biased) distribution leads
to a larger minimum pairwise distance between data points, denoted by
$h_{\min}$, and prove that a smaller $h_{\min}$ can slow down the training
dynamics of gradient descent (GD). Moreover, we theoretically show that the
approximation error of neural networks decreases as $h_{\min}$ increases. Our
analysis introduces a convergence framework for GD beyond the Neural Tangent
Kernel (NTK) regime, applicable to a broad class of architectures, including
transformers, without requiring Lipschitz smoothness. This framework further
provides theoretical justification for the use of residual connections and
function compositions in deep neural architectures. In the end, we conduct
comprehensive experiments for supervised fine-tuning across various settings,
including different optimization strategies, model sizes, and training
datasets. The results consistently demonstrate that selecting data by
maximizing pairwise distance significantly accelerates training and achieves
comparable or better performance in LLMs across diverse datasets. Code and
Datasets are available at the link:
https://github.com/SafeRL-Lab/data-uniformity.

</details>


### [157] [Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives](https://arxiv.org/abs/2506.24124)
*Dong Sixun,Fan Wei,Teresa Wu,Fu Yanjie*

Main category: cs.LG

TL;DR: A multimodal contrastive learning framework enhances time series forecasting by aligning visual and textual representations derived from numerical sequences, outperforming unimodal and cross-modal baselines.


<details>
  <summary>Details</summary>
Motivation: Traditional unimodal numerical inputs for time series forecasting fail to capture high-level semantic patterns, while text-based methods lack perceptual intuition. This paper addresses these limitations by proposing a multimodal approach.

Method: The framework transforms raw time series into structured visual and textual perspectives, aligns them via contrastive learning, and uses a variate selection module for informative variable identification.

Result: The approach outperforms unimodal and cross-modal baselines on fifteen short-term and six long-term forecasting benchmarks.

Conclusion: Multimodal alignment significantly enhances time series forecasting, demonstrating the effectiveness of the proposed framework.

Abstract: Time series forecasting traditionally relies on unimodal numerical inputs,
which often struggle to capture high-level semantic patterns due to their dense
and unstructured nature. While recent approaches have explored representing
time series as text using large language models (LLMs), these methods remain
limited by the discrete nature of token sequences and lack the perceptual
intuition humans typically apply, such as interpreting visual patterns. In this
paper, we propose a multimodal contrastive learning framework that transforms
raw time series into structured visual and textual perspectives. Rather than
using natural language or real-world images, we construct both modalities
directly from numerical sequences. We then align these views in a shared
semantic space via contrastive learning, enabling the model to capture richer
and more complementary representations. Furthermore, we introduce a variate
selection module that leverages the aligned representations to identify the
most informative variables for multivariate forecasting. Extensive experiments
on fifteen short-term and six long-term forecasting benchmarks demonstrate that
our approach consistently outperforms strong unimodal and cross-modal
baselines, highlighting the effectiveness of multimodal alignment in enhancing
time series forecasting. Code is available at:
https://github.com/Ironieser/TimesCLIP.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [158] [Robust Perspective Correction for Real-World Crack Evolution Tracking in Image-Based Structural Health Monitoring](https://arxiv.org/abs/2506.22437)
*Xinxin Sun,Peter Chang*

Main category: cs.CV

TL;DR: A physics-informed alignment framework improves crack localization in structural health monitoring by adapting KAZE architecture, outperforming traditional methods like SIFT and SURF.


<details>
  <summary>Details</summary>
Motivation: Traditional feature detectors (e.g., SIFT, SURF) and lightweight alternatives (e.g., ORB, BRISK) are inadequate for thin crack localization due to high-frequency edge suppression or poor repeatability.

Method: The framework uses nonlinear anisotropic diffusion for crack-preserving scale space and RANSAC-based homography estimation for geometric correction, requiring no training or tuning.

Result: The method reduces crack area and spine length errors by up to 70% and 90%, respectively, with sub-5% alignment error in key metrics.

Conclusion: The approach is robust, interpretable, and lightweight, suitable for scalable deployment in real-world SHM applications.

Abstract: Accurate image alignment is essential for monitoring crack evolution in
structural health monitoring (SHM), particularly under real-world conditions
involving perspective distortion, occlusion, and low contrast. However,
traditional feature detectors such as SIFT and SURF, which rely on
Gaussian-based scale spaces, tend to suppress high-frequency edges, making them
unsuitable for thin crack localization. Lightweight binary alternatives like
ORB and BRISK, while computationally efficient, often suffer from poor keypoint
repeatability on textured or shadowed surfaces. This study presents a
physics-informed alignment framework that adapts the open KAZE architecture to
SHM-specific challenges. By utilizing nonlinear anisotropic diffusion to
construct a crack-preserving scale space, and integrating RANSAC-based
homography estimation, the framework enables accurate geometric correction
without the need for training, parameter tuning, or prior calibration. The
method is validated on time-lapse images of masonry and concrete acquired via
handheld smartphone under varied field conditions, including shadow
interference, cropping, oblique viewing angles, and surface clutter. Compared
to classical detectors, the proposed framework reduces crack area and spine
length errors by up to 70 percent and 90 percent, respectively, while
maintaining sub-5 percent alignment error in key metrics. Unsupervised,
interpretable, and computationally lightweight, this approach supports scalable
deployment via UAVs and mobile platforms. By tailoring nonlinear scale-space
modeling to SHM image alignment, this work offers a robust and physically
grounded alternative to conventional techniques for tracking real-world crack
evolution.

</details>


### [159] [Counting with Confidence: Accurate Pest Monitoring in Water Traps](https://arxiv.org/abs/2506.22438)
*Xumin Gao,Mark Stevens,Grzegorz Cielniak*

Main category: cs.CV

TL;DR: The paper proposes a method to evaluate pest counting confidence by combining counting results and environmental factors, improving accuracy over baseline methods.


<details>
  <summary>Details</summary>
Motivation: Existing pest counting models lack reliability assessment in real-world deployments due to missing ground truth. The study aims to address this gap.

Method: Uses a pest detection network, image quality/complexity assessments, and pest distribution uniformity analysis. Includes a multi-factor sensitivity analysis and adaptive DBSCAN clustering.

Result: Reduces MSE by 31.7% and improves R2 by 15.2% on pest counting confidence compared to baseline.

Conclusion: The study is the first to comprehensively evaluate counting confidence, providing a reliable model for precision agriculture.

Abstract: Accurate pest population monitoring and tracking their dynamic changes are
crucial for precision agriculture decision-making. A common limitation in
existing vision-based automatic pest counting research is that models are
typically evaluated on datasets with ground truth but deployed in real-world
scenarios without assessing the reliability of counting results due to the lack
of ground truth. To this end, this paper proposed a method for comprehensively
evaluating pest counting confidence in the image, based on information related
to counting results and external environmental conditions. First, a pest
detection network is used for pest detection and counting, extracting counting
result-related information. Then, the pest images undergo image quality
assessment, image complexity assessment, and pest distribution uniformity
assessment. And the changes in image clarity caused by stirring during image
acquisition are quantified by calculating the average gradient magnitude.
Notably, we designed a hypothesis-driven multi-factor sensitivity analysis
method to select the optimal image quality assessment and image complexity
assessment methods. And we proposed an adaptive DBSCAN clustering algorithm for
pest distribution uniformity assessment. Finally, the obtained information
related to counting results and external environmental conditions is input into
a regression model for prediction, resulting in the final pest counting
confidence. To the best of our knowledge, this is the first study dedicated to
comprehensively evaluating counting confidence in counting tasks, and
quantifying the relationship between influencing factors and counting
confidence through a model. Experimental results show our method reduces MSE by
31.7% and improves R2 by 15.2% on the pest counting confidence test set,
compared to the baseline built primarily on information related to counting
results.

</details>


### [160] [Modulated Diffusion: Accelerating Generative Modeling with Modulated Quantization](https://arxiv.org/abs/2506.22463)
*Weizhi Gao,Zhichao Hou,Junqi Yin,Feiyi Wang,Linyu Peng,Xiaorui Liu*

Main category: cs.CV

TL;DR: MoDiff accelerates diffusion models via modulated quantization and error compensation, reducing activation quantization to 3 bits without performance loss.


<details>
  <summary>Details</summary>
Motivation: High computation cost in iterative sampling of diffusion models is a bottleneck; existing acceleration techniques have limitations in error and quality.

Method: Introduces Modulated Diffusion (MoDiff), a framework combining modulated quantization and error compensation.

Result: Reduces activation quantization from 8 to 3 bits without performance degradation, validated on CIFAR-10 and LSUN datasets.

Conclusion: MoDiff is a principled, general framework for accelerating diffusion models, supported by theory and experiments.

Abstract: Diffusion models have emerged as powerful generative models, but their high
computation cost in iterative sampling remains a significant bottleneck. In
this work, we present an in-depth and insightful study of state-of-the-art
acceleration techniques for diffusion models, including caching and
quantization, revealing their limitations in computation error and generation
quality. To break these limits, this work introduces Modulated Diffusion
(MoDiff), an innovative, rigorous, and principled framework that accelerates
generative modeling through modulated quantization and error compensation.
MoDiff not only inherents the advantages of existing caching and quantization
methods but also serves as a general framework to accelerate all diffusion
models. The advantages of MoDiff are supported by solid theoretical insight and
analysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate
that MoDiff significant reduces activation quantization from 8 bits to 3 bits
without performance degradation in post-training quantization (PTQ). Our code
implementation is available at https://github.com/WeizhiGao/MoDiff.

</details>


### [161] [ViFusionTST: Deep Fusion of Time-Series Image Representations from Load Signals for Early Bed-Exit Prediction](https://arxiv.org/abs/2506.22498)
*Hao Liu,Yu Hu,Rakiba Rayhana,Ling Bai,Zheng Liu*

Main category: cs.CV

TL;DR: A method using low-cost load cells and image-based fusion predicts bed-exit intent early, outperforming baselines in accuracy and F1 score for fall prevention.


<details>
  <summary>Details</summary>
Motivation: Bed-related falls are a major injury source in healthcare; existing alarms often trigger too late.

Method: Uses four load cells under bed legs, converts signals into images (RGB line plot and texture maps), and processes them with ViFusionTST, a dual-stream Swin Transformer.

Result: Achieves 0.885 accuracy and 0.794 F1 score on real-world data, surpassing other time-series methods.

Conclusion: Image-based fusion of load-sensor signals is effective for real-time, privacy-preserving fall prevention.

Abstract: Bed-related falls remain a leading source of injury in hospitals and
long-term-care facilities, yet many commercial alarms trigger only after a
patient has already left the bed. We show that early bed-exit intent can be
predicted using only four low-cost load cells mounted under the bed legs. The
resulting load signals are first converted into a compact set of complementary
images: an RGB line plot that preserves raw waveforms and three texture maps -
recurrence plot, Markov transition field, and Gramian angular field - that
expose higher-order dynamics. We introduce ViFusionTST, a dual-stream Swin
Transformer that processes the line plot and texture maps in parallel and fuses
them through cross-attention to learn data-driven modality weights.
  To provide a realistic benchmark, we collected six months of continuous data
from 95 beds in a long-term-care facility. On this real-world dataset
ViFusionTST reaches an accuracy of 0.885 and an F1 score of 0.794, surpassing
recent 1D and 2D time-series baselines across F1, recall, accuracy, and AUPRC.
The results demonstrate that image-based fusion of load-sensor signals for time
series classification is a practical and effective solution for real-time,
privacy-preserving fall prevention.

</details>


### [162] [Scalable Dynamic Origin-Destination Demand Estimation Enhanced by High-Resolution Satellite Imagery Data](https://arxiv.org/abs/2506.22499)
*Jiachao Liu,Pablo Guarda,Koichiro Niinuma,Sean Qian*

Main category: cs.CV

TL;DR: A novel framework integrates satellite imagery with local sensor data for dynamic origin-destination demand estimation (DODE) in multi-class mesoscopic networks, improving accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of sparse local sensors by leveraging city-wide satellite imagery for consistent traffic data.

Method: Develops a computer vision pipeline for vehicle detection and map matching, then formulates a computational graph-based DODE model to calibrate network states.

Result: Out-of-sample tests show improved estimation performance, especially for unsensed links, and real-world experiments confirm scalability.

Conclusion: The framework enhances DODE accuracy and scalability, making it viable for practical deployment in cities of varying sizes.

Abstract: This study presents a novel integrated framework for dynamic
origin-destination demand estimation (DODE) in multi-class mesoscopic network
models, leveraging high-resolution satellite imagery together with conventional
traffic data from local sensors. Unlike sparse local detectors, satellite
imagery offers consistent, city-wide road and traffic information of both
parking and moving vehicles, overcoming data availability limitations. To
extract information from imagery data, we design a computer vision pipeline for
class-specific vehicle detection and map matching, generating link-level
traffic density observations by vehicle class. Building upon this information,
we formulate a computational graph-based DODE model that calibrates dynamic
network states by jointly matching observed traffic counts and travel times
from local sensors with density measurements derived from satellite imagery. To
assess the accuracy and scalability of the proposed framework, we conduct a
series of numerical experiments using both synthetic and real-world data. The
results of out-of-sample tests demonstrate that supplementing traditional data
with satellite-derived density significantly improves estimation performance,
especially for links without local sensors. Real-world experiments also confirm
the framework's capability to handle large-scale networks, supporting its
potential for practical deployment in cities of varying sizes. Sensitivity
analysis further evaluates the impact of data quality related to satellite
imagery data.

</details>


### [163] [Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models](https://arxiv.org/abs/2506.22500)
*Weiyi Zhao,Xiaoyu Tan,Liang Liu,Sijia Li,Youwei Song,Xihe Qiu*

Main category: cs.CV

TL;DR: The paper introduces a synthetic dataset (OR-VSKC) to address visual-semantic knowledge conflicts in MLLMs for surgical risk detection, improving performance on trained entities but showing limitations on untrained ones.


<details>
  <summary>Details</summary>
Motivation: To improve automated operating room risk detection by addressing visual-semantic knowledge conflicts in multimodal large language models (MLLMs).

Method: Created a dataset of 34,000 synthetic images (and 214 human-annotated ones) depicting safety rule violations, then fine-tuned MLLMs on this dataset.

Result: Fine-tuning improved detection of trained conflict entities and generalised to new viewpoints, but performance on untrained entities remained poor.

Conclusion: The OR-VSKC dataset and methodology help expose and address VS-KC in MLLMs, though comprehensive training is needed for broader applicability.

Abstract: Surgical risk identification is critical for patient safety and reducing
preventable medical errors. While multimodal large language models (MLLMs) show
promise for automated operating room (OR) risk detection, they often exhibit
visual-semantic knowledge conflicts (VS-KC), failing to identify visual safety
violations despite understanding textual rules. To address this, we introduce a
dataset comprising over 34,000 synthetic images generated by diffusion models,
depicting operating room scenes containing entities that violate established
safety rules. These images were created to alleviate data scarcity and examine
MLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated
images that serve as a gold-standard reference for validation. This
comprehensive dataset, spanning diverse perspectives, stages, and
configurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC
significantly improves MLLMs' detection of trained conflict entities and
generalizes well to new viewpoints for these entities, but performance on
untrained entity types remains poor, highlighting learning specificity and the
need for comprehensive training. The main contributions of this work include:
(1) a data generation methodology tailored for rule-violation scenarios; (2)
the release of the OR-VSKC dataset and its associated benchmark as open-source
resources; and (3) an empirical analysis of violation-sensitive knowledge
consistency in representative MLLMs. The dataset and appendix are available at
https://github.com/zgg2577/VS-KC.

</details>


### [164] [How Can Multimodal Remote Sensing Datasets Transform Classification via SpatialNet-ViT?](https://arxiv.org/abs/2506.22501)
*Gautam Siddharth Kashyap,Manaswi Kulahara,Nipun Joshi,Usman Naseem*

Main category: cs.CV

TL;DR: SpatialNet-ViT, a novel model combining Vision Transformers and Multi-Task Learning, improves remote sensing classification accuracy and scalability by integrating spatial awareness and contextual understanding.


<details>
  <summary>Details</summary>
Motivation: Existing studies focus on narrow tasks or datasets, limiting generalization across remote sensing classification challenges.

Method: Proposes SpatialNet-ViT, leveraging Vision Transformers and Multi-Task Learning, with techniques like data augmentation, transfer learning, and multi-task learning.

Result: Enhanced classification accuracy and scalability, with improved robustness and generalization across diverse datasets.

Conclusion: SpatialNet-ViT effectively addresses limitations of narrow-focused studies, offering a versatile solution for remote sensing classification tasks.

Abstract: Remote sensing datasets offer significant promise for tackling key
classification tasks such as land-use categorization, object presence
detection, and rural/urban classification. However, many existing studies tend
to focus on narrow tasks or datasets, which limits their ability to generalize
across various remote sensing classification challenges. To overcome this, we
propose a novel model, SpatialNet-ViT, leveraging the power of Vision
Transformers (ViTs) and Multi-Task Learning (MTL). This integrated approach
combines spatial awareness with contextual understanding, improving both
classification accuracy and scalability. Additionally, techniques like data
augmentation, transfer learning, and multi-task learning are employed to
enhance model robustness and its ability to generalize across diverse datasets

</details>


### [165] [What Makes a Dribble Successful? Insights From 3D Pose Tracking Data](https://arxiv.org/abs/2506.22503)
*Michiel Schepers,Pieter Robberechts,Jan Van Haaren,Jesse Davis*

Main category: cs.CV

TL;DR: The study uses 3D pose tracking data to enhance dribble evaluation in soccer, showing improved predictive performance over traditional 2D methods.


<details>
  <summary>Details</summary>
Motivation: Current 2D positional data lacks depth in capturing dribbling skills like balance and orientation, prompting the need for 3D pose tracking.

Method: Analyzed 1,736 dribbles from the 2022/23 Champions League, extracting pose-based features (e.g., balance, attacker-defender alignment).

Result: Pose-based features, especially balance and orientation alignment, significantly improve dribble success prediction when combined with 2D data.

Conclusion: 3D pose tracking offers deeper insights into dribbling, enhancing performance evaluation beyond traditional 2D methods.

Abstract: Data analysis plays an increasingly important role in soccer, offering new
ways to evaluate individual and team performance. One specific application is
the evaluation of dribbles: one-on-one situations where an attacker attempts to
bypass a defender with the ball. While previous research has primarily relied
on 2D positional tracking data, this fails to capture aspects like balance,
orientation, and ball control, limiting the depth of current insights. This
study explores how pose tracking data (capturing players' posture and movement
in three dimensions) can improve our understanding of dribbling skills. We
extract novel pose-based features from 1,736 dribbles in the 2022/23 Champions
League season and evaluate their impact on dribble success. Our results
indicate that features capturing the attacker's balance and the alignment of
the orientation between the attacker and defender are informative for
predicting dribble success. Incorporating these pose-based features on top of
features derived from traditional 2D positional data leads to a measurable
improvement in model performance.

</details>


### [166] [Patch2Loc: Learning to Localize Patches for Unsupervised Brain Lesion Detection](https://arxiv.org/abs/2506.22504)
*Hassan Baker,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: Patch2Loc is an unsupervised method for detecting brain lesions in MRI by learning from normal patches and identifying abnormalities through prediction errors.


<details>
  <summary>Details</summary>
Motivation: Radiologists need efficient tools for detecting brain abnormalities like tumors. Supervised methods require annotated data, but Patch2Loc offers an unsupervised alternative.

Method: Train a neural network to map normal MRI patches to their spatial locations. Detect abnormalities via higher prediction errors/variance during inference.

Result: Outperforms state-of-the-art unsupervised segmentation on BraTS2021, MSLUB, ATLAS, and WMH datasets.

Conclusion: Patch2Loc is effective for unsupervised brain lesion detection, offering finer-grained segmentation without annotated data.

Abstract: Detecting brain lesions as abnormalities observed in magnetic resonance
imaging (MRI) is essential for diagnosis and treatment. In the search of
abnormalities, such as tumors and malformations, radiologists may benefit from
computer-aided diagnostics that use computer vision systems trained with
machine learning to segment normal tissue from abnormal brain tissue. While
supervised learning methods require annotated lesions, we propose a new
unsupervised approach (Patch2Loc) that learns from normal patches taken from
structural MRI. We train a neural network model to map a patch back to its
spatial location within a slice of the brain volume. During inference, abnormal
patches are detected by the relatively higher error and/or variance of the
location prediction. This generates a heatmap that can be integrated into
pixel-wise methods to achieve finer-grained segmentation. We demonstrate the
ability of our model to segment abnormal brain tissues by applying our approach
to the detection of tumor tissues in MRI on T2-weighted images from BraTS2021
and MSLUB datasets and T1-weighted images from ATLAS and WMH datasets. We show
that it outperforms the state-of-the art in unsupervised segmentation. The
codebase for this work can be found on our
\href{https://github.com/bakerhassan/Patch2Loc}{GitHub page}.

</details>


### [167] [Weakly Supervised Object Segmentation by Background Conditional Divergence](https://arxiv.org/abs/2506.22505)
*Hassan Baker,Matthew S. Emigh,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: The paper proposes a weakly supervised method for binary object segmentation using image-wise labels and counterfactual background images, achieving success in specialized domains like sonar and natural images.


<details>
  <summary>Details</summary>
Motivation: Automatic object segmentation is challenging in specialized domains due to the lack of labeled data. Pixel-wise masks are expensive, so the paper explores weak supervision (image-wise labels) as a more feasible alternative.

Method: The method trains a masking network using weak supervision (image-wise labels). It creates counterfactual images by blending segmented objects into clustered backgrounds and uses divergence and supervised loss for training.

Result: The approach outperforms unsupervised baselines in sonar images and shows reasonable performance in natural images without relying on pretrained or generative networks.

Conclusion: The method is effective for binary segmentation in data-scarce domains, offering a practical alternative to fully supervised approaches.

Abstract: As a computer vision task, automatic object segmentation remains challenging
in specialized image domains without massive labeled data, such as synthetic
aperture sonar images, remote sensing, biomedical imaging, etc. In any domain,
obtaining pixel-wise segmentation masks is expensive. In this work, we propose
a method for training a masking network to perform binary object segmentation
using weak supervision in the form of image-wise presence or absence of an
object of interest, which provides less information but may be obtained more
quickly from manual or automatic labeling. A key step in our method is that the
segmented objects can be placed into background-only images to create
realistic, images of the objects with counterfactual backgrounds. To create a
contrast between the original and counterfactual background images, we propose
to first cluster the background-only images, and then during learning create
counterfactual images that blend objects segmented from their original source
backgrounds to backgrounds chosen from a targeted cluster. One term in the
training loss is the divergence between these counterfactual images and the
real object images with backgrounds of the target cluster. The other term is a
supervised loss for background-only images. While an adversarial critic could
provide the divergence, we use sample-based divergences. We conduct experiments
on side-scan and synthetic aperture sonar in which our approach succeeds
compared to previous unsupervised segmentation baselines that were only tested
on natural images. Furthermore, to show generality we extend our experiments to
natural images, obtaining reasonable performance with our method that avoids
pretrained networks, generative networks, and adversarial critics. The basecode
for this work can be found at
\href{GitHub}{https://github.com/bakerhassan/WSOS}.

</details>


### [168] [FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment](https://arxiv.org/abs/2506.22509)
*Hang Xu,Jie Huang,Linjiang Huang,Dong Li,Yidi Liu,Feng Zhao*

Main category: cs.CV

TL;DR: The paper introduces a training-free Domain Noise Alignment (DNA) method for Diffusion-based Dense Prediction (DDP) models to enhance domain adaptation (DA) by aligning noise statistics between domains.


<details>
  <summary>Details</summary>
Motivation: The exposure bias in diffusion models causes domain shift, and noise prediction statistics can capture domain differences, motivating a training-free DA solution.

Method: Proposes DNA, which aligns noise statistics of target domains with source domains (or high-confidence regions in source-free DA) during diffusion sampling.

Result: Demonstrates effectiveness in enhancing DA for DDP models across four dense prediction tasks.

Conclusion: DNA is a viable training-free approach for DA in DDP frameworks, leveraging noise statistics alignment.

Abstract: Domain Adaptation(DA) for dense prediction tasks is an important topic, which
enhances the dense prediction model's performance when tested on its unseen
domain. Recently, with the development of Diffusion-based Dense Prediction
(DDP) models, the exploration of DA designs tailored to this framework is worth
exploring, since the diffusion model is effective in modeling the distribution
transformation that comprises domain information. In this work, we propose a
training-free mechanism for DDP frameworks, endowing them with DA capabilities.
Our motivation arises from the observation that the exposure bias (e.g., noise
statistics bias) in diffusion brings domain shift, and different domains in
conditions of DDP models can also be effectively captured by the noise
prediction statistics. Based on this, we propose a training-free Domain Noise
Alignment (DNA) approach, which alleviates the variations of noise statistics
to domain changes during the diffusion sampling process, thereby achieving
domain adaptation. Specifically, when the source domain is available, we
directly adopt the DNA method to achieve domain adaptation by aligning the
noise statistics of the target domain with those of the source domain. For the
more challenging source-free DA, inspired by the observation that regions
closer to the source domain exhibit higher confidence meeting variations of
sampling noise, we utilize the statistics from the high-confidence regions
progressively to guide the noise statistic adjustment during the sampling
process. Notably, our method demonstrates the effectiveness of enhancing the DA
capability of DDP models across four common dense prediction tasks. Code is
available at
\href{https://github.com/xuhang07/FreeDNA}{https://github.com/xuhang07/FreeDNA}.

</details>


### [169] [Lightning the Night with Generative Artificial Intelligence](https://arxiv.org/abs/2506.22511)
*Tingting Zhou,Feng Zhang,Haoyang Fu,Baoxiang Pan,Renhe Zhang,Feng Lu,Zhixin Yang*

Main category: cs.CV

TL;DR: The study introduces RefDiff, a generative diffusion model, to retrieve visible light reflectance at night using thermal infrared data from FY4B satellite, improving accuracy and enabling uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: Visible light reflectance data is unavailable at night, limiting continuous weather monitoring. This study aims to overcome this gap using thermal infrared data.

Method: Developed RefDiff, a generative diffusion model, using multi-band thermal infrared brightness temperature data from FY4B's AGRI for nighttime visible light reflectance retrieval.

Result: RefDiff achieves high accuracy (SSIM 0.90), especially in complex cloud areas, and performs comparably to daytime models when validated with VIIRS data.

Conclusion: The research advances nighttime visible light reflectance retrieval, expanding the potential applications of nighttime visible light data.

Abstract: The visible light reflectance data from geostationary satellites is crucial
for meteorological observations and plays an important role in weather
monitoring and forecasting. However, due to the lack of visible light at night,
it is impossible to conduct continuous all-day weather observations using
visible light reflectance data. This study pioneers the use of generative
diffusion models to address this limitation. Based on the multi-band thermal
infrared brightness temperature data from the Advanced Geostationary Radiation
Imager (AGRI) onboard the Fengyun-4B (FY4B) geostationary satellite, we
developed a high-precision visible light reflectance retrieval model, called
Reflectance Diffusion (RefDiff), which enables 0.47~\mu\mathrm{m},
0.65~\mu\mathrm{m}, and 0.825~\mu\mathrm{m} bands visible light reflectance
retrieval at night. Compared to the classical models, RefDiff not only
significantly improves accuracy through ensemble averaging but also provides
uncertainty estimation. Specifically, the SSIM index of RefDiff can reach 0.90,
with particularly significant improvements in areas with complex cloud
structures and thick clouds. The model's nighttime retrieval capability was
validated using VIIRS nighttime product, demonstrating comparable performance
to its daytime counterpart. In summary, this research has made substantial
progress in the ability to retrieve visible light reflectance at night, with
the potential to expand the application of nighttime visible light data.

</details>


### [170] [Automated Defect Identification and Categorization in NDE 4.0 with the Application of Artificial Intelligence](https://arxiv.org/abs/2506.22513)
*Aditya Sharma*

Main category: cs.CV

TL;DR: An automated framework for fault detection in radiography using NDE 4.0, leveraging virtual defect augmentation and a modified U-net model, achieves high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of sufficiently explained information in radiography and explores the potential of virtual defect augmentation for fault detection.

Method: Compiles 223 CR images of airplane welds, uses data augmentation (virtual defect and standard), and trains a modified U-net model for semantic fault segmentation.

Result: Achieves high defect detection awareness (a90/95), efficient processing of large images, and validation by professional controllers.

Conclusion: The framework is viable, efficient, and promising as a support tool in radiography testing, despite equipment and software limitations.

Abstract: This investigation attempts to create an automated framework for fault
detection and organization for usage in contemporary radiography, as per NDE
4.0. The review's goals are to address the lack of information that is
sufficiently explained, learn how to make the most of virtual defect increase,
and determine whether the framework is viable by using NDE measurements. As its
basic information source, the technique consists of compiling and categorizing
223 CR photographs of airplane welds. Information expansion systems, such as
virtual defect increase and standard increase, are used to work on the
preparation dataset. A modified U-net model is prepared using the improved data
to produce semantic fault division veils. To assess the effectiveness of the
model, NDE boundaries such as Case, estimating exactness, and misleading call
rate are used. Tiny a90/95 characteristics, which provide strong
differentiating evidence of flaws, reveal that the suggested approach achieves
exceptional awareness in defect detection. Considering a 90/95, size error, and
fake call rate in the weld area, the consolidated expansion approach clearly
wins. Due to the framework's fast derivation speed, large images can be broken
down efficiently and quickly. Professional controllers evaluate the transmitted
system in the field and believe that it has a guarantee as a support device in
the testing cycle, irrespective of particular equipment cut-off points and
programming resemblance.

</details>


### [171] [Container damage detection using advanced computer vision model Yolov12 vs Yolov11 vs RF-DETR A comparative analysis](https://arxiv.org/abs/2506.22517)
*Subhadip Kumar*

Main category: cs.CV

TL;DR: The paper compares three computer vision models (Yolov11, Yolov12, RF-DETR) for detecting container damage, finding RF-DETR superior for uncommon damages despite lower mAP scores.


<details>
  <summary>Details</summary>
Motivation: Timely detection of container damage is crucial for safety and prolonging service life in logistics.

Method: Three models were trained and tested on 278 annotated images, comparing mAP and precision.

Result: Yolov11 and Yolov12 had higher mAP@50 (81.9%), but RF-DETR outperformed for uncommon damages.

Conclusion: RF-DETR is better for detecting diverse damage types, despite lower overall mAP.

Abstract: Containers are an integral part of the logistics industry and act as a
barrier for cargo. A typical service life for a container is more than 20
years. However, overtime containers suffer various types of damage due to the
mechanical as well as natural factors. A damaged container is a safety hazard
for the employees handling it and a liability for the logistic company.
Therefore, a timely inspection and detection of the damaged container is a key
for prolonging service life as well as avoiding safety hazards. In this paper,
we will compare the performance of the damage detection by three
state-of-the-art advanced computer vision models Yolov12, Yolov11 and RF-DETR.
We will use a dataset of 278 annotated images to train, validate and test the
model. We will compare the mAP and precision of the model. The objective of
this paper is to identify the model that is best suited for container damage
detection. The result is mixed. mAP@50 score of Yolov11 and 12 was 81.9%
compared to RF-DETR, which was 77.7%. However, while testing the model for
not-so-common damaged containers, the RF-DETR model outperformed the others
overall, exhibiting superiority to accurately detecting both damaged containers
as well as damage occurrences with high confidence.

</details>


### [172] [Preserve Anything: Controllable Image Synthesis with Object Preservation](https://arxiv.org/abs/2506.22531)
*Prasen Kumar Sharma,Neeraj Matiyali,Siddharth Srivastava,Gaurav Sharma*

Main category: cs.CV

TL;DR: The paper introduces 	extit{Preserve Anything}, a method for controlled image synthesis that improves object preservation, semantic consistency, and user control in text-to-image generation.


<details>
  <summary>Details</summary>
Motivation: Existing T2I methods struggle with preserving multiple objects, maintaining semantic alignment, and providing explicit scene composition control.

Method: The method uses an N-channel ControlNet with modules for object preservation, background guidance, lighting consistency, and high-frequency detail retention. A benchmark dataset of 240K natural and 18K synthetic images is introduced.

Result: The method achieves state-of-the-art performance (FID 15.26, CLIP-S 32.85) and shows significant improvements in user study metrics (e.g., ~25% better prompt alignment).

Conclusion: 	extit{Preserve Anything} effectively addresses key T2I limitations, offering superior fidelity, control, and semantic alignment.

Abstract: We introduce \textit{Preserve Anything}, a novel method for controlled image
synthesis that addresses key limitations in object preservation and semantic
consistency in text-to-image (T2I) generation. Existing approaches often fail
(i) to preserve multiple objects with fidelity, (ii) maintain semantic
alignment with prompts, or (iii) provide explicit control over scene
composition. To overcome these challenges, the proposed method employs an
N-channel ControlNet that integrates (i) object preservation with size and
placement agnosticism, color and detail retention, and artifact elimination,
(ii) high-resolution, semantically consistent backgrounds with accurate
shadows, lighting, and prompt adherence, and (iii) explicit user control over
background layouts and lighting conditions. Key components of our framework
include object preservation and background guidance modules, enforcing lighting
consistency and a high-frequency overlay module to retain fine details while
mitigating unwanted artifacts. We introduce a benchmark dataset consisting of
240K natural images filtered for aesthetic quality and 18K 3D-rendered
synthetic images with metadata such as lighting, camera angles, and object
relationships. This dataset addresses the deficiencies of existing benchmarks
and allows a complete evaluation. Empirical results demonstrate that our method
achieves state-of-the-art performance, significantly improving feature-space
fidelity (FID 15.26) and semantic alignment (CLIP-S 32.85) while maintaining
competitive aesthetic quality. We also conducted a user study to demonstrate
the efficacy of the proposed work on unseen benchmark and observed a remarkable
improvement of $\sim25\%$, $\sim19\%$, $\sim13\%$, and $\sim14\%$ in terms of
prompt alignment, photorealism, the presence of AI artifacts, and natural
aesthetics over existing works.

</details>


### [173] [Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset](https://arxiv.org/abs/2506.22554)
*Vasu Agrawal,Akinniyi Akinyemi,Kathryn Alvero,Morteza Behrooz,Julia Buffalini,Fabio Maria Carlucci,Joy Chen,Junming Chen,Zhang Chen,Shiyang Cheng,Praveen Chowdary,Joe Chuang,Antony D'Avirro,Jon Daly,Ning Dong,Mark Duppenthaler,Cynthia Gao,Jeff Girard,Martin Gleize,Sahir Gomez,Hongyu Gong,Srivathsan Govindarajan,Brandon Han,Sen He,Denise Hernandez,Yordan Hristov,Rongjie Huang,Hirofumi Inaguma,Somya Jain,Raj Janardhan,Qingyao Jia,Christopher Klaiber,Dejan Kovachev,Moneish Kumar,Hang Li,Yilei Li,Pavel Litvin,Wei Liu,Guangyao Ma,Jing Ma,Martin Ma,Xutai Ma,Lucas Mantovani,Sagar Miglani,Sreyas Mohan,Louis-Philippe Morency,Evonne Ng,Kam-Woh Ng,Tu Anh Nguyen,Amia Oberai,Benjamin Peloquin,Juan Pino,Jovan Popovic,Omid Poursaeed,Fabian Prada,Alice Rakotoarison,Alexander Richard,Christophe Ropers,Safiyyah Saleem,Vasu Sharma,Alex Shcherbyna,Jia Shen,Jie Shen,Anastasis Stathopoulos,Anna Sun,Paden Tomasello,Tuan Tran,Arina Turkatenko,Bo Wan,Chao Wang,Jeff Wang,Mary Williamson,Carleigh Wood,Tao Xiang,Yilin Yang,Julien Yao,Chen Zhang,Jiemin Zhang,Xinyue Zhang,Jason Zheng,Pavlo Zhyzheria,Jan Zikes,Michael Zollhoefer*

Main category: cs.CV

TL;DR: The paper introduces the Seamless Interaction Dataset and models for AI to understand and generate dyadic behavioral dynamics, enhancing human-AI interactions.


<details>
  <summary>Details</summary>
Motivation: To develop socially intelligent AI by comprehending and generating dyadic behavioral dynamics in human communication.

Method: Creation of a large-scale dataset (Seamless Interaction Dataset) and development of models for generating dyadic motion gestures and facial expressions aligned with speech.

Result: Models capable of generating contextually aligned gestures and expressions, with controllable emotional responses and expressivity, improving virtual agents and telepresence.

Conclusion: The work advances intuitive and responsive human-AI interactions, with potential applications in virtual agents and multimodal content analysis.

Abstract: Human communication involves a complex interplay of verbal and nonverbal
signals, essential for conveying meaning and achieving interpersonal goals. To
develop socially intelligent AI technologies, it is crucial to develop models
that can both comprehend and generate dyadic behavioral dynamics. To this end,
we introduce the Seamless Interaction Dataset, a large-scale collection of over
4,000 hours of face-to-face interaction footage from over 4,000 participants in
diverse contexts. This dataset enables the development of AI technologies that
understand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,
telepresence experiences, and multimodal content analysis tools. We also
develop a suite of models that utilize the dataset to generate dyadic motion
gestures and facial expressions aligned with human speech. These models can
take as input both the speech and visual behavior of their interlocutors. We
present a variant with speech from an LLM model and integrations with 2D and 3D
rendering methods, bringing us closer to interactive virtual agents.
Additionally, we describe controllable variants of our motion models that can
adapt emotional responses and expressivity levels, as well as generating more
semantically-relevant gestures. Finally, we discuss methods for assessing the
quality of these dyadic motion models, which are demonstrating the potential
for more intuitive and responsive human-AI interactions.

</details>


### [174] [Recomposed realities: animating still images via patch clustering and randomness](https://arxiv.org/abs/2506.22556)
*Markus Juvonen,Samuli Siltanen*

Main category: cs.CV

TL;DR: A method for animating still images by reconstructing them using patches from datasets, grouped via k-means clustering.


<details>
  <summary>Details</summary>
Motivation: To bring still images to life through motion by leveraging existing image data without strict replication.

Method: Uses k-means clustering to group image patches from datasets, then reconstructs target images by matching and randomly sampling from these clusters.

Result: Enables reinterpretation of source images, allowing conceptual differences between source and target while preserving local structures.

Conclusion: The method successfully animates still images by creatively reusing existing data, emphasizing reinterpretation over replication.

Abstract: We present a patch-based image reconstruction and animation method that uses
existing image data to bring still images to life through motion. Image patches
from curated datasets are grouped using k-means clustering and a new target
image is reconstructed by matching and randomly sampling from these clusters.
This approach emphasizes reinterpretation over replication, allowing the source
and target domains to differ conceptually while sharing local structures.

</details>


### [175] [Improving Token-based Object Detection with Video](https://arxiv.org/abs/2506.22562)
*Abhineet Singh,Nilanjan Ray*

Main category: cs.CV

TL;DR: The paper extends Pix2Seq for video object detection, introducing a method that uses discrete tokens for object representation and outputs 3D boxes or tracklets, improving scalability and performance.


<details>
  <summary>Details</summary>
Motivation: To address limitations of conventional video detectors, such as loss sparsity and heuristics-based postprocessing, by leveraging sequence-based object representation and integrated 3D output.

Method: Represents objects as variable-length sequences of discrete tokens and outputs them as 3D boxes or tracklets, eliminating the need for box sampling and postprocessing.

Result: Shows consistent improvement over Pix2Seq and competitive performance with state-of-the-art video detectors, despite computational bottlenecks.

Conclusion: The proposed method offers a scalable and efficient approach to video object detection, with potential for generalization to multi-object tracking.

Abstract: This paper improves upon the Pix2Seq object detector by extending it for
videos. In the process, it introduces a new way to perform end-to-end video
object detection that improves upon existing video detectors in two key ways.
First, by representing objects as variable-length sequences of discrete tokens,
we can succinctly represent widely varying numbers of video objects, with
diverse shapes and locations, without having to inject any localization cues in
the training process. This eliminates the need to sample the space of all
possible boxes that constrains conventional detectors and thus solves the dual
problems of loss sparsity during training and heuristics-based postprocessing
during inference. Second, it conceptualizes and outputs the video objects as
fully integrated and indivisible 3D boxes or tracklets instead of generating
image-specific 2D boxes and linking these boxes together to construct the video
object, as done in most conventional detectors. This allows it to scale
effortlessly with available computational resources by simply increasing the
length of the video subsequence that the network takes as input, even
generalizing to multi-object tracking if the subsequence can span the entire
video. We compare our video detector with the baseline Pix2Seq static detector
on several datasets and demonstrate consistent improvement, although with
strong signs of being bottlenecked by our limited computational resources. We
also compare it with several video detectors on UA-DETRAC to show that it is
competitive with the current state of the art even with the computational
bottleneck. We make our code and models publicly available.

</details>


### [176] [Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation](https://arxiv.org/abs/2506.22567)
*Shansong Wang,Zhecheng Jin,Mingzhe Hu,Mojtaba Safari,Feng Zhao,Chih-Wei Chang,Richard LJ Qiu,Justin Roper,David S. Yu,Xiaofeng Yang*

Main category: cs.CV

TL;DR: MMKD-CLIP is a biomedical foundation model that distills knowledge from multiple CLIP models to overcome data scarcity and heterogeneity in biomedicine, outperforming teacher models across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: The scarcity of large-scale biomedical image-text corpora and fragmented data standards hinder the development of a unified biomedical foundation model.

Method: MMKD-CLIP uses a two-stage training pipeline: CLIP-style pretraining on 2.9M biomedical image-text pairs, followed by feature-level distillation from nine teacher models using 19.2M feature pairs.

Result: MMKD-CLIP outperforms all teacher models on 58 datasets across six task types, showing robustness and generalization.

Conclusion: Multi-teacher knowledge distillation is scalable and effective for building high-performing biomedical foundation models under real-world data constraints.

Abstract: CLIP models pretrained on natural images with billion-scale image-text pairs
have demonstrated impressive capabilities in zero-shot classification,
cross-modal retrieval, and open-ended visual answering. However, transferring
this success to biomedicine is hindered by the scarcity of large-scale
biomedical image-text corpora, the heterogeneity of image modalities, and
fragmented data standards across institutions. These limitations hinder the
development of a unified and generalizable biomedical foundation model trained
from scratch. To overcome this, we introduce MMKD-CLIP, a generalist biomedical
foundation model developed via Multiple Medical CLIP Knowledge Distillation.
Rather than relying on billion-scale raw data, MMKD-CLIP distills knowledge
from nine state-of-the-art domain-specific or generalist biomedical CLIP
models, each pretrained on millions of biomedical image-text pairs. Our
two-stage training pipeline first performs CLIP-style pretraining on over 2.9
million biomedical image-text pairs from 26 image modalities, followed by
feature-level distillation using over 19.2 million feature pairs extracted from
teacher models. We evaluate MMKD-CLIP on 58 diverse biomedical datasets,
encompassing over 10.8 million biomedical images across nine image modalities.
The evaluation spans six core task types: zero-shot classification, linear
probing, cross-modal retrieval, visual question answering, survival prediction,
and cancer diagnosis. MMKD-CLIP consistently outperforms all teacher models
while demonstrating remarkable robustness and generalization across image
domains and task settings. These results underscore that multi-teacher
knowledge distillation is a scalable and effective paradigm for building
high-performing biomedical foundation models under the practical constraints of
real-world data availability.

</details>


### [177] [Dual Atrous Separable Convolution for Improving Agricultural Semantic Segmentation](https://arxiv.org/abs/2506.22570)
*Chee Mei Ling,Thangarajah Akilan,Aparna Ravinda Phalke*

Main category: cs.CV

TL;DR: A novel Dual Atrous Separable Convolution (DAS Conv) module is integrated into a DeepLabV3-based framework for efficient agricultural image segmentation, balancing performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve crop management and productivity by accurately segmenting farmland anomalies in agricultural imagery.

Method: Proposes a DAS Conv module with optimal dilation rates and padding, and a strategic skip connection in a DeepLabV3 framework.

Result: Outperforms baseline and matches SOTA transformer models with 66% efficiency improvement on the Agriculture Vision dataset.

Conclusion: The study presents a lightweight, high-performance solution for agricultural semantic segmentation.

Abstract: Agricultural image semantic segmentation is a pivotal component of modern
agriculture, facilitating accurate visual data analysis to improve crop
management, optimize resource utilization, and boost overall productivity. This
study proposes an efficient image segmentation method for precision
agriculture, focusing on accurately delineating farmland anomalies to support
informed decision-making and proactive interventions. A novel Dual Atrous
Separable Convolution (DAS Conv) module is integrated within the
DeepLabV3-based segmentation framework. The DAS Conv module is meticulously
designed to achieve an optimal balance between dilation rates and padding size,
thereby enhancing model performance without compromising efficiency. The study
also incorporates a strategic skip connection from an optimal stage in the
encoder to the decoder to bolster the model's capacity to capture fine-grained
spatial features. Despite its lower computational complexity, the proposed
model outperforms its baseline and achieves performance comparable to highly
complex transformer-based state-of-the-art (SOTA) models on the Agriculture
Vision benchmark dataset. It achieves more than 66% improvement in efficiency
when considering the trade-off between model complexity and performance,
compared to the SOTA model. This study highlights an efficient and effective
solution for improving semantic segmentation in remote sensing applications,
offering a computationally lightweight model capable of high-quality
performance in agricultural imagery.

</details>


### [178] [LIGHT: Multi-Modal Text Linking on Historical Maps](https://arxiv.org/abs/2506.22589)
*Yijun Lin,Rhett Olson,Junhan Wu,Yao-Yi Chiang,Jerod Weinman*

Main category: cs.CV

TL;DR: LIGHT is a multi-modal approach integrating linguistic, image, and geometric features to link text on historical maps, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Text on historical maps varies in orientation and placement, making linking fragments challenging. Existing methods neglect geometric information, crucial for map text.

Method: LIGHT combines geometry-aware embeddings (polygon coordinates) with visual and linguistic features from LayoutLMv3, using a bi-directional learning strategy for reading-order prediction.

Result: LIGHT outperforms existing methods on the ICDAR 2024/2025 MapText Competition data.

Conclusion: Multi-modal learning, especially integrating geometric features, is effective for linking text on historical maps.

Abstract: Text on historical maps provides valuable information for studies in history,
economics, geography, and other related fields. Unlike structured or
semi-structured documents, text on maps varies significantly in orientation,
reading order, shape, and placement. Many modern methods can detect and
transcribe text regions, but they struggle to effectively ``link'' the
recognized text fragments, e.g., determining a multi-word place name. Existing
layout analysis methods model word relationships to improve text understanding
in structured documents, but they primarily rely on linguistic features and
neglect geometric information, which is essential for handling map text. To
address these challenges, we propose LIGHT, a novel multi-modal approach that
integrates linguistic, image, and geometric features for linking text on
historical maps. In particular, LIGHT includes a geometry-aware embedding
module that encodes the polygonal coordinates of text regions to capture
polygon shapes and their relative spatial positions on an image. LIGHT unifies
this geometric information with the visual and linguistic token embeddings from
LayoutLMv3, a pretrained layout analysis model. LIGHT uses the cross-modal
information to predict the reading-order successor of each text instance
directly with a bi-directional learning strategy that enhances sequence
robustness. Experimental results show that LIGHT outperforms existing methods
on the ICDAR 2024/2025 MapText Competition data, demonstrating the
effectiveness of multi-modal learning for historical map text linking.

</details>


### [179] [BrainMT: A Hybrid Mamba-Transformer Architecture for Modeling Long-Range Dependencies in Functional MRI Data](https://arxiv.org/abs/2506.22591)
*Arunkumar Kannan,Martin A. Lindquist,Brian Caffo*

Main category: cs.CV

TL;DR: BrainMT is a hybrid framework combining Mamba and transformer blocks to model long-range spatiotemporal dependencies in fMRI data, outperforming existing methods in classification and regression tasks.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning approaches struggle with long-range spatial and temporal dependencies in fMRI data, limiting their predictive performance.

Method: BrainMT uses a bidirectional Mamba block for temporal interactions and a transformer block for spatial relationships, applied to fMRI data.

Result: BrainMT achieves state-of-the-art performance on sex prediction and cognitive intelligence prediction tasks, surpassing other methods.

Conclusion: BrainMT effectively addresses the limitations of current methods by integrating long-range spatiotemporal modeling, demonstrating superior performance in fMRI-based prediction tasks.

Abstract: Recent advances in deep learning have made it possible to predict phenotypic
measures directly from functional magnetic resonance imaging (fMRI) brain
volumes, sparking significant interest in the neuroimaging community. However,
existing approaches, primarily based on convolutional neural networks or
transformer architectures, often struggle to model the complex relationships
inherent in fMRI data, limited by their inability to capture long-range spatial
and temporal dependencies. To overcome these shortcomings, we introduce
BrainMT, a novel hybrid framework designed to efficiently learn and integrate
long-range spatiotemporal attributes in fMRI data. Our framework operates in
two stages: (1) a bidirectional Mamba block with a temporal-first scanning
mechanism to capture global temporal interactions in a computationally
efficient manner; and (2) a transformer block leveraging self-attention to
model global spatial relationships across the deep features processed by the
Mamba block. Extensive experiments on two large-scale public datasets,
UKBioBank and the Human Connectome Project, demonstrate that BrainMT achieves
state-of-the-art performance on both classification (sex prediction) and
regression (cognitive intelligence prediction) tasks, outperforming existing
methods by a significant margin. Our code and implementation details will be
made publicly available at this
https://github.com/arunkumar-kannan/BrainMT-fMRI

</details>


### [180] [Seg-R1: Segmentation Can Be Surprisingly Simple with Reinforcement Learning](https://arxiv.org/abs/2506.22624)
*Zuyao You,Zuxuan Wu*

Main category: cs.CV

TL;DR: Seg-R1 uses reinforcement learning to improve pixel-level understanding in large multimodal models, achieving strong performance in segmentation tasks without complex modifications.


<details>
  <summary>Details</summary>
Motivation: Enhancing pixel-level reasoning in large multimodal models for better segmentation tasks like camouflaged and salient object detection.

Method: Utilizes reinforcement learning (RL) with Group Relative Policy Optimization (GRPO) to train the model for generating prompts and segmentation masks.

Result: Achieves .873 S-measure on COD10K and strong zero-shot performance on referring and reasoning segmentation tasks.

Conclusion: Seg-R1 demonstrates the effectiveness of pure RL training for segmentation, with notable generalization capabilities.

Abstract: We present Seg-R1, a preliminary exploration of using reinforcement learning
(RL) to enhance the pixel-level understanding and reasoning capabilities of
large multimodal models (LMMs). Starting with foreground segmentation tasks,
specifically camouflaged object detection (COD) and salient object detection
(SOD), our approach enables the LMM to generate point and bounding box prompts
in the next-token fashion, which are then used to guide SAM2 in producing
segmentation masks. We introduce Group Relative Policy Optimization (GRPO) into
the segmentation domain, equipping the LMM with pixel-level comprehension
through a carefully designed training strategy. Notably, Seg-R1 achieves
remarkable performance with purely RL-based training, achieving .873 S-measure
on COD10K without complex model modification. Moreover, we found that pure RL
training demonstrates strong open-world generalization. Despite being trained
solely on foreground segmentation image-mask pairs without text supervision,
Seg-R1 achieves impressive zero-shot performance on referring segmentation and
reasoning segmentation tasks, with 71.4 cIoU on RefCOCOg test and 56.7 gIoU on
ReasonSeg test, outperforming models fully supervised on these datasets.

</details>


### [181] [ReCo: Reminder Composition Mitigates Hallucinations in Vision-Language Models](https://arxiv.org/abs/2506.22636)
*Sotirios Panagiotis Chytas,Miso Choi,Hyunwoo J. Kim,Vikas Singh*

Main category: cs.CV

TL;DR: The paper addresses the hallucination issue in Vision Language Models (VLMs) by proposing a lightweight module (ReCo) to mitigate the fading memory effect, improving performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: VLMs often hallucinate due to over-reliance on language and fading memory of visual input, leading to ungrounded or contradictory outputs.

Method: Introduces ReCo, a small trainable module based on geometric algebra and relational compositions, added atop existing VLMs without further modifications.

Result: ReCo improves performance on three popular VLMs (InstructBLIP, LlaVA, MiniGPT4) across benchmarks and enhances other hallucination-reduction methods.

Conclusion: ReCo effectively mitigates the fading memory effect in VLMs, offering a lightweight solution to hallucination without extensive model changes.

Abstract: Vision Language Models (VLMs) show impressive capabilities in integrating and
reasoning with both visual and language data. But these models make mistakes. A
common finding -- similar to LLMs -- is their tendency to hallucinate, i.e.,
generate plausible sounding text which is not grounded in the visual input, or
at worst, is contradictory. A growing consensus attributes this behavior to an
over-reliance on language -- especially as the generation progresses, the model
suffers from a ``fading memory effect'' with respect to the provided visual
input. We study mechanisms by which this behavior can be controlled.
Specifically, using ideas from geometric algebra and relational compositions,
we propose the addition of a small, trainable module (named ReCo) on top of any
VLM -- no other modification is needed. We show that such a lightweight module
is able to mitigate the fading memory effect on three of the most widely used
VLMs (InstructBLIP, LlaVA, MiniGPT4), where we see performance improvements on
multiple benchmarks. Additionally, we show that our module can be combined with
many of the other approaches for reducing hallucination where we achieve
improved results for each one.

</details>


### [182] [CaO$_2$: Rectifying Inconsistencies in Diffusion-Based Dataset Distillation](https://arxiv.org/abs/2506.22637)
*Haoxuan Wang,Zhenghao Zhao,Junyi Wu,Yuzhang Shang,Gaowen Liu,Yan Yan*

Main category: cs.CV

TL;DR: CaO$_2$ introduces a two-stage diffusion-based framework to address inconsistencies in dataset distillation, achieving state-of-the-art performance on ImageNet.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based dataset distillation methods suffer from objective and condition inconsistencies, leading to suboptimal performance.

Method: CaO$_2$ uses a two-stage approach: probability-informed sample selection and latent representation refinement.

Result: CaO$_2$ outperforms baselines by 2.3% accuracy on ImageNet and subsets.

Conclusion: The proposed framework effectively aligns distillation with evaluation objectives, improving performance.

Abstract: The recent introduction of diffusion models in dataset distillation has shown
promising potential in creating compact surrogate datasets for large,
high-resolution target datasets, offering improved efficiency and performance
over traditional bi-level/uni-level optimization methods. However, current
diffusion-based dataset distillation approaches overlook the evaluation process
and exhibit two critical inconsistencies in the distillation process: (1)
Objective Inconsistency, where the distillation process diverges from the
evaluation objective, and (2) Condition Inconsistency, leading to mismatches
between generated images and their corresponding conditions. To resolve these
issues, we introduce Condition-aware Optimization with Objective-guided
Sampling (CaO$_2$), a two-stage diffusion-based framework that aligns the
distillation process with the evaluation objective. The first stage employs a
probability-informed sample selection pipeline, while the second stage refines
the corresponding latent representations to improve conditional likelihood.
CaO$_2$ achieves state-of-the-art performance on ImageNet and its subsets,
surpassing the best-performing baselines by an average of 2.3% accuracy.

</details>


### [183] [3D Shape Generation: A Survey](https://arxiv.org/abs/2506.22678)
*Nicolas Caytuiro,Ivan Sipiran*

Main category: cs.CV

TL;DR: A survey on 3D shape generation, covering representations, methods, and evaluation, with future research directions.


<details>
  <summary>Details</summary>
Motivation: To provide a structured overview of advancements in 3D shape generation and guide future research.

Method: Categorizes 3D representations, reviews generative methods, and summarizes datasets and metrics.

Result: Highlights current state-of-the-art, challenges, and future directions in 3D shape generation.

Conclusion: The survey serves as a reference for understanding and advancing 3D shape generation.

Abstract: Recent advances in deep learning have significantly transformed the field of
3D shape generation, enabling the synthesis of complex, diverse, and
semantically meaningful 3D objects. This survey provides a comprehensive
overview of the current state of the art in 3D shape generation, organizing the
discussion around three core components: shape representations, generative
modeling approaches, and evaluation protocols. We begin by categorizing 3D
representations into explicit, implicit, and hybrid setups, highlighting their
structural properties, advantages, and limitations. Next, we review a wide
range of generation methods, focusing on feedforward architectures. We further
summarize commonly used datasets and evaluation metrics that assess fidelity,
diversity, and realism of generated shapes. Finally, we identify open
challenges and outline future research directions that could drive progress in
controllable, efficient, and high-quality 3D shape generation. This survey aims
to serve as a valuable reference for researchers and practitioners seeking a
structured and in-depth understanding of this rapidly evolving field.

</details>


### [184] [LightBSR: Towards Lightweight Blind Super-Resolution via Discriminative Implicit Degradation Representation Learning](https://arxiv.org/abs/2506.22710)
*Jiang Yuan,JI Ma,Bo Wang,Guanzhou Ke,Weiming Hu*

Main category: cs.CV

TL;DR: LightBSR improves blind super-resolution by optimizing implicit degradation representation (IDR) discriminability using a knowledge distillation framework, achieving high performance with minimal complexity.


<details>
  <summary>Details</summary>
Motivation: Existing IDE-BSR methods overlook IDR discriminability, complicating adaptation and increasing model size. LightBSR addresses this gap.

Method: Uses a knowledge distillation framework with degradation-prior-constrained contrastive learning (teacher stage) and feature alignment (student stage).

Result: LightBSR achieves outstanding performance with minimal complexity in blind SR tasks.

Conclusion: Optimizing IDR discriminability is effective for BSR, and LightBSR provides a lightweight, high-performance solution.

Abstract: Implicit degradation estimation-based blind super-resolution (IDE-BSR) hinges
on extracting the implicit degradation representation (IDR) of the LR image and
adapting it to LR image features to guide HR detail restoration. Although
IDE-BSR has shown potential in dealing with noise interference and complex
degradations, existing methods ignore the importance of IDR discriminability
for BSR and instead over-complicate the adaptation process to improve effect,
resulting in a significant increase in the model's parameters and computations.
In this paper, we focus on the discriminability optimization of IDR and propose
a new powerful and lightweight BSR model termed LightBSR. Specifically, we
employ a knowledge distillation-based learning framework. We first introduce a
well-designed degradation-prior-constrained contrastive learning technique
during teacher stage to make the model more focused on distinguishing different
degradation types. Then we utilize a feature alignment technique to transfer
the degradation-related knowledge acquired by the teacher to the student for
practical inferencing. Extensive experiments demonstrate the effectiveness of
IDR discriminability-driven BSR model design. The proposed LightBSR can achieve
outstanding performance with minimal complexity across a range of blind SR
tasks. Our code is accessible at: https://github.com/MJ-NCEPU/LightBSR.

</details>


### [185] [Part Segmentation and Motion Estimation for Articulated Objects with Dynamic 3D Gaussians](https://arxiv.org/abs/2506.22718)
*Jun-Jee Chao,Qingyuan Jiang,Volkan Isler*

Main category: cs.CV

TL;DR: A method for joint part segmentation and motion estimation from point clouds of articulated objects, handling occlusions and asynchronous data by representing objects as 3D Gaussians with shared parameters across time.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of analyzing articulated object motion when point clouds are not from fixed points, due to occlusions or asynchronous sensor data.

Method: Representing objects as 3D Gaussians with time-dependent transformations (rotations, translations, scales) shared across time steps, enabling part segmentation and motion estimation without point correspondences.

Result: Outperforms point-correspondence-based methods, especially under occlusions, with a 13% improvement in part segmentation accuracy.

Conclusion: The proposed Gaussian-based representation is robust to occlusions and missing data, offering superior performance in part segmentation and motion estimation.

Abstract: Part segmentation and motion estimation are two fundamental problems for
articulated object motion analysis. In this paper, we present a method to solve
these two problems jointly from a sequence of observed point clouds of a single
articulated object. The main challenge in our problem setting is that the point
clouds are not assumed to be generated by a fixed set of moving points.
Instead, each point cloud in the sequence could be an arbitrary sampling of the
object surface at that particular time step. Such scenarios occur when the
object undergoes major occlusions, or if the dataset is collected using
measurements from multiple sensors asynchronously. In these scenarios, methods
that rely on tracking point correspondences are not appropriate. We present an
alternative approach based on a compact but effective representation where we
represent the object as a collection of simple building blocks modeled as 3D
Gaussians. We parameterize the Gaussians with time-dependent rotations,
translations, and scales that are shared across all time steps. With our
representation, part segmentation can be achieved by building correspondences
between the observed points and the Gaussians. Moreover, the transformation of
each point across time can be obtained by following the poses of the assigned
Gaussian (even when the point is not observed). Experiments show that our
method outperforms existing methods that solely rely on finding point
correspondences. Additionally, we extend existing datasets to emulate
real-world scenarios by considering viewpoint occlusions. We further
demonstrate that our method is more robust to missing points as compared to
existing approaches on these challenging datasets, even when some parts are
completely occluded in some time-steps. Notably, our part segmentation
performance outperforms the state-of-the-art method by 13% on point clouds with
occlusions.

</details>


### [186] [Deterministic Object Pose Confidence Region Estimation](https://arxiv.org/abs/2506.22720)
*Jinghao Wang,Zhang Li,Zi Wang,Banglei Guan,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: A deterministic method for 6D pose confidence region estimation using inductive conformal prediction and the implicit function theorem, addressing inefficiencies and inflated regions in sampling-based approaches.


<details>
  <summary>Details</summary>
Motivation: Current sampling-based methods for 6D pose confidence region estimation are slow and produce excessively large regions, limiting practical use.

Method: Uses inductive conformal prediction to calibrate Gaussian keypoint distributions into 2D confidence regions, then propagates these into 6D pose regions via the implicit function theorem.

Result: Achieves higher accuracy and faster computation, reducing confidence region volumes by up to 99.9% for rotations and 99.8% for translations.

Conclusion: The proposed method provides compact, efficient, and accurate 6D pose confidence regions, outperforming sampling-based approaches.

Abstract: 6D pose confidence region estimation has emerged as a critical direction,
aiming to perform uncertainty quantification for assessing the reliability of
estimated poses. However, current sampling-based approach suffers from critical
limitations that severely impede their practical deployment: 1) the sampling
speed significantly decreases as the number of samples increases. 2) the
derived confidence regions are often excessively large. To address these
challenges, we propose a deterministic and efficient method for estimating pose
confidence regions. Our approach uses inductive conformal prediction to
calibrate the deterministically regressed Gaussian keypoint distributions into
2D keypoint confidence regions. We then leverage the implicit function theorem
to propagate these keypoint confidence regions directly into 6D pose confidence
regions. This method avoids the inefficiency and inflated region sizes
associated with sampling and ensembling. It provides compact confidence regions
that cover the ground-truth poses with a user-defined confidence level.
Experimental results on the LineMOD Occlusion and SPEED datasets show that our
method achieves higher pose estimation accuracy with reduced computational
time. For the same coverage rate, our method yields significantly smaller
confidence region volumes, reducing them by up to 99.9\% for rotations and
99.8\% for translations. The code will be available soon.

</details>


### [187] [XTransfer: Cross-Modality Model Transfer for Human Sensing with Few Data at the Edge](https://arxiv.org/abs/2506.22726)
*Yu Zhang,Xi Zhang,Hualin zhou,Xinyuan Chen,Shang Gao,Hong Jia,Jianfei Yang,Yuankai Qi,Tao Gu*

Main category: cs.CV

TL;DR: XTransfer is a novel method for efficient, modality-agnostic model transfer in edge-based human sensing, addressing issues like modality shift and resource constraints.


<details>
  <summary>Details</summary>
Motivation: Limited sensor data and edge system resources hinder deep learning for human sensing, while current transfer methods face accuracy loss and high resource demands.

Method: XTransfer uses model repairing to fix modality shifts and layer recombining to create compact models from pre-trained layers.

Result: XTransfer outperforms baselines, reducing costs in data collection, training, and deployment while maintaining high accuracy.

Conclusion: XTransfer is a resource-efficient, adaptable solution for human sensing on edge systems.

Abstract: Deep learning for human sensing on edge systems offers significant
opportunities for smart applications. However, its training and development are
hindered by the limited availability of sensor data and resource constraints of
edge systems. Current methods that rely on transferring pre-trained models
often encounter issues such as modality shift and high resource demands,
resulting in substantial accuracy loss, resource overhead, and poor
adaptability across different sensing applications. In this paper, we propose
XTransfer, a first-of-its-kind method for resource-efficient, modality-agnostic
model transfer. XTransfer freely leverages single or multiple pre-trained
models and transfers knowledge across different modalities by (i) model
repairing that safely repairs modality shift in pre-trained model layers with
only few sensor data, and (ii) layer recombining that efficiently searches and
recombines layers of interest from source models in a layer-wise manner to
create compact models. We benchmark various baselines across diverse human
sensing datasets spanning different modalities. Comprehensive results
demonstrate that XTransfer achieves state-of-the-art performance on human
sensing tasks while significantly reducing the costs of sensor data collection,
model training, and edge deployment.

</details>


### [188] [UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image Fusion Under Diverse Degradations and Misalignments](https://arxiv.org/abs/2506.22736)
*Dayong Su,Yafei Zhang,Huafeng Li,Jinxing Li,Yu Liu*

Main category: cs.CV

TL;DR: UniFuse is a unified framework for multimodal medical image fusion that addresses misalignment and degradation by integrating alignment, restoration, and fusion into a single-stage process.


<details>
  <summary>Details</summary>
Motivation: Current fusion methods rely on high-quality, aligned images, but their performance drops with misaligned or degraded inputs. UniFuse aims to overcome these limitations.

Method: UniFuse incorporates degradation-aware prompt learning, Omni Unified Feature Representation, and a Universal Feature Restoration & Fusion module with Adaptive LoRA Synergistic Network (ALSN).

Result: Experiments show UniFuse outperforms existing methods in handling misaligned and degraded medical images.

Conclusion: UniFuse successfully unifies alignment, restoration, and fusion, offering a robust solution for multimodal medical image fusion.

Abstract: Current multimodal medical image fusion typically assumes that source images
are of high quality and perfectly aligned at the pixel level. Its effectiveness
heavily relies on these conditions and often deteriorates when handling
misaligned or degraded medical images. To address this, we propose UniFuse, a
general fusion framework. By embedding a degradation-aware prompt learning
module, UniFuse seamlessly integrates multi-directional information from input
images and correlates cross-modal alignment with restoration, enabling joint
optimization of both tasks within a unified framework. Additionally, we design
an Omni Unified Feature Representation scheme, which leverages Spatial Mamba to
encode multi-directional features and mitigate modality differences in feature
alignment. To enable simultaneous restoration and fusion within an All-in-One
configuration, we propose a Universal Feature Restoration & Fusion module,
incorporating the Adaptive LoRA Synergistic Network (ALSN) based on LoRA
principles. By leveraging ALSN's adaptive feature representation along with
degradation-type guidance, we enable joint restoration and fusion within a
single-stage framework. Compared to staged approaches, UniFuse unifies
alignment, restoration, and fusion within a single framework. Experimental
results across multiple datasets demonstrate the method's effectiveness and
significant advantages over existing approaches.

</details>


### [189] [Deep Learning based Joint Geometry and Attribute Up-sampling for Large-Scale Colored Point Clouds](https://arxiv.org/abs/2506.22749)
*Yun Zhang,Feifan Chen,Na Li,Zhiwei Guo,Xu Wang,Fen Miao,Sam Kwong*

Main category: cs.CV

TL;DR: The paper proposes a deep learning-based Joint Geometry and Attribute Up-sampling (JGAU) method for colored point clouds, achieving superior quality and performance over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To generate large-scale, high-quality colored point clouds by leveraging spatial attribute correlations and improving up-sampling techniques.

Method: A JGAU framework with geometry and attribute up-sampling networks, using coarse attribute up-sampling methods (GDWAI and DLAI) and an attribute enhancement module.

Result: JGAU achieves PSNR gains of 2.11 to 2.47 decibels over state-of-the-art methods across four up-sampling rates.

Conclusion: The proposed JGAU method significantly improves the quality of colored point clouds, outperforming existing techniques.

Abstract: Colored point cloud, which includes geometry and attribute components, is a
mainstream representation enabling realistic and immersive 3D applications. To
generate large-scale and denser colored point clouds, we propose a deep
learning-based Joint Geometry and Attribute Up-sampling (JGAU) method that
learns to model both geometry and attribute patterns while leveraging spatial
attribute correlations. First, we establish and release a large-scale dataset
for colored point cloud up-sampling called SYSU-PCUD, containing 121
large-scale colored point clouds with diverse geometry and attribute
complexities across six categories and four sampling rates. Second, to improve
the quality of up-sampled point clouds, we propose a deep learning-based JGAU
framework that jointly up-samples geometry and attributes. It consists of a
geometry up-sampling network and an attribute up-sampling network, where the
latter leverages the up-sampled auxiliary geometry to model neighborhood
correlations of the attributes. Third, we propose two coarse attribute
up-sampling methods, Geometric Distance Weighted Attribute Interpolation
(GDWAI) and Deep Learning-based Attribute Interpolation (DLAI), to generate
coarse up-sampled attributes for each point. Then, an attribute enhancement
module is introduced to refine these up-sampled attributes and produce
high-quality point clouds by further exploiting intrinsic attribute and
geometry patterns. Extensive experiments show that the Peak Signal-to-Noise
Ratio (PSNR) achieved by the proposed JGAU method is 33.90 decibels, 32.10
decibels, 31.10 decibels, and 30.39 decibels for up-sampling rates of 4 times,
8 times, 12 times, and 16 times, respectively. Compared to state-of-the-art
methods, JGAU achieves average PSNR gains of 2.32 decibels, 2.47 decibels, 2.28
decibels, and 2.11 decibels at these four up-sampling rates, demonstrating
significant improvement.

</details>


### [190] [Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography](https://arxiv.org/abs/2506.22753)
*Jianing Zhang,Jiayi Zhu,Feiyu Ji,Xiaokang Yang,Xiaoyun Yuan*

Main category: cs.CV

TL;DR: A novel framework, Degradation-Modeled Multipath Diffusion, is introduced for tunable metalens photography, leveraging pretrained models and pseudo data augmentation to overcome challenges in computational imaging.


<details>
  <summary>Details</summary>
Motivation: Metalenses face issues like optical degradation and computational restoration difficulties, with existing methods requiring precise calibration or large datasets, leading to artifacts.

Method: The framework uses positive, neutral, and negative-prompt paths for detail generation, structural fidelity, and degradation suppression, alongside a tunable decoder and SVDA module for adaptive degradation modeling.

Result: The approach outperforms state-of-the-art methods, achieving high-fidelity and sharp image reconstruction, validated with a MetaCamera.

Conclusion: The proposed method effectively addresses metalens imaging challenges, offering controlled trade-offs between fidelity and perceptual quality.

Abstract: Metalenses offer significant potential for ultra-compact computational
imaging but face challenges from complex optical degradation and computational
restoration difficulties. Existing methods typically rely on precise optical
calibration or massive paired datasets, which are non-trivial for real-world
imaging systems. Furthermore, a lack of control over the inference process
often results in undesirable hallucinated artifacts. We introduce
Degradation-Modeled Multipath Diffusion for tunable metalens photography,
leveraging powerful natural image priors from pretrained models instead of
large datasets. Our framework uses positive, neutral, and negative-prompt paths
to balance high-frequency detail generation, structural fidelity, and
suppression of metalens-specific degradation, alongside \textit{pseudo} data
augmentation. A tunable decoder enables controlled trade-offs between fidelity
and perceptual quality. Additionally, a spatially varying degradation-aware
attention (SVDA) module adaptively models complex optical and sensor-induced
degradation. Finally, we design and build a millimeter-scale MetaCamera for
real-world validation. Extensive results show that our approach outperforms
state-of-the-art methods, achieving high-fidelity and sharp image
reconstruction. More materials: https://dmdiff.github.io/.

</details>


### [191] [RoboPearls: Editable Video Simulation for Robot Manipulation](https://arxiv.org/abs/2506.22756)
*Tao Tang,Likui Zhang,Youpeng Wen,Kaidong Zhang,Jia-Wang Bian,xia zhou,Tianyi Yan,Kun Zhan,Peng Jia,Hefeng Wu,Liang Lin,Xiaodan Liang*

Main category: cs.CV

TL;DR: RoboPearls is a video simulation framework for robotic manipulation, using 3D Gaussian Splatting and LLMs to automate simulation production and enhance performance.


<details>
  <summary>Details</summary>
Motivation: High cost and inefficiency of real-world demonstration data collection hinder scalability, and existing simulation platforms struggle with the sim-to-real gap.

Method: RoboPearls uses 3DGS for photo-realistic simulations, ISD and 3D-NNFM for object manipulations, and LLMs/VLMs for automation and performance analysis.

Result: Extensive experiments on datasets like RLBench and real-world robots show satisfactory simulation performance.

Conclusion: RoboPearls effectively addresses scalability and sim-to-real challenges in robotic manipulation.

Abstract: The development of generalist robot manipulation policies has seen
significant progress, driven by large-scale demonstration data across diverse
environments. However, the high cost and inefficiency of collecting real-world
demonstrations hinder the scalability of data acquisition. While existing
simulation platforms enable controlled environments for robotic learning, the
challenge of bridging the sim-to-real gap remains. To address these challenges,
we propose RoboPearls, an editable video simulation framework for robotic
manipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the
construction of photo-realistic, view-consistent simulations from demonstration
videos, and supports a wide range of simulation operators, including various
object manipulations, powered by advanced modules like Incremental Semantic
Distillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by
incorporating large language models (LLMs), RoboPearls automates the simulation
production process in a user-friendly manner through flexible command
interpretation and execution. Furthermore, RoboPearls employs a vision-language
model (VLM) to analyze robotic learning issues to close the simulation loop for
performance enhancement. To demonstrate the effectiveness of RoboPearls, we
conduct extensive experiments on multiple datasets and scenes, including
RLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which
demonstrate our satisfactory simulation performance.

</details>


### [192] [VSRM: A Robust Mamba-Based Framework for Video Super-Resolution](https://arxiv.org/abs/2506.22762)
*Dinh Phu Tran,Dao Duy Hung,Daeyoung Kim*

Main category: cs.CV

TL;DR: VSRM, a novel video super-resolution framework using Mamba, introduces Spatial-to-Temporal and Temporal-to-Spatial Mamba blocks for efficient long-range spatio-temporal feature extraction. It includes a Deformable Cross-Mamba Alignment module and a Frequency Charbonnier-like loss, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Video super-resolution (VSR) faces challenges with CNNs' limited receptive fields and Transformers' quadratic complexity. Mamba's linear complexity and long-sequence modeling capabilities offer a promising alternative.

Method: VSRM employs Spatial-to-Temporal and Temporal-to-Spatial Mamba blocks for feature extraction, a Deformable Cross-Mamba Alignment module for dynamic frame alignment, and a Frequency Charbonnier-like loss for frequency domain optimization.

Result: VSRM achieves state-of-the-art performance on diverse benchmarks, demonstrating superior efficiency and visual quality.

Conclusion: VSRM sets a new standard for video super-resolution, leveraging Mamba's strengths for efficient, high-quality results, and provides a foundation for future research.

Abstract: Video super-resolution remains a major challenge in low-level vision tasks.
To date, CNN- and Transformer-based methods have delivered impressive results.
However, CNNs are limited by local receptive fields, while Transformers
struggle with quadratic complexity, posing challenges for processing long
sequences in VSR. Recently, Mamba has drawn attention for its long-sequence
modeling, linear complexity, and large receptive fields. In this work, we
propose VSRM, a novel \textbf{V}ideo \textbf{S}uper-\textbf{R}esolution
framework that leverages the power of \textbf{M}amba. VSRM introduces
Spatial-to-Temporal Mamba and Temporal-to-Spatial Mamba blocks to extract
long-range spatio-temporal features and enhance receptive fields efficiently.
To better align adjacent frames, we propose Deformable Cross-Mamba Alignment
module. This module utilizes a deformable cross-mamba mechanism to make the
compensation stage more dynamic and flexible, preventing feature distortions.
Finally, we minimize the frequency domain gaps between reconstructed and
ground-truth frames by proposing a simple yet effective Frequency
Charbonnier-like loss that better preserves high-frequency content and enhances
visual quality. Through extensive experiments, VSRM achieves state-of-the-art
results on diverse benchmarks, establishing itself as a solid foundation for
future research.

</details>


### [193] [PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection](https://arxiv.org/abs/2506.22783)
*Oguzhan Baser,Ahmet Ege Tanriverdi,Sriram Vishwanath,Sandeep P. Chinchali*

Main category: cs.CV

TL;DR: The paper introduces PhonemeFake (PF), a deepfake attack method that outperforms existing datasets in deceiving humans and benchmarks, and proposes a detection model for scalable defense.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake datasets fail to mimic real-world attack effectiveness, highlighting the need for more realistic attack vectors.

Method: PhonemeFake manipulates critical speech segments using language reasoning, and a bilevel detection model is introduced to prioritize compute on manipulated regions.

Result: PF reduces human perception by 42% and benchmark accuracies by 94%. The detection model cuts EER by 91% and speeds up by 90% with minimal overhead.

Conclusion: PF and the detection model offer a scalable solution for realistic deepfake threats, outperforming existing methods.

Abstract: Deepfake (DF) attacks pose a growing threat as generative models become
increasingly advanced. However, our study reveals that existing DF datasets
fail to deceive human perception, unlike real DF attacks that influence public
discourse. It highlights the need for more realistic DF attack vectors. We
introduce PhonemeFake (PF), a DF attack that manipulates critical speech
segments using language reasoning, significantly reducing human perception by
up to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF
dataset on HuggingFace and open-source bilevel DF segment detection model that
adaptively prioritizes compute on manipulated regions. Our extensive
experiments across three known DF datasets reveal that our detection model
reduces EER by 91% while achieving up to 90% speed-up, with minimal compute
overhead and precise localization beyond existing models as a scalable
solution.

</details>


### [194] [Single-Frame Point-Pixel Registration via Supervised Cross-Modal Feature Matching](https://arxiv.org/abs/2506.22784)
*Yu Han,Zhiwei Huang,Yanting Zhang,Fangjun Ding,Shen Cai,Rui Fan*

Main category: cs.CV

TL;DR: A detector-free framework for direct point-pixel matching between LiDAR and camera views, addressing modality gaps and sparsity in single-frame LiDAR, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: The modality gap between unstructured LiDAR point clouds and structured images, especially under sparse single-frame LiDAR settings, challenges point-pixel registration. Existing methods struggle with sparsity and noise, often requiring multi-frame accumulation.

Method: Projects LiDAR intensity maps into 2D views and uses an attention-based detector-free matching network for cross-modal correspondence. Introduces a repeatability scoring mechanism to enhance reliability.

Result: Achieves state-of-the-art performance on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks, outperforming prior methods even with single-frame LiDAR.

Conclusion: The proposed framework effectively bridges the modality gap and improves robustness under sparse input, setting a new benchmark for point-pixel registration.

Abstract: Point-pixel registration between LiDAR point clouds and camera images is a
fundamental yet challenging task in autonomous driving and robotic perception.
A key difficulty lies in the modality gap between unstructured point clouds and
structured images, especially under sparse single-frame LiDAR settings.
Existing methods typically extract features separately from point clouds and
images, then rely on hand-crafted or learned matching strategies. This separate
encoding fails to bridge the modality gap effectively, and more critically,
these methods struggle with the sparsity and noise of single-frame LiDAR, often
requiring point cloud accumulation or additional priors to improve reliability.
Inspired by recent progress in detector-free matching paradigms (e.g.
MatchAnything), we revisit the projection-based approach and introduce the
detector-free framework for direct point-pixel matching between LiDAR and
camera views. Specifically, we project the LiDAR intensity map into a 2D view
from the LiDAR perspective and feed it into an attention-based detector-free
matching network, enabling cross-modal correspondence estimation without
relying on multi-frame accumulation. To further enhance matching reliability,
we introduce a repeatability scoring mechanism that acts as a soft visibility
prior. This guides the network to suppress unreliable matches in regions with
low intensity variation, improving robustness under sparse input. Extensive
experiments on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks demonstrate that
our method achieves state-of-the-art performance, outperforming prior
approaches on nuScenes (even those relying on accumulated point clouds),
despite using only single-frame LiDAR.

</details>


### [195] [RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via Diffusion Priors](https://arxiv.org/abs/2506.22800)
*Sicong Du,Jiarun Liu,Qifeng Chen,Hao-Xiang Chen,Tai-Jiang Mu,Sheng Yang*

Main category: cs.CV

TL;DR: RGE-GS is a novel framework for expansive road scene reconstruction, combining diffusion-based generation with reward-guided Gaussian integration to improve quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Incomplete road scans from single-pass driving clips necessitate better reconstruction methods for sensor simulators, but current 3DGS extensions with diffusion priors introduce physical inconsistencies and inefficiencies.

Method: RGE-GS uses a reward network to prioritize stable diffusion outputs and a differentiated training strategy for adaptive Gaussian optimization.

Result: RGE-GS achieves state-of-the-art reconstruction quality on public datasets.

Conclusion: The framework effectively addresses prior limitations, offering improved performance and stability in scene reconstruction.

Abstract: A single-pass driving clip frequently results in incomplete scanning of the
road structure, making reconstructed scene expanding a critical requirement for
sensor simulators to effectively regress driving actions. Although contemporary
3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction
quality, their direct extension through the integration of diffusion priors
often introduces cumulative physical inconsistencies and compromises training
efficiency. To address these limitations, we present RGE-GS, a novel expansive
reconstruction framework that synergizes diffusion-based generation with
reward-guided Gaussian integration. The RGE-GS framework incorporates two key
innovations: First, we propose a reward network that learns to identify and
prioritize consistently generated patterns prior to reconstruction phases,
thereby enabling selective retention of diffusion outputs for spatial
stability. Second, during the reconstruction process, we devise a
differentiated training strategy that automatically adjust Gaussian
optimization progress according to scene converge metrics, which achieving
better convergence than baseline methods. Extensive evaluations of publicly
available datasets demonstrate that RGE-GS achieves state-of-the-art
performance in reconstruction quality. Our source-code will be made publicly
available at https://github.com/CN-ADLab/RGE-GS. (Camera-ready version
incorporating reviewer suggestions will be updated soon.)

</details>


### [196] [Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding](https://arxiv.org/abs/2506.22803)
*Nuoye Xiong,Anqi Dong,Ning Wang,Cong Hua,Guangming Zhu,Mei Lin,Peiyi Shen,Liang Zhang*

Main category: cs.CV

TL;DR: The paper introduces CBM-HNMU, a method to improve interpretability and accuracy of deep learning models by refining concepts in the Concept Bottleneck Model and distilling corrected knowledge back into the model.


<details>
  <summary>Details</summary>
Motivation: Deep learning models are becoming less interpretable due to complexity, and existing explanation methods lack effective interventions or model modifications.

Method: Uses the Concept Bottleneck Model (CBM) to approximate black-box reasoning, identifies and refines detrimental concepts globally, and distills corrected knowledge back into the model.

Result: Evaluated on multiple datasets, achieving up to 2.64% accuracy improvement and 1.03% average accuracy increase.

Conclusion: CBM-HNMU enhances both interpretability and accuracy of deep learning models by refining and distilling conceptual knowledge.

Abstract: Recent advances in deep learning have led to increasingly complex models with
deeper layers and more parameters, reducing interpretability and making their
decisions harder to understand. While many methods explain black-box reasoning,
most lack effective interventions or only operate at sample-level without
modifying the model itself. To address this, we propose the Concept Bottleneck
Model for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU).
CBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable
framework to approximate black-box reasoning and communicate conceptual
understanding. Detrimental concepts are automatically identified and refined
(removed/replaced) based on global gradient contributions. The modified CBM
then distills corrected knowledge back into the black-box model, enhancing both
interpretability and accuracy. We evaluate CBM-HNMU on various CNN and
transformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft,
and CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum
increase in average accuracy across 1.03%. Source code is available at:
https://github.com/XiGuaBo/CBM-HNMU.

</details>


### [197] [Concept Pinpoint Eraser for Text-to-image Diffusion Models via Residual Attention Gate](https://arxiv.org/abs/2506.22806)
*Byung Hyun Lee,Sungjin Lim,Seunggyu Lee,Dong Un Kang,Se Young Chun*

Main category: cs.CV

TL;DR: The paper introduces Concept Pinpoint Eraser (CPE), a method to selectively erase target concepts in diffusion models while preserving others, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Addressing concerns about inappropriate or trademarked content in text-to-image diffusion models by improving concept erasure without distorting other concepts.

Method: Proposes CPE with nonlinear Residual Attention Gates (ResAGs) and an attention anchoring loss, trained adversarially with learnable text embeddings for robustness.

Result: CPE effectively erases target concepts (e.g., celebrities, styles, explicit content) while preserving diverse remaining concepts and resisting adversarial attacks.

Conclusion: CPE advances concept erasure in diffusion models, offering better performance and robustness compared to existing methods.

Abstract: Remarkable progress in text-to-image diffusion models has brought a major
concern about potentially generating images on inappropriate or trademarked
concepts. Concept erasing has been investigated with the goals of deleting
target concepts in diffusion models while preserving other concepts with
minimal distortion. To achieve these goals, recent concept erasing methods
usually fine-tune the cross-attention layers of diffusion models. In this work,
we first show that merely updating the cross-attention layers in diffusion
models, which is mathematically equivalent to adding \emph{linear} modules to
weights, may not be able to preserve diverse remaining concepts. Then, we
propose a novel framework, dubbed Concept Pinpoint Eraser (CPE), by adding
\emph{nonlinear} Residual Attention Gates (ResAGs) that selectively erase (or
cut) target concepts while safeguarding remaining concepts from broad
distributions by employing an attention anchoring loss to prevent the
forgetting. Moreover, we adversarially train CPE with ResAG and learnable text
embeddings in an iterative manner to maximize erasing performance and enhance
robustness against adversarial attacks. Extensive experiments on the erasure of
celebrities, artistic styles, and explicit contents demonstrated that the
proposed CPE outperforms prior arts by keeping diverse remaining concepts while
deleting the target concepts with robustness against attack prompts. Code is
available at https://github.com/Hyun1A/CPE

</details>


### [198] [FreqDGT: Frequency-Adaptive Dynamic Graph Networks with Transformer for Cross-subject EEG Emotion Recognition](https://arxiv.org/abs/2506.22807)
*Yueyang Li,Shengyu Gong,Weiming Zeng,Nizhuan Wang,Wai Ting Siok*

Main category: cs.CV

TL;DR: FreqDGT, a frequency-adaptive dynamic graph transformer, improves cross-subject EEG emotion recognition by integrating frequency-adaptive processing, adaptive dynamic graph learning, and multi-scale temporal disentanglement.


<details>
  <summary>Details</summary>
Motivation: Cross-subject EEG emotion recognition is challenging due to individual variability. FreqDGT aims to address this by dynamically adapting to frequency bands and brain connectivity patterns.

Method: FreqDGT combines frequency-adaptive processing (FAP), adaptive dynamic graph learning (ADGL), and a multi-scale temporal disentanglement network (MTDN) for robust modeling.

Result: FreqDGT significantly enhances cross-subject emotion recognition accuracy, validating its integrated approach.

Conclusion: FreqDGT effectively addresses individual variability in EEG emotion recognition through dynamic frequency, spatial, and temporal modeling.

Abstract: Electroencephalography (EEG) serves as a reliable and objective signal for
emotion recognition in affective brain-computer interfaces, offering unique
advantages through its high temporal resolution and ability to capture
authentic emotional states that cannot be consciously controlled. However,
cross-subject generalization remains a fundamental challenge due to individual
variability, cognitive traits, and emotional responses. We propose FreqDGT, a
frequency-adaptive dynamic graph transformer that systematically addresses
these limitations through an integrated framework. FreqDGT introduces
frequency-adaptive processing (FAP) to dynamically weight emotion-relevant
frequency bands based on neuroscientific evidence, employs adaptive dynamic
graph learning (ADGL) to learn input-specific brain connectivity patterns, and
implements multi-scale temporal disentanglement network (MTDN) that combines
hierarchical temporal transformers with adversarial feature disentanglement to
capture both temporal dynamics and ensure cross-subject robustness.
Comprehensive experiments demonstrate that FreqDGT significantly improves
cross-subject emotion recognition accuracy, confirming the effectiveness of
integrating frequency-adaptive, spatial-dynamic, and temporal-hierarchical
modeling while ensuring robustness to individual differences. The code is
available at https://github.com/NZWANG/FreqDGT.

</details>


### [199] [Efficient Multi-Crop Saliency Partitioning for Automatic Image Cropping](https://arxiv.org/abs/2506.22814)
*Andrew Hamara,Andrew C. Freeman*

Main category: cs.CV

TL;DR: Extended Fixed Aspect Ratio Cropping algorithm to efficiently extract multiple non-overlapping crops in linear time, dynamically adjusting attention thresholds.


<details>
  <summary>Details</summary>
Motivation: Traditional methods optimize a single bounding box, making them ineffective for applications requiring multiple disjoint crops.

Method: Extends the Fixed Aspect Ratio Cropping algorithm to dynamically adjust attention thresholds and remove selected crops without recomputing the saliency map.

Result: Efficient extraction of multiple non-overlapping crops in linear time.

Conclusion: Qualitative results are discussed, with potential for future datasets and benchmarks.

Abstract: Automatic image cropping aims to extract the most visually salient regions
while preserving essential composition elements. Traditional saliency-aware
cropping methods optimize a single bounding box, making them ineffective for
applications requiring multiple disjoint crops. In this work, we extend the
Fixed Aspect Ratio Cropping algorithm to efficiently extract multiple
non-overlapping crops in linear time. Our approach dynamically adjusts
attention thresholds and removes selected crops from consideration without
recomputing the entire saliency map. We discuss qualitative results and
introduce the potential for future datasets and benchmarks.

</details>


### [200] [Unleashing the Multi-View Fusion Potential: Noise Correction in VLM for Open-Vocabulary 3D Scene Understanding](https://arxiv.org/abs/2506.22817)
*Xingyilang Yin,Jiale Wang,Xi Yang,Mutian Xu,Xu Gu,Nannan Wang*

Main category: cs.CV

TL;DR: MVOV3D improves open-vocabulary 3D scene understanding by reducing noise in 2D multi-view fusion without training, leveraging CLIP encoders and 3D geometric priors.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with diverse object categories due to limited 3D data and noisy 2D multi-view fusion.

Method: MVOV3D refines 2D multi-view features using CLIP encoders and 3D geometric priors, avoiding training.

Result: Achieves 14.7% mIoU on ScanNet200 and 16.2% mIoU on Matterport160, outperforming trained 3D networks.

Conclusion: MVOV3D effectively enhances open-vocabulary 3D scene understanding by optimizing multi-view fusion.

Abstract: Recent open-vocabulary 3D scene understanding approaches mainly focus on
training 3D networks through contrastive learning with point-text pairs or by
distilling 2D features into 3D models via point-pixel alignment. While these
methods show considerable performance in benchmarks with limited vocabularies,
they struggle to handle diverse object categories as the limited amount of 3D
data upbound training strong open-vocabulary 3d models. We observe that 2D
multi-view fusion methods take precedence in understanding diverse concepts in
3D scenes. However, inherent noises in vision-language models lead multi-view
fusion to sub-optimal performance. To this end, we introduce MVOV3D, a novel
approach aimed at unleashing the potential of 2D multi-view fusion for
open-vocabulary 3D scene understanding. We focus on reducing the inherent
noises without training, thereby preserving the generalizability while
enhancing open-world capabilities. Specifically, MVOV3D improves multi-view 2D
features by leveraging precise region-level image features and text features
encoded by CLIP encoders and incorporates 3D geometric priors to optimize
multi-view fusion. Extensive experiments on various datasets demonstrate the
effectiveness of our method. Notably, our MVOV3D achieves a new record with
14.7% mIoU on ScanNet200 and 16.2% mIoU on Matterport160 for challenge
open-vocabulary semantic segmentation, outperforming current leading trained 3D
networks by a significant margin.

</details>


### [201] [Prompting without Panic: Attribute-aware, Zero-shot, Test-Time Calibration](https://arxiv.org/abs/2506.22819)
*Ramya Hebbalaguppe,Tamoghno Kandar,Abhinav Nagpal,Chetan Arora*

Main category: cs.CV

TL;DR: The paper addresses the issue of confidence calibration degradation in vision-language models (VLMs) during test-time prompt tuning (TPT), proposing a method to improve calibration by leveraging LLM knowledge and a novel regularization loss.


<details>
  <summary>Details</summary>
Motivation: Current TPT methods focus on accuracy but degrade confidence calibration, limiting their use in critical applications. The paper aims to address this by improving calibration through better prompt initialization and regularization.

Method: The proposed method, TCA, initializes prompts using prior knowledge from LLMs and introduces a regularization loss to reduce intraclass distance and increase inter-class distance during TPT.

Result: Experiments show TCA reduces expected calibration error (ECE) to 4.11, outperforming other methods (vanilla TPT: 11.7, C-TPT: 6.12, DiffTPT: 6.78, PromptAlign: 8.43).

Conclusion: TCA effectively improves calibration in VLMs during TPT, making it more reliable for critical applications.

Abstract: Vision-language models (VLM) have demonstrated impressive performance in
image recognition by leveraging self-supervised training on large datasets.
Their performance can be further improved by adapting to the test sample using
test-time prompt tuning (TPT). Unfortunately, the singular focus of TPT
approaches on improving the accuracy suffers from tunnel vision, and leads to
degradation in confidence calibration. This limits the applicability of TPT in
critical applications.
  We make three contributions in this work. (1) We posit that random or naive
initialization of prompts leads to overfitting on a particular test sample, and
is the main reason for miscalibration of the VLM after TPT. To mitigate the
problem, we propose careful initialization of test time prompt using prior
knowledge about the target label attributes from a large language model (LLM);
(2) To further maintain the quality of prompts during \tpt, we propose a novel
regularization loss to reduce intraclass distance, and increase inter-class
distance between the learnt
  Through extensive experiments on different CLIP architectures and 15
datasets, we show that our approach can effectively improve the calibration
after TPT. We report an average expected calibration error (ECE) of 4.11 with
our method, TCA, compared to 11.7 for vanilla TPT, 6.12 for C-TPT (ICLR'24),
6.78 for DiffTPT (CVPR'23), and 8.43 for PromptAlign (NeurIPS'23). The code is
publicly accessible at:
https://github.com/rhebbalaguppe/TCA_PromptWithoutPanic.

</details>


### [202] [Listener-Rewarded Thinking in VLMs for Image Preferences](https://arxiv.org/abs/2506.22832)
*Alexander Gambashidze,Li Pengyi,Matvey Skripkin,Andrey Galichin,Anton Gusarov,Konstantin Sobolev,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.CV

TL;DR: The paper introduces a listener-augmented GRPO framework to improve reward models for human visual preferences by addressing reasoning contradictions and enhancing generalization.


<details>
  <summary>Details</summary>
Motivation: Current reward models for human visual preferences often fail to generalize, and supervised fine-tuning leads to memorization, demanding complex annotation pipelines.

Method: The proposed method uses a listener-augmented GRPO framework, where a frozen vision-language model re-evaluates the reasoner's chain-of-thought to provide a dense, calibrated confidence score, shaping the RL reward signal.

Result: The listener-shaped reward scheme achieves 67.4% accuracy on the ImageReward benchmark, improves OOD performance by up to +6%, and reduces reasoning contradictions compared to baselines.

Conclusion: Listener-based rewards offer a scalable, data-efficient way to align vision-language models with nuanced human preferences.

Abstract: Training robust and generalizable reward models for human visual preferences
is essential for aligning text-to-image and text-to-video generative models
with human intent. However, current reward models often fail to generalize, and
supervised fine-tuning leads to memorization, demanding complex annotation
pipelines. While reinforcement learning (RL), specifically Group Relative
Policy Optimization (GRPO), improves generalization, we uncover a key failure
mode: a significant drop in reasoning accuracy occurs when a model's reasoning
trace contradicts that of an independent, frozen vision-language model
("listener") evaluating the same output. To address this, we introduce a
listener-augmented GRPO framework. Here, the listener re-evaluates the
reasoner's chain-of-thought to provide a dense, calibrated confidence score,
shaping the RL reward signal. This encourages the reasoner not only to answer
correctly, but to produce explanations that are persuasive to an independent
model. Our listener-shaped reward scheme achieves best accuracy on the
ImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)
performance on a large-scale human preference dataset (1.2M votes, up to +6%
over naive reasoner), and reduces reasoning contradictions compared to strong
GRPO and SFT baselines. These results demonstrate that listener-based rewards
provide a scalable, data-efficient path to aligning vision-language models with
nuanced human preferences. We will release our reasoning model here:
https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.

</details>


### [203] [SemFaceEdit: Semantic Face Editing on Generative Radiance Manifolds](https://arxiv.org/abs/2506.22833)
*Shashikant Verma,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: SemFaceEdit introduces a method for localized facial editing in 3D-aware GANs by generating semantic fields on generative radiance manifolds, enabling precise control over geometry and appearance.


<details>
  <summary>Details</summary>
Motivation: Existing 3D-aware GANs lack localized editing capabilities, limiting their practicality for fine-grained facial modifications.

Method: SemFaceEdit uses two modules: Geometry (for semantic radiance and occupancy fields) and Appearance (for RGB radiance), trained adversarially with latent codes for disentanglement.

Result: The method achieves superior radiance field disentanglement and precise editing of facial semantics while preserving other regions.

Conclusion: SemFaceEdit advances localized editing in 3D-aware GANs, offering improved control and disentanglement for facial semantics.

Abstract: Despite multiple view consistency offered by 3D-aware GAN techniques, the
resulting images often lack the capacity for localized editing. In response,
generative radiance manifolds emerge as an efficient approach for constrained
point sampling within volumes, effectively reducing computational demands and
enabling the learning of fine details. This work introduces SemFaceEdit, a
novel method that streamlines the appearance and geometric editing process by
generating semantic fields on generative radiance manifolds. Utilizing latent
codes, our method effectively disentangles the geometry and appearance
associated with different facial semantics within the generated image. In
contrast to existing methods that can change the appearance of the entire
radiance field, our method enables the precise editing of particular facial
semantics while preserving the integrity of other regions. Our network
comprises two key modules: the Geometry module, which generates semantic
radiance and occupancy fields, and the Appearance module, which is responsible
for predicting RGB radiance. We jointly train both modules in adversarial
settings to learn semantic-aware geometry and appearance descriptors. The
appearance descriptors are then conditioned on their respective semantic latent
codes by the Appearance Module, facilitating disentanglement and enhanced
control. Our experiments highlight SemFaceEdit's superior performance in
semantic field-based editing, particularly in achieving improved radiance field
disentanglement.

</details>


### [204] [FOCUS: Fine-grained Optimization with Semantic Guided Understanding for Pedestrian Attributes Recognition](https://arxiv.org/abs/2506.22836)
*Hongyan An,Kuan Zhu,Xin He,Haiyun Guo,Chaoyang Zhao,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: The paper introduces FOCUS, a method for Pedestrian Attribute Recognition (PAR) that adaptively extracts fine-grained attribute-level features using semantic guidance, improving performance and generalization to unseen attributes.


<details>
  <summary>Details</summary>
Motivation: Existing PAR methods rely on regional features, which may compromise fine-grained patterns and fail to generalize to unseen attributes. The goal is to overcome these limitations.

Method: Proposes FOCUS with Multi-Granularity Mix Tokens (MGMT) for diverse feature capture, Attribute-guided Visual Feature Extraction (AVFE) for semantic-guided retrieval, and Region-Aware Contrastive Learning (RACL) for consistent attention.

Result: Demonstrates effectiveness and strong generalization on PA100K, PETA, and RAPv1 datasets.

Conclusion: FOCUS advances PAR by adaptively extracting fine-grained features and generalizing to unseen attributes, outperforming existing methods.

Abstract: Pedestrian attribute recognition (PAR) is a fundamental perception task in
intelligent transportation and security. To tackle this fine-grained task, most
existing methods focus on extracting regional features to enrich attribute
information. However, a regional feature is typically used to predict a fixed
set of pre-defined attributes in these methods, which limits the performance
and practicality in two aspects: 1) Regional features may compromise
fine-grained patterns unique to certain attributes in favor of capturing common
characteristics shared across attributes. 2) Regional features cannot
generalize to predict unseen attributes in the test time. In this paper, we
propose the \textbf{F}ine-grained \textbf{O}ptimization with semanti\textbf{C}
g\textbf{U}ided under\textbf{S}tanding (FOCUS) approach for PAR, which
adaptively extracts fine-grained attribute-level features for each attribute
individually, regardless of whether the attributes are seen or not during
training. Specifically, we propose the Multi-Granularity Mix Tokens (MGMT) to
capture latent features at varying levels of visual granularity, thereby
enriching the diversity of the extracted information. Next, we introduce the
Attribute-guided Visual Feature Extraction (AVFE) module, which leverages
textual attributes as queries to retrieve their corresponding visual attribute
features from the Mix Tokens using a cross-attention mechanism. To ensure that
textual attributes focus on the appropriate Mix Tokens, we further incorporate
a Region-Aware Contrastive Learning (RACL) method, encouraging attributes
within the same region to share consistent attention maps. Extensive
experiments on PA100K, PETA, and RAPv1 datasets demonstrate the effectiveness
and strong generalization ability of our method.

</details>


### [205] [AG-VPReID 2025: Aerial-Ground Video-based Person Re-identification Challenge Results](https://arxiv.org/abs/2506.22843)
*Kien Nguyen,Clinton Fookes,Sridha Sridharan,Huy Nguyen,Feng Liu,Xiaoming Liu,Arun Ross,Dana Michalski,Tamás Endrei,Ivan DeAndres-Tame,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez,Javier Ortega-Garcia,Zijing Gong,Yuhao Wang,Xuehu Liu,Pingping Zhang,Md Rashidunnabi,Hugo Proença,Kailash A. Hambarde,Saeid Rezaei*

Main category: cs.CV

TL;DR: The AG-VPReID 2025 Challenge introduces a large-scale video-based competition for aerial-ground person re-identification (ReID), addressing challenges like viewpoint differences and scale variations. The dataset includes 3,027 identities and 3.7 million frames, with top-performing methods achieving over 70% Rank-1 accuracy.


<details>
  <summary>Details</summary>
Motivation: Bridging the aerial-ground domain gap in ReID is critical for surveillance and public safety, but challenges like viewpoint differences and occlusions persist.

Method: The challenge involved four teams using multi-stream architectures, transformer-based temporal reasoning, and physics-informed modeling.

Result: The leading method, X-TFCLIP, achieved 72.28% aerial-to-ground and 70.77% ground-to-aerial Rank-1 accuracy.

Conclusion: The AG-VPReID 2025 Challenge advances aerial-ground ReID, showcasing the dataset's complexity and the potential of innovative methods.

Abstract: Person re-identification (ReID) across aerial and ground vantage points has
become crucial for large-scale surveillance and public safety applications.
Although significant progress has been made in ground-only scenarios, bridging
the aerial-ground domain gap remains a formidable challenge due to extreme
viewpoint differences, scale variations, and occlusions. Building upon the
achievements of the AG-ReID 2023 Challenge, this paper introduces the AG-VPReID
2025 Challenge - the first large-scale video-based competition focused on
high-altitude (80-120m) aerial-ground ReID. Constructed on the new AG-VPReID
dataset with 3,027 identities, over 13,500 tracklets, and approximately 3.7
million frames captured from UAVs, CCTV, and wearable cameras, the challenge
featured four international teams. These teams developed solutions ranging from
multi-stream architectures to transformer-based temporal reasoning and
physics-informed modeling. The leading approach, X-TFCLIP from UAM, attained
72.28% Rank-1 accuracy in the aerial-to-ground ReID setting and 70.77% in the
ground-to-aerial ReID setting, surpassing existing baselines while highlighting
the dataset's complexity. For additional details, please refer to the official
website at https://agvpreid25.github.io.

</details>


### [206] [DMD-Net: Deep Mesh Denoising Network](https://arxiv.org/abs/2506.22850)
*Aalok Gangopadhyay,Shashikant Verma,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: DMD-Net is a deep learning framework for mesh denoising using a dual-stream GCN and a Feature Guided Transformer, achieving state-of-the-art performance even with high noise.


<details>
  <summary>Details</summary>
Motivation: To address the mesh denoising problem effectively, leveraging deep learning for robust and high-quality results.

Method: Uses a dual-stream Graph Convolutional Neural Network (primal and dual graphs) with a Feature Guided Transformer (feature extractor, transformer, denoiser) for denoising.

Result: Competitive or superior performance compared to state-of-the-art methods, robust to various noise levels.

Conclusion: DMD-Net is an effective and robust solution for mesh denoising, excelling even under extreme noise conditions.

Abstract: We present Deep Mesh Denoising Network (DMD-Net), an end-to-end deep learning
framework, for solving the mesh denoising problem. DMD-Net consists of a Graph
Convolutional Neural Network in which aggregation is performed in both the
primal as well as the dual graph. This is realized in the form of an asymmetric
two-stream network, which contains a primal-dual fusion block that enables
communication between the primal-stream and the dual-stream. We develop a
Feature Guided Transformer (FGT) paradigm, which consists of a feature
extractor, a transformer, and a denoiser. The feature extractor estimates the
local features, that guide the transformer to compute a transformation, which
is applied to the noisy input mesh to obtain a useful intermediate
representation. This is further processed by the denoiser to obtain the
denoised mesh. Our network is trained on a large scale dataset of 3D objects.
We perform exhaustive ablation studies to demonstrate that each component in
our network is essential for obtaining the best performance. We show that our
method obtains competitive or better results when compared with the
state-of-the-art mesh denoising algorithms. We demonstrate that our method is
robust to various kinds of noise. We observe that even in the presence of
extremely high noise, our method achieves excellent performance.

</details>


### [207] [Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval](https://arxiv.org/abs/2506.22864)
*Li-Cheng Shen,Jih-Kang Hsieh,Wei-Hua Li,Chu-Song Chen*

Main category: cs.CV

TL;DR: MaTIR unifies text-to-image retrieval (TIR) and referring expression segmentation (RES) for efficient search and accurate segmentation, using a two-stage framework with SAM and Alpha-CLIP for offline mask generation and MLLM for reranking.


<details>
  <summary>Details</summary>
Motivation: Existing TIR lacks interpretability, while RES is computationally expensive for large datasets. MaTIR bridges this gap.

Method: Two-stage framework: 1) Segmentation-aware retrieval with SAM and Alpha-CLIP for offline mask/embedding generation. 2) MLLM for reranking and object grounding.

Result: Improved retrieval accuracy and segmentation quality on COCO and D$^3$ datasets.

Conclusion: MaTIR effectively combines TIR and RES, offering scalable and accurate results.

Abstract: Text-to-image retrieval (TIR) aims to find relevant images based on a textual
query, but existing approaches are primarily based on whole-image captions and
lack interpretability. Meanwhile, referring expression segmentation (RES)
enables precise object localization based on natural language descriptions but
is computationally expensive when applied across large image collections. To
bridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies
TIR and RES, requiring both efficient image search and accurate object
segmentation. To address this task, we propose a two-stage framework,
comprising a first stage for segmentation-aware image retrieval and a second
stage for reranking and object grounding with a multimodal large language model
(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract
region-level embeddings offline at first, enabling effective and scalable
online retrieval. Secondly, MLLM is used to refine retrieval rankings and
generate bounding boxes, which are matched to segmentation masks. We evaluate
our approach on COCO and D$^3$ datasets, demonstrating significant improvements
in both retrieval accuracy and segmentation quality over previous methods.

</details>


### [208] [Region-Aware CAM: High-Resolution Weakly-Supervised Defect Segmentation via Salient Region Perception](https://arxiv.org/abs/2506.22866)
*Hang-Cheng Dong,Lu Zou,Bingguo Liu,Dong Ye,Guodong Liu*

Main category: cs.CV

TL;DR: A novel weakly supervised semantic segmentation framework for industrial defect detection, using region-aware CAM and pseudo-label training, outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the reliance on large annotated datasets in defect detection by proposing a weakly supervised approach.

Method: Combines filtering-guided backpropagation (FGBP) for refined target regions and a region-aware weighted module for spatial precision, followed by pseudo-label training.

Result: Superior performance on industrial defect datasets, bridging the gap between weak supervision and high-precision segmentation.

Conclusion: The framework offers a practical solution for defect detection in resource-constrained settings.

Abstract: Surface defect detection plays a critical role in industrial quality
inspection. Recent advances in artificial intelligence have significantly
enhanced the automation level of detection processes. However, conventional
semantic segmentation and object detection models heavily rely on large-scale
annotated datasets, which conflicts with the practical requirements of defect
detection tasks. This paper proposes a novel weakly supervised semantic
segmentation framework comprising two key components: a region-aware class
activation map (CAM) and pseudo-label training. To address the limitations of
existing CAM methods, especially low-resolution thermal maps, and insufficient
detail preservation, we introduce filtering-guided backpropagation (FGBP),
which refines target regions by filtering gradient magnitudes to identify areas
with higher relevance to defects. Building upon this, we further develop a
region-aware weighted module to enhance spatial precision. Finally,
pseudo-label segmentation is implemented to refine the model's performance
iteratively. Comprehensive experiments on industrial defect datasets
demonstrate the superiority of our method. The proposed framework effectively
bridges the gap between weakly supervised learning and high-precision defect
segmentation, offering a practical solution for resource-constrained industrial
scenarios.

</details>


### [209] [STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing](https://arxiv.org/abs/2506.22868)
*Junsung Lee,Junoh Kang,Bohyung Han*

Main category: cs.CV

TL;DR: STR-Match is a training-free video editing method using latent optimization guided by a novel STR score to improve spatiotemporal consistency and visual quality.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations like temporal inconsistency, motion distortion, and limited domain transformation in text-guided video editing by modeling spatiotemporal pixel relevance.

Method: Proposes STR-Match, leveraging 2D spatial attention and 1D temporal modules in T2V diffusion models for latent optimization with a latent mask.

Result: Outperforms existing methods in visual quality and spatiotemporal consistency, even under significant domain transformations.

Conclusion: STR-Match effectively enhances video editing by ensuring coherence and preserving source attributes without expensive 3D attention.

Abstract: Previous text-guided video editing methods often suffer from temporal
inconsistency, motion distortion, and-most notably-limited domain
transformation. We attribute these limitations to insufficient modeling of
spatiotemporal pixel relevance during the editing process. To address this, we
propose STR-Match, a training-free video editing algorithm that produces
visually appealing and spatiotemporally coherent videos through latent
optimization guided by our novel STR score. The score captures spatiotemporal
pixel relevance across adjacent frames by leveraging 2D spatial attention and
1D temporal modules in text-to-video (T2V) diffusion models, without the
overhead of computationally expensive 3D attention mechanisms. Integrated into
a latent optimization framework with a latent mask, STR-Match generates
temporally consistent and visually faithful videos, maintaining strong
performance even under significant domain transformations while preserving key
visual attributes of the source. Extensive experiments demonstrate that
STR-Match consistently outperforms existing methods in both visual quality and
spatiotemporal consistency.

</details>


### [210] [Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and Grounder](https://arxiv.org/abs/2506.22880)
*Dang Jisheng,Wu Xudong,Wang Bimei,Lv Ning,Chen Jiayu,Jingwen Zhao,Yichu liu,Jizhao Liu,Juncheng Li,Teng Wang*

Main category: cs.CV

TL;DR: DeSa2VA introduces a decoupling-enhanced prompting scheme to improve segmentation accuracy by disentangling visual and textual features, outperforming existing methods in multiple tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods like Sa2VA entangle dynamic visual and static semantic features, degrading segmentation accuracy. DeSa2VA aims to systematically mitigate this issue.

Method: DeSa2VA uses text pre-training, a linear decoupling module, and dynamic mask fusion to disentangle and synergistically combine textual and visual features.

Result: The method achieves state-of-the-art performance in image/video segmentation and question answering tasks.

Conclusion: DeSa2VA effectively addresses feature entanglement, enhancing segmentation accuracy and semantic grounding.

Abstract: Existing video segmenter and grounder approaches, exemplified by Sa2VA,
directly fuse features within segmentation models. This often results in an
undesirable entanglement of dynamic visual information and static semantics,
thereby degrading segmentation accuracy. To systematically mitigate this issue,
we propose DeSa2VA, a decoupling-enhanced prompting scheme integrating text
pre-training and a linear decoupling module to address the information
processing limitations inherent in SAM-2. Specifically, first, we devise a
pre-training paradigm that converts textual ground-truth labels into
point-level prompts while generating corresponding text masks. These masks are
refined through a hybrid loss function to strengthen the model's semantic
grounding capabilities. Next, we employ linear projection to disentangle hidden
states that generated by a large language model into distinct textual and
visual feature subspaces. Finally, a dynamic mask fusion strategy
synergistically combines these decoupled features through triple supervision
from predicted text/visual masks and ground-truth annotations. Extensive
experiments demonstrate state-of-the-art performance across diverse tasks,
including image segmentation, image question answering, video segmentation, and
video question answering. Our codes are available at
https://github.com/longmalongma/DeSa2VA.

</details>


### [211] [How Semantically Informative is an Image?: Measuring the Covariance-Weighted Norm of Contrastive Learning Embeddings](https://arxiv.org/abs/2506.22881)
*Fumiya Uchiyama,Rintaro Yanagi,Shohei Taniguchi,Shota Takashiro,Masahiro Suzuki,Hirokatsu Kataoka,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CV

TL;DR: The paper introduces a metric for semantic informativeness in multimodal contrastive learning, extending Information Gain to vision and language, and validates it with strong empirical results.


<details>
  <summary>Details</summary>
Motivation: To address the gap in representing absolute semantic informativeness in contrastive learning models, despite their success in relational similarity.

Method: Proposes a semantic informativeness metric using contrastive learning, redefines Information Gain for vision and language, and validates it with OpenCLIP and SigLIP models.

Result: Strong correlation (0.98-1.00) between the proposed metric and empirical results, with low computational cost and compatibility with open-weight models.

Conclusion: The method effectively quantifies semantic informativeness, offering practical applications in multimodal learning with scalable and efficient computation.

Abstract: Contrastive learning has the capacity to model multimodal probability
distributions by embedding and aligning visual representations with semantics
from captions. This approach enables the estimation of relational semantic
similarity; however, it remains unclear whether it can also represent absolute
semantic informativeness. In this work, we introduce a semantic informativeness
metric for an image calculated from text samples via a contrastive learning
model; similarly, the informativeness of a text is calculated from image
samples. We propose a redefinition of the concept of Information Gain, a
concept previously explored in natural language processing, extending its
application to the domains of vision and language. Our metric quantifies how
conditioning on an image distorts the distribution of associated texts, and
vice versa for text conditioning on image distributions. In OpenCLIP's
empirical results, we observe that images with the lowest Information Gain
scores often correspond to placeholder icons such as "image not found."
Furthermore, we propose to measure a norm-based metric of the embedding to
estimate the Information Gain, following the theoretical results for Skip-Gram
with Negative Sampling (SGNS) word embedding. Information Gain can be measured
using either CLIP or SigLIP, and the results demonstrate a strong correlation
with a coefficient of determination ranging from 0.98 to 1.00. After obtaining
the mean and the covariance of the sample embedding, the computational cost of
this method is independent of the sample size, and it is compatible with
publicly available, open-weight models.

</details>


### [212] [CP-Guard: A Unified, Probability-Agnostic, and Adaptive Framework for Malicious Agent Detection and Defense in Multi-Agent Embodied Perception Systems](https://arxiv.org/abs/2506.22890)
*Senkang Hu,Yihang Tao,Guowen Xu,Xinyuan Qian,Yiqin Deng,Xianhao Chen,Sam Tak Wu Kwong,Yuguang Fang*

Main category: cs.CV

TL;DR: CP-Guard is a defense framework for Collaborative Perception (CP) that detects and eliminates malicious agents by ensuring consensus among collaborators, using PASAC, CCLoss, and adaptive thresholds.


<details>
  <summary>Details</summary>
Motivation: CP is vulnerable to attacks from malicious agents, necessitating a robust defense mechanism to maintain system reliability.

Method: Develops PASAC for sampling and consensus verification, defines CCLoss for discrepancy measurement, and uses adaptive thresholds for dynamic environments.

Result: Demonstrates effectiveness in detecting and eliminating malicious agents, ensuring reliable CP performance.

Conclusion: CP-Guard provides a unified, adaptive solution to secure CP systems against malicious attacks.

Abstract: Collaborative Perception (CP) has been shown to be a promising technique for
multi-agent autonomous driving and multi-agent robotic systems, where multiple
agents share their perception information to enhance the overall perception
performance and expand the perception range. However, in CP, an ego agent needs
to receive messages from its collaborators, which makes it vulnerable to
attacks from malicious agents. To address this critical issue, we propose a
unified, probability-agnostic, and adaptive framework, namely, CP-Guard, which
is a tailored defense mechanism for CP deployed by each agent to accurately
detect and eliminate malicious agents in its collaboration network. Our key
idea is to enable CP to reach a consensus rather than a conflict against an ego
agent's perception results. Based on this idea, we first develop a
probability-agnostic sample consensus (PASAC) method to effectively sample a
subset of the collaborators and verify the consensus without prior
probabilities of malicious agents. Furthermore, we define collaborative
consistency loss (CCLoss) for object detection task and bird's eye view (BEV)
segmentation task to capture the discrepancy between an ego agent and its
collaborators, which is used as a verification criterion for consensus. In
addition, we propose online adaptive threshold via dual sliding windows to
dynamically adjust the threshold for consensus verification and ensure the
reliability of the systems in dynamic environments. Finally, we conduct
extensive experiments and demonstrate the effectiveness of our framework. Code
will be released at https://github.com/CP-Security/CP-Guard

</details>


### [213] [Neural Cellular Automata: From Cells to Pixels](https://arxiv.org/abs/2506.22899)
*Ehsan Pajouheshgar,Yitao Xu,Ali Abbasi,Alexander Mordvintsev,Wenzel Jakob,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: The paper introduces an implicit decoder to scale Neural Cellular Automata (NCAs) to high-resolution grids, addressing training, memory, and computational limitations while preserving emergent properties.


<details>
  <summary>Details</summary>
Motivation: NCAs are limited to low-resolution grids due to quadratic growth in training time, local information propagation, and high compute demands. The goal is to enable high-resolution outputs efficiently.

Method: Pair NCA with a tiny shared implicit decoder for rendering high-resolution outputs. Introduce novel loss functions for morphogenesis and texture synthesis tailored for minimal overhead.

Result: The proposed framework allows NCAs to generate full-HD outputs in real time, maintaining self-organizing properties and efficiency across 2D, 3D grids, and meshes.

Conclusion: The implicit decoder and tailored loss functions enable NCAs to scale to high-resolution outputs with minimal computational overhead, enhancing quality and performance.

Abstract: Neural Cellular Automata (NCAs) are bio-inspired systems in which identical
cells self-organize to form complex and coherent patterns by repeatedly
applying simple local rules. NCAs display striking emergent behaviors including
self-regeneration, generalization and robustness to unseen situations, and
spontaneous motion. Despite their success in texture synthesis and
morphogenesis, NCAs remain largely confined to low-resolution grids. This
limitation stems from (1) training time and memory requirements that grow
quadratically with grid size, (2) the strictly local propagation of information
which impedes long-range cell communication, and (3) the heavy compute demands
of real-time inference at high resolution. In this work, we overcome this
limitation by pairing NCA with a tiny, shared implicit decoder, inspired by
recent advances in implicit neural representations. Following NCA evolution on
a coarse grid, a lightweight decoder renders output images at arbitrary
resolution. We also propose novel loss functions for both morphogenesis and
texture synthesis tasks, specifically tailored for high-resolution output with
minimal memory and computation overhead. Combining our proposed architecture
and loss functions brings substantial improvement in quality, efficiency, and
performance. NCAs equipped with our implicit decoder can generate full-HD
outputs in real time while preserving their self-organizing, emergent
properties. Moreover, because each MLP processes cell states independently,
inference remains highly parallelizable and efficient. We demonstrate the
applicability of our approach across multiple NCA variants (on 2D, 3D grids,
and 3D meshes) and multiple tasks, including texture generation and
morphogenesis (growing patterns from a seed), showing that with our proposed
framework, NCAs seamlessly scale to high-resolution outputs with minimal
computational overhead.

</details>


### [214] [MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering](https://arxiv.org/abs/2506.22900)
*Mai A. Shaaban,Tausifa Jan Saleem,Vijay Ram Papineni,Mohammad Yaqub*

Main category: cs.CV

TL;DR: MOTOR improves MedVQA accuracy by 6.45% using multimodal retrieval and re-ranking with grounded captions and optimal transport.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs for MedVQA often produce incorrect answers, and retrieval-augmented methods risk irrelevant context, ignoring visual/multimodal cues crucial for medical diagnosis.

Method: MOTOR combines grounded captions and optimal transport to align query and context using textual and visual information.

Result: MOTOR outperforms state-of-the-art methods by 6.45% on MedVQA datasets.

Conclusion: MOTOR enhances clinical relevance in MedVQA by integrating multimodal context, improving accuracy significantly.

Abstract: Medical visual question answering (MedVQA) plays a vital role in clinical
decision-making by providing contextually rich answers to image-based queries.
Although vision-language models (VLMs) are widely used for this task, they
often generate factually incorrect answers. Retrieval-augmented generation
addresses this challenge by providing information from external sources, but
risks retrieving irrelevant context, which can degrade the reasoning
capabilities of VLMs. Re-ranking retrievals, as introduced in existing
approaches, enhances retrieval relevance by focusing on query-text alignment.
However, these approaches neglect the visual or multimodal context, which is
particularly crucial for medical diagnosis. We propose MOTOR, a novel
multimodal retrieval and re-ranking approach that leverages grounded captions
and optimal transport. It captures the underlying relationships between the
query and the retrieved context based on textual and visual information.
Consequently, our approach identifies more clinically relevant contexts to
augment the VLM input. Empirical analysis and human expert evaluation
demonstrate that MOTOR achieves higher accuracy on MedVQA datasets,
outperforming state-of-the-art methods by an average of 6.45%. Code is
available at https://github.com/BioMedIA-MBZUAI/MOTOR.

</details>


### [215] [Point Cloud Compression and Objective Quality Assessment: A Survey](https://arxiv.org/abs/2506.22902)
*Yiling Xu,Yujie Zhang,Shuting Xia,Kaifa Yang,He Huang,Ziyu Shan,Wenjie Huang,Qi Yang,Le Yang*

Main category: cs.CV

TL;DR: A survey on point cloud compression (PCC) and quality assessment (PCQA), highlighting challenges, methods, and future directions for efficient 3D applications.


<details>
  <summary>Details</summary>
Motivation: The surge in 3D point cloud data necessitates efficient compression and quality assessment due to its irregular structure and high volume, especially for real-time and perceptually relevant applications.

Method: Analyzes handcrafted and learning-based PCC algorithms and objective PCQA metrics, benchmarking them on emerging datasets for detailed comparisons.

Result: Identifies strengths and limitations of current methods, noting challenges like visual fidelity, latency, and multimodal data support.

Conclusion: Proposes future directions, such as hybrid compression frameworks and advanced feature extraction, to improve 3D applications.

Abstract: The rapid growth of 3D point cloud data, driven by applications in autonomous
driving, robotics, and immersive environments, has led to criticals demand for
efficient compression and quality assessment techniques. Unlike traditional 2D
media, point clouds present unique challenges due to their irregular structure,
high data volume, and complex attributes. This paper provides a comprehensive
survey of recent advances in point cloud compression (PCC) and point cloud
quality assessment (PCQA), emphasizing their significance for real-time and
perceptually relevant applications. We analyze a wide range of handcrafted and
learning-based PCC algorithms, along with objective PCQA metrics. By
benchmarking representative methods on emerging datasets, we offer detailed
comparisons and practical insights into their strengths and limitations.
Despite notable progress, challenges such as enhancing visual fidelity,
reducing latency, and supporting multimodal data remain. This survey outlines
future directions, including hybrid compression frameworks and advanced feature
extraction strategies, to enable more efficient, immersive, and intelligent 3D
applications.

</details>


### [216] [MagShield: Towards Better Robustness in Sparse Inertial Motion Capture Under Magnetic Disturbances](https://arxiv.org/abs/2506.22907)
*Yunzhe Shao,Xinyu Yi,Lu Yin,Shihui Guo,Junhai Yong,Feng Xu*

Main category: cs.CV

TL;DR: MagShield is a novel method to mitigate magnetic interference in sparse inertial MoCap systems by detecting and correcting disturbances using multi-IMU analysis and motion priors.


<details>
  <summary>Details</summary>
Motivation: Existing IMU systems suffer from orientation errors in magnetically disturbed environments, limiting real-world usability.

Method: MagShield uses a 'detect-then-correct' approach: detecting disturbances via multi-IMU joint analysis and correcting errors with human motion priors.

Result: MagShield improves motion capture accuracy under interference and is compatible with various sparse inertial MoCap systems.

Conclusion: MagShield effectively addresses magnetic interference, enhancing the practicality of sparse inertial MoCap systems.

Abstract: This paper proposes a novel method called MagShield, designed to address the
issue of magnetic interference in sparse inertial motion capture (MoCap)
systems. Existing Inertial Measurement Unit (IMU) systems are prone to
orientation estimation errors in magnetically disturbed environments, limiting
their practical application in real-world scenarios. To address this problem,
MagShield employs a "detect-then-correct" strategy, first detecting magnetic
disturbances through multi-IMU joint analysis, and then correcting orientation
errors using human motion priors. MagShield can be integrated with most
existing sparse inertial MoCap systems, improving their performance in
magnetically disturbed environments. Experimental results demonstrate that
MagShield significantly enhances the accuracy of motion capture under magnetic
interference and exhibits good compatibility across different sparse inertial
MoCap systems.

</details>


### [217] [Attention to Burstiness: Low-Rank Bilinear Prompt Tuning](https://arxiv.org/abs/2506.22908)
*Yuzhu Wang,Manni Duan,Shu Kong*

Main category: cs.CV

TL;DR: Visual Prompt Tuning (VPT) is enhanced by whitening data to address non-Gaussian distributions, leading to Bilinear Prompt Tuning (BPT), which improves accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The non-Gaussian distributions in patch embeddings and key/query projectors hinder prompt learning in VPT, motivating the need for whitening.

Method: Proposes whitening data and a bilinear model (BPT) to de-correlate and equalize variance, improving prompt tuning.

Result: BPT significantly boosts accuracy (e.g., +25 points on CUB) and reduces parameters/computation.

Conclusion: BPT outperforms VPT methods, offering a more efficient and effective approach to prompt tuning.

Abstract: Visual Prompt Tuning (VPT) is a parameter-efficient fune-tuning technique
that adapts a pre-trained vision Transformer (ViT) by learning a small set of
parameters in the input space, known as prompts. In VPT, we uncover
``burstiness'' in the values arising from the interaction of image patch
embeddings, and the key and query projectors within Transformer's
self-attention module. Furthermore, the values of patch embeddings and the key
and query projectors exhibit Laplacian and hyper-Laplacian distribution,
respectively. Intuitively, these non-Gaussian distributions pose challenges for
learning prompts. To address this, we propose whitening these data,
de-correlating them and equalizing their variance towards more Gaussian before
learning prompts. We derive the whitening matrix over random image patch
embeddings and ViT's key and query projectors, and multiply it with the prompt
to be learned in a bilinear manner. Surprisingly, this method significantly
accelerates prompt tuning and boosts accuracy, e.g., $>$25 accuracy points on
the CUB dataset; interestingly, it learns ``bursty prompts''. Extending the
bilinear model which is known to introduce burstiness, we present a compact,
low-rank version by learning two smaller matrices whose multiplication yields
the final prompts. We call the proposed methods Bilinear Prompt Tuning (BPT).
Extensive experiments across multiple benchmark datasets demonstrate that BPT
methods not only outperform various VPT methods but also reduce parameter count
and computation overhead.

</details>


### [218] [Towards Explainable Bilingual Multimodal Misinformation Detection and Localization](https://arxiv.org/abs/2506.22930)
*Yiwei He,Xiangtai Li,Zhenglin Huang,Yi Dong,Hao Fei,Jiangning Zhang,Baoyuan Wu,Guangliang Cheng*

Main category: cs.CV

TL;DR: BiMi is a bilingual multimodal framework for detecting misinformation in news media by analyzing region-level edits and cross-lingual inconsistencies, outperforming baselines in accuracy and explanation quality.


<details>
  <summary>Details</summary>
Motivation: Misinformation in bilingual news media is harder to detect due to subtle edits and cross-lingual inconsistencies, requiring advanced multimodal analysis.

Method: BiMi combines region-level localization, cross-modal/lingual consistency detection, and natural language explanation, enhanced by GRPO for better interpretability.

Result: BiMi outperforms baselines by +8.9 in classification accuracy, +15.9 in localization accuracy, and +2.5 in explanation BERTScore.

Conclusion: BiMi advances multilingual misinformation detection with superior performance and interpretability, supported by the BiMiBench dataset.

Abstract: The increasing realism of multimodal content has made misinformation more
subtle and harder to detect, especially in news media where images are
frequently paired with bilingual (e.g., Chinese-English) subtitles. Such
content often includes localized image edits and cross-lingual inconsistencies
that jointly distort meaning while remaining superficially plausible. We
introduce BiMi, a bilingual multimodal framework that jointly performs
region-level localization, cross-modal and cross-lingual consistency detection,
and natural language explanation for misinformation analysis. To support
generalization, BiMi integrates an online retrieval module that supplements
model reasoning with up-to-date external context. We further release BiMiBench,
a large-scale and comprehensive benchmark constructed by systematically editing
real news images and subtitles, comprising 104,000 samples with realistic
manipulations across visual and linguistic modalities. To enhance
interpretability, we apply Group Relative Policy Optimization (GRPO) to improve
explanation quality, marking the first use of GRPO in this domain. Extensive
experiments demonstrate that BiMi outperforms strong baselines by up to +8.9 in
classification accuracy, +15.9 in localization accuracy, and +2.5 in
explanation BERTScore, advancing state-of-the-art performance in realistic,
multilingual misinformation detection. Code, models, and datasets will be
released.

</details>


### [219] [Utilizing a Novel Deep Learning Method for Scene Categorization in Remote Sensing Data](https://arxiv.org/abs/2506.22939)
*Ghufran A. Omran,Wassan Saad Abduljabbar Hayale,Ahmad AbdulQadir AlRababah,Israa Ibraheem Al-Barazanchi,Ravi Sekhar,Pritesh Shah,Sushma Parihar,Harshavardhan Reddy Penubadi*

Main category: cs.CV

TL;DR: The paper introduces CO-BRNN for scene categorization in remote sensing, achieving 97% accuracy, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: High accuracy in scene categorization from remote sensing is challenging due to noise and data variety.

Method: Proposes Cuttlefish Optimized Bidirectional Recurrent Neural Network (CO-BRNN) and compares it with MLP-CNN, CNN-LSTM, LSTM-CRF, GB, MIRM-CF, and CNN-DA.

Result: CO-BRNN achieved 97% accuracy, surpassing LSTM-CRF (90%), MLP-CNN (85%), and CNN-LSTM (80%).

Conclusion: CO-BRNN is effective for remote sensing scene categorization, emphasizing the need for physical validation of satellite data.

Abstract: Scene categorization (SC) in remotely acquired images is an important subject
with broad consequences in different fields, including catastrophe control,
ecological observation, architecture for cities, and more. Nevertheless, its
several apps, reaching a high degree of accuracy in SC from distant observation
data has demonstrated to be difficult. This is because traditional conventional
deep learning models require large databases with high variety and high levels
of noise to capture important visual features. To address these problems, this
investigation file introduces an innovative technique referred to as the
Cuttlefish Optimized Bidirectional Recurrent Neural Network (CO- BRNN) for type
of scenes in remote sensing data. The investigation compares the execution of
CO-BRNN with current techniques, including Multilayer Perceptron- Convolutional
Neural Network (MLP-CNN), Convolutional Neural Network-Long Short Term Memory
(CNN-LSTM), and Long Short Term Memory-Conditional Random Field (LSTM-CRF),
Graph-Based (GB), Multilabel Image Retrieval Model (MIRM-CF), Convolutional
Neural Networks Data Augmentation (CNN-DA). The results demonstrate that
CO-BRNN attained the maximum accuracy of 97%, followed by LSTM-CRF with 90%,
MLP-CNN with 85%, and CNN-LSTM with 80%. The study highlights the significance
of physical confirmation to ensure the efficiency of satellite data.

</details>


### [220] [YM-WML: A new Yolo-based segmentation Model with Weighted Multi-class Loss for medical imaging](https://arxiv.org/abs/2506.22955)
*Haniyeh Nikkhah,Jafar Tanha,Mahdi Zarrin,SeyedEhsan Roshan,Amin Kazempour*

Main category: cs.CV

TL;DR: YM-WML, a novel model for cardiac image segmentation, integrates a robust backbone, YOLOv11 neck, and attention-based head, achieving superior performance with a Dice score of 91.02.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like class imbalance and complex structures in medical image segmentation.

Method: Proposes YM-WML with a robust backbone, YOLOv11 neck, and attention-based segmentation head, plus the WME loss function for class imbalance.

Result: Achieves a Dice Similarity Coefficient of 91.02 on the ACDC dataset, outperforming state-of-the-art methods.

Conclusion: YM-WML sets a new benchmark in cardiac segmentation with stable training, accuracy, and strong generalization.

Abstract: Medical image segmentation poses significant challenges due to class
imbalance and the complex structure of medical images. To address these
challenges, this study proposes YM-WML, a novel model for cardiac image
segmentation. The model integrates a robust backbone for effective feature
extraction, a YOLOv11 neck for multi-scale feature aggregation, and an
attention-based segmentation head for precise and accurate segmentation. To
address class imbalance, we introduce the Weighted Multi-class Exponential
(WME) loss function. On the ACDC dataset, YM-WML achieves a Dice Similarity
Coefficient of 91.02, outperforming state-of-the-art methods. The model
demonstrates stable training, accurate segmentation, and strong generalization,
setting a new benchmark in cardiac segmentation tasks.

</details>


### [221] [Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image Watermarking Technique for AI-Generated Images](https://arxiv.org/abs/2506.22960)
*Shreyas Dixit,Ashhar Aziz,Shashwat Bajpai,Vasu Sharma,Aman Chadha,Vinija Jain,Amitava Das*

Main category: cs.CV

TL;DR: PECCAVI is a new watermarking technique for AI-generated content that resists visual paraphrase attacks by embedding watermarks in Non-Melting Points (NMPs) and using multi-channel frequency domain methods.


<details>
  <summary>Details</summary>
Motivation: The rise of synthetic content and its potential for disinformation has led to regulatory measures like watermarking, but existing methods are vulnerable to attacks like visual paraphrasing.

Method: PECCAVI embeds watermarks in NMPs (core semantic regions) and uses multi-channel frequency domain watermarking with noisy burnishing to prevent reverse-engineering.

Result: PECCAVI is the first technique to resist visual paraphrase attacks while maintaining image quality.

Conclusion: PECCAVI offers a robust, model-agnostic solution for watermarking AI-generated content, addressing vulnerabilities in current methods.

Abstract: A report by the European Union Law Enforcement Agency predicts that by 2026,
up to 90 percent of online content could be synthetically generated, raising
concerns among policymakers, who cautioned that "Generative AI could act as a
force multiplier for political disinformation. The combined effect of
generative text, images, videos, and audio may surpass the influence of any
single modality." In response, California's Bill AB 3211 mandates the
watermarking of AI-generated images, videos, and audio. However, concerns
remain regarding the vulnerability of invisible watermarking techniques to
tampering and the potential for malicious actors to bypass them entirely.
Generative AI-powered de-watermarking attacks, especially the newly introduced
visual paraphrase attack, have shown an ability to fully remove watermarks,
resulting in a paraphrase of the original image. This paper introduces PECCAVI,
the first visual paraphrase attack-safe and distortion-free image watermarking
technique. In visual paraphrase attacks, an image is altered while preserving
its core semantic regions, termed Non-Melting Points (NMPs). PECCAVI
strategically embeds watermarks within these NMPs and employs multi-channel
frequency domain watermarking. It also incorporates noisy burnishing to counter
reverse-engineering efforts aimed at locating NMPs to disrupt the embedded
watermark, thereby enhancing durability. PECCAVI is model-agnostic. All
relevant resources and codes will be open-sourced.

</details>


### [222] [ActAlign: Zero-Shot Fine-Grained Video Classification via Language-Guided Sequence Alignment](https://arxiv.org/abs/2506.22967)
*Amir Aghdam,Vincent Tao Hu*

Main category: cs.CV

TL;DR: ActAlign, a zero-shot framework, uses sequence alignment and language priors for fine-grained video classification without video-text supervision, outperforming larger models.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive vision-language models lack temporal structure understanding for fine-grained video classification.

Method: ActAlign aligns sub-action sequences (generated by a large language model) with video frames using Dynamic Time Warping (DTW) in a shared embedding space.

Result: Achieves 30.5% accuracy on ActionAtlas (human accuracy: 61.6%) with 8x fewer parameters than larger models.

Conclusion: Structured language priors and classical alignment techniques enhance vision-language models for fine-grained video understanding.

Abstract: We address the task of zero-shot fine-grained video classification, where no
video examples or temporal annotations are available for unseen action classes.
While contrastive vision-language models such as SigLIP demonstrate strong
open-set recognition via mean-pooled image-text similarity, they fail to
capture the temporal structure critical for distinguishing fine-grained
activities. We introduce ActAlign, a zero-shot framework that formulates video
classification as sequence alignment. For each class, a large language model
generates an ordered sub-action sequence, which is aligned with video frames
using Dynamic Time Warping (DTW) in a shared embedding space. Without any
video-text supervision or fine-tuning, ActAlign achieves 30.5% accuracy on the
extremely challenging ActionAtlas benchmark, where human accuracy is only
61.6%. ActAlign outperforms billion-parameter video-language models while using
approximately 8x less parameters. These results demonstrate that structured
language priors, combined with classical alignment techniques, offer a scalable
and general approach to unlocking the open-set recognition potential of
vision-language models for fine-grained video understanding.

</details>


### [223] [Probabilistic Prototype Calibration of Vision-Language Models for Generalized Few-shot Semantic Segmentation](https://arxiv.org/abs/2506.22979)
*Jie Liu,Jiayi Shen,Pan Zhou,Jan-Jakob Sonke,Efstratios Gavves*

Main category: cs.CV

TL;DR: FewCLIP introduces a probabilistic prototype calibration framework for GFSS, improving adaptability and generalization over deterministic methods.


<details>
  <summary>Details</summary>
Motivation: Existing prototype-based methods in GFSS are deterministic, limiting adaptability to diverse samples, especially for novel classes with few annotations.

Method: FewCLIP refines frozen textual prototypes with learnable visual calibration prototypes and introduces distribution regularization for uncertainty-aware learning.

Result: FewCLIP outperforms state-of-the-art methods on PASCAL-5$^i$ and COCO-20$^i$ datasets in GFSS and class-incremental settings.

Conclusion: The probabilistic approach of FewCLIP enhances prototype adaptability and generalization, addressing limitations of deterministic methods.

Abstract: Generalized Few-Shot Semantic Segmentation (GFSS) aims to extend a
segmentation model to novel classes with only a few annotated examples while
maintaining performance on base classes. Recently, pretrained vision-language
models (VLMs) such as CLIP have been leveraged in GFSS to improve
generalization on novel classes through multi-modal prototypes learning.
However, existing prototype-based methods are inherently deterministic,
limiting the adaptability of learned prototypes to diverse samples,
particularly for novel classes with scarce annotations. To address this, we
propose FewCLIP, a probabilistic prototype calibration framework over
multi-modal prototypes from the pretrained CLIP, thus providing more adaptive
prototype learning for GFSS. Specifically, FewCLIP first introduces a prototype
calibration mechanism, which refines frozen textual prototypes with learnable
visual calibration prototypes, leading to a more discriminative and adaptive
representation. Furthermore, unlike deterministic prototype learning
techniques, FewCLIP introduces distribution regularization over these
calibration prototypes. This probabilistic formulation ensures structured and
uncertainty-aware prototype learning, effectively mitigating overfitting to
limited novel class data while enhancing generalization. Extensive experimental
results on PASCAL-5$^i$ and COCO-20$^i$ datasets demonstrate that our proposed
FewCLIP significantly outperforms state-of-the-art approaches across both GFSS
and class-incremental setting. The code is available at
https://github.com/jliu4ai/FewCLIP.

</details>


### [224] [Revisiting CroPA: A Reproducibility Study and Enhancements for Cross-Prompt Adversarial Transferability in Vision-Language Models](https://arxiv.org/abs/2506.22982)
*Atharv Mittal,Agam Pandey,Amritanshu Tiwari,Sukrit Jindal,Swadesh Swain*

Main category: cs.CV

TL;DR: The study validates and improves the Cross-Prompt Attack (CroPA) on Vision-Language Models (VLMs), enhancing adversarial transferability and attack success rates.


<details>
  <summary>Details</summary>
Motivation: VLMs are vulnerable to adversarial attacks, especially when both visual and textual inputs are manipulated. This work aims to reproduce and improve existing adversarial methods.

Method: The study replicates CroPA and introduces three improvements: a novel initialization strategy, investigation of cross-image transferability, and a new loss function targeting vision encoder attention.

Result: The improvements consistently boost adversarial effectiveness across VLMs like Flamingo, BLIP-2, InstructBLIP, and LLaVA.

Conclusion: The work highlights the need to study VLM vulnerabilities and provides a robust framework for generating transferable adversarial examples, impacting real-world VLM security.

Abstract: Large Vision-Language Models (VLMs) have revolutionized computer vision,
enabling tasks such as image classification, captioning, and visual question
answering. However, they remain highly vulnerable to adversarial attacks,
particularly in scenarios where both visual and textual modalities can be
manipulated. In this study, we conduct a comprehensive reproducibility study of
"An Image is Worth 1000 Lies: Adversarial Transferability Across Prompts on
Vision-Language Models" validating the Cross-Prompt Attack (CroPA) and
confirming its superior cross-prompt transferability compared to existing
baselines. Beyond replication we propose several key improvements: (1) A novel
initialization strategy that significantly improves Attack Success Rate (ASR).
(2) Investigate cross-image transferability by learning universal
perturbations. (3) A novel loss function targeting vision encoder attention
mechanisms to improve generalization. Our evaluation across prominent VLMs --
including Flamingo, BLIP-2, and InstructBLIP as well as extended experiments on
LLaVA validates the original results and demonstrates that our improvements
consistently boost adversarial effectiveness. Our work reinforces the
importance of studying adversarial vulnerabilities in VLMs and provides a more
robust framework for generating transferable adversarial examples, with
significant implications for understanding the security of VLMs in real-world
applications.

</details>


### [225] [A Novel Frame Identification and Synchronization Technique for Smartphone Visible Light Communication Systems Based on Convolutional Neural Networks](https://arxiv.org/abs/2506.23004)
*Vaigai Nayaki Yokar,Hoa Le-Minh,Xicong Li,Wai Lok Woo,Luis Nero Alves,Stanislav Zvanovec,Tran The Son,Zabih Ghassemlooy*

Main category: cs.CV

TL;DR: A lightweight CNN-based method for frame identification and synchronization in S2C VLC systems achieves 98.74% accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve short-link communication performance in S2C VLC systems by addressing challenges like blurring, cropping, and rotated images.

Method: A supervised CNN model developed in Python/TensorFlow Keras, trained on a custom dataset via real-time experiments.

Result: 98.74% accuracy in frame identification and synchronization.

Conclusion: The proposed CNN model is effective for enhancing S2C VLC system performance.

Abstract: This paper proposes a novel, robust, and lightweight supervised Convolutional
Neural Network (CNN)-based technique for frame identification and
synchronization, designed to enhance short-link communication performance in a
screen-to-camera (S2C) based visible light communication (VLC) system.
Developed using Python and the TensorFlow Keras framework, the proposed CNN
model was trained through three real-time experimental investigations conducted
in Jupyter Notebook. These experiments incorporated a dataset created from
scratch to address various real-time challenges in S2C communication, including
blurring, cropping, and rotated images in mobility scenarios. Overhead frames
were introduced for synchronization, which leads to enhanced system
performance. The experimental results demonstrate that the proposed model
achieves an overall accuracy of approximately 98.74%, highlighting its
effectiveness in identifying and synchronizing frames in S2C VLC systems.

</details>


### [226] [MusiXQA: Advancing Visual Music Understanding in Multimodal Large Language Models](https://arxiv.org/abs/2506.23009)
*Jian Chen,Wenye Ma,Penghang Liu,Wei Wang,Tengwei Song,Ming Li,Chenguang Wang,Ruiyi Zhang,Changyou Chen*

Main category: cs.CV

TL;DR: The paper introduces MusiXQA, a dataset for evaluating MLLMs in music sheet understanding, and presents Phi-3-MusiX, a fine-tuned model outperforming GPT-based methods.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs lack exploration in music sheet interpretation, despite their success in other visual reasoning tasks.

Method: The authors create MusiXQA, a synthetic dataset with structured annotations, and develop Phi-3-MusiX, a fine-tuned MLLM.

Result: Evaluations show current MLLMs struggle with music sheets, while Phi-3-MusiX achieves significant performance improvements.

Conclusion: MusiXQA and Phi-3-MusiX provide a foundation for advancing MLLMs in music sheet understanding.

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable visual
reasoning abilities in natural images, text-rich documents, and graphic
designs. However, their ability to interpret music sheets remains
underexplored. To bridge this gap, we introduce MusiXQA, the first
comprehensive dataset for evaluating and advancing MLLMs in music sheet
understanding. MusiXQA features high-quality synthetic music sheets generated
via MusiXTeX, with structured annotations covering note pitch and duration,
chords, clefs, key/time signatures, and text, enabling diverse visual QA tasks.
Through extensive evaluations, we reveal significant limitations of current
state-of-the-art MLLMs in this domain. Beyond benchmarking, we developed
Phi-3-MusiX, an MLLM fine-tuned on our dataset, achieving significant
performance gains over GPT-based methods. The proposed dataset and model
establish a foundation for future advances in MLLMs for music sheet
understanding. Code, data, and model will be released upon acceptance.

</details>


### [227] [VisionScores -- A system-segmented image score dataset for deep learning tasks](https://arxiv.org/abs/2506.23030)
*Alejandro Romero Amezcua,Mariano José Juan Rivera Meraz*

Main category: cs.CV

TL;DR: VisionScores introduces the first system-segmented image score dataset for machine learning, focusing on two-handed piano pieces with 24.8k samples in two scenarios: same composition type (Sonatinas) from different composers and different composition types from Franz Liszt.


<details>
  <summary>Details</summary>
Motivation: To provide a structured, high-density image dataset for machine/deep learning, capturing graphic similarity and composition patterns specific to piano pieces.

Method: Dataset includes 24.8k grayscale images (128x512 pixels) of two-handed piano scores, segmented by system. Two scenarios: 14k samples of Sonatinas from various composers and 10.8k samples of diverse compositions by Liszt. Metadata and full-page scores are also provided.

Result: A comprehensive dataset (VisionScores) with structured, segmented piano scores, supporting machine learning tasks with rich metadata and additional resources.

Conclusion: VisionScores fills a gap in structured musical score datasets, offering valuable resources for ML/DL applications in music analysis.

Abstract: VisionScores presents a novel proposal being the first system-segmented image
score dataset, aiming to offer structure-rich, high information-density images
for machine and deep learning tasks. Delimited to two-handed piano pieces, it
was built to consider not only certain graphic similarity but also composition
patterns, as this creative process is highly instrument-dependent. It provides
two scenarios in relation to composer and composition type. The first, formed
by 14k samples, considers works from different authors but the same composition
type, specifically, Sonatinas. The latter, consisting of 10.8K samples,
presents the opposite case, various composition types from the same author,
being the one selected Franz Liszt. All of the 24.8k samples are formatted as
grayscale jpg images of $128 \times 512$ pixels. VisionScores supplies the
users not only the formatted samples but the systems' order and pieces'
metadata. Moreover, unsegmented full-page scores and the pre-formatted images
are included for further analysis.

</details>


### [228] [Inpainting is All You Need: A Diffusion-based Augmentation Method for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2506.23038)
*Xinrong Hu,Yiyu Shi*

Main category: cs.CV

TL;DR: AugPaint is a data augmentation framework using inpainting to generate synthetic medical image-label pairs from limited labeled data, improving segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Pixel-level labeling in medical datasets is costly and time-consuming, and enhancing segmentation with scarce labeled data is a challenge.

Method: AugPaint uses latent diffusion models for inpainting, conditioning on cropped foreground areas during denoising to generate accurate image-label pairs.

Result: AugPaint outperforms state-of-the-art methods on four medical datasets (CT, MRI, skin imaging), significantly boosting segmentation performance.

Conclusion: AugPaint effectively addresses limited annotations by generating high-quality synthetic data, enhancing segmentation model training.

Abstract: Collecting pixel-level labels for medical datasets can be a laborious and
expensive process, and enhancing segmentation performance with a scarcity of
labeled data is a crucial challenge. This work introduces AugPaint, a data
augmentation framework that utilizes inpainting to generate image-label pairs
from limited labeled data. AugPaint leverages latent diffusion models, known
for their ability to generate high-quality in-domain images with low overhead,
and adapts the sampling process for the inpainting task without need for
retraining. Specifically, given a pair of image and label mask, we crop the
area labeled with the foreground and condition on it during reversed denoising
process for every noise level. Masked background area would gradually be filled
in, and all generated images are paired with the label mask. This approach
ensures the accuracy of match between synthetic images and label masks, setting
it apart from existing dataset generation methods. The generated images serve
as valuable supervision for training downstream segmentation models,
effectively addressing the challenge of limited annotations. We conducted
extensive evaluations of our data augmentation method on four public medical
image segmentation datasets, including CT, MRI, and skin imaging. Results
across all datasets demonstrate that AugPaint outperforms state-of-the-art
label-efficient methodologies, significantly improving segmentation
performance.

</details>


### [229] [From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient 3D Gaussian Splatting](https://arxiv.org/abs/2506.23042)
*Hung Nguyen,An Le,Runfa Li,Truong Nguyen*

Main category: cs.CV

TL;DR: AutoOpti3DGS reduces Gaussian proliferation in 3D Gaussian Splatting by using learnable wavelet transforms, maintaining visual quality while optimizing memory usage.


<details>
  <summary>Details</summary>
Motivation: The growing set of Gaussian primitives in 3D Gaussian Splatting strains memory and bandwidth, necessitating a method to control proliferation without losing fidelity.

Method: Uses learnable Forward and Inverse Discrete Wavelet Transforms with fixed low-pass and learnable high-pass filters, activated by an orthogonality loss for coarse-to-fine refinement.

Result: Produces sparser scene representations, compatible with memory-constrained hardware, with minimal hyper-parameter tuning.

Conclusion: AutoOpti3DGS effectively balances detail and efficiency in 3DGS, enhancing practicality for constrained hardware.

Abstract: 3D Gaussian Splatting has emerged as a powerful approach in novel view
synthesis, delivering rapid training and rendering but at the cost of an
ever-growing set of Gaussian primitives that strains memory and bandwidth. We
introduce AutoOpti3DGS, a training-time framework that automatically restrains
Gaussian proliferation without sacrificing visual fidelity. The key idea is to
feed the input images to a sequence of learnable Forward and Inverse Discrete
Wavelet Transforms, where low-pass filters are kept fixed, high-pass filters
are learnable and initialized to zero, and an auxiliary orthogonality loss
gradually activates fine frequencies. This wavelet-driven, coarse-to-fine
process delays the formation of redundant fine Gaussians, allowing 3DGS to
capture global structure first and refine detail only when necessary. Through
extensive experiments, AutoOpti3DGS requires just a single filter learning-rate
hyper-parameter, integrates seamlessly with existing efficient 3DGS frameworks,
and consistently produces sparser scene representations more compatible with
memory or storage-constrained hardware.

</details>


### [230] [Ovis-U1 Technical Report](https://arxiv.org/abs/2506.23044)
*Guo-Hua Wang,Shanshan Zhao,Xinjie Zhang,Liangfu Cao,Pengxin Zhan,Lunhao Duan,Shiyin Lu,Minghao Fu,Xiaohao Chen,Jianshan Zhao,Yang Li,Qing-Guo Chen*

Main category: cs.CV

TL;DR: Ovis-U1 is a 3B-parameter unified model excelling in multimodal understanding, text-to-image generation, and image editing, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To integrate multimodal understanding, generation, and editing into a single model, improving performance over specialized models.

Method: Uses a diffusion-based visual decoder and bidirectional token refiner, trained with a unified approach from a language model.

Result: Achieves top scores on benchmarks like OpenCompass (69.6), DPG-Bench (83.72), and ImgEdit-Bench (4.00).

Conclusion: Ovis-U1 advances multimodal capabilities, setting a new standard for unified models.

Abstract: In this report, we introduce Ovis-U1, a 3-billion-parameter unified model
that integrates multimodal understanding, text-to-image generation, and image
editing capabilities. Building on the foundation of the Ovis series, Ovis-U1
incorporates a diffusion-based visual decoder paired with a bidirectional token
refiner, enabling image generation tasks comparable to leading models like
GPT-4o. Unlike some previous models that use a frozen MLLM for generation
tasks, Ovis-U1 utilizes a new unified training approach starting from a
language model. Compared to training solely on understanding or generation
tasks, unified training yields better performance, demonstrating the
enhancement achieved by integrating these two tasks. Ovis-U1 achieves a score
of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent
state-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In
text-to-image generation, it excels with scores of 83.72 and 0.89 on the
DPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves
4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the
initial version of the Ovis unified model series, Ovis-U1 pushes the boundaries
of multimodal understanding, generation, and editing.

</details>


### [231] [Empowering Small VLMs to Think with Dynamic Memorization and Exploration](https://arxiv.org/abs/2506.23061)
*Jiazhen Liu,Yuchuan Deng,Long Chen*

Main category: cs.CV

TL;DR: DyME is a novel training paradigm for Small-scale Vision-Language Models (SVLMs) that dynamically switches between Memorization (SFT) and Exploration (RLVR) modes to enhance thinking reliability and performance.


<details>
  <summary>Details</summary>
Motivation: Existing training paradigms (SFT and RLVR) are too demanding for SVLMs, leading to pseudo thinking traces and advantage collapse. A combined approach is needed but struggles with sub-optimal convergence.

Method: DyME dynamically selects between SFT (Memorization) and RLVR (Exploration) modes at each optimization step to balance trade-offs and improve performance.

Result: DyME consistently balances the trade-off and delivers substantial performance improvements across diverse domains.

Conclusion: DyME is a practical and effective solution for enhancing SVLMs' thinking capabilities.

Abstract: Empowering Small-scale Vision-Language Models (SVLMs) with reliable thinking
capabilities remains fundamentally challenging due to their limited parameter
capacity and weak instruction-following abilities. Existing training paradigms,
including Supervised Fine-Tuning (SFT) and Reinforcement Learning with
Verifiable Reward (RLVR), impose substantial demands on the base VLM, exceeding
the capabilities of SVLMs. Consequently, directly applying these paradigms to
SVLMs often suffers from severe pseudo thinking traces and advantage collapse,
ultimately undermining both thinking reliability and task performance. A
natural solution is to combine SFT and RLVR, leveraging their complementarity
to reduce the dependence on model capacity. However, the widely adopted
two-stage training paradigm still performs poorly on SVLMs, as their tendency
toward sub-optimal convergence hinders the trade-off and limits the benefits of
the combination. To address this, we propose DyME, a novel training paradigm
that Dynamically selects between Memorization (via SFT) and Exploration (via
RLVR) modes at each optimization step, ensuring that every update contributes
to the trade-off. Extensive experiments across diverse domains demonstrate that
DyME consistently achieves this balance, and thus delivers substantial
performance improvements. These results establish DyME as a practical and
effective solution for empowering SVLMs with reliable thinking capabilities.
GitHub: https://github.com/HKUST-LongGroup/DyME

</details>


### [232] [CoreMark: Toward Robust and Universal Text Watermarking Technique](https://arxiv.org/abs/2506.23066)
*Jiale Meng,Yiming Li,Zheming Lu,Zewei He,Hao Luo,Tianwei Zhang*

Main category: cs.CV

TL;DR: CoreMark introduces a text watermarking framework using CORE segments for robust, generalizable, and imperceptible data embedding across languages and fonts.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in text watermarking like robustness, generalizability, and imperceptibility.

Method: Uses CORE segments for embedding, selects robust characters, adjusts thickness for data embedding, and employs an adaptive strength modulator.

Result: Outperforms existing methods in resisting attacks (screenshot, print-scan, print-camera) while maintaining imperceptibility.

Conclusion: CoreMark is a highly effective and adaptable text watermarking solution.

Abstract: Text watermarking schemes have gained considerable attention in recent years,
yet still face critical challenges in achieving simultaneous robustness,
generalizability, and imperceptibility. This paper introduces a new embedding
paradigm,termed CORE, which comprises several consecutively aligned black pixel
segments. Its key innovation lies in its inherent noise resistance during
transmission and broad applicability across languages and fonts. Based on the
CORE, we present a text watermarking framework named CoreMark. Specifically,
CoreMark first dynamically extracts COREs from characters. Then, the characters
with stronger robustness are selected according to the lengths of COREs. By
modifying the thickness of the CORE, the hidden data is embedded into the
selected characters without causing significant visual distortions. Moreover, a
general plug-and-play embedding strength modulator is proposed, which can
adaptively enhance the robustness for small font sizes by adjusting the
embedding strength according to the font size. Experimental evaluation
indicates that CoreMark demonstrates outstanding generalizability across
multiple languages and fonts. Compared to existing methods, CoreMark achieves
significant improvements in resisting screenshot, print-scan, and print camera
attacks, while maintaining satisfactory imperceptibility.

</details>


### [233] [Unsupervised 3D Braided Hair Reconstruction from a Single-View Image](https://arxiv.org/abs/2506.23072)
*Jing Gao*

Main category: cs.CV

TL;DR: A novel unsupervised pipeline for 3D braided hair reconstruction from single-view images, outperforming existing methods in accuracy and realism.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with braided hair due to its complex structure; this work aims to address this gap.

Method: Uses a synthetic braid model inspired by braid theory to capture intricate braid structures.

Result: Outperforms state-of-the-art methods in accuracy, realism, and efficiency.

Conclusion: Supports expressive hairstyle modeling in digital humans.

Abstract: Reconstructing 3D braided hairstyles from single-view images remains a
challenging task due to the intricate interwoven structure and complex
topologies of braids. Existing strand-based hair reconstruction methods
typically focus on loose hairstyles and often struggle to capture the
fine-grained geometry of braided hair. In this paper, we propose a novel
unsupervised pipeline for efficiently reconstructing 3D braided hair from
single-view RGB images. Leveraging a synthetic braid model inspired by braid
theory, our approach effectively captures the complex intertwined structures of
braids. Extensive experiments demonstrate that our method outperforms
state-of-the-art approaches, providing superior accuracy, realism, and
efficiency in reconstructing 3D braided hairstyles, supporting expressive
hairstyle modeling in digital humans.

</details>


### [234] [Learning Counterfactually Decoupled Attention for Open-World Model Attribution](https://arxiv.org/abs/2506.23074)
*Yu Zheng,Boyang Gong,Fanye Kong,Yueqi Duan,Bingyao Yu,Wenzhao Zheng,Lei Chen,Jiwen Lu,Jie Zhou*

Main category: cs.CV

TL;DR: Proposes CDAL for open-world model attribution, addressing spurious correlations and novel attacks by modeling causal relationships and decoupling artifacts.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with spurious correlations and novel attacks due to handcrafted designs.

Method: CDAL models causal relationships between attentional traces and attribution, decoupling artifacts from biases.

Result: Improves state-of-the-art models significantly, especially for unseen attacks, with minimal overhead.

Conclusion: CDAL effectively generalizes to unseen models by focusing on essential generation patterns.

Abstract: In this paper, we propose a Counterfactually Decoupled Attention Learning
(CDAL) method for open-world model attribution. Existing methods rely on
handcrafted design of region partitioning or feature space, which could be
confounded by the spurious statistical correlations and struggle with novel
attacks in open-world scenarios. To address this, CDAL explicitly models the
causal relationships between the attentional visual traces and source model
attribution, and counterfactually decouples the discriminative model-specific
artifacts from confounding source biases for comparison. In this way, the
resulting causal effect provides a quantification on the quality of learned
attention maps, thus encouraging the network to capture essential generation
patterns that generalize to unseen source models by maximizing the effect.
Extensive experiments on existing open-world model attribution benchmarks show
that with minimal computational overhead, our method consistently improves
state-of-the-art models by large margins, particularly for unseen novel
attacks. Source code: https://github.com/yzheng97/CDAL.

</details>


### [235] [Dynamic Contrastive Learning for Hierarchical Retrieval: A Case Study of Distance-Aware Cross-View Geo-Localization](https://arxiv.org/abs/2506.23077)
*Suofei Zhang,Xinxin Wang,Xiaofu Wu,Quan Zhou,Haifeng Hu*

Main category: cs.CV

TL;DR: The paper introduces Distance-Aware Cross-View Geo-Localization (DACVGL) and a benchmark (DA-Campus) with precise distance annotations. It proposes Dynamic Contrastive Learning (DyCL) for hierarchical retrieval, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on cross-domain image matching but lack contextual understanding and error minimization in geo-localization.

Method: Proposes DyCL, a contrastive learning framework aligning features hierarchically, tested on the DA-Campus benchmark.

Result: DyCL improves hierarchical retrieval and geo-localization accuracy, complementing multi-scale metric learning.

Conclusion: DyCL effectively addresses DACVGL, offering a robust solution for cross-view geo-localization.

Abstract: Existing deep learning-based cross-view geo-localization methods primarily
focus on improving the accuracy of cross-domain image matching, rather than
enabling models to comprehensively capture contextual information around the
target and minimize the cost of localization errors. To support systematic
research into this Distance-Aware Cross-View Geo-Localization (DACVGL) problem,
we construct Distance-Aware Campus (DA-Campus), the first benchmark that pairs
multi-view imagery with precise distance annotations across three spatial
resolutions. Based on DA-Campus, we formulate DACVGL as a hierarchical
retrieval problem across different domains. Our study further reveals that, due
to the inherent complexity of spatial relationships among buildings, this
problem can only be addressed via a contrastive learning paradigm, rather than
conventional metric learning. To tackle this challenge, we propose Dynamic
Contrastive Learning (DyCL), a novel framework that progressively aligns
feature representations according to hierarchical spatial margins. Extensive
experiments demonstrate that DyCL is highly complementary to existing
multi-scale metric learning methods and yields substantial improvements in both
hierarchical retrieval performance and overall cross-view geo-localization
accuracy. Our code and benchmark are publicly available at
https://github.com/anocodetest1/DyCL.

</details>


### [236] [Frequency-enhanced Multi-granularity Context Network for Efficient Vertebrae Segmentation](https://arxiv.org/abs/2506.23086)
*Jian Shi,Tianqi You,Pingping Zhang,Hongli Zhang,Rui Xu,Haojie Li*

Main category: cs.CV

TL;DR: FMC-Net improves vertebrae segmentation in 3D CT/MRI by using wavelet transform and multi-granularity context modeling to handle blurring and similar vertebrae.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with image blurring and distinguishing similar vertebrae in spinal imaging, limiting clinical applications.

Method: Uses wavelet transform for lossless downsampling, processes high/low-frequency components separately (HFR for high-frequency, MG-SSM for low-frequency), and aggregates multi-granularity contexts.

Result: Outperforms state-of-the-art methods on CT/MRI datasets, with publicly available source code.

Conclusion: FMC-Net effectively addresses blurring and vertebrae similarity, enhancing segmentation accuracy for clinical use.

Abstract: Automated and accurate segmentation of individual vertebra in 3D CT and MRI
images is essential for various clinical applications. Due to the limitations
of current imaging techniques and the complexity of spinal structures, existing
methods still struggle with reducing the impact of image blurring and
distinguishing similar vertebrae. To alleviate these issues, we introduce a
Frequency-enhanced Multi-granularity Context Network (FMC-Net) to improve the
accuracy of vertebrae segmentation. Specifically, we first apply wavelet
transform for lossless downsampling to reduce the feature distortion in blurred
images. The decomposed high and low-frequency components are then processed
separately. For the high-frequency components, we apply a High-frequency
Feature Refinement (HFR) to amplify the prominence of key features and filter
out noises, restoring fine-grained details in blurred images. For the
low-frequency components, we use a Multi-granularity State Space Model (MG-SSM)
to aggregate feature representations with different receptive fields,
extracting spatially-varying contexts while capturing long-range dependencies
with linear complexity. The utilization of multi-granularity contexts is
essential for distinguishing similar vertebrae and improving segmentation
accuracy. Extensive experiments demonstrate that our method outperforms
state-of-the-art approaches on both CT and MRI vertebrae segmentation datasets.
The source code is publicly available at https://github.com/anaanaa/FMCNet.

</details>


### [237] [Where, What, Why: Towards Explainable Driver Attention Prediction](https://arxiv.org/abs/2506.23088)
*Yuchen Zhou,Jiayu Tang,Xiaoyan Xiao,Yueyao Lin,Linkai Liu,Zipeng Guo,Hao Fei,Xiaobo Xia,Chao Gou*

Main category: cs.CV

TL;DR: The paper introduces Explainable Driver Attention Prediction, a task paradigm predicting spatial attention, semantics, and reasoning in driving. It presents W3DA, a dataset with semantic and causal annotations, and LLada, a framework for unified attention modeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods predict spatial attention but lack cognitive reasoning, limiting understanding of attention mechanisms in driving.

Method: Proposes W3DA dataset with detailed annotations and LLada, a Large Language model-driven framework for end-to-end attention prediction.

Result: LLada shows robust generalization across datasets and driving conditions.

Conclusion: This work advances understanding of driver attention, benefiting autonomous driving and human-computer interaction.

Abstract: Modeling task-driven attention in driving is a fundamental challenge for both
autonomous vehicles and cognitive science. Existing methods primarily predict
where drivers look by generating spatial heatmaps, but fail to capture the
cognitive motivations behind attention allocation in specific contexts, which
limits deeper understanding of attention mechanisms. To bridge this gap, we
introduce Explainable Driver Attention Prediction, a novel task paradigm that
jointly predicts spatial attention regions (where), parses attended semantics
(what), and provides cognitive reasoning for attention allocation (why). To
support this, we present W3DA, the first large-scale explainable driver
attention dataset. It enriches existing benchmarks with detailed semantic and
causal annotations across diverse driving scenarios, including normal
conditions, safety-critical situations, and traffic accidents. We further
propose LLada, a Large Language model-driven framework for driver attention
prediction, which unifies pixel modeling, semantic parsing, and cognitive
reasoning within an end-to-end architecture. Extensive experiments demonstrate
the effectiveness of LLada, exhibiting robust generalization across datasets
and driving conditions. This work serves as a key step toward a deeper
understanding of driver attention mechanisms, with significant implications for
autonomous driving, intelligent driver training, and human-computer
interaction.

</details>


### [238] [DC-TTA: Divide-and-Conquer Framework for Test-Time Adaptation of Interactive Segmentation](https://arxiv.org/abs/2506.23104)
*Jihun Kim,Hoyong Kwon,Hyeokjun Kweon,Wooseong Jeong,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: DC-TTA is a test-time adaptation framework that improves SAM's interactive segmentation by dividing user clicks into subsets for localized updates, enhancing performance in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: SAM struggles in specialized domains and complex scenarios like camouflaged objects, prompting the need for a more adaptive approach.

Method: DC-TTA partitions user clicks into coherent subsets, processes each independently via TTA, and merges adapted models for unified predictions.

Result: DC-TTA outperforms SAM's zero-shot results and conventional TTA methods, achieving better accuracy with fewer interactions.

Conclusion: DC-TTA effectively addresses SAM's limitations, offering a robust solution for complex interactive segmentation tasks.

Abstract: Interactive segmentation (IS) allows users to iteratively refine object
boundaries with minimal cues, such as positive and negative clicks. While the
Segment Anything Model (SAM) has garnered attention in the IS community for its
promptable segmentation capabilities, it often struggles in specialized domains
or when handling complex scenarios (e.g., camouflaged or multi-part objects).
To overcome these challenges, we propose DC-TTA, a novel test-time adaptation
(TTA) framework that adapts SAM on a per-sample basis by leveraging user
interactions as supervision. Instead of forcing a single model to incorporate
all user clicks at once, DC-TTA partitions the clicks into more coherent
subsets, each processed independently via TTA with a separated model. This
Divide-and-Conquer strategy reduces conflicts among diverse cues and enables
more localized updates. Finally, we merge the adapted models to form a unified
predictor that integrates the specialized knowledge from each subset.
Experimental results across various benchmarks demonstrate that DC-TTA
significantly outperforms SAM's zero-shot results and conventional TTA methods,
effectively handling complex tasks such as camouflaged object segmentation with
fewer interactions and improved accuracy.

</details>


### [239] [Computer-Aided Multi-Stroke Character Simplification by Stroke Removal](https://arxiv.org/abs/2506.23106)
*Ryo Ishiyama,Shinnosuke Matsuo,Seiichi Uchida*

Main category: cs.CV

TL;DR: A framework simplifies multi-stroke Chinese/Japanese characters by removing strokes while maintaining legibility, aiding learners and font design.


<details>
  <summary>Details</summary>
Motivation: Simplify complex characters to reduce learning barriers, improve font design, and enhance communication systems.

Method: Uses a character recognition model to assess and remove strokes with minimal impact on legibility.

Result: Tests on 1,256 characters show many remain distinguishable even after stroke removal.

Conclusion: Suggests potential for formalized simplification strategies in character design.

Abstract: Multi-stroke characters in scripts such as Chinese and Japanese can be highly
complex, posing significant challenges for both native speakers and,
especially, non-native learners. If these characters can be simplified without
degrading their legibility, it could reduce learning barriers for non-native
speakers, facilitate simpler and legible font designs, and contribute to
efficient character-based communication systems. In this paper, we propose a
framework to systematically simplify multi-stroke characters by selectively
removing strokes while preserving their overall legibility. More specifically,
we use a highly accurate character recognition model to assess legibility and
remove those strokes that minimally impact it. Experimental results on 1,256
character classes with 5, 10, 15, and 20 strokes reveal several key findings,
including the observation that even after removing multiple strokes, many
characters remain distinguishable. These findings suggest the potential for
more formalized simplification strategies.

</details>


### [240] [Hierarchical Corpus-View-Category Refinement for Carotid Plaque Risk Grading in Ultrasound](https://arxiv.org/abs/2506.23108)
*Zhiyuan Zhu,Jian Wang,Yong Jiang,Tong Han,Yuhao Huang,Ang Zhang,Kaiwen Yang,Mingyuan Luo,Zhe Liu,Yaofei Duan,Dong Ni,Tianhong Tang,Xin Yang*

Main category: cs.CV

TL;DR: A novel Corpus-View-Category Refinement Framework (CVC-RF) is proposed for accurate carotid plaque grading (CPG), addressing multi-view classification challenges by enhancing representation learning and feature differences.


<details>
  <summary>Details</summary>
Motivation: Accurate CPG is crucial for assessing cardiovascular and cerebrovascular risks, but existing deep learning methods neglect representation learning and class feature differences.

Method: CVC-RF processes information at Corpus-, View-, and Category-levels, using center-memory contrastive loss, cascaded down-sampling attention, and a mixture-of-experts weighting strategy.

Result: CVC-RF achieves state-of-the-art performance in CPG by effectively modeling global features through multi-level refinement.

Conclusion: The proposed framework advances CPG by addressing key limitations in existing methods, demonstrating superior performance.

Abstract: Accurate carotid plaque grading (CPG) is vital to assess the risk of
cardiovascular and cerebrovascular diseases. Due to the small size and high
intra-class variability of plaque, CPG is commonly evaluated using a
combination of transverse and longitudinal ultrasound views in clinical
practice. However, most existing deep learning-based multi-view classification
methods focus on feature fusion across different views, neglecting the
importance of representation learning and the difference in class features. To
address these issues, we propose a novel Corpus-View-Category Refinement
Framework (CVC-RF) that processes information from Corpus-, View-, and
Category-levels, enhancing model performance. Our contribution is four-fold.
First, to the best of our knowledge, we are the foremost deep learning-based
method for CPG according to the latest Carotid Plaque-RADS guidelines. Second,
we propose a novel center-memory contrastive loss, which enhances the network's
global modeling capability by comparing with representative cluster centers and
diverse negative samples at the Corpus level. Third, we design a cascaded
down-sampling attention module to fuse multi-scale information and achieve
implicit feature interaction at the View level. Finally, a parameter-free
mixture-of-experts weighting strategy is introduced to leverage class
clustering knowledge to weight different experts, enabling feature decoupling
at the Category level. Experimental results indicate that CVC-RF effectively
models global features via multi-level refinement, achieving state-of-the-art
performance in the challenging CPG task.

</details>


### [241] [MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings](https://arxiv.org/abs/2506.23115)
*Haonan Chen,Hong Liu,Yuping Luo,Liang Wang,Nan Yang,Furu Wei,Zhicheng Dou*

Main category: cs.CV

TL;DR: MoCa transforms causal VLMs into bidirectional multimodal embedding models via two stages: continual pre-training with joint reconstruction and contrastive fine-tuning with diverse data, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Current VLMs have suboptimal causal attention for embeddings, scalability issues, and limited training diversity.

Method: Two-stage framework: Modality-aware Continual Pre-training (joint reconstruction) and Heterogeneous Contrastive Fine-tuning (diverse data).

Result: MoCa improves performance on MMEB and ViDoRe-v2 benchmarks, showing scalability with model size and data.

Conclusion: MoCa effectively addresses VLM limitations, enhancing bidirectional reasoning and representation robustness.

Abstract: Multimodal embedding models, built upon causal Vision Language Models (VLMs),
have shown promise in various tasks. However, current approaches face three key
limitations: the use of causal attention in VLM backbones is suboptimal for
embedding tasks; scalability issues due to reliance on high-quality labeled
paired data for contrastive learning; and limited diversity in training
objectives and data. To address these issues, we propose MoCa, a two-stage
framework for transforming pre-trained VLMs into effective bidirectional
multimodal embedding models. The first stage, Modality-aware Continual
Pre-training, introduces a joint reconstruction objective that simultaneously
denoises interleaved text and image inputs, enhancing bidirectional
context-aware reasoning. The second stage, Heterogeneous Contrastive
Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple
image-caption pairs to enhance generalization and alignment. Our method
addresses the stated limitations by introducing bidirectional attention through
continual pre-training, scaling effectively with massive unlabeled datasets via
joint reconstruction objectives, and utilizing diverse multimodal data for
enhanced representation robustness. Experiments demonstrate that MoCa
consistently improves performance across MMEB and ViDoRe-v2 benchmarks,
achieving new state-of-the-art results, and exhibits strong scalability with
both model size and training data on MMEB.

</details>


### [242] [Enhancing Spatial Reasoning in Multimodal Large Language Models through Reasoning-based Segmentation](https://arxiv.org/abs/2506.23120)
*Zhenhua Ning,Zhuotao Tian,Shaoshuai Shi,Guangming Lu,Daojing He,Wenjie Pei,Li Jiang*

Main category: cs.CV

TL;DR: The paper introduces R$^2$S, a reasoning-based segmentation framework for 3D point clouds, and a new dataset, 3D ReasonSeg, to address challenges in spatial reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with complex spatial reasoning in 3D point cloud perception despite detailed spatial cues.

Method: R$^2$S decomposes spatial reasoning into two stages: identifying relevant elements and processing instructions using visual priors.

Result: R$^2$S and 3D ReasonSeg improve spatial reasoning in 3D point cloud perception, validated by experiments.

Conclusion: The framework and dataset serve as a new baseline for future work in 3D point cloud perception.

Abstract: Recent advances in point cloud perception have demonstrated remarkable
progress in scene understanding through vision-language alignment leveraging
large language models (LLMs). However, existing methods may still encounter
challenges in handling complex instructions that require accurate spatial
reasoning, even if the 3D point cloud data provides detailed spatial cues such
as size and position for identifying the targets. To tackle this issue, we
propose Relevant Reasoning Segmentation (R$^2$S), a reasoning-based
segmentation framework. The framework emulates human cognitive processes by
decomposing spatial reasoning into two sequential stages: first identifying
relevant elements, then processing instructions guided by their associated
visual priors. Furthermore, acknowledging the inadequacy of existing datasets
in complex reasoning tasks, we introduce 3D ReasonSeg, a reasoning-based
segmentation dataset comprising 25,185 training samples and 3,966 validation
samples with precise annotations. Both quantitative and qualitative experiments
demonstrate that the R$^2$S and 3D ReasonSeg effectively endow 3D point cloud
perception with stronger spatial reasoning capabilities, and we hope that they
can serve as a new baseline and benchmark for future work.

</details>


### [243] [Dare to Plagiarize? Plagiarized Painting Recognition and Retrieval](https://arxiv.org/abs/2506.23132)
*Sophie Zhou,Shu Kong*

Main category: cs.CV

TL;DR: The paper proposes a method for detecting art plagiarism by retrieving visually similar authentic artworks, using a dataset of paintings and AI-synthesized plagiarized versions. A baseline method with DINOv2 achieves high recognition accuracy but low retrieval precision, while finetuning improves retrieval but reduces accuracy.


<details>
  <summary>Details</summary>
Motivation: To protect artists' copyrights by detecting and explaining plagiarized paintings through retrieval of similar authentic artworks.

Method: Construct a dataset of paintings and AI-synthesized plagiarized versions. Use DINOv2 for baseline retrieval and classification, then finetune with metric learning to improve retrieval.

Result: Baseline: 97.2% recognition accuracy, 29.0% AP retrieval. Finetuned model: 12% AP improvement but 92.7% accuracy.

Conclusion: Finetuning improves retrieval but reduces accuracy. Future research directions are outlined.

Abstract: Art plagiarism detection plays a crucial role in protecting artists'
copyrights and intellectual property, yet it remains a challenging problem in
forensic analysis. In this paper, we address the task of recognizing
plagiarized paintings and explaining the detected plagarisms by retrieving
visually similar authentic artworks. To support this study, we construct a
dataset by collecting painting photos and synthesizing plagiarized versions
using generative AI, tailored to specific artists' styles. We first establish a
baseline approach using off-the-shelf features from the visual foundation model
DINOv2 to retrieve the most similar images in the database and classify
plagiarism based on a similarity threshold. Surprisingly, this non-learned
method achieves a high recognition accuracy of 97.2\% but suffers from low
retrieval precision 29.0\% average precision (AP). To improve retrieval
quality, we finetune DINOv2 with a metric learning loss using positive and
negative sample pairs sampled in the database. The finetuned model greatly
improves retrieval performance by 12\% AP over the baseline, though it
unexpectedly results in a lower recognition accuracy (92.7\%). We conclude with
insightful discussions and outline directions for future research.

</details>


### [244] [RoboScape: Physics-informed Embodied World Model](https://arxiv.org/abs/2506.23135)
*Yu Shang,Xin Zhang,Yinzhou Tang,Lei Jin,Chen Gao,Wei Wu,Yong Li*

Main category: cs.CV

TL;DR: RoboScape is a physics-informed world model that improves video generation for robotic scenarios by integrating 3D geometry and motion dynamics.


<details>
  <summary>Details</summary>
Motivation: Current embodied world models lack physical awareness, leading to unrealistic video generation for contact-rich robotic tasks.

Method: RoboScape jointly learns RGB video generation and physics knowledge, using temporal depth prediction and keypoint dynamics learning.

Result: RoboScape generates videos with high visual fidelity and physical plausibility, validated in downstream robotic applications.

Conclusion: RoboScape advances embodied intelligence research by efficiently integrating physics into world models.

Abstract: World models have become indispensable tools for embodied intelligence,
serving as powerful simulators capable of generating realistic robotic videos
while addressing critical data scarcity challenges. However, current embodied
world models exhibit limited physical awareness, particularly in modeling 3D
geometry and motion dynamics, resulting in unrealistic video generation for
contact-rich robotic scenarios. In this paper, we present RoboScape, a unified
physics-informed world model that jointly learns RGB video generation and
physics knowledge within an integrated framework. We introduce two key
physics-informed joint training tasks: temporal depth prediction that enhances
3D geometric consistency in video rendering, and keypoint dynamics learning
that implicitly encodes physical properties (e.g., object shape and material
characteristics) while improving complex motion modeling. Extensive experiments
demonstrate that RoboScape generates videos with superior visual fidelity and
physical plausibility across diverse robotic scenarios. We further validate its
practical utility through downstream applications including robotic policy
training with generated data and policy evaluation. Our work provides new
insights for building efficient physics-informed world models to advance
embodied intelligence research. The code is available at:
https://github.com/tsinghua-fib-lab/RoboScape.

</details>


### [245] [VisualPrompter: Prompt Optimization with Visual Feedback for Text-to-Image Synthesis](https://arxiv.org/abs/2506.23138)
*Shiyu Wu,Mingzhen Sun,Weining Wang,Yequan Wang,Jing Liu*

Main category: cs.CV

TL;DR: VisualPrompter is a training-free framework for refining user prompts to improve text-to-image alignment in diffusion models, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the gap between user-provided and model-preferred prompts, ensuring semantic alignment in generated images.

Method: Uses self-reflection to identify missing concepts and fine-grained prompt optimization.

Result: Achieves state-of-the-art performance in text-image alignment benchmarks.

Conclusion: VisualPrompter is effective, adaptable, and enhances semantic alignment without training.

Abstract: Since there exists a notable gap between user-provided and model-preferred
prompts, generating high-quality and satisfactory images using diffusion models
often requires prompt engineering to optimize user inputs. Current studies on
text-to-image prompt engineering can effectively enhance the style and
aesthetics of generated images. However, they often neglect the semantic
alignment between generated images and user descriptions, resulting in visually
appealing but content-wise unsatisfying outputs. In this work, we propose
VisualPrompter, a novel training-free prompt engineering framework that refines
user inputs to model-preferred sentences. In particular, VisualPrompter
utilizes an automatic self-reflection module to identify the missing concepts
in generated images and a target-specific prompt optimization mechanism to
revise the prompts in a fine-grained manner. Extensive experiments demonstrate
the effectiveness of our VisualPrompter, which achieves new state-of-the-art
performance on multiple benchmarks for text-image alignment evaluation.
Additionally, our framework features a plug-and-play design, making it highly
adaptable to various generative models.

</details>


### [246] [AlignCVC: Aligning Cross-View Consistency for Single-Image-to-3D Generation](https://arxiv.org/abs/2506.23150)
*Xinyue Liang,Zhiyuan Ma,Lingchen Sun,Yanjun Guo,Lei Zhang*

Main category: cs.CV

TL;DR: AlignCVC introduces a distribution alignment framework to improve cross-view consistency in single-image-to-3D generation, enhancing quality and speeding up inference.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with noisy and unstable reconstruction outputs, limiting cross-view consistency (CVC) improvement in single-image-to-3D tasks.

Method: AlignCVC aligns generated and reconstructed multi-view distributions with ground-truth using a soft-hard alignment strategy, distinct for generation and reconstruction models.

Result: The method enhances generation quality, accelerates inference to as few as 4 steps, and integrates seamlessly with various models.

Conclusion: AlignCVC provides a principled and efficient solution for improving CVC in single-image-to-3D generation.

Abstract: Single-image-to-3D models typically follow a sequential generation and
reconstruction workflow. However, intermediate multi-view images synthesized by
pre-trained generation models often lack cross-view consistency (CVC),
significantly degrading 3D reconstruction performance. While recent methods
attempt to refine CVC by feeding reconstruction results back into the
multi-view generator, these approaches struggle with noisy and unstable
reconstruction outputs that limit effective CVC improvement. We introduce
AlignCVC, a novel framework that fundamentally re-frames single-image-to-3D
generation through distribution alignment rather than relying on strict
regression losses. Our key insight is to align both generated and reconstructed
multi-view distributions toward the ground-truth multi-view distribution,
establishing a principled foundation for improved CVC. Observing that generated
images exhibit weak CVC while reconstructed images display strong CVC due to
explicit rendering, we propose a soft-hard alignment strategy with distinct
objectives for generation and reconstruction models. This approach not only
enhances generation quality but also dramatically accelerates inference to as
few as 4 steps. As a plug-and-play paradigm, our method, namely AlignCVC,
seamlessly integrates various multi-view generation models with 3D
reconstruction models. Extensive experiments demonstrate the effectiveness and
efficiency of AlignCVC for single-image-to-3D generation.

</details>


### [247] [MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation](https://arxiv.org/abs/2506.23151)
*Vladislav Bargatin,Egor Chistov,Alexander Yakovenko,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: MEMFOF is a memory-efficient multi-frame optical flow method that balances accuracy and GPU memory usage, achieving state-of-the-art performance with minimal memory overhead.


<details>
  <summary>Details</summary>
Motivation: Address the high GPU memory consumption in optical flow estimation, especially for high-resolution inputs, without sacrificing accuracy.

Method: Revisits RAFT-like architectures, integrates reduced correlation volumes, high-resolution training, and multi-frame estimation.

Result: Outperforms alternatives in accuracy and efficiency, ranking first on benchmarks like Spring, Sintel, and KITTI-2015.

Conclusion: MEMFOF is a robust solution for high-resolution flow estimation, offering significant memory savings and superior performance.

Abstract: Recent advances in optical flow estimation have prioritized accuracy at the
cost of growing GPU memory consumption, particularly for high-resolution
(FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical
flow method that identifies a favorable trade-off between multi-frame
estimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU
memory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely
positions our method to be trained at native 1080p without the need for
cropping or downsampling. We systematically revisit design choices from
RAFT-like architectures, integrating reduced correlation volumes and
high-resolution training protocols alongside multi-frame estimation, to achieve
state-of-the-art performance across multiple benchmarks while substantially
reducing memory overhead. Our method outperforms more resource-intensive
alternatives in both accuracy and runtime efficiency, validating its robustness
for flow estimation at high resolutions. At the time of submission, our method
ranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289,
leads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the
best Fl-all error on KITTI-2015 at 2.94%. The code is available at
https://github.com/msu-video-group/memfof.

</details>


### [248] [Dynamic View Synthesis from Small Camera Motion Videos](https://arxiv.org/abs/2506.23153)
*Huiqiang Sun,Xingyi Li,Juewen Peng,Liao Shen,Zhiguo Cao,Ke Xian,Guosheng Lin*

Main category: cs.CV

TL;DR: The paper addresses challenges in novel view synthesis for dynamic 3D scenes with limited camera motion, proposing a Distribution-based Depth Regularization (DDR) and camera parameter learning to improve geometry representation and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing NeRF-based methods struggle with small camera motion, leading to incorrect geometry and inaccurate camera parameters.

Method: Proposes DDR for accurate rendering weight distribution and introduces constraints for correct geometry. Also includes camera parameter learning during training.

Result: The approach outperforms state-of-the-art methods in scenes with small camera motion.

Conclusion: DDR and camera parameter learning effectively address challenges in dynamic scene synthesis with limited camera motion.

Abstract: Novel view synthesis for dynamic $3$D scenes poses a significant challenge.
Many notable efforts use NeRF-based approaches to address this task and yield
impressive results. However, these methods rely heavily on sufficient motion
parallax in the input images or videos. When the camera motion range becomes
limited or even stationary (i.e., small camera motion), existing methods
encounter two primary challenges: incorrect representation of scene geometry
and inaccurate estimation of camera parameters. These challenges make prior
methods struggle to produce satisfactory results or even become invalid. To
address the first challenge, we propose a novel Distribution-based Depth
Regularization (DDR) that ensures the rendering weight distribution to align
with the true distribution. Specifically, unlike previous methods that use
depth loss to calculate the error of the expectation, we calculate the
expectation of the error by using Gumbel-softmax to differentiably sample
points from discrete rendering weight distribution. Additionally, we introduce
constraints that enforce the volume density of spatial points before the object
boundary along the ray to be near zero, ensuring that our model learns the
correct geometry of the scene. To demystify the DDR, we further propose a
visualization tool that enables observing the scene geometry representation at
the rendering weight level. For the second challenge, we incorporate camera
parameter learning during training to enhance the robustness of our model to
camera parameters. We conduct extensive experiments to demonstrate the
effectiveness of our approach in representing scenes with small camera motion
input, and our results compare favorably to state-of-the-art methods.

</details>


### [249] [Self-Supervised Contrastive Learning for Multi-Label Images](https://arxiv.org/abs/2506.23156)
*Jiale Chen*

Main category: cs.CV

TL;DR: The paper adapts self-supervised learning (SSL) for multi-label images, proposing a block-wise augmentation module and image-aware contrastive loss to improve representation learning with fewer samples.


<details>
  <summary>Details</summary>
Motivation: Mainstream SSL methods rely on single-label datasets like ImageNet, ignoring multi-label images' richer semantics and causing high pre-training costs.

Method: Introduces a block-wise augmentation module for multi-label images and an image-aware contrastive loss to link views for consistent representations.

Result: Validated through linear fine-tuning and transfer learning, the approach shows competitiveness despite limited sample quality and quantity.

Conclusion: The tailored SSL method effectively learns representations from fewer multi-label images, offering broader applicability.

Abstract: Self-supervised learning (SSL) has demonstrated its effectiveness in learning
representations through comparison methods that align with human intuition.
However, mainstream SSL methods heavily rely on high body datasets with single
label, such as ImageNet, resulting in intolerable pre-training overhead.
Besides, more general multi-label images are frequently overlooked in SSL,
despite their potential for richer semantic information and broader
applicability in downstream scenarios. Therefore, we tailor the mainstream SSL
approach to guarantee excellent representation learning capabilities using
fewer multi-label images. Firstly, we propose a block-wise augmentation module
aimed at extracting additional potential positive view pairs from multi-label
images. Subsequently, an image-aware contrastive loss is devised to establish
connections between these views, thereby facilitating the extraction of
semantically consistent representations. Comprehensive linear fine-tuning and
transfer learning validate the competitiveness of our approach despite
challenging sample quality and quantity.

</details>


### [250] [STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene](https://arxiv.org/abs/2506.23157)
*Hanyu Zhou,Haonan Wang,Haoyue Liu,Yuxing Duan,Luxin Yan,Gim Hee Lee*

Main category: cs.CV

TL;DR: A spatiotemporal-disentangled Gaussian splatting framework is proposed for high-dynamic scene reconstruction, using event cameras to complement frame cameras and addressing spatiotemporal mismatches between background and objects.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to handle discontinuous temporal features and heterogeneous spatial features in dynamic scenes due to unified representation models.

Method: The approach disentangles spatiotemporal features into latent representations, uses event cameras alongside frame cameras, and employs clustering to distinguish features between background and objects.

Result: The framework improves spatiotemporal discrimination, enabling time-continuous dynamic scene rendering, validated by extensive experiments.

Conclusion: The proposed method effectively addresses spatiotemporal mismatches in high-dynamic scenes, outperforming existing approaches.

Abstract: High-dynamic scene reconstruction aims to represent static background with
rigid spatial features and dynamic objects with deformed continuous
spatiotemporal features. Typically, existing methods adopt unified
representation model (e.g., Gaussian) to directly match the spatiotemporal
features of dynamic scene from frame camera. However, this unified paradigm
fails in the potential discontinuous temporal features of objects due to frame
imaging and the heterogeneous spatial features between background and objects.
To address this issue, we disentangle the spatiotemporal features into various
latent representations to alleviate the spatiotemporal mismatching between
background and objects. In this work, we introduce event camera to compensate
for frame camera, and propose a spatiotemporal-disentangled Gaussian splatting
framework for high-dynamic scene reconstruction. As for dynamic scene, we
figure out that background and objects have appearance discrepancy in
frame-based spatial features and motion discrepancy in event-based temporal
features, which motivates us to distinguish the spatiotemporal features between
background and objects via clustering. As for dynamic object, we discover that
Gaussian representations and event data share the consistent spatiotemporal
characteristic, which could serve as a prior to guide the spatiotemporal
disentanglement of object Gaussians. Within Gaussian splatting framework, the
cumulative scene-object disentanglement can improve the spatiotemporal
discrimination between background and objects to render the time-continuous
dynamic scene. Extensive experiments have been performed to verify the
superiority of the proposed method.

</details>


### [251] [Trident: Detecting Face Forgeries with Adversarial Triplet Learning](https://arxiv.org/abs/2506.23189)
*Mustafa Hakan Kara,Aysegul Dundar,Uğur Güdükbay*

Main category: cs.CV

TL;DR: Trident is a face forgery detection framework using triplet learning and adversarial training to improve adaptability and robustness against unseen manipulations.


<details>
  <summary>Details</summary>
Motivation: The increasing sophistication of deepfake technologies necessitates advanced detection methods to ensure digital media integrity and combat disinformation.

Method: Trident employs triplet learning with a Siamese network and domain-adversarial training to capture fine-grained forgery features and enhance generalizability.

Result: The framework demonstrates effectiveness across multiple benchmarks, showing improved robustness to unseen manipulations.

Conclusion: Trident offers a promising solution for detecting diverse face forgeries, with code to be released for further research.

Abstract: As face forgeries generated by deep neural networks become increasingly
sophisticated, detecting face manipulations in digital media has posed a
significant challenge, underscoring the importance of maintaining digital media
integrity and combating visual disinformation. Current detection models,
predominantly based on supervised training with domain-specific data, often
falter against forgeries generated by unencountered techniques. In response to
this challenge, we introduce \textit{Trident}, a face forgery detection
framework that employs triplet learning with a Siamese network architecture for
enhanced adaptability across diverse forgery methods. \textit{Trident} is
trained on curated triplets to isolate nuanced differences of forgeries,
capturing fine-grained features that distinguish pristine samples from
manipulated ones while controlling for other variables. To further enhance
generalizability, we incorporate domain-adversarial training with a forgery
discriminator. This adversarial component guides our embedding model towards
forgery-agnostic representations, improving its robustness to unseen
manipulations. In addition, we prevent gradient flow from the classifier head
to the embedding model, avoiding overfitting induced by artifacts peculiar to
certain forgeries. Comprehensive evaluations across multiple benchmarks and
ablation studies demonstrate the effectiveness of our framework. We will
release our code in a GitHub repository.

</details>


### [252] [DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding](https://arxiv.org/abs/2506.23196)
*Mona Ahmadian,Amir Shirian,Frank Guerin,Andrew Gilbert*

Main category: cs.CV

TL;DR: DEL is a framework for dense semantic action localization in videos, using multimodal interaction modeling to achieve state-of-the-art performance on TAL datasets.


<details>
  <summary>Details</summary>
Motivation: Real-world videos have overlapping events and complex temporal dependencies, making multimodal interaction modeling challenging.

Method: DEL aligns audio and visual features with masked self-attention and refines multimodal interactions across scales.

Result: Achieves notable mAP gains on UnAV-100 (+3.3%), THUMOS14 (+2.6%), ActivityNet 1.3 (+1.2%), and EPIC-Kitchens-100 (+1.7% verb, +1.4% noun).

Conclusion: DEL outperforms previous methods, demonstrating effectiveness in dense action localization.

Abstract: Real-world videos often contain overlapping events and complex temporal
dependencies, making multimodal interaction modeling particularly challenging.
We introduce DEL, a framework for dense semantic action localization, aiming to
accurately detect and classify multiple actions at fine-grained temporal
resolutions in long untrimmed videos. DEL consists of two key modules: the
alignment of audio and visual features that leverage masked self-attention to
enhance intra-mode consistency and a multimodal interaction refinement module
that models cross-modal dependencies across multiple scales, enabling
high-level semantics and fine-grained details. Our method achieves
state-of-the-art performance on multiple real-world Temporal Action
Localization (TAL) datasets, UnAV-100, THUMOS14, ActivityNet 1.3, and
EPIC-Kitchens-100, surpassing previous approaches with notable average mAP
gains of +3.3%, +2.6%, +1.2%, +1.7% (verb), and +1.4% (noun), respectively.

</details>


### [253] [UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding](https://arxiv.org/abs/2506.23219)
*Jie Feng,Shengyuan Wang,Tianhui Liu,Yanxin Xi,Yong Li*

Main category: cs.CV

TL;DR: UrbanLLaVA is a multi-modal large language model designed for urban research, outperforming general MLLMs by processing diverse urban data types through a curated dataset and multi-stage training.


<details>
  <summary>Details</summary>
Motivation: Current urban research methods lack a unified framework for multi-modal data, limiting comprehensive analysis. MLLMs offer a solution.

Method: UrbanLLaVA uses a diverse urban instruction dataset and a multi-stage training framework to enhance spatial reasoning and domain knowledge.

Result: UrbanLLaVA outperforms open-source and proprietary MLLMs in urban tasks, showing robust generalization across cities.

Conclusion: UrbanLLaVA provides a scalable and effective solution for multi-modal urban research, with open-source availability for community use.

Abstract: Urban research involves a wide range of scenarios and tasks that require the
understanding of multi-modal data. Current methods often focus on specific data
types and lack a unified framework in urban field for processing them
comprehensively. The recent success of multi-modal large language models
(MLLMs) presents a promising opportunity to overcome this limitation. In this
paper, we introduce $\textit{UrbanLLaVA}$, a multi-modal large language model
designed to process these four types of data simultaneously and achieve strong
performance across diverse urban tasks compared with general MLLMs. In
$\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset
encompassing both single-modal and cross-modal urban data, spanning from
location view to global view of urban environment. Additionally, we propose a
multi-stage training framework that decouples spatial reasoning enhancement
from domain knowledge learning, thereby improving the compatibility and
downstream performance of $\textit{UrbanLLaVA}$ across diverse urban tasks.
Finally, we also extend existing benchmark for urban research to assess the
performance of MLLMs across a wide range of urban tasks. Experimental results
from three cities demonstrate that $\textit{UrbanLLaVA}$ outperforms
open-source and proprietary MLLMs in both single-modal tasks and complex
cross-modal tasks and shows robust generalization abilities across cities.
Source codes and data are openly accessible to the research community via
https://github.com/tsinghua-fib-lab/UrbanLLaVA.

</details>


### [254] [Transformer-Based Person Search with High-Frequency Augmentation and Multi-Wave Mixing](https://arxiv.org/abs/2506.23202)
*Qilin Shu,Qixian Zhang,Qi Zhang,Hongyun Zhang,Duoqian Miao,Cairong Zhao*

Main category: cs.CV

TL;DR: The paper proposes HAMW, a method to enhance transformer-based person search by addressing high-frequency feature suppression and high computational costs.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models for person search face challenges like high-frequency feature suppression and high computational costs, limiting performance.

Method: HAMW introduces high-frequency augmentation and multi-wave mixing, replacing self-attention with multi-level Haar wavelet fusion for better feature extraction and efficiency.

Result: HAMW achieves state-of-the-art performance on CUHK-SYSU and PRW datasets.

Conclusion: HAMW effectively improves transformer-based person search by enhancing feature extraction and reducing computational overhead.

Abstract: The person search task aims to locate a target person within a set of scene
images. In recent years, transformer-based models in this field have made some
progress. However, they still face three primary challenges: 1) the
self-attention mechanism tends to suppress high-frequency components in the
features, which severely impacts model performance; 2) the computational cost
of transformers is relatively high. To address these issues, we propose a novel
High-frequency Augmentation and Multi-Wave mixing (HAMW) method for person
search. HAMW is designed to enhance the discriminative feature extraction
capabilities of transformers while reducing computational overhead and
improving efficiency. Specifically, we develop a three-stage framework that
progressively optimizes both detection and re-identification performance. Our
model enhances the perception of high-frequency features by learning from
augmented inputs containing additional high-frequency components. Furthermore,
we replace the self-attention layers in the transformer with a strategy based
on multi-level Haar wavelet fusion to capture multi-scale features. This not
only lowers the computational complexity but also alleviates the suppression of
high-frequency features and enhances the ability to exploit multi-scale
information. Extensive experiments demonstrate that HAMW achieves
state-of-the-art performance on both the CUHK-SYSU and PRW datasets.

</details>


### [255] [BridgeShape: Latent Diffusion Schrödinger Bridge for 3D Shape Completion](https://arxiv.org/abs/2506.23205)
*Dequan Kong,Zhe Zhu,Honghua Chen,Mingqiang Wei*

Main category: cs.CV

TL;DR: BridgeShape introduces a latent diffusion Schr\"odinger bridge framework for 3D shape completion, addressing suboptimal completions and resolution constraints by modeling optimal transport and using a Depth-Enhanced VQ-VAE.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to model optimal global transport paths and face resolution limits in voxel space, leading to suboptimal completions and lack of fine details.

Method: BridgeShape formulates shape completion as an optimal transport problem and uses a Depth-Enhanced VQ-VAE to encode shapes into a compact latent space, leveraging multi-view depth and DINOv2 features.

Result: The framework achieves state-of-the-art performance, enabling high-fidelity completions at higher resolutions and for unseen classes.

Conclusion: BridgeShape effectively addresses limitations of existing methods, offering superior fidelity and efficiency in 3D shape completion.

Abstract: Existing diffusion-based 3D shape completion methods typically use a
conditional paradigm, injecting incomplete shape information into the denoising
network via deep feature interactions (e.g., concatenation, cross-attention) to
guide sampling toward complete shapes, often represented by voxel-based
distance functions. However, these approaches fail to explicitly model the
optimal global transport path, leading to suboptimal completions. Moreover,
performing diffusion directly in voxel space imposes resolution constraints,
limiting the generation of fine-grained geometric details. To address these
challenges, we propose BridgeShape, a novel framework for 3D shape completion
via latent diffusion Schr\"odinger bridge. The key innovations lie in two
aspects: (i) BridgeShape formulates shape completion as an optimal transport
problem, explicitly modeling the transition between incomplete and complete
shapes to ensure a globally coherent transformation. (ii) We introduce a
Depth-Enhanced Vector Quantized Variational Autoencoder (VQ-VAE) to encode 3D
shapes into a compact latent space, leveraging self-projected multi-view depth
information enriched with strong DINOv2 features to enhance geometric
structural perception. By operating in a compact yet structurally informative
latent space, BridgeShape effectively mitigates resolution constraints and
enables more efficient and high-fidelity 3D shape completion. BridgeShape
achieves state-of-the-art performance on large-scale 3D shape completion
benchmarks, demonstrating superior fidelity at higher resolutions and for
unseen object classes.

</details>


### [256] [TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints](https://arxiv.org/abs/2506.23207)
*Zhen Tan,Xieyuanli Chen,Lei Feng,Yangbing Ge,Shuaifeng Zhi,Jiaxiong Liu,Dewen Hu*

Main category: cs.CV

TL;DR: TVG-SLAM improves RGB-only 3DGS SLAM by using tri-view geometry for robust tracking and mapping, outperforming prior methods in challenging outdoor environments.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS SLAM systems rely heavily on photometric loss, which lacks robustness in unbounded outdoor scenes with viewpoint and illumination changes.

Method: TVG-SLAM introduces tri-view matching for geometric constraints, hybrid geometric-photometric tracking, probabilistic Gaussian initialization, and dynamic rendering trust attenuation.

Result: TVG-SLAM reduces Absolute Trajectory Error by 69.0% in challenging datasets while maintaining high rendering quality.

Conclusion: TVG-SLAM offers a robust and accurate RGB-only SLAM solution for outdoor environments, with open-source implementation planned.

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled RGB-only SLAM
systems to achieve high-fidelity scene representation. However, the heavy
reliance of existing systems on photometric rendering loss for camera tracking
undermines their robustness, especially in unbounded outdoor environments with
severe viewpoint and illumination changes. To address these challenges, we
propose TVG-SLAM, a robust RGB-only 3DGS SLAM system that leverages a novel
tri-view geometry paradigm to ensure consistent tracking and high-quality
mapping. We introduce a dense tri-view matching module that aggregates reliable
pairwise correspondences into consistent tri-view matches, forming robust
geometric constraints across frames. For tracking, we propose Hybrid Geometric
Constraints, which leverage tri-view matches to construct complementary
geometric cues alongside photometric loss, ensuring accurate and stable pose
estimation even under drastic viewpoint shifts and lighting variations. For
mapping, we propose a new probabilistic initialization strategy that encodes
geometric uncertainty from tri-view correspondences into newly initialized
Gaussians. Additionally, we design a Dynamic Attenuation of Rendering Trust
mechanism to mitigate tracking drift caused by mapping latency. Experiments on
multiple public outdoor datasets show that our TVG-SLAM outperforms prior
RGB-only 3DGS-based SLAM systems. Notably, in the most challenging dataset, our
method improves tracking robustness, reducing the average Absolute Trajectory
Error (ATE) by 69.0\% while achieving state-of-the-art rendering quality. The
implementation of our method will be released as open-source.

</details>


### [257] [A Hierarchical Slice Attention Network for Appendicitis Classification in 3D CT Scans](https://arxiv.org/abs/2506.23209)
*Chia-Wen Huang,Haw Hwai,Chien-Chang Lee,Pei-Yuan Wu*

Main category: cs.CV

TL;DR: A deep learning model using 3D CT scans with Slice Attention and hierarchical classification improves appendicitis diagnosis accuracy.


<details>
  <summary>Details</summary>
Motivation: Timely and accurate diagnosis of appendicitis is crucial to prevent complications, but CT imaging overload can delay diagnoses.

Method: Proposes a deep learning model with 3D CT scans, Slice Attention, and hierarchical classification using pre-trained 2D models.

Result: Improves AUC by 3% for appendicitis and 5.9% for complicated appendicitis.

Conclusion: The model offers a more efficient and reliable diagnostic solution compared to prior methods.

Abstract: Timely and accurate diagnosis of appendicitis is critical in clinical
settings to prevent serious complications. While CT imaging remains the
standard diagnostic tool, the growing number of cases can overwhelm
radiologists, potentially causing delays. In this paper, we propose a deep
learning model that leverages 3D CT scans for appendicitis classification,
incorporating Slice Attention mechanisms guided by external 2D datasets to
enhance small lesion detection. Additionally, we introduce a hierarchical
classification framework using pre-trained 2D models to differentiate between
simple and complicated appendicitis. Our approach improves AUC by 3% for
appendicitis and 5.9% for complicated appendicitis, offering a more efficient
and reliable diagnostic solution compared to previous work.

</details>


### [258] [High-quality Pseudo-labeling for Point Cloud Segmentation with Scene-level Annotation](https://arxiv.org/abs/2506.23227)
*Lunhao Duan,Shanshan Zhao,Xingxing Weng,Jing Zhang,Gui-Song Xia*

Main category: cs.CV

TL;DR: The paper proposes a framework for high-quality pseudo-label generation in indoor point cloud semantic segmentation using scene-level annotations, leveraging multi-modal information and region-point consistency to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with generating accurate pseudo-labels for point cloud segmentation under scene-level annotations, limiting performance.

Method: The framework includes a cross-modal feature guidance module for 2D-3D alignment and a region-point semantic consistency module to refine predictions.

Result: The method outperforms previous works on ScanNet v2 and S3DIS datasets, with ablation studies confirming the contributions of its components.

Conclusion: The proposed approach effectively enhances pseudo-label quality and segmentation performance under scene-level annotations.

Abstract: This paper investigates indoor point cloud semantic segmentation under
scene-level annotation, which is less explored compared to methods relying on
sparse point-level labels. In the absence of precise point-level labels,
current methods first generate point-level pseudo-labels, which are then used
to train segmentation models. However, generating accurate pseudo-labels for
each point solely based on scene-level annotations poses a considerable
challenge, substantially affecting segmentation performance. Consequently, to
enhance accuracy, this paper proposes a high-quality pseudo-label generation
framework by exploring contemporary multi-modal information and region-point
semantic consistency. Specifically, with a cross-modal feature guidance module,
our method utilizes 2D-3D correspondences to align point cloud features with
corresponding 2D image pixels, thereby assisting point cloud feature learning.
To further alleviate the challenge presented by the scene-level annotation, we
introduce a region-point semantic consistency module. It produces regional
semantics through a region-voting strategy derived from point-level semantics,
which are subsequently employed to guide the point-level semantic predictions.
Leveraging the aforementioned modules, our method can rectify inaccurate
point-level semantic predictions during training and obtain high-quality
pseudo-labels. Significant improvements over previous works on ScanNet v2 and
S3DIS datasets under scene-level annotation can demonstrate the effectiveness.
Additionally, comprehensive ablation studies validate the contributions of our
approach's individual components. The code is available at
https://github.com/LHDuan/WSegPC .

</details>


### [259] [Towards an Automated Multimodal Approach for Video Summarization: Building a Bridge Between Text, Audio and Facial Cue-Based Summarization](https://arxiv.org/abs/2506.23714)
*Md Moinul Islam,Sofoklis Kakouros,Janne Heikkilä,Mourad Oussalah*

Main category: cs.CV

TL;DR: A multimodal video summarization framework integrates text, audio, and visual cues to generate behavior-aware summaries, outperforming traditional methods in both text and video metrics.


<details>
  <summary>Details</summary>
Motivation: The growing volume of video content demands advanced summarization techniques that leverage multiple modalities for more effective and expressive summaries.

Method: The framework combines prosodic features, textual cues, and visual indicators to identify semantically and emotionally important moments, including bonus words emphasized across modalities.

Result: The method significantly outperforms traditional approaches, with ROUGE-1 improving from 0.4769 to 0.7929 and F1-Score by 23% in video evaluation.

Conclusion: Multimodal integration enhances the quality and behavioral relevance of video summaries, demonstrating its superiority over unimodal methods.

Abstract: The increasing volume of video content in educational, professional, and
social domains necessitates effective summarization techniques that go beyond
traditional unimodal approaches. This paper proposes a behaviour-aware
multimodal video summarization framework that integrates textual, audio, and
visual cues to generate timestamp-aligned summaries. By extracting prosodic
features, textual cues and visual indicators, the framework identifies
semantically and emotionally important moments. A key contribution is the
identification of bonus words, which are terms emphasized across multiple
modalities and used to improve the semantic relevance and expressive clarity of
the summaries. The approach is evaluated against pseudo-ground truth (pGT)
summaries generated using LLM-based extractive method. Experimental results
demonstrate significant improvements over traditional extractive method, such
as the Edmundson method, in both text and video-based evaluation metrics.
Text-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore
from 0.9152 to 0.9536, while in video-based evaluation, our proposed framework
improves F1-Score by almost 23%. The findings underscore the potential of
multimodal integration in producing comprehensive and behaviourally informed
video summaries.

</details>


### [260] [VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions](https://arxiv.org/abs/2506.23236)
*Marko Mihajlovic,Siwei Zhang,Gen Li,Kaifeng Zhao,Lea Müller,Siyu Tang*

Main category: cs.CV

TL;DR: VolumetricSMPL introduces a neural volumetric body model using Neural Blend Weights (NBW) for efficient MLP decoders, outperforming prior models in speed, memory, and accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional surface mesh models struggle with interactions involving other geometric entities, and existing volumetric models are either inefficient or lack robustness for complex articulations.

Method: VolumetricSMPL uses NBW to dynamically blend a small set of learned weight matrices, improving efficiency while maintaining expressiveness.

Result: The model achieves 10x faster inference, 6x lower GPU memory usage, better accuracy, and includes an SDF for contact modeling. It excels in tasks like human-object interaction reconstruction and scene-constrained motion synthesis.

Conclusion: VolumetricSMPL offers significant performance and efficiency gains, demonstrating broad applicability in computer graphics and vision tasks.

Abstract: Parametric human body models play a crucial role in computer graphics and
vision, enabling applications ranging from human motion analysis to
understanding human-environment interactions. Traditionally, these models use
surface meshes, which pose challenges in efficiently handling interactions with
other geometric entities, such as objects and scenes, typically represented as
meshes or point clouds. To address this limitation, recent research has
explored volumetric neural implicit body models. However, existing works are
either insufficiently robust for complex human articulations or impose high
computational and memory costs, limiting their widespread use. To this end, we
introduce VolumetricSMPL, a neural volumetric body model that leverages Neural
Blend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike
prior approaches that rely on large MLPs, NBW dynamically blends a small set of
learned weight matrices using predicted shape- and pose-dependent coefficients,
significantly improving computational efficiency while preserving
expressiveness. VolumetricSMPL outperforms prior volumetric occupancy model
COAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy,
and a Signed Distance Function (SDF) for efficient and differentiable contact
modeling. We demonstrate VolumetricSMPL's strengths across four challenging
tasks: (1) reconstructing human-object interactions from in-the-wild images,
(2) recovering human meshes in 3D scenes from egocentric views, (3)
scene-constrained motion synthesis, and (4) resolving self-intersections. Our
results highlight its broad applicability and significant performance and
efficiency gains.

</details>


### [261] [Aggregating Local Saliency Maps for Semi-Global Explainable Image Classification](https://arxiv.org/abs/2506.23247)
*James Hinns,David Martens*

Main category: cs.CV

TL;DR: Proposes Segment Attribution Tables (SATs) to summarize local saliency explanations into semi-global insights for analyzing image classifiers.


<details>
  <summary>Details</summary>
Motivation: Addresses the gap between oversimplified global methods and overly detailed local explanations in understanding model predictions.

Method: Uses saliency maps and image segments (e.g., 'eyes') to quantify segment influence, revealing model reliance and spurious correlations.

Result: SATs provide actionable insights into model behavior, identifying recurring patterns and potential biases.

Conclusion: SATs offer a practical tool for bridging the gap between local and global explanations, aiding in classifier analysis and debugging.

Abstract: Deep learning dominates image classification tasks, yet understanding how
models arrive at predictions remains a challenge. Much research focuses on
local explanations of individual predictions, such as saliency maps, which
visualise the influence of specific pixels on a model's prediction. However,
reviewing many of these explanations to identify recurring patterns is
infeasible, while global methods often oversimplify and miss important local
behaviours. To address this, we propose Segment Attribution Tables (SATs), a
method for summarising local saliency explanations into (semi-)global insights.
SATs take image segments (such as "eyes" in Chihuahuas) and leverage saliency
maps to quantify their influence. These segments highlight concepts the model
relies on across instances and reveal spurious correlations, such as reliance
on backgrounds or watermarks, even when out-of-distribution test performance
sees little change. SATs can explain any classifier for which a form of
saliency map can be produced, using segmentation maps that provide named
segments. SATs bridge the gap between oversimplified global summaries and
overly detailed local explanations, offering a practical tool for analysing and
debugging image classifiers.

</details>


### [262] [DGE-YOLO: Dual-Branch Gathering and Attention for Accurate UAV Object Detection](https://arxiv.org/abs/2506.23252)
*Kunwei Lv,Ping Lan*

Main category: cs.CV

TL;DR: DGE-YOLO is an enhanced YOLO-based framework for multi-modal UAV object detection, featuring dual-branch architecture, EMA mechanism, and Gather-and-Distribute module, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of detecting small objects in complex aerial scenarios and the limitations of existing approaches prioritizing speed over multi-modal input handling.

Method: Introduces DGE-YOLO with dual-branch architecture for infrared and visible images, EMA mechanism for multi-scale feature learning, and Gather-and-Distribute module for feature aggregation.

Result: Superior performance on the Drone Vehicle dataset compared to state-of-the-art methods.

Conclusion: DGE-YOLO effectively addresses multi-modal UAV object detection challenges, proving its robustness and efficiency.

Abstract: The rapid proliferation of unmanned aerial vehicles (UAVs) has highlighted
the importance of robust and efficient object detection in diverse aerial
scenarios. Detecting small objects under complex conditions, however, remains a
significant challenge. Existing approaches often prioritize inference speed,
leading to degraded performance when handling multi-modal inputs. To address
this, we present DGE-YOLO, an enhanced YOLO-based detection framework designed
to effectively fuse multi-modal information. Specifically, we introduce a
dual-branch architecture for modality-specific feature extraction, enabling the
model to process both infrared and visible images. To further enrich semantic
representation, we propose an Efficient Multi-scale Attention (EMA) mechanism
that enhances feature learning across spatial scales. Additionally, we replace
the conventional neck with a Gather-and-Distribute module to mitigate
information loss during feature aggregation. Extensive experiments on the Drone
Vehicle dataset demonstrate that DGE-YOLO achieves superior performance over
state-of-the-art methods, validating its effectiveness in multi-modal UAV
object detection tasks.

</details>


### [263] [Ella: Embodied Social Agents with Lifelong Memory](https://arxiv.org/abs/2506.24019)
*Hongxin Zhang,Zheyuan Zhang,Zeyuan Wang,Zunzhe Zhang,Lixing Fang,Qinhong Zhou,Chuang Gan*

Main category: cs.CV

TL;DR: Ella is an embodied social agent with lifelong learning in a 3D open world, using a multimodal memory system and foundation models for decision-making and social interactions.


<details>
  <summary>Details</summary>
Motivation: To advance embodied intelligence by integrating structured memory systems with foundation models for lifelong learning in dynamic social environments.

Method: Ella uses a name-centric semantic memory and spatiotemporal episodic memory, combined with foundation models, to store, update, and retrieve information for decision-making and social interactions.

Result: Ella successfully influences, leads, and cooperates with other agents in a dynamic 3D world, demonstrating effective learning through observation and social interaction.

Conclusion: The integration of structured memory systems with foundation models shows transformative potential for advancing embodied intelligence.

Abstract: We introduce Ella, an embodied social agent capable of lifelong learning
within a community in a 3D open world, where agents accumulate experiences and
acquire knowledge through everyday visual observations and social interactions.
At the core of Ella's capabilities is a structured, long-term multimodal memory
system that stores, updates, and retrieves information effectively. It consists
of a name-centric semantic memory for organizing acquired knowledge and a
spatiotemporal episodic memory for capturing multimodal experiences. By
integrating this lifelong memory system with foundation models, Ella retrieves
relevant information for decision-making, plans daily activities, builds social
relationships, and evolves autonomously while coexisting with other intelligent
beings in the open world. We conduct capability-oriented evaluations in a
dynamic 3D open world where 15 agents engage in social activities for days and
are assessed with a suite of unseen controlled evaluations. Experimental
results show that Ella can influence, lead, and cooperate with other agents
well to achieve goals, showcasing its ability to learn effectively through
observation and social interaction. Our findings highlight the transformative
potential of combining structured memory systems with foundation models for
advancing embodied intelligence. More videos can be found at
https://umass-embodied-agi.github.io/Ella/.

</details>


### [264] [PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution](https://arxiv.org/abs/2506.23254)
*Aradhana Mishra,Bumshik Lee*

Main category: cs.CV

TL;DR: PixelBoost, a novel diffusion model, improves image super-resolution by leveraging Brownian motion's stochastic nature, enhancing realism and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between realistic image generation and computational efficiency in diffusion-model-based super-resolution.

Method: Integrates controlled stochasticity into training, uses sigmoidal noise sequencing, and adapts to Brownian noise patterns.

Result: Superior performance in LPIPS, LOE, PSNR, SSIM, and visual quality, with enhanced edge reconstruction.

Conclusion: PixelBoost effectively balances realism and efficiency, advancing super-resolution techniques.

Abstract: Diffusion-model-based image super-resolution techniques often face a
trade-off between realistic image generation and computational efficiency. This
issue is exacerbated when inference times by decreasing sampling steps,
resulting in less realistic and hazy images. To overcome this challenge, we
introduce a novel diffusion model named PixelBoost that underscores the
significance of embracing the stochastic nature of Brownian motion in advancing
image super-resolution, resulting in a high degree of realism, particularly
focusing on texture and edge definitions. By integrating controlled
stochasticity into the training regimen, our proposed model avoids convergence
to local optima, effectively capturing and reproducing the inherent uncertainty
of image textures and patterns. Our proposed model demonstrates superior
objective results in terms of learned perceptual image patch similarity
(LPIPS), lightness order error (LOE), peak signal-to-noise ratio(PSNR),
structural similarity index measure (SSIM), as well as visual quality. To
determine the edge enhancement, we evaluated the gradient magnitude and pixel
value, and our proposed model exhibited a better edge reconstruction
capability. Additionally, our model demonstrates adaptive learning capabilities
by effectively adjusting to Brownian noise patterns and introduces a sigmoidal
noise sequencing method that simplifies training, resulting in faster inference
speeds.

</details>


### [265] [MotionGPT3: Human Motion as a Second Modality](https://arxiv.org/abs/2506.24086)
*Bingfan Zhu,Biao Jiang,Sunyi Wang,Shixiang Tang,Tao Chen,Linjie Luo,Youyi Zheng,Xin Chen*

Main category: cs.CV

TL;DR: MotionGPT3 is a bimodal motion-language model addressing challenges in unified motion-language tasks by decoupling motion modeling and preserving language intelligence.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between continuous motion and discrete language representation while maintaining language capabilities in multimodal models.

Method: Uses a mixture of experts approach with separate motion and text branches, leveraging a motion VAE and diffusion head for motion prediction.

Result: Achieves competitive performance in motion understanding and generation without degrading language intelligence.

Conclusion: Establishes a unified bimodal framework for motion-language tasks, enabling effective cross-modal interaction.

Abstract: Though recent advances in multimodal models have demonstrated strong
capabilities and opportunities in unified understanding and generation, the
development of unified motion-language models remains underexplored. To enable
such models with high-fidelity human motion, two core challenges must be
addressed. The first is the reconstruction gap between the continuous motion
modality and discrete representation in an autoregressive manner, and the
second is the degradation of language intelligence during unified training.
Inspired by the mixture of experts, we propose MotionGPT3, a bimodal
motion-language model that treats human motion as a second modality, decoupling
motion modeling via separate model parameters and enabling both effective
cross-modal interaction and efficient multimodal scaling training. To preserve
language intelligence, the text branch retains the original structure and
parameters of the pretrained language model, while a new motion branch is
integrated via a shared attention mechanism, enabling bidirectional information
flow between two modalities. We first employ a motion Variational Autoencoder
(VAE) to encode raw human motion into latent representations. Based on this
continuous latent space, the motion branch predicts motion latents directly
from intermediate hidden states using a diffusion head, bypassing discrete
tokenization. Extensive experiments show that our approach achieves competitive
performance on both motion understanding and generation tasks while preserving
strong language capabilities, establishing a unified bimodal motion diffusion
framework within an autoregressive manner.

</details>


### [266] [PCLVis: Visual Analytics of Process Communication Latency in Large-Scale Simulation](https://arxiv.org/abs/2506.23257)
*Chongke Bi,Xin Gao,Baofeng Fu,Yuheng Zhao,Siming Chen,Ying Zhao,Yunhai Wang*

Main category: cs.CV

TL;DR: PCLVis is a framework for analyzing process communication latency (PCL) in large-scale simulations using MPI data, improving simulation efficiency.


<details>
  <summary>Details</summary>
Motivation: Address scalability issues in supercomputing simulations caused by high communication costs, without needing physical link layer data.

Method: Uses MPI process communication data, spatial PCL event locating, correlation clustering, DAG-based propagation analysis, and CS-Glyphs for visualization.

Result: Demonstrated effectiveness on TH-1A supercomputer, enabling users to optimize simulations.

Conclusion: PCLVis helps users analyze and improve simulation efficiency by focusing on communication latency.

Abstract: Large-scale simulations on supercomputers have become important tools for
users. However, their scalability remains a problem due to the huge
communication cost among parallel processes. Most of the existing communication
latency analysis methods rely on the physical link layer information, which is
only available to administrators. In this paper, a framework called PCLVis is
proposed to help general users analyze process communication latency (PCL)
events. Instead of the physical link layer information, the PCLVis uses the MPI
process communication data for the analysis. First, a spatial PCL event
locating method is developed. All processes with high correlation are
classified into a single cluster by constructing a process-correlation tree.
Second, the propagation path of PCL events is analyzed by constructing a
communication-dependency-based directed acyclic graph (DAG), which can help
users interactively explore a PCL event from the temporal evolution of a
located PCL event cluster. In this graph, a sliding window algorithm is
designed to generate the PCL events abstraction. Meanwhile, a new glyph called
the communication state glyph (CS-Glyph) is designed for each process to show
its communication states, including its in/out messages and load balance. Each
leaf node can be further unfolded to view additional information. Third, a PCL
event attribution strategy is formulated to help users optimize their
simulations. The effectiveness of the PCLVis framework is demonstrated by
analyzing the PCL events of several simulations running on the TH-1A
supercomputer. By using the proposed framework, users can greatly improve the
efficiency of their simulations.

</details>


### [267] [Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis](https://arxiv.org/abs/2506.23263)
*Lei-lei Li,Jianwu Fang,Junbin Xiao,Shanmin Pang,Hongkai Yu,Chen Lv,Jianru Xue,Tat-Seng Chua*

Main category: cs.CV

TL;DR: Causal-VidSyn is a novel diffusion model for synthesizing egocentric traffic accident videos, leveraging cause descriptions and driver fixations to improve causal entity grounding. It outperforms state-of-the-art models in frame quality and causal sensitivity.


<details>
  <summary>Details</summary>
Motivation: Understanding and simulating car accidents is vital for self-driving car safety, but synthesizing realistic accident videos with causal relations is challenging.

Method: Proposes Causal-VidSyn, a diffusion model using cause descriptions and driver fixations, aided by accident reason answering and gaze-conditioned selection modules.

Result: Causal-VidSyn excels in frame quality and causal sensitivity, demonstrated in tasks like accident video editing and text-to-video generation.

Conclusion: The model and dataset (Drive-Gaze) advance the synthesis of realistic, causally-aware accident videos, enhancing self-driving car testing.

Abstract: Egocentricly comprehending the causes and effects of car accidents is crucial
for the safety of self-driving cars, and synthesizing causal-entity reflected
accident videos can facilitate the capability test to respond to unaffordable
accidents in reality. However, incorporating causal relations as seen in
real-world videos into synthetic videos remains challenging. This work argues
that precisely identifying the accident participants and capturing their
related behaviors are of critical importance. In this regard, we propose a
novel diffusion model, Causal-VidSyn, for synthesizing egocentric traffic
accident videos. To enable causal entity grounding in video diffusion,
Causal-VidSyn leverages the cause descriptions and driver fixations to identify
the accident participants and behaviors, facilitated by accident reason
answering and gaze-conditioned selection modules. To support Causal-VidSyn, we
further construct Drive-Gaze, the largest driver gaze dataset (with 1.54M
frames of fixations) in driving accident scenarios. Extensive experiments show
that Causal-VidSyn surpasses state-of-the-art video diffusion models in terms
of frame quality and causal sensitivity in various tasks, including accident
video editing, normal-to-accident video diffusion, and text-to-video
generation.

</details>


### [268] [Token Activation Map to Visually Explain Multimodal LLMs](https://arxiv.org/abs/2506.23270)
*Yi Li,Hualiang Wang,Xinpeng Ding,Haonan Wang,Xiaomeng Li*

Main category: cs.CV

TL;DR: The paper introduces Token Activation Map (TAM), a method to improve the explainability of Multimodal Large Language Models (MLLMs) by addressing redundant activations and context interference.


<details>
  <summary>Details</summary>
Motivation: The explainability of MLLMs is understudied, and existing methods overlook redundant activations that harm reliability.

Method: Proposes an estimated causal inference method with a rank Gaussian filter to mitigate context interference, termed Token Activation Map (TAM).

Result: TAM outperforms state-of-the-art methods, providing high-quality visualizations for various applications.

Conclusion: TAM enhances MLLM explainability and is versatile for multiple use cases.

Abstract: Multimodal large language models (MLLMs) are broadly empowering various
fields. Despite their advancements, the explainability of MLLMs remains less
explored, hindering deeper understanding, model credibility, and effective
visualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that
produce a single output, MLLMs generate sequences of tokens progressively,
where each generated token depends on the previous context. Therefore, earlier
context tokens can introduce redundant activations that interfere with the
explanation of later tokens beyond their original information. Existing studies
often overlook this issue, but our observations reveal that these redundant
correlations can significantly hurt the reliability of explanations. To address
this, we propose an estimated causal inference method to mitigate the
interference of context to achieve high-quality MLLM explanation, with a novel
rank Gaussian filter to further reduce activation noises. We term this method
Token Activation Map (TAM) to highlight the consideration of interactions
between tokens. TAM also indicates that it excels at explaining multiple tokens
of MLLM, which is different from the Class Activation Map (CAM) for a single
prediction. Our TAM method significantly outperforms existing SoTA methods,
showcasing high-quality visualization results that can be utilized for various
scenarios, such as object localization, failure case analysis, video
visualization, MLLMs visual comparison, and model understanding (e.g., color,
shape, action, location, visual reasoning, multi-turn conversation, etc). The
code is available atgithub.com/xmed-lab/TAM.

</details>


### [269] [Mettle: Meta-Token Learning for Memory-Efficient Audio-Visual Adaptation](https://arxiv.org/abs/2506.23271)
*Jinxing Zhou,Zhihui Li,Yongqiang Yu,Yanghao Zhou,Ruohao Guo,Guangyao Li,Yuxin Mao,Mingfei Han,Xiaojun Chang,Meng Wang*

Main category: cs.CV

TL;DR: Mettle is a memory-efficient method for adapting pretrained transformers to audio-visual tasks using Layer-Centric Distillation (LCD) and Meta-Token Injection (MTI).


<details>
  <summary>Details</summary>
Motivation: To efficiently adapt large-scale pretrained transformers for downstream audio-visual tasks while preserving pretrained knowledge and enabling task-specific adaptation.

Method: Uses LCD to distill audio/visual features into meta-tokens in parallel and MTI to guide feature adaptation in earlier layers for fine-grained tasks.

Result: Reduces memory usage and training time while maintaining parameter efficiency and competitive accuracy.

Conclusion: Mettle offers a lightweight, efficient solution for adapting transformers to diverse audio-visual tasks.

Abstract: We present \textbf{Met}a-\textbf{T}oken \textbf{Le}arning (Mettle), a simple
and memory-efficient method for adapting large-scale pretrained transformer
models to downstream audio-visual tasks. Instead of sequentially modifying the
output feature distribution of the transformer backbone, Mettle utilizes a
lightweight \textit{Layer-Centric Distillation (LCD)} module to distill in
parallel the intact audio or visual features embedded by each transformer layer
into compact meta-tokens. This distillation process considers both pretrained
knowledge preservation and task-specific adaptation. The obtained meta-tokens
can be directly applied to classification tasks, such as audio-visual event
localization and audio-visual video parsing. To further support fine-grained
segmentation tasks, such as audio-visual segmentation, we introduce a
\textit{Meta-Token Injection (MTI)} module, which utilizes the audio and visual
meta-tokens distilled from the top transformer layer to guide feature
adaptation in earlier layers. Extensive experiments on multiple audiovisual
benchmarks demonstrate that our method significantly reduces memory usage and
training time while maintaining parameter efficiency and competitive accuracy.

</details>


### [270] [Why Settle for One? Text-to-ImageSet Generation and Evaluation](https://arxiv.org/abs/2506.23275)
*Chengyou Jia,Xin Shen,Zhuohang Dang,Zhuohang Dang,Changliang Xia,Weijia Wu,Xinyu Zhang,Hangwei Qian,Ivor W. Tsang,Minnan Luo*

Main category: cs.CV

TL;DR: The paper introduces Text-to-ImageSet (T2IS) generation, a novel problem for creating coherent image sets with diverse consistency requirements. It proposes T2IS-Bench, T2IS-Eval, and AutoT2IS to address the challenge, demonstrating superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for consistent image generation are domain-specific, limiting generalizability. The paper aims to address this by tackling the broader T2IS problem.

Method: The authors introduce T2IS-Bench for diverse instructions, T2IS-Eval for assessment, and AutoT2IS, a training-free framework leveraging Diffusion Transformers for consistency.

Result: AutoT2IS outperforms existing methods on T2IS-Bench, handling diverse consistency challenges and enabling real-world applications.

Conclusion: The proposed framework advances T2IS generation, offering practical value and outperforming specialized approaches.

Abstract: Despite remarkable progress in Text-to-Image models, many real-world
applications require generating coherent image sets with diverse consistency
requirements. Existing consistent methods often focus on a specific domain with
specific aspects of consistency, which significantly constrains their
generalizability to broader applications. In this paper, we propose a more
challenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate
sets of images that meet various consistency requirements based on user
instructions. To systematically study this problem, we first introduce
$\textbf{T2IS-Bench}$ with 596 diverse instructions across 26 subcategories,
providing comprehensive coverage for T2IS generation. Building on this, we
propose $\textbf{T2IS-Eval}$, an evaluation framework that transforms user
instructions into multifaceted assessment criteria and employs effective
evaluators to adaptively assess consistency fulfillment between criteria and
generated sets. Subsequently, we propose $\textbf{AutoT2IS}$, a training-free
framework that maximally leverages pretrained Diffusion Transformers'
in-context capabilities to harmonize visual elements to satisfy both
image-level prompt alignment and set-level visual consistency. Extensive
experiments on T2IS-Bench reveal that diverse consistency challenges all
existing methods, while our AutoT2IS significantly outperforms current
generalized and even specialized approaches. Our method also demonstrates the
ability to enable numerous underexplored real-world applications, confirming
its substantial practical value. Visit our project in
https://chengyou-jia.github.io/T2IS-Home.

</details>


### [271] [Autoregressive Denoising Score Matching is a Good Video Anomaly Detector](https://arxiv.org/abs/2506.23282)
*Hanwen Zhang,Congqi Cao,Qinyi Lv,Lingtong Min,Yanning Zhang*

Main category: cs.CV

TL;DR: The paper proposes a novel method for video anomaly detection (VAD) using a noise-conditioned score transformer and motion-aware scoring to address gaps in scene, motion, and appearance. It achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Likelihood-based VAD methods fail to detect anomalies near learned distributions. The paper aims to address this by tackling gaps in scene, motion, and appearance.

Method: Uses a noise-conditioned score transformer for denoising score matching, introduces scene-dependent and motion-aware scoring, and employs autoregressive denoising for enhanced anomaly detection.

Result: Demonstrates state-of-the-art performance on three VAD benchmarks.

Conclusion: The proposed method effectively addresses gaps in VAD, improving anomaly detection by considering scene, motion, and appearance.

Abstract: Video anomaly detection (VAD) is an important computer vision problem. Thanks
to the mode coverage capabilities of generative models, the likelihood-based
paradigm is catching growing interest, as it can model normal distribution and
detect out-of-distribution anomalies. However, these likelihood-based methods
are blind to the anomalies located in local modes near the learned
distribution. To handle these ``unseen" anomalies, we dive into three gaps
uniquely existing in VAD regarding scene, motion and appearance. Specifically,
we first build a noise-conditioned score transformer for denoising score
matching. Then, we introduce a scene-dependent and motion-aware score function
by embedding the scene condition of input sequences into our model and
assigning motion weights based on the difference between key frames of input
sequences. Next, to solve the problem of blindness in principle, we integrate
unaffected visual information via a novel autoregressive denoising score
matching mechanism for inference. Through autoregressively injecting
intensifying Gaussian noise into the denoised data and estimating the
corresponding score function, we compare the denoised data with the original
data to get a difference and aggregate it with the score function for an
enhanced appearance perception and accumulate the abnormal context. With all
three gaps considered, we can compute a more comprehensive anomaly indicator.
Experiments on three popular VAD benchmarks demonstrate the state-of-the-art
performance of our method.

</details>


### [272] [MoMa: Modulating Mamba for Adapting Image Foundation Models to Video Recognition](https://arxiv.org/abs/2506.23283)
*Yuhuan Yang,Chaofan Ma,Zhenjie Mao,Jiangchao Yao,Ya Zhang,Yanfeng Wang*

Main category: cs.CV

TL;DR: MoMa is an efficient adapter framework for video understanding, integrating Mamba's selective state space modeling into image foundation models (IFMs) for full spatial-temporal modeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods for adapting IFMs to video often separate spatial and temporal information, failing to capture video dynamics fully.

Method: Proposes MoMa with SeqMod operation to inject spatial-temporal information into IFMs without disrupting original features, using a Divide-and-Modulate architecture.

Result: MoMa achieves superior performance on video benchmarks with reduced computational cost.

Conclusion: MoMa effectively enhances video understanding by integrating spatial-temporal modeling efficiently.

Abstract: Video understanding is a complex challenge that requires effective modeling
of spatial-temporal dynamics. With the success of image foundation models
(IFMs) in image understanding, recent approaches have explored
parameter-efficient fine-tuning (PEFT) to adapt IFMs for video. However, most
of these methods tend to process spatial and temporal information separately,
which may fail to capture the full intricacy of video dynamics. In this paper,
we propose MoMa, an efficient adapter framework that achieves full
spatial-temporal modeling by integrating Mamba's selective state space modeling
into IFMs. We propose a novel SeqMod operation to inject spatial-temporal
information into pre-trained IFMs, without disrupting their original features.
By incorporating SeqMod into a Divide-and-Modulate architecture, MoMa enhances
video understanding while maintaining computational efficiency. Extensive
experiments on multiple video benchmarks demonstrate the effectiveness of MoMa,
achieving superior performance with reduced computational cost.

</details>


### [273] [Competitive Distillation: A Simple Learning Strategy for Improving Visual Classification](https://arxiv.org/abs/2506.23285)
*Daqian Shi,Xiaolei Diao,Xu Chen,Cédric M. John*

Main category: cs.CV

TL;DR: The paper proposes a competitive distillation strategy for DNN training, where networks dynamically act as teachers based on performance, improving learning efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Current distillation methods like mutual learning and self-distillation have limited improvements due to unclear learning direction impacts across iterations.

Method: Introduces competitive distillation, where networks compete and act as teachers based on performance, with stochastic perturbation to enhance representations.

Result: Competitive distillation shows promising performance across diverse tasks and datasets.

Conclusion: The strategy effectively enhances DNN training by leveraging competition and dynamic teacher roles.

Abstract: Deep Neural Networks (DNNs) have significantly advanced the field of computer
vision. To improve DNN training process, knowledge distillation methods
demonstrate their effectiveness in accelerating network training by introducing
a fixed learning direction from the teacher network to student networks. In
this context, several distillation-based optimization strategies are proposed,
e.g., deep mutual learning and self-distillation, as an attempt to achieve
generic training performance enhancement through the cooperative training of
multiple networks. However, such strategies achieve limited improvements due to
the poor understanding of the impact of learning directions among networks
across different iterations. In this paper, we propose a novel competitive
distillation strategy that allows each network in a group to potentially act as
a teacher based on its performance, enhancing the overall learning performance.
Competitive distillation organizes a group of networks to perform a shared task
and engage in competition, where competitive optimization is proposed to
improve the parameter updating process. We further introduce stochastic
perturbation in competitive distillation, aiming to motivate networks to induce
mutations to achieve better visual representations and global optimum. The
experimental results show that competitive distillation achieves promising
performance in diverse tasks and datasets.

</details>


### [274] [DDL: A Dataset for Interpretable Deepfake Detection and Localization in Real-World Scenarios](https://arxiv.org/abs/2506.23292)
*Changtao Miao,Yi Zhang,Weize Gao,Man Luo,Weiwei Feng,Zhiya Tan,Jianshu Li,Ajian Liu,Yunfeng Diao,Qi Chu,Tao Gong,Zhe Li,Weibin Yao,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: A new large-scale deepfake detection and localization dataset (DDL) is introduced to address the lack of interpretability and diversity in existing methods, featuring 1.8M samples and 75 deepfake methods.


<details>
  <summary>Details</summary>
Motivation: The misuse of deepfake content necessitates reliable detection methods with interpretability, especially in critical domains like law, where current datasets are limited in diversity and scale.

Method: The DDL dataset is constructed with diverse forgery scenarios, comprehensive deepfake methods, varied manipulation modes, and fine-grained annotations.

Result: The DDL dataset provides a challenging benchmark for real-world forgeries and supports next-generation detection and interpretability methods.

Conclusion: The DDL dataset addresses limitations of existing datasets, enhancing deepfake detection and interpretability for complex real-world scenarios.

Abstract: Recent advances in AIGC have exacerbated the misuse of malicious deepfake
content, making the development of reliable deepfake detection methods an
essential means to address this challenge. Although existing deepfake detection
models demonstrate outstanding performance in detection metrics, most methods
only provide simple binary classification results, lacking interpretability. In
critical domains such as law, interpretability is crucial for enhancing the
credibility and authority of decisions. Recent studies attempt to improve the
interpretability of classification results by providing spatial manipulation
masks or temporal forgery segments. However, the practical effectiveness of
these methods remains suboptimal due to limitations of the forgery data. Most
current deepfake datasets predominantly offer binary labels, only a few
datasets with localization annotations. However, they suffer from restricted
forgery scenarios, limited diversity in deepfake types, and insufficient data
scale, making them inadequate for complex real-world scenarios. To address this
predicament, we construct a novel large-scale deepfake detection and
localization ($\textbf{DDL}$) dataset containing over $\textbf{1.8M}$ forged
samples and encompassing up to $\textbf{75}$ distinct deepfake methods. The DDL
design incorporates four key innovations: (1) $\textbf{Diverse Forgery
Scenarios}$, (2) $\textbf{Comprehensive Deepfake Methods}$, (3) $\textbf{Varied
Manipulation Modes}$, and (4) $\textbf{Fine-grained Forgery Annotations}$.
Through these improvements, our DDL not only provides a more challenging
benchmark for complex real-world forgeries, but also offers crucial support for
building next-generation deepfake detection, localization, and interpretability
methods. The DDL dataset project page is on
https://deepfake-workshop-ijcai2025.github.io/main/index.html.

</details>


### [275] [DiffFit: Disentangled Garment Warping and Texture Refinement for Virtual Try-On](https://arxiv.org/abs/2506.23295)
*Xiang Xu*

Main category: cs.CV

TL;DR: DiffFit is a two-stage latent diffusion framework for high-fidelity virtual try-on, addressing garment detail preservation, alignment, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing VTON methods struggle with preserving garment details, alignment, efficiency, and generalization to diverse poses/styles.

Method: DiffFit uses a two-stage approach: geometry-aware garment warping followed by texture refinement via cross-modal conditional diffusion.

Result: DiffFit outperforms state-of-the-art methods in preserving garment attributes and alignment, validated by benchmarks.

Conclusion: DiffFit effectively decouples geometric alignment and appearance refinement, enhancing realism and stability in virtual try-on.

Abstract: Virtual try-on (VTON) aims to synthesize realistic images of a person wearing
a target garment, with broad applications in e-commerce and digital fashion.
While recent advances in latent diffusion models have substantially improved
visual quality, existing approaches still struggle with preserving fine-grained
garment details, achieving precise garment-body alignment, maintaining
inference efficiency, and generalizing to diverse poses and clothing styles. To
address these challenges, we propose DiffFit, a novel two-stage latent
diffusion framework for high-fidelity virtual try-on. DiffFit adopts a
progressive generation strategy: the first stage performs geometry-aware
garment warping, aligning the garment with the target body through fine-grained
deformation and pose adaptation. The second stage refines texture fidelity via
a cross-modal conditional diffusion model that integrates the warped garment,
the original garment appearance, and the target person image for high-quality
rendering. By decoupling geometric alignment and appearance refinement, DiffFit
effectively reduces task complexity and enhances both generation stability and
visual realism. It excels in preserving garment-specific attributes such as
textures, wrinkles, and lighting, while ensuring accurate alignment with the
human body. Extensive experiments on large-scale VTON benchmarks demonstrate
that DiffFit achieves superior performance over existing state-of-the-art
methods in both quantitative metrics and perceptual evaluations.

</details>


### [276] [Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting](https://arxiv.org/abs/2506.23308)
*Yiming Huang,Long Bai,Beilei Cui,Yanheng Li,Tong Chen,Jie Wang,Jinlin Wu,Zhen Lei,Hongbin Liu,Hongliang Ren*

Main category: cs.CV

TL;DR: Endo-4DGX improves 3D Gaussian Splatting for endoscopic scenes by addressing illumination challenges, achieving better rendering under low-light and over-exposure conditions.


<details>
  <summary>Details</summary>
Motivation: Accurate soft tissue reconstruction in robotic surgery is hindered by varying illumination, which degrades 3D-GS performance.

Method: Endo-4DGX uses illumination embeddings, a region-aware enhancement module, and a spatial-aware adjustment module to adapt to lighting variations.

Result: The method outperforms state-of-the-art techniques in challenging lighting, maintaining geometric accuracy.

Conclusion: Endo-4DGX advances robotic surgery by enabling reliable reconstruction under uneven lighting.

Abstract: Accurate reconstruction of soft tissue is crucial for advancing automation in
image-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS)
techniques and their variants, 4DGS, achieve high-quality renderings of dynamic
surgical scenes in real-time. However, 3D-GS-based methods still struggle in
scenarios with varying illumination, such as low light and over-exposure.
Training 3D-GS in such extreme light conditions leads to severe optimization
problems and devastating rendering quality. To address these challenges, we
present Endo-4DGX, a novel reconstruction method with illumination-adaptive
Gaussian Splatting designed specifically for endoscopic scenes with uneven
lighting. By incorporating illumination embeddings, our method effectively
models view-dependent brightness variations. We introduce a region-aware
enhancement module to model the sub-area lightness at the Gaussian level and a
spatial-aware adjustment module to learn the view-consistent brightness
adjustment. With the illumination adaptive design, Endo-4DGX achieves superior
rendering performance under both low-light and over-exposure conditions while
maintaining geometric accuracy. Additionally, we employ an exposure control
loss to restore the appearance from adverse exposure to the normal level for
illumination-adaptive optimization. Experimental results demonstrate that
Endo-4DGX significantly outperforms combinations of state-of-the-art
reconstruction and restoration methods in challenging lighting environments,
underscoring its potential to advance robot-assisted surgical applications. Our
code is available at https://github.com/lastbasket/Endo-4DGX.

</details>


### [277] [FastSeg: Efficient Training-Free Open-Vocabulary Segmentation via Hierarchical Attention Refinement Method](https://arxiv.org/abs/2506.23323)
*Quang-Huy Che,Vinh-Tiep Nguyen*

Main category: cs.CV

TL;DR: FastSeg is a training-free framework for open-vocabulary semantic segmentation, using a pretrained diffusion model with minimal steps, achieving high efficiency and state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods lose spatial precision (contrastive learning) or struggle with iteration-quality balance (diffusion models). FastSeg aims to bridge this gap.

Method: Uses (1+1)-step reverse process of a pretrained diffusion model, dual-prompt mechanism, HARD for attention refinement, and TTF for spatial consistency.

Result: Achieves 43.8% mIoU on PASCAL VOC, PASCAL Context, and COCO Object benchmarks with superior efficiency.

Conclusion: FastSeg balances segmentation quality and efficiency, offering a strong foundation for future extensions.

Abstract: Open-vocabulary semantic segmentation (OVSS) aims to segment objects from
arbitrary text categories without requiring densely annotated datasets.
Although contrastive learning based models enable zero-shot segmentation, they
often lose fine spatial precision at pixel level, due to global representation
bias. In contrast, diffusion-based models naturally encode fine-grained spatial
features via attention mechanisms that capture both global context and local
details. However, they often face challenges in balancing the number of
iterations with the quality of the segmentation. In this work, we propose
FastSeg, a novel and efficient training-free framework with only (1+1)-step of
reverse process of a pretrained diffusion model (e.g., Stable Diffusion).
Moreover, instead of running multiple times for different classes, FastSeg
performs segmentation for all classes at once. To further enhance the
segmentation quality, FastSeg introduces three key components: (i) a
dual-prompt mechanism for discriminative, class-aware attention extraction,
(ii) a Hierarchical Attention Refinement Method (HARD) that enhances fused
cross-attention using scale-aligned selfattention maps, and (iii) a Test-Time
Flipping (TTF) scheme designed to improve spatial consistency. Extensive
experiments show that FastSeg achieves state-of-the-art training-free
performance, obtaining 43.8% average mIoU across PASCAL VOC, PASCAL Context,
and COCO Object benchmarks while maintaining superior inference efficiency. Our
results demonstrate that FastSeg provides a strong foundation for
extendability, bridging the gap between segmentation quality and inference
efficiency.

</details>


### [278] [IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering](https://arxiv.org/abs/2506.23329)
*Parker Liu,Chenxin Li,Zhengxin Li,Yipeng Wu,Wuyang Li,Zhiqin Yang,Zhenyuan Zhang,Yunlong Lin,Sirui Han,Brandon Y. Feng*

Main category: cs.CV

TL;DR: IR3D-Bench is a benchmark challenging VLMs to demonstrate scene understanding by actively recreating 3D structures from images using tools, moving beyond passive recognition.


<details>
  <summary>Details</summary>
Motivation: To assess if VLMs truly understand scenes by testing their ability to actively create rather than passively describe.

Method: VLAs use programming and rendering tools to recreate 3D structures from images, following an analysis-by-synthesis paradigm.

Result: Initial experiments reveal limitations in visual precision, not tool usage, among state-of-the-art VLMs.

Conclusion: IR3D-Bench enables systematic study of tool-using VLAs for genuine scene understanding through creation.

Abstract: Vision-language models (VLMs) excel at descriptive tasks, but whether they
truly understand scenes from visual observations remains uncertain. We
introduce IR3D-Bench, a benchmark challenging VLMs to demonstrate understanding
through active creation rather than passive recognition. Grounded in the
analysis-by-synthesis paradigm, IR3D-Bench tasks Vision-Language Agents (VLAs)
with actively using programming and rendering tools to recreate the underlying
3D structure of an input image, achieving agentic inverse rendering through
tool use. This "understanding-by-creating" approach probes the tool-using
generative capacity of VLAs, moving beyond the descriptive or conversational
capacity measured by traditional scene understanding benchmarks. We provide a
comprehensive suite of metrics to evaluate geometric accuracy, spatial
relations, appearance attributes, and overall plausibility. Initial experiments
on agentic inverse rendering powered by various state-of-the-art VLMs highlight
current limitations, particularly in visual precision rather than basic tool
usage. IR3D-Bench, including data and evaluation protocols, is released to
facilitate systematic study and development of tool-using VLAs towards genuine
scene understanding by creating.

</details>


### [279] [CycleVAR: Repurposing Autoregressive Model for Unsupervised One-Step Image Translation](https://arxiv.org/abs/2506.23347)
*Yi Liu,Shengqian Li,Zuzeng Lin,Feng Wang,Si Liu*

Main category: cs.CV

TL;DR: CycleVAR introduces Softmax Relaxed Quantization and a dual-mode autoregressive approach for unsupervised image translation, outperforming existing methods like CycleGAN-Turbo.


<details>
  <summary>Details</summary>
Motivation: Addressing the gradient flow disruption in traditional Vector Quantization-based frameworks for unsupervised image translation.

Method: Proposes Softmax Relaxed Quantization for continuous gradient flow and CycleVAR, which uses multi-scale source image tokens for autoregressive generation in serial or parallel modes.

Result: Parallel one-step generation achieves better translation quality and faster inference than serial mode, surpassing state-of-the-art models.

Conclusion: CycleVAR effectively improves unsupervised image translation by preserving gradients and leveraging autoregressive generation.

Abstract: The current conditional autoregressive image generation methods have shown
promising results, yet their potential remains largely unexplored in the
practical unsupervised image translation domain, which operates without
explicit cross-domain correspondences. A critical limitation stems from the
discrete quantization inherent in traditional Vector Quantization-based
frameworks, which disrupts gradient flow between the Variational Autoencoder
decoder and causal Transformer, impeding end-to-end optimization during
adversarial training in image space. To tackle this issue, we propose using
Softmax Relaxed Quantization, a novel approach that reformulates codebook
selection as a continuous probability mixing process via Softmax, thereby
preserving gradient propagation. Building upon this differentiable foundation,
we introduce CycleVAR, which reformulates image-to-image translation as
image-conditional visual autoregressive generation by injecting multi-scale
source image tokens as contextual prompts, analogous to prefix-based
conditioning in language models. CycleVAR exploits two modes to generate the
target image tokens, including (1) serial multi-step generation, enabling
iterative refinement across scales, and (2) parallel one-step generation
synthesizing all resolution outputs in a single forward pass. Experimental
findings indicate that the parallel one-step generation mode attains superior
translation quality with quicker inference speed than the serial multi-step
mode in unsupervised scenarios. Furthermore, both quantitative and qualitative
results indicate that CycleVAR surpasses previous state-of-the-art unsupervised
image translation models, \textit{e}.\textit{g}., CycleGAN-Turbo.

</details>


### [280] [GeoProg3D: Compositional Visual Reasoning for City-Scale 3D Language Fields](https://arxiv.org/abs/2506.23352)
*Shunsuke Yasuki,Taiki Miyanishi,Nakamasa Inoue,Shuhei Kurita,Koya Sakamoto,Daichi Azuma,Masato Taki,Yutaka Matsuo*

Main category: cs.CV

TL;DR: GeoProg3D is a visual programming framework for natural language-driven interactions with city-scale 3D scenes, combining geography-aware 3D language fields and specialized vision APIs, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing 3D language approaches lack scalability and compositional reasoning for large urban settings.

Method: GeoProg3D uses a Geography-aware City-scale 3D Language Field (GCLF) and Geographical Vision APIs (GV-APIs), powered by LLMs for dynamic task execution.

Result: GeoProg3D outperforms existing methods on the GeoEval3D benchmark across five tasks.

Conclusion: GeoProg3D is the first framework enabling compositional geographic reasoning in city-scale 3D environments via natural language.

Abstract: The advancement of 3D language fields has enabled intuitive interactions with
3D scenes via natural language. However, existing approaches are typically
limited to small-scale environments, lacking the scalability and compositional
reasoning capabilities necessary for large, complex urban settings. To overcome
these limitations, we propose GeoProg3D, a visual programming framework that
enables natural language-driven interactions with city-scale high-fidelity 3D
scenes. GeoProg3D consists of two key components: (i) a Geography-aware
City-scale 3D Language Field (GCLF) that leverages a memory-efficient
hierarchical 3D model to handle large-scale data, integrated with geographic
information for efficiently filtering vast urban spaces using directional cues,
distance measurements, elevation data, and landmark references; and (ii)
Geographical Vision APIs (GV-APIs), specialized geographic vision tools such as
area segmentation and object detection. Our framework employs large language
models (LLMs) as reasoning engines to dynamically combine GV-APIs and operate
GCLF, effectively supporting diverse geographic vision tasks. To assess
performance in city-scale reasoning, we introduce GeoEval3D, a comprehensive
benchmark dataset containing 952 query-answer pairs across five challenging
tasks: grounding, spatial reasoning, comparison, counting, and measurement.
Experiments demonstrate that GeoProg3D significantly outperforms existing 3D
language fields and vision-language models across multiple tasks. To our
knowledge, GeoProg3D is the first framework enabling compositional geographic
reasoning in high-fidelity city-scale 3D environments via natural language. The
code is available at https://snskysk.github.io/GeoProg3D/.

</details>


### [281] [Layer Decomposition and Morphological Reconstruction for Task-Oriented Infrared Image Enhancement](https://arxiv.org/abs/2506.23353)
*Siyuan Chai,Xiaodong Guo,Tong Liu*

Main category: cs.CV

TL;DR: A task-oriented infrared image enhancement method improves contrast for autonomous driving by decomposing layers and extracting saliency information, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Infrared images enhance perception in complex weather but suffer from low contrast, especially for non-heat-emitting targets, impacting downstream vision tasks.

Method: Proposes layer decomposition to preserve dark region features and morphological reconstruction-based saliency extraction to enhance targets without noise amplification.

Result: Improves image quality for object detection and semantic segmentation, outperforming state-of-the-art methods.

Conclusion: The method effectively addresses contrast enhancement challenges in infrared images, benefiting high-level vision tasks.

Abstract: Infrared image helps improve the perception capabilities of autonomous
driving in complex weather conditions such as fog, rain, and low light.
However, infrared image often suffers from low contrast, especially in
non-heat-emitting targets like bicycles, which significantly affects the
performance of downstream high-level vision tasks. Furthermore, achieving
contrast enhancement without amplifying noise and losing important information
remains a challenge. To address these challenges, we propose a task-oriented
infrared image enhancement method. Our approach consists of two key components:
layer decomposition and saliency information extraction. First, we design an
layer decomposition method for infrared images, which enhances scene details
while preserving dark region features, providing more features for subsequent
saliency information extraction. Then, we propose a morphological
reconstruction-based saliency extraction method that effectively extracts and
enhances target information without amplifying noise. Our method improves the
image quality for object detection and semantic segmentation tasks. Extensive
experiments demonstrate that our approach outperforms state-of-the-art methods.

</details>


### [282] [OmniVCus: Feedforward Subject-driven Video Customization with Multimodal Control Conditions](https://arxiv.org/abs/2506.23361)
*Yuanhao Cai,He Zhang,Xi Chen,Jinbo Xing,Yiwei Hu,Yuqian Zhou,Kai Zhang,Zhifei Zhang,Soo Ye Kim,Tianyu Wang,Yulun Zhang,Xiaokang Yang,Zhe Lin,Alan Yuille*

Main category: cs.CV

TL;DR: The paper introduces VideoCus-Factory for multi-subject video customization data construction and OmniVCus, a diffusion Transformer framework with Lottery Embedding and Temporally Aligned Embedding for improved video editing.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of multi-subject training data and unexplored control signals (depth, mask, camera, text prompts) in video customization.

Method: Proposes VideoCus-Factory for data construction and OmniVCus framework with Lottery Embedding (LE) and Temporally Aligned Embedding (TAE) for multi-subject inference and control signal utilization.

Result: Outperforms state-of-the-art methods in quantitative and qualitative evaluations.

Conclusion: The approach enables effective multi-subject video customization and control signal integration, with promising results.

Abstract: Existing feedforward subject-driven video customization methods mainly study
single-subject scenarios due to the difficulty of constructing multi-subject
training data pairs. Another challenging problem that how to use the signals
such as depth, mask, camera, and text prompts to control and edit the subject
in the customized video is still less explored. In this paper, we first propose
a data construction pipeline, VideoCus-Factory, to produce training data pairs
for multi-subject customization from raw videos without labels and control
signals such as depth-to-video and mask-to-video pairs. Based on our
constructed data, we develop an Image-Video Transfer Mixed (IVTM) training with
image editing data to enable instructive editing for the subject in the
customized video. Then we propose a diffusion Transformer framework, OmniVCus,
with two embedding mechanisms, Lottery Embedding (LE) and Temporally Aligned
Embedding (TAE). LE enables inference with more subjects by using the training
subjects to activate more frame embeddings. TAE encourages the generation
process to extract guidance from temporally aligned control signals by
assigning the same frame embeddings to the control and noise tokens.
Experiments demonstrate that our method significantly surpasses
state-of-the-art methods in both quantitative and qualitative evaluations.
Video demos are at our project page:
https://caiyuanhao1998.github.io/project/OmniVCus/. Our code will be released
at https://github.com/caiyuanhao1998/Open-OmniVCus

</details>


### [283] [SIEDD: Shared-Implicit Encoder with Discrete Decoders](https://arxiv.org/abs/2506.23382)
*Vikram Rangarajan,Shishira Maiya,Max Ehrlich,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: SIEDD accelerates INR video encoding by 20-30X without sacrificing quality or control, using a shared encoder and discrete decoders.


<details>
  <summary>Details</summary>
Motivation: Slow encoding times of INRs hinder adoption despite their high fidelity for video compression.

Method: SIEDD uses a shared encoder for global features and lightweight decoders for frame groups, with aggressive sampling for speed.

Result: Achieves 20-30X faster encoding than state-of-the-art INR codecs while maintaining quality and compression ratios.

Conclusion: SIEDD advances neural video compression practicality, enabling real-world deployment with scalable efficiency.

Abstract: Implicit Neural Representations (INRs) offer exceptional fidelity for video
compression by learning per-video optimized functions, but their adoption is
crippled by impractically slow encoding times. Existing attempts to accelerate
INR encoding often sacrifice reconstruction quality or crucial coordinate-level
control essential for adaptive streaming and transcoding. We introduce SIEDD
(Shared-Implicit Encoder with Discrete Decoders), a novel architecture that
fundamentally accelerates INR encoding without these compromises. SIEDD first
rapidly trains a shared, coordinate-based encoder on sparse anchor frames to
efficiently capture global, low-frequency video features. This encoder is then
frozen, enabling massively parallel training of lightweight, discrete decoders
for individual frame groups, further expedited by aggressive coordinate-space
sampling. This synergistic design delivers a remarkable 20-30X encoding
speed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while
maintaining competitive reconstruction quality and compression ratios.
Critically, SIEDD retains full coordinate-based control, enabling continuous
resolution decoding and eliminating costly transcoding. Our approach
significantly advances the practicality of high-fidelity neural video
compression, demonstrating a scalable and efficient path towards real-world
deployment. Our codebase is available at
https://github.com/VikramRangarajan/SIEDD .

</details>


### [284] [A High-Throughput Platform to Bench Test Smartphone-Based Heart Rate Measurements Derived From Video](https://arxiv.org/abs/2506.23414)
*Ming-Zher Poh,Jonathan Wang,Jonathan Hsu,Lawrence Cai,Eric Teasley,James A. Taylor,Jameson K. Rogers,Anupam Pathak,Shwetak Patel*

Main category: cs.CV

TL;DR: A high-throughput bench-testing platform for smartphone-based heart rate (HR) apps addresses device variability and lack of standardized testing. It uses synthetic PPG videos and parallel testing, achieving high accuracy and device compatibility.


<details>
  <summary>Details</summary>
Motivation: Challenges in evaluating smartphone HR apps due to device variability and lack of standardized testing methods.

Method: A system with a test rig for 12 smartphones, synthetic PPG video generation, and a host machine for coordination and data logging.

Result: Achieved 0.11% MAPE in HR accuracy and 0.92 correlation for PPG signals, with all tested devices meeting ANSI/CTA standards.

Conclusion: The platform provides scalable pre-deployment testing to enhance app performance and device compatibility in mobile health.

Abstract: Smartphone-based heart rate (HR) monitoring apps using finger-over-camera
photoplethysmography (PPG) face significant challenges in performance
evaluation and device compatibility due to device variability and
fragmentation. Manual testing is impractical, and standardized methods are
lacking. This paper presents a novel, high-throughput bench-testing platform to
address this critical need. We designed a system comprising a test rig capable
of holding 12 smartphones for parallel testing, a method for generating
synthetic PPG test videos with controllable HR and signal quality, and a host
machine for coordinating video playback and data logging. The system achieved a
mean absolute percentage error (MAPE) of 0.11% +/- 0.001% between input and
measured HR, and a correlation coefficient of 0.92 +/- 0.008 between input and
measured PPG signals using a clinically-validated smartphone-based HR app.
Bench-testing results of 20 different smartphone models correctly classified
all the devices as meeting the ANSI/CTA accuracy standards for HR monitors
(MAPE <10%) when compared to a prospective clinical study with 80 participants,
demonstrating high positive predictive value. This platform offers a scalable
solution for pre-deployment testing of smartphone HR apps to improve app
performance, ensure device compatibility, and advance the field of mobile
health.

</details>


### [285] [Why Settle for Mid: A Probabilistic Viewpoint to Spatial Relationship Alignment in Text-to-image Models](https://arxiv.org/abs/2506.23418)
*Parham Rezaei,Arash Marioriyad,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: The paper addresses spatial misalignment in text-to-image models by introducing a probabilistic framework (PoS) and two contributions: PSE for evaluation and PSG for generation, improving spatial accuracy.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models struggle with accurately representing spatial relationships in input prompts, leading to misaligned compositions.

Method: Proposes PoS-based Evaluation (PSE) for assessing spatial alignment and PoS-based Generation (PSG) for improving generation without fine-tuning, using gradient-based or search-based strategies.

Result: PSE aligns better with human judgment than traditional metrics, and PSG outperforms state-of-the-art methods in generating spatially accurate images.

Conclusion: The proposed framework and methods significantly enhance spatial relationship accuracy in text-to-image generation, validated by experiments.

Abstract: Despite the ability of text-to-image models to generate high-quality,
realistic, and diverse images, they face challenges in compositional
generation, often struggling to accurately represent details specified in the
input prompt. A prevalent issue in compositional generation is the misalignment
of spatial relationships, as models often fail to faithfully generate images
that reflect the spatial configurations specified between objects in the input
prompts. To address this challenge, we propose a novel probabilistic framework
for modeling the relative spatial positioning of objects in a scene, leveraging
the concept of Probability of Superiority (PoS). Building on this insight, we
make two key contributions. First, we introduce a novel evaluation metric,
PoS-based Evaluation (PSE), designed to assess the alignment of 2D and 3D
spatial relationships between text and image, with improved adherence to human
judgment. Second, we propose PoS-based Generation (PSG), an inference-time
method that improves the alignment of 2D and 3D spatial relationships in T2I
models without requiring fine-tuning. PSG employs a Part-of-Speech PoS-based
reward function that can be utilized in two distinct ways: (1) as a
gradient-based guidance mechanism applied to the cross-attention maps during
the denoising steps, or (2) as a search-based strategy that evaluates a set of
initial noise vectors to select the best one. Extensive experiments demonstrate
that the PSE metric exhibits stronger alignment with human judgment compared to
traditional center-based metrics, providing a more nuanced and reliable measure
of complex spatial relationship accuracy in text-image alignment. Furthermore,
PSG significantly enhances the ability of text-to-image models to generate
images with specified spatial configurations, outperforming state-of-the-art
methods across multiple evaluation metrics and benchmarks.

</details>


### [286] [Detecting What Matters: A Novel Approach for Out-of-Distribution 3D Object Detection in Autonomous Vehicles](https://arxiv.org/abs/2506.23426)
*Menna Taha,Aya Ahmed,Mohammed Karmoose,Yasser Gadallah*

Main category: cs.CV

TL;DR: The paper proposes a novel object detection method for AVs that focuses on classifying objects as 'harmful' or 'harmless' instead of traditional class-based detection, improving safety for OOD objects.


<details>
  <summary>Details</summary>
Motivation: Current AV object detection methods struggle with OOD objects, posing safety risks due to misclassification or failure to detect.

Method: The approach evaluates object harmfulness based on position and trajectory relative to the AV, bypassing class-based classification.

Result: The model effectively detects OOD objects, assesses their danger, and enhances AV decision-making in dynamic environments.

Conclusion: The proposed method improves AV safety by focusing on harmfulness rather than class, addressing OOD challenges.

Abstract: Autonomous vehicles (AVs) use object detection models to recognize their
surroundings and make driving decisions accordingly. Conventional object
detection approaches classify objects into known classes, which limits the AV's
ability to detect and appropriately respond to Out-of-Distribution (OOD)
objects. This problem is a significant safety concern since the AV may fail to
detect objects or misclassify them, which can potentially lead to hazardous
situations such as accidents. Consequently, we propose a novel object detection
approach that shifts the emphasis from conventional class-based classification
to object harmfulness determination. Instead of object detection by their
specific class, our method identifies them as either 'harmful' or 'harmless'
based on whether they pose a danger to the AV. This is done based on the object
position relative to the AV and its trajectory. With this metric, our model can
effectively detect previously unseen objects to enable the AV to make safer
real-time decisions. Our results demonstrate that the proposed model
effectively detects OOD objects, evaluates their harmfulness, and classifies
them accordingly, thus enhancing the AV decision-making effectiveness in
dynamic environments.

</details>


### [287] [Towards foundational LiDAR world models with efficient latent flow matching](https://arxiv.org/abs/2506.23434)
*Tianran Liu,Shengwen Zhao,Nicholas Rhinehart*

Main category: cs.CV

TL;DR: A study on improving LiDAR world models' transferability across domains, achieving significant performance gains with less data and higher efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing LiDAR world models lack transferability across domains, requiring domain-specific training. This work aims to develop models that generalize better.

Method: Conducted domain transfer studies across three scenarios, proposed a latent conditional flow matching (CFM)-based framework for efficient training and compression.

Result: Achieved up to 11% absolute improvement over training from scratch, reduced labeled data reliance by 95%, and improved computational efficiency (23x faster).

Conclusion: The proposed CFM-based framework enhances transferability, efficiency, and performance of LiDAR world models, setting new benchmarks.

Abstract: LiDAR-based world models offer more structured and geometry-aware
representations than their image-based counterparts. However, existing LiDAR
world models are narrowly trained; each model excels only in the domain for
which it was built. Can we develop LiDAR world models that exhibit strong
transferability across multiple domains? We conduct the first systematic domain
transfer study across three demanding scenarios: (i) outdoor to indoor
generalization, (ii) sparse-beam \& dense-beam adaptation, and (iii)
non-semantic to semantic transfer. Given different amounts of fine-tuning data,
our experiments show that a single pre-trained model can achieve up to 11%
absolute improvement (83\% relative) over training from scratch and outperforms
training from scratch in 30/36 of our comparisons. This transferability of
dynamic learning significantly reduces the reliance on manually annotated data
for semantic occupancy forecasting: our method exceed the previous semantic
occupancy forecasting models with only 5% of the labeled training data required
by prior models. We also observed inefficiencies of current LiDAR world models,
mainly through their under-compression of LiDAR data and inefficient training
objectives. To address this, we propose a latent conditional flow matching
(CFM)-based frameworks that achieves state-of-the-art reconstruction accuracy
using only half the training data and a compression ratio 6 times higher than
that of prior methods. Our model achieves SOTA performance on
future-trajectory-conditioned semantic occupancy forecasting while being 23x
more computationally efficient (a 28x FPS speedup); and achieves SOTA
performance on semantic occupancy forecasting while being 2x more
computationally efficient (a 1.1x FPS speedup).

</details>


### [288] [PathDiff: Histopathology Image Synthesis with Unpaired Text and Mask Conditions](https://arxiv.org/abs/2506.23440)
*Mahesh Bhosale,Abdul Wasi,Yuanhao Zhai,Yunjie Tian,Samuel Border,Nan Xi,Pinaki Sarder,Junsong Yuan,David Doermann,Xuan Gong*

Main category: cs.CV

TL;DR: PathDiff is a diffusion framework for generating histopathology images by integrating unpaired mask-text data, improving control over semantics and spatial details.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity in histopathology due to privacy constraints and the lack of paired text-mask datasets for joint use in image generation.

Method: Proposes PathDiff, a diffusion framework that learns from unpaired mask-text data by unifying both modalities in a conditioning space.

Result: Generates high-quality, semantically accurate images with improved fidelity, text-image alignment, and faithfulness for downstream tasks.

Conclusion: PathDiff outperforms existing methods, offering enhanced control and quality in histopathology image synthesis.

Abstract: Diffusion-based generative models have shown promise in synthesizing
histopathology images to address data scarcity caused by privacy constraints.
Diagnostic text reports provide high-level semantic descriptions, and masks
offer fine-grained spatial structures essential for representing distinct
morphological regions. However, public datasets lack paired text and mask data
for the same histopathological images, limiting their joint use in image
generation. This constraint restricts the ability to fully exploit the benefits
of combining both modalities for enhanced control over semantics and spatial
details. To overcome this, we propose PathDiff, a diffusion framework that
effectively learns from unpaired mask-text data by integrating both modalities
into a unified conditioning space. PathDiff allows precise control over
structural and contextual features, generating high-quality, semantically
accurate images. PathDiff also improves image fidelity, text-image alignment,
and faithfulness, enhancing data augmentation for downstream tasks like nuclei
segmentation and classification. Extensive experiments demonstrate its
superiority over existing methods.

</details>


### [289] [Contrastive Learning with Diffusion Features for Weakly Supervised Medical Image Segmentation](https://arxiv.org/abs/2506.23460)
*Dewen Zeng,Xinrong Hu,Yu-Jen Chen,Yawen Wu,Xiaowei Xu,Yiyu Shi*

Main category: cs.CV

TL;DR: A novel method, CLDF, uses contrastive learning with diffusion features to improve weakly supervised semantic segmentation by addressing noise in CDM-generated saliency maps.


<details>
  <summary>Details</summary>
Motivation: Traditional CAM-based methods struggle with partial activations and imprecise boundaries, while CDM-generated saliency maps are noisy.

Method: CLDF integrates gradient maps from CDM with CAMs for contrastive learning, training a pixel decoder to map diffusion features for segmentation.

Result: CLDF outperforms baselines on four segmentation tasks across two medical datasets.

Conclusion: CLDF effectively reduces noise and improves segmentation accuracy in weakly supervised settings.

Abstract: Weakly supervised semantic segmentation (WSSS) methods using class labels
often rely on class activation maps (CAMs) to localize objects. However,
traditional CAM-based methods struggle with partial activations and imprecise
object boundaries due to optimization discrepancies between classification and
segmentation. Recently, the conditional diffusion model (CDM) has been used as
an alternative for generating segmentation masks in WSSS, leveraging its strong
image generation capabilities tailored to specific class distributions. By
modifying or perturbing the condition during diffusion sampling, the related
objects can be highlighted in the generated images. Yet, the saliency maps
generated by CDMs are prone to noise from background alterations during reverse
diffusion. To alleviate the problem, we introduce Contrastive Learning with
Diffusion Features (CLDF), a novel method that uses contrastive learning to
train a pixel decoder to map the diffusion features from a frozen CDM to a
low-dimensional embedding space for segmentation. Specifically, we integrate
gradient maps generated from CDM external classifier with CAMs to identify
foreground and background pixels with fewer false positives/negatives for
contrastive learning, enabling robust pixel embedding learning. Experimental
results on four segmentation tasks from two public medical datasets demonstrate
that our method significantly outperforms existing baselines.

</details>


### [290] [Time-variant Image Inpainting via Interactive Distribution Transition Estimation](https://arxiv.org/abs/2506.23461)
*Yun Xing,Qing Guo,Xiaoguang Li,Yihao Huang,Xiaofeng Cao,Di Lin,Ivor Tsang,Lei Ma*

Main category: cs.CV

TL;DR: The paper introduces Time-vAriant iMage inPainting (TAMP), a task to restore damaged images using time-variant reference images. It proposes the InDiTE-Diff method, combining interactive distribution transition estimation with diffusion models, and introduces a new dataset, TAMP-Street. Results show superiority over SOTA methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of restoring damaged images using time-variant reference images, which often have significant content differences and may also be damaged. Existing methods fail in this scenario.

Method: Proposes InDiTE-Diff, integrating the InDiTE module (for adaptive semantics) with a diffusion model, and introduces latent cross-reference during sampling.

Result: InDiTE-Diff outperforms SOTA reference-guided inpainting methods on the TAMP-Street dataset under two settings.

Conclusion: The proposed method effectively addresses the TAMP task, demonstrating superior performance and providing a new benchmark for future research.

Abstract: In this work, we focus on a novel and practical task, i.e., Time-vAriant
iMage inPainting (TAMP). The aim of TAMP is to restore a damaged target image
by leveraging the complementary information from a reference image, where both
images captured the same scene but with a significant time gap in between,
i.e., time-variant images. Different from conventional reference-guided image
inpainting, the reference image under TAMP setup presents significant content
distinction to the target image and potentially also suffers from damages. Such
an application frequently happens in our daily lives to restore a damaged image
by referring to another reference image, where there is no guarantee of the
reference image's source and quality. In particular, our study finds that even
state-of-the-art (SOTA) reference-guided image inpainting methods fail to
achieve plausible results due to the chaotic image complementation. To address
such an ill-posed problem, we propose a novel Interactive Distribution
Transition Estimation (InDiTE) module which interactively complements the
time-variant images with adaptive semantics thus facilitate the restoration of
damaged regions. To further boost the performance, we propose our TAMP
solution, namely Interactive Distribution Transition Estimation-driven
Diffusion (InDiTE-Diff), which integrates InDiTE with SOTA diffusion model and
conducts latent cross-reference during sampling. Moreover, considering the lack
of benchmarks for TAMP task, we newly assembled a dataset, i.e., TAMP-Street,
based on existing image and mask datasets. We conduct experiments on the
TAMP-Street datasets under two different time-variant image inpainting
settings, which show our method consistently outperform SOTA reference-guided
image inpainting methods for solving TAMP.

</details>


### [291] [Sanitizing Manufacturing Dataset Labels Using Vision-Language Models](https://arxiv.org/abs/2506.23465)
*Nazanin Mahjourian,Vinh Nguyen*

Main category: cs.CV

TL;DR: VLSR is a vision-language framework for cleaning and refining noisy labels in manufacturing datasets using CLIP embeddings and cosine similarity.


<details>
  <summary>Details</summary>
Motivation: Large-scale datasets, especially in manufacturing, often have noisy labels due to crowd-sourcing or web-scraping, making high-quality labels costly and rare.

Method: Uses CLIP to embed images and labels into a shared space, computes cosine similarity for sanitization, and applies density-based clustering to merge similar labels.

Result: VLSR effectively identifies and fixes problematic labels, reduces label vocabulary, and improves dataset quality with minimal human effort.

Conclusion: VLSR enhances label consistency and dataset quality, benefiting industrial machine learning applications.

Abstract: The success of machine learning models in industrial applications is heavily
dependent on the quality of the datasets used to train the models. However,
large-scale datasets, specially those constructed from crowd-sourcing and
web-scraping, often suffer from label noise, inconsistencies, and errors. This
problem is particularly pronounced in manufacturing domains, where obtaining
high-quality labels is costly and time-consuming. This paper introduces
Vision-Language Sanitization and Refinement (VLSR), which is a
vision-language-based framework for label sanitization and refinement in
multi-label manufacturing image datasets. This method embeds both images and
their associated textual labels into a shared semantic space leveraging the
CLIP vision-language model. Then two key tasks are addressed in this process by
computing the cosine similarity between embeddings. First, label sanitization
is performed to identify irrelevant, misspelled, or semantically weak labels,
and surface the most semantically aligned label for each image by comparing
image-label pairs using cosine similarity between image and label embeddings.
Second, the method applies density-based clustering on text embeddings,
followed by iterative cluster merging, to group semantically similar labels
into unified label groups. The Factorynet dataset, which includes noisy labels
from both human annotations and web-scraped sources, is employed to evaluate
the effectiveness of the proposed framework. Experimental results demonstrate
that the VLSR framework successfully identifies problematic labels and improves
label consistency. This method enables a significant reduction in label
vocabulary through clustering, which ultimately enhances the dataset's quality
for training robust machine learning models in industrial applications with
minimal human intervention.

</details>


### [292] [AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training for Chest X-rays](https://arxiv.org/abs/2506.23467)
*Chenlang Yi,Zizhan Xiong,Qi Qi,Xiyuan Wei,Girish Bathla,Ching-Long Lin,Bobak Jack Mortazavi,Tianbao Yang*

Main category: cs.CV

TL;DR: AdFair-CLIP improves fairness and accuracy in CLIP-based medical image classification by suppressing demographic biases.


<details>
  <summary>Details</summary>
Motivation: Address fairness concerns in CLIP models, particularly demographic biases affecting race and gender, which lead to disparities in diagnostic outcomes.

Method: Introduces AdFair-CLIP, a framework using adversarial feature intervention to mitigate spurious correlations.

Result: Significantly enhances fairness and diagnostic accuracy in chest X-ray datasets, with robust generalization in zero-shot and few-shot scenarios.

Conclusion: Sets new benchmarks for fairness-aware learning in CLIP-based medical diagnostic models, especially for CXR analysis.

Abstract: Contrastive Language-Image Pre-training (CLIP) models have demonstrated
superior performance across various visual tasks including medical image
classification. However, fairness concerns, including demographic biases, have
received limited attention for CLIP models. This oversight leads to critical
issues, particularly those related to race and gender, resulting in disparities
in diagnostic outcomes and reduced reliability for underrepresented groups. To
address these challenges, we introduce AdFair-CLIP, a novel framework employing
adversarial feature intervention to suppress sensitive attributes, thereby
mitigating spurious correlations and improving prediction fairness. We conduct
comprehensive experiments on chest X-ray (CXR) datasets, and show that
AdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while
maintaining robust generalization in zero-shot and few-shot scenarios. These
results establish new benchmarks for fairness-aware learning in CLIP-based
medical diagnostic models, particularly for CXR analysis.

</details>


### [293] [NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments](https://arxiv.org/abs/2506.23468)
*Xuan Yao,Junyu Gao,Changsheng Xu*

Main category: cs.CV

TL;DR: NavMorph is a self-evolving world model framework for VLN-CE tasks, improving generalization and adaptability through compact latent representations and Contextual Evolution Memory.


<details>
  <summary>Details</summary>
Motivation: Current VLN-CE methods struggle with novel environments and dynamic changes, prompting the need for a more adaptive and generalizable approach.

Method: NavMorph uses compact latent representations to model environmental dynamics and integrates Contextual Evolution Memory for scene-contextual navigation.

Result: NavMorph achieves significant performance improvements on VLN-CE benchmarks.

Conclusion: NavMorph enhances VLN-CE performance by combining adaptive planning and contextual memory, demonstrating its effectiveness in dynamic environments.

Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires
agents to execute sequential navigation actions in complex environments guided
by natural language instructions. Current approaches often struggle with
generalizing to novel environments and adapting to ongoing changes during
navigation. Inspired by human cognition, we present NavMorph, a self-evolving
world model framework that enhances environmental understanding and
decision-making in VLN-CE tasks. NavMorph employs compact latent
representations to model environmental dynamics, equipping agents with
foresight for adaptive planning and policy refinement. By integrating a novel
Contextual Evolution Memory, NavMorph leverages scene-contextual information to
support effective navigation while maintaining online adaptability. Extensive
experiments demonstrate that our method achieves notable performance
improvements on popular VLN-CE benchmarks. Code is available at
\href{https://github.com/Feliciaxyao/NavMorph}{this https URL}.

</details>


### [294] [Interactive Interface For Semantic Segmentation Dataset Synthesis](https://arxiv.org/abs/2506.23470)
*Ngoc-Do Tran,Minh-Tuan Huynh,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: SynthLab is a modular platform for visual data synthesis with a user-friendly interface, addressing the challenges of high-quality dataset creation for semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: The high cost and privacy concerns of creating annotated datasets for AI and computer vision tasks motivate the need for an efficient, scalable solution.

Method: SynthLab uses a modular architecture for data synthesis and an interactive interface for easy customization, enabling flexible and adaptable usage.

Result: User studies show SynthLab is accessible and usable by diverse users, including those without technical expertise.

Conclusion: SynthLab provides a scalable, user-friendly solution for generating high-quality datasets, reducing resource and privacy barriers.

Abstract: The rapid advancement of AI and computer vision has significantly increased
the demand for high-quality annotated datasets, particularly for semantic
segmentation. However, creating such datasets is resource-intensive, requiring
substantial time, labor, and financial investment, and often raises privacy
concerns due to the use of real-world data. To mitigate these challenges, we
present SynthLab, consisting of a modular platform for visual data synthesis
and a user-friendly interface. The modular architecture of SynthLab enables
easy maintenance, scalability with centralized updates, and seamless
integration of new features. Each module handles distinct aspects of computer
vision tasks, enhancing flexibility and adaptability. Meanwhile, its
interactive, user-friendly interface allows users to quickly customize their
data pipelines through drag-and-drop actions. Extensive user studies involving
a diverse range of users across different ages, professions, and expertise
levels, have demonstrated flexible usage, and high accessibility of SynthLab,
enabling users without deep technical expertise to harness AI for real-world
applications.

</details>


### [295] [GeoCD: A Differential Local Approximation for Geodesic Chamfer Distance](https://arxiv.org/abs/2506.23478)
*Pedro Alonso,Tianrui Li,Chongshou Li*

Main category: cs.CV

TL;DR: GeoCD, a topology-aware geodesic distance metric, outperforms Chamfer Distance (CD) in 3D point cloud learning by better capturing intrinsic geometry.


<details>
  <summary>Details</summary>
Motivation: Chamfer Distance (CD) is limited by its reliance on Euclidean distances, which fail to capture the intrinsic geometry of 3D shapes.

Method: Proposed GeoCD, a topology-aware and differentiable geodesic distance approximation, and fine-tuned models trained with CD using GeoCD.

Result: GeoCD consistently improves reconstruction quality over CD, with significant gains observed after just one epoch of fine-tuning.

Conclusion: GeoCD is a superior metric for 3D point cloud learning, enhancing performance across architectures and datasets.

Abstract: Chamfer Distance (CD) is a widely adopted metric in 3D point cloud learning
due to its simplicity and efficiency. However, it suffers from a fundamental
limitation: it relies solely on Euclidean distances, which often fail to
capture the intrinsic geometry of 3D shapes. To address this limitation, we
propose GeoCD, a topology-aware and fully differentiable approximation of
geodesic distance designed to serve as a metric for 3D point cloud learning.
Our experiments show that GeoCD consistently improves reconstruction quality
over standard CD across various architectures and datasets. We demonstrate this
by fine-tuning several models, initially trained with standard CD, using GeoCD.
Remarkably, fine-tuning for a single epoch with GeoCD yields significant gains
across multiple evaluation metrics.

</details>


### [296] [Instant GaussianImage: A Generalizable and Self-Adaptive Image Representation via 2D Gaussian Splatting](https://arxiv.org/abs/2506.23479)
*Zhaojie Zeng,Yuesong Wang,Chao Yang,Tao Guan,Lili Ju*

Main category: cs.CV

TL;DR: Proposes a faster, adaptive 2D Gaussian Splatting method for image representation, reducing training time significantly while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Address the high GPU resource demands and slow training of existing methods like GaussianImage, and improve adaptability to varying image complexity.

Method: Uses a network to generate a coarse Gaussian representation quickly, followed by minimal fine-tuning, and dynamically adjusts Gaussian points based on image complexity.

Result: Achieves comparable or better rendering quality than GaussianImage with up to 10x faster training and adaptive Gaussian point allocation.

Conclusion: The method offers a practical, efficient solution for high-quality image representation with reduced computational costs.

Abstract: Implicit Neural Representation (INR) has demonstrated remarkable advances in
the field of image representation but demands substantial GPU resources.
GaussianImage recently pioneered the use of Gaussian Splatting to mitigate this
cost, however, the slow training process limits its practicality, and the fixed
number of Gaussians per image limits its adaptability to varying information
entropy. To address these issues, we propose in this paper a generalizable and
self-adaptive image representation framework based on 2D Gaussian Splatting.
Our method employs a network to quickly generate a coarse Gaussian
representation, followed by minimal fine-tuning steps, achieving comparable
rendering quality of GaussianImage while significantly reducing training time.
Moreover, our approach dynamically adjusts the number of Gaussian points based
on image complexity to further enhance flexibility and efficiency in practice.
Experiments on DIV2K and Kodak datasets show that our method matches or exceeds
GaussianImage's rendering performance with far fewer iterations and shorter
training times. Specifically, our method reduces the training time by up to one
order of magnitude while achieving superior rendering performance with the same
number of Gaussians.

</details>


### [297] [Evaluation of Geolocation Capabilities of Multimodal Large Language Models and Analysis of Associated Privacy Risks](https://arxiv.org/abs/2506.23481)
*Xian Zhang,Xiang Cheng*

Main category: cs.CV

TL;DR: MLLMs can geolocate images with 49% accuracy within 1km, raising privacy concerns. The study reviews techniques, evaluates performance, and suggests countermeasures.


<details>
  <summary>Details</summary>
Motivation: To analyze the privacy risks posed by MLLMs' ability to infer geographic locations from images, focusing on street views.

Method: Systematic review of geolocation techniques and evaluation of state-of-the-art visual reasoning models on street view imagery.

Result: Advanced models achieve 49% accuracy within 1km, highlighting their ability to extract geographic cues.

Conclusion: Identifies key visual elements for geolocation and discusses privacy implications and countermeasures.

Abstract: Objectives: The rapid advancement of Multimodal Large Language Models (MLLMs)
has significantly enhanced their reasoning capabilities, enabling a wide range
of intelligent applications. However, these advancements also raise critical
concerns regarding privacy and ethics. MLLMs are now capable of inferring the
geographic location of images -- such as those shared on social media or
captured from street views -- based solely on visual content, thereby posing
serious risks of privacy invasion, including doxxing, surveillance, and other
security threats.
  Methods: This study provides a comprehensive analysis of existing geolocation
techniques based on MLLMs. It systematically reviews relevant litera-ture and
evaluates the performance of state-of-the-art visual reasoning models on
geolocation tasks, particularly in identifying the origins of street view
imagery.
  Results: Empirical evaluation reveals that the most advanced visual large
models can successfully localize the origin of street-level imagery with up to
$49\%$ accuracy within a 1-kilometer radius. This performance underscores the
models' powerful capacity to extract and utilize fine-grained geographic cues
from visual data.
  Conclusions: Building on these findings, the study identifies key visual
elements that contribute to suc-cessful geolocation, such as text,
architectural styles, and environmental features. Furthermore, it discusses the
potential privacy implications associated with MLLM-enabled geolocation and
discuss several technical and policy-based coun-termeasures to mitigate
associated risks. Our code and dataset are available at
https://github.com/zxyl1003/MLLM-Geolocation-Evaluation.

</details>


### [298] [MTADiffusion: Mask Text Alignment Diffusion Model for Object Inpainting](https://arxiv.org/abs/2506.23482)
*Jun Huang,Ting Liu,Yihang Wu,Xiaochao Qu,Luoqi Liu,Xiaolin Hu*

Main category: cs.CV

TL;DR: MTADiffusion is a Mask-Text Alignment diffusion model for object inpainting, addressing issues like semantic misalignment, structural distortion, and style inconsistency. It introduces MTAPipeline for mask annotation, a large MTADataset, multi-task training, and a style-consistency loss, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing inpainting methods often suffer from semantic misalignment, structural distortion, and style inconsistency, limiting their effectiveness.

Method: Proposes MTADiffusion with MTAPipeline for mask annotation, a multi-task training strategy (inpainting and edge prediction), and a style-consistency loss using VGG and Gram matrix.

Result: Achieves state-of-the-art performance on BrushBench and EditBench benchmarks.

Conclusion: MTADiffusion effectively addresses key challenges in inpainting, offering improved semantic alignment, structural stability, and style consistency.

Abstract: Advancements in generative models have enabled image inpainting models to
generate content within specific regions of an image based on provided prompts
and masks. However, existing inpainting methods often suffer from problems such
as semantic misalignment, structural distortion, and style inconsistency. In
this work, we present MTADiffusion, a Mask-Text Alignment diffusion model
designed for object inpainting. To enhance the semantic capabilities of the
inpainting model, we introduce MTAPipeline, an automatic solution for
annotating masks with detailed descriptions. Based on the MTAPipeline, we
construct a new MTADataset comprising 5 million images and 25 million mask-text
pairs. Furthermore, we propose a multi-task training strategy that integrates
both inpainting and edge prediction tasks to improve structural stability. To
promote style consistency, we present a novel inpainting style-consistency loss
using a pre-trained VGG network and the Gram matrix. Comprehensive evaluations
on BrushBench and EditBench demonstrate that MTADiffusion achieves
state-of-the-art performance compared to other methods.

</details>


### [299] [When Test-Time Adaptation Meets Self-Supervised Models](https://arxiv.org/abs/2506.23529)
*Jisu Han,Jihee Park,Dongyoon Han,Wonjun Hwang*

Main category: cs.CV

TL;DR: The paper explores test-time adaptation (TTA) for self-supervised learning (SSL) models without relying on source pretraining, proposing a collaborative framework to enhance performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing TTA methods when applied to SSL models with low source accuracy, enabling continuous improvement without source pretraining.

Method: Introduces a self-supervised TTA protocol and a collaborative learning framework combining SSL and TTA, using contrastive learning and knowledge distillation for refinement.

Result: Validated on SSL models (DINO, MoCo, iBOT) across TTA benchmarks, achieving competitive performance without source pretraining.

Conclusion: The proposed framework effectively enhances SSL models via TTA, demonstrating practical applicability without dependency on source pretraining.

Abstract: Training on test-time data enables deep learning models to adapt to dynamic
environmental changes, enhancing their practical applicability. Online
adaptation from source to target domains is promising but it remains highly
reliant on the performance of source pretrained model. In this paper, we
investigate whether test-time adaptation (TTA) methods can continuously improve
models trained via self-supervised learning (SSL) without relying on source
pretraining. We introduce a self-supervised TTA protocol after observing that
existing TTA approaches struggle when directly applied to self-supervised
models with low accuracy on the source domain. Furthermore, we propose a
collaborative learning framework that integrates SSL and TTA models, leveraging
contrastive learning and knowledge distillation for stepwise representation
refinement. We validate our method on diverse self-supervised models, including
DINO, MoCo, and iBOT, across TTA benchmarks. Extensive experiments validate the
effectiveness of our approach in SSL, showing that it achieves competitive
performance even without source pretraining.

</details>


### [300] [Qwen-GUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding](https://arxiv.org/abs/2506.23491)
*ZongHan Hsieh,Tzer-Jen Wei*

Main category: cs.CV

TL;DR: Qwen-GUI-3B is a lightweight Vision-Language Model for GUI grounding, achieving competitive performance with larger models while being trainable on a single GPU. Key innovations include cross-platform dataset, two-stage fine-tuning, and data curation strategies.


<details>
  <summary>Details</summary>
Motivation: Address the impracticality of large-scale VLMs for consumer-grade hardware by developing a lightweight model that maintains strong grounding accuracy.

Method: Combines a diverse dataset, two-stage fine-tuning (cross-platform training followed by specialized fine-tuning), and data redundancy reduction.

Result: Achieves 84.9% on ScreenSpot and 86.4% on ScreenSpot-v2, surpassing prior models under 4B parameters.

Conclusion: Qwen-GUI-3B demonstrates that lightweight models can achieve high accuracy with innovative training and data strategies, making GUI grounding more accessible.

Abstract: This paper introduces Qwen-GUI-3B, a lightweight Vision-Language Model (VLM)
specifically designed for Graphical User Interface grounding tasks, achieving
performance competitive with significantly larger models. Unlike large-scale
VLMs (>7B parameters) that are computationally intensive and impractical for
consumer-grade hardware, Qwen-GUI-3B delivers strong grounding accuracy while
being fully trainable on a single GPU (RTX 4090). The model incorporates
several key innovations: (i) combine cross-platform, multi-resolution dataset
of 24K examples from diverse sources including mobile, desktop, and web GUI
screenshots to effectively address data scarcity in high-resolution desktop
environments; (ii) a two-stage fine-tuning strategy, where initial
cross-platform training establishes robust GUI understanding, followed by
specialized fine-tuning on high-resolution data to significantly enhance model
adaptability; and (iii) data curation and redundancy reduction strategies,
demonstrating that randomly sampling a smaller subset with reduced redundancy
achieves performance comparable to larger datasets, emphasizing data diversity
over sheer volume. Empirical evaluation on standard GUI grounding
benchmarks-including ScreenSpot, ScreenSpot-v2, and the challenging
ScreenSpot-Pro, highlights Qwen-GUI-3B's exceptional accuracy, achieving 84.9%
on ScreenSpot and 86.4% on ScreenSpot-v2, surpassing prior models under 4B
parameters. Ablation studies validate the critical role of balanced sampling
and two-stage fine-tuning in enhancing robustness, particularly in
high-resolution desktop scenarios. The Qwen-GUI-3B is available at:
https://github.com/Han1018/Qwen-GUI-3B

</details>


### [301] [GViT: Representing Images as Gaussians for Visual Recognition](https://arxiv.org/abs/2506.23532)
*Jefferson Hernandez,Ruozhen He,Guha Balakrishnan,Alexander C. Berg,Vicente Ordonez*

Main category: cs.CV

TL;DR: GVIT replaces pixel/patch grids with learnable 2D Gaussians for image classification, achieving competitive performance with ViT-B on Imagenet-1k.


<details>
  <summary>Details</summary>
Motivation: To explore alternative input representations beyond traditional pixel or patch grids for vision tasks.

Method: Encodes images as optimized 2D Gaussians, jointly trained with a ViT classifier using gradients for guidance and a differentiable renderer for reconstruction.

Result: Achieves 76.9% top-1 accuracy on Imagenet-1k with ViT-B, comparable to patch-based ViT.

Conclusion: GVIT demonstrates the viability of Gaussian-based representations for efficient and effective image classification.

Abstract: We introduce GVIT, a classification framework that abandons conventional
pixel or patch grid input representations in favor of a compact set of
learnable 2D Gaussians. Each image is encoded as a few hundred Gaussians whose
positions, scales, orientations, colors, and opacities are optimized jointly
with a ViT classifier trained on top of these representations. We reuse the
classifier gradients as constructive guidance, steering the Gaussians toward
class-salient regions while a differentiable renderer optimizes an image
reconstruction loss. We demonstrate that by 2D Gaussian input representations
coupled with our GVIT guidance, using a relatively standard ViT architecture,
closely matches the performance of a traditional patch-based ViT, reaching a
76.9% top-1 accuracy on Imagenet-1k using a ViT-B architecture.

</details>


### [302] [LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text Matching](https://arxiv.org/abs/2506.23502)
*Mengxiao Tian,Xinxiao Wu,Shuo Yang*

Main category: cs.CV

TL;DR: The paper introduces an LLM-enhanced action-aware multi-modal prompt-tuning method to improve CLIP's fine-grained action-level understanding, addressing its limitations in object attributes and spatial relationships.


<details>
  <summary>Details</summary>
Motivation: CLIP lacks fine-grained understanding of actions and object states, which are crucial for detailed image-text matching.

Method: The method uses LLM-generated action-related knowledge, designing action triplet and state prompts, and an adaptive interaction module for visual feature aggregation.

Result: Experiments on benchmark datasets show the method's effectiveness in improving action-aware visual representations.

Conclusion: The proposed method successfully enhances CLIP's action-level understanding, demonstrating significant performance improvements.

Abstract: Driven by large-scale contrastive vision-language pre-trained models such as
CLIP, recent advancements in the image-text matching task have achieved
remarkable success in representation learning. Due to image-level
visual-language alignment, CLIP falls short in understanding fine-grained
details such as object attributes and spatial relationships between objects.
Recent efforts have attempted to compel CLIP to acquire structured visual
representations by introducing prompt learning to achieve object-level
alignment. While achieving promising results, they still lack the capability to
perceive actions, which are crucial for describing the states or relationships
between objects. Therefore, we propose to endow CLIP with fine-grained
action-level understanding by introducing an LLM-enhanced action-aware
multi-modal prompt-tuning method, incorporating the action-related external
knowledge generated by large language models (LLMs). Specifically, we design an
action triplet prompt and an action state prompt to exploit compositional
semantic knowledge and state-related causal knowledge implicitly stored in
LLMs. Subsequently, we propose an adaptive interaction module to aggregate
attentive visual features conditioned on action-aware prompted knowledge for
establishing discriminative and action-aware visual representations, which
further improves the performance. Comprehensive experimental results on two
benchmark datasets demonstrate the effectiveness of our method.

</details>


### [303] [Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound](https://arxiv.org/abs/2506.23538)
*Yuhao Huang,Yueyue Xu,Haoran Dou,Jiaxiao Deng,Xin Yang,Hongyu Zheng,Dong Ni*

Main category: cs.CV

TL;DR: Proposes an intelligent system for automated plane localization and CUA diagnosis using 3D ultrasound, combining denoising diffusion, reinforcement learning, and text-driven uncertainty modeling.


<details>
  <summary>Details</summary>
Motivation: CUAs cause infertility and pregnancy complications; 3D US improves diagnosis accuracy over 2D US by visualizing uterine morphology.

Method: Uses a denoising diffusion model with local/global guidance, reinforcement learning for key slice extraction, and text-driven uncertainty modeling for classification adjustment.

Result: Effective plane localization and CUA diagnosis demonstrated on a large 3D uterine US dataset.

Conclusion: The system enhances CUA diagnosis accuracy and efficiency, with code publicly available.

Abstract: Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage,
preterm birth, and an increased risk of pregnancy complications. Compared to
traditional 2D ultrasound (US), 3D US can reconstruct the coronal plane,
providing a clear visualization of the uterine morphology for assessing CUAs
accurately. In this paper, we propose an intelligent system for simultaneous
automated plane localization and CUA diagnosis. Our highlights are: 1) we
develop a denoising diffusion model with local (plane) and global (volume/text)
guidance, using an adaptive weighting strategy to optimize attention allocation
to different conditions; 2) we introduce a reinforcement learning-based
framework with unsupervised rewards to extract the key slice summary from
redundant sequences, fully integrating information across multiple planes to
reduce learning difficulty; 3) we provide text-driven uncertainty modeling for
coarse prediction, and leverage it to adjust the classification probability for
overall performance improvement. Extensive experiments on a large 3D uterine US
dataset show the efficacy of our method, in terms of plane localization and CUA
diagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.

</details>


### [304] [Improve Underwater Object Detection through YOLOv12 Architecture and Physics-informed Augmentation](https://arxiv.org/abs/2506.23505)
*Tinh Nguyen*

Main category: cs.CV

TL;DR: The paper introduces YOLOv12, integrating physics-informed augmentation techniques for underwater object detection, achieving high accuracy and real-time performance in challenging conditions.


<details>
  <summary>Details</summary>
Motivation: Underwater object detection is hindered by poor visibility, occlusion, and turbidity, requiring solutions that balance accuracy and computational efficiency.

Method: The study combines YOLOv12 with Residual ELAN blocks and Area Attention, along with domain-specific augmentations like turbulence blurring and spectral HSV transformations.

Result: YOLOv12 achieves 98.30% mAP at 142 FPS, improving occlusion robustness by 18.9%, small-object recall by 22.4%, and detection precision by up to 7.94%.

Conclusion: The work provides a precise, efficient solution for underwater robotics and conservation, validated by ablation studies.

Abstract: Underwater object detection is crucial for autonomous navigation,
environmental monitoring, and marine exploration, but it is severely hampered
by light attenuation, turbidity, and occlusion. Current methods balance
accuracy and computational efficiency, but they have trouble deploying in
real-time under low visibility conditions. Through the integration of
physics-informed augmentation techniques with the YOLOv12 architecture, this
study advances underwater detection. With Residual ELAN blocks to preserve
structural features in turbid waters and Area Attention to maintain large
receptive fields for occluded objects while reducing computational complexity.
Underwater optical properties are addressed by domain-specific augmentations
such as turbulence adaptive blurring, biologically grounded occlusion
simulation, and spectral HSV transformations for color distortion. Extensive
tests on four difficult datasets show state-of-the-art performance, with
Brackish data registering 98.30% mAP at 142 FPS. YOLOv12 improves occlusion
robustness by 18.9%, small-object recall by 22.4%, and detection precision by
up to 7.94% compared to previous models. The crucial role of augmentation
strategy is validated by ablation studies. This work offers a precise and
effective solution for conservation and underwater robotics applications.

</details>


### [305] [ViewPoint: Panoramic Video Generation with Pretrained Diffusion Models](https://arxiv.org/abs/2506.23513)
*Zixun Fang,Kai Zhu,Zhiheng Liu,Yu Liu,Wei Zhai,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: A novel framework for generating high-quality 360-degree videos by leveraging pretrained perspective video models and a new panorama representation called ViewPoint map.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with synthesizing panoramic videos due to the modality gap between panoramic and perspective data, which limits the quality of outputs.

Method: Introduces a ViewPoint map for panorama representation and a Pano-Perspective attention mechanism to utilize pretrained perspective priors and capture panoramic spatial correlations.

Result: The method produces highly dynamic and spatially consistent panoramic videos, outperforming previous approaches.

Conclusion: The proposed framework effectively bridges the modality gap, achieving state-of-the-art performance in panoramic video generation.

Abstract: Panoramic video generation aims to synthesize 360-degree immersive videos,
holding significant importance in the fields of VR, world models, and spatial
intelligence. Existing works fail to synthesize high-quality panoramic videos
due to the inherent modality gap between panoramic data and perspective data,
which constitutes the majority of the training data for modern diffusion
models. In this paper, we propose a novel framework utilizing pretrained
perspective video models for generating panoramic videos. Specifically, we
design a novel panorama representation named ViewPoint map, which possesses
global spatial continuity and fine-grained visual details simultaneously. With
our proposed Pano-Perspective attention mechanism, the model benefits from
pretrained perspective priors and captures the panoramic spatial correlations
of the ViewPoint map effectively. Extensive experiments demonstrate that our
method can synthesize highly dynamic and spatially consistent panoramic videos,
achieving state-of-the-art performance and surpassing previous methods.

</details>


### [306] [Metadata, Wavelet, and Time Aware Diffusion Models for Satellite Image Super Resolution](https://arxiv.org/abs/2506.23566)
*Luigi Sigillo,Renato Giamba,Danilo Comminiello*

Main category: cs.CV

TL;DR: MWT-Diff is a framework for satellite image super-resolution using latent diffusion models and wavelet transforms, outperforming recent methods in perceptual quality.


<details>
  <summary>Details</summary>
Motivation: High-resolution satellite imagery is limited by sensor constraints and high costs, hindering applications like environmental monitoring and disaster response.

Method: Combines latent diffusion models with wavelet transforms, using a novel MWT-Encoder to capture metadata, multi-scale frequency, and temporal relationships for hierarchical reconstruction.

Result: MWT-Diff outperforms recent approaches in perceptual quality metrics (FID, LPIPS) across multiple datasets.

Conclusion: The framework effectively reconstructs high-resolution satellite imagery while preserving critical spatial details, addressing key limitations in remote sensing.

Abstract: The acquisition of high-resolution satellite imagery is often constrained by
the spatial and temporal limitations of satellite sensors, as well as the high
costs associated with frequent observations. These challenges hinder
applications such as environmental monitoring, disaster response, and
agricultural management, which require fine-grained and high-resolution data.
In this paper, we propose MWT-Diff, an innovative framework for satellite image
super-resolution (SR) that combines latent diffusion models with wavelet
transforms to address these challenges. At the core of the framework is a novel
metadata-, wavelet-, and time-aware encoder (MWT-Encoder), which generates
embeddings that capture metadata attributes, multi-scale frequency information,
and temporal relationships. The embedded feature representations steer the
hierarchical diffusion dynamics, through which the model progressively
reconstructs high-resolution satellite imagery from low-resolution inputs. This
process preserves critical spatial characteristics including textural patterns,
boundary discontinuities, and high-frequency spectral components essential for
detailed remote sensing analysis. The comparative analysis of MWT-Diff across
multiple datasets demonstrated favorable performance compared to recent
approaches, as measured by standard perceptual quality metrics including FID
and LPIPS.

</details>


### [307] [WAVE: Warp-Based View Guidance for Consistent Novel View Synthesis Using a Single Image](https://arxiv.org/abs/2506.23518)
*Jiwoo Park,Tae Eun Choi,Youngjun Jun,Seong Jae Hwang*

Main category: cs.CV

TL;DR: A method to improve view consistency in novel view synthesis using diffusion models without extra modules, enhancing attention and noise handling.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of maintaining structural coherence (view consistency) in novel view synthesis from a single image, especially with diffusion models.

Method: Uses a training-free approach with adaptive attention manipulation and noise reinitialization, leveraging view-guided warping for consistency.

Result: Improves view consistency across diffusion models, validated by a comprehensive metric framework.

Conclusion: The method is broadly applicable and effective for enhancing view consistency in novel view synthesis.

Abstract: Generating high-quality novel views of a scene from a single image requires
maintaining structural coherence across different views, referred to as view
consistency. While diffusion models have driven advancements in novel view
synthesis, they still struggle to preserve spatial continuity across views.
Diffusion models have been combined with 3D models to address the issue, but
such approaches lack efficiency due to their complex multi-step pipelines. This
paper proposes a novel view-consistent image generation method which utilizes
diffusion models without additional modules. Our key idea is to enhance
diffusion models with a training-free method that enables adaptive attention
manipulation and noise reinitialization by leveraging view-guided warping to
ensure view consistency. Through our comprehensive metric framework suitable
for novel-view datasets, we show that our method improves view consistency
across various diffusion models, demonstrating its broader applicability.

</details>


### [308] [PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection](https://arxiv.org/abs/2506.23581)
*Xiao Li,Yiming Zhu,Yifan Huang,Wei Zhang,Yingzhe He,Jie Shi,Xiaolin Hu*

Main category: cs.CV

TL;DR: The paper introduces PBCAT, a unified adversarial training method to defend against various physically realizable attacks on object detectors, improving robustness significantly.


<details>
  <summary>Details</summary>
Motivation: Object detectors are vulnerable to physically realizable attacks like adversarial patches and textures, but existing adversarial training methods are limited in scope.

Method: Proposes PBCAT, combining small-area gradient-guided adversarial patches and imperceptible global perturbations for training.

Result: PBCAT improves detection accuracy by 29.7% under adversarial texture attacks and outperforms state-of-the-art defenses.

Conclusion: PBCAT is a robust and versatile defense against diverse physically realizable attacks on object detectors.

Abstract: Object detection plays a crucial role in many security-sensitive
applications. However, several recent studies have shown that object detectors
can be easily fooled by physically realizable attacks, \eg, adversarial patches
and recent adversarial textures, which pose realistic and urgent threats.
Adversarial Training (AT) has been recognized as the most effective defense
against adversarial attacks. While AT has been extensively studied in the
$l_\infty$ attack settings on classification models, AT against physically
realizable attacks on object detectors has received limited exploration. Early
attempts are only performed to defend against adversarial patches, leaving AT
against a wider range of physically realizable attacks under-explored. In this
work, we consider defending against various physically realizable attacks with
a unified AT method. We propose PBCAT, a novel Patch-Based Composite
Adversarial Training strategy. PBCAT optimizes the model by incorporating the
combination of small-area gradient-guided adversarial patches and imperceptible
global adversarial perturbations covering the entire image. With these designs,
PBCAT has the potential to defend against not only adversarial patches but also
unseen physically realizable attacks such as adversarial textures. Extensive
experiments in multiple settings demonstrated that PBCAT significantly improved
robustness against various physically realizable attacks over state-of-the-art
defense methods. Notably, it improved the detection accuracy by 29.7\% over
previous defense methods under one recent adversarial texture attack.

</details>


### [309] [From Sight to Insight: Unleashing Eye-Tracking in Weakly Supervised Video Salient Object Detection](https://arxiv.org/abs/2506.23519)
*Qi Qin,Runmin Cong,Gen Zhan,Yiting Liao,Sam Kwong*

Main category: cs.CV

TL;DR: The paper introduces fixation information to improve video salient object detection (VSOD) under weak supervision, using a Position and Semantic Embedding (PSE) module and a Semantics and Locality Query (SLQ) Competitor, along with an Intra-Inter Mixed Contrastive (IIMC) model for better spatiotemporal feature modeling.


<details>
  <summary>Details</summary>
Motivation: Eye-tracking annotations are more accessible and align with human visual patterns, making them useful for weakly supervised VSOD.

Method: Proposes PSE for location and semantic guidance, SLQ for feature selection, and IIMC for contrastive learning.

Result: Outperforms competitors on five VSOD benchmarks across multiple metrics.

Conclusion: The approach effectively leverages fixation data and weak supervision to enhance VSOD performance.

Abstract: The eye-tracking video saliency prediction (VSP) task and video salient
object detection (VSOD) task both focus on the most attractive objects in video
and show the result in the form of predictive heatmaps and pixel-level saliency
masks, respectively. In practical applications, eye tracker annotations are
more readily obtainable and align closely with the authentic visual patterns of
human eyes. Therefore, this paper aims to introduce fixation information to
assist the detection of video salient objects under weak supervision. On the
one hand, we ponder how to better explore and utilize the information provided
by fixation, and then propose a Position and Semantic Embedding (PSE) module to
provide location and semantic guidance during the feature learning process. On
the other hand, we achieve spatiotemporal feature modeling under weak
supervision from the aspects of feature selection and feature contrast. A
Semantics and Locality Query (SLQ) Competitor with semantic and locality
constraints is designed to effectively select the most matching and accurate
object query for spatiotemporal modeling. In addition, an Intra-Inter Mixed
Contrastive (IIMC) model improves the spatiotemporal modeling capabilities
under weak supervision by forming an intra-video and inter-video contrastive
learning paradigm. Experimental results on five popular VSOD benchmarks
indicate that our model outperforms other competitors on various evaluation
metrics.

</details>


### [310] [Brain Tumor Detection through Thermal Imaging and MobileNET](https://arxiv.org/abs/2506.23627)
*Roham Maiti,Debasmita Bhoumik*

Main category: cs.CV

TL;DR: The paper proposes using MobileNET for efficient brain tumor detection from MRI scans, addressing limitations of traditional methods and classical ML models, achieving 98.5% accuracy.


<details>
  <summary>Details</summary>
Motivation: Brain tumors pose significant health risks, and traditional detection methods like biopsies and MRI/CT scans are costly and require expertise. Classical ML models for detection are computationally demanding and inefficient.

Method: The research employs the MobileNET model for tumor detection, focusing on reduced computational resources and faster processing while maintaining accuracy through image processing techniques.

Result: The proposed method achieved an average accuracy of 98.5% in detecting brain tumors from MRI scans.

Conclusion: The MobileNET-based approach offers an efficient, accurate, and resource-friendly solution for brain tumor detection, improving accessibility and speed compared to traditional and classical ML methods.

Abstract: Brain plays a crucial role in regulating body functions and cognitive
processes, with brain tumors posing significant risks to human health. Precise
and prompt detection is a key factor in proper treatment and better patient
outcomes. Traditional methods for detecting brain tumors, that include
biopsies, MRI, and CT scans often face challenges due to their high costs and
the need for specialized medical expertise. Recent developments in machine
learning (ML) and deep learning (DL) has exhibited strong capabilities in
automating the identification and categorization of brain tumors from medical
images, especially MRI scans. However, these classical ML models have
limitations, such as high computational demands, the need for large datasets,
and long training times, which hinder their accessibility and efficiency. Our
research uses MobileNET model for efficient detection of these tumors. The
novelty of this project lies in building an accurate tumor detection model
which use less computing re-sources and runs in less time followed by efficient
decision making through the use of image processing technique for accurate
results. The suggested method attained an average accuracy of 98.5%.

</details>


### [311] [Lightweight Temporal Transformer Decomposition for Federated Autonomous Driving](https://arxiv.org/abs/2506.23523)
*Tuong Do,Binh X. Nguyen,Quang D. Tran,Erman Tjiputra,Te-Chuan Chiu,Anh Nguyen*

Main category: cs.CV

TL;DR: Proposes a lightweight temporal transformer decomposition method for autonomous driving, improving efficiency and performance by breaking down large attention maps.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of single-image inputs and resource-intensive fusion networks in autonomous driving systems.

Method: Lightweight temporal transformer decomposition processes sequential image frames and steering data by decomposing large attention maps into smaller matrices.

Result: Outperforms recent methods on three datasets and achieves real-time performance, confirmed by real robot experiments.

Conclusion: The method effectively enhances autonomous driving performance while being efficient and practical for real-time applications.

Abstract: Traditional vision-based autonomous driving systems often face difficulties
in navigating complex environments when relying solely on single-image inputs.
To overcome this limitation, incorporating temporal data such as past image
frames or steering sequences, has proven effective in enhancing robustness and
adaptability in challenging scenarios. While previous high-performance methods
exist, they often rely on resource-intensive fusion networks, making them
impractical for training and unsuitable for federated learning. To address
these challenges, we propose lightweight temporal transformer decomposition, a
method that processes sequential image frames and temporal steering data by
breaking down large attention maps into smaller matrices. This approach reduces
model complexity, enabling efficient weight updates for convergence and
real-time predictions while leveraging temporal information to enhance
autonomous driving performance. Intensive experiments on three datasets
demonstrate that our method outperforms recent approaches by a clear margin
while achieving real-time performance. Additionally, real robot experiments
further confirm the effectiveness of our method.

</details>


### [312] [On the Domain Robustness of Contrastive Vision-Language Models](https://arxiv.org/abs/2506.23663)
*Mario Koddenbrock,Rudolf Hoffmann,David Brodmann,Erik Rodner*

Main category: cs.CV

TL;DR: Deepbench is a framework to evaluate domain-specific robustness of vision-language models (VLMs) using LLM-generated corruptions, revealing variability in performance across domains.


<details>
  <summary>Details</summary>
Motivation: Practitioners use pretrained foundation models despite limited transparency, but their performance drops under domain shifts. Deepbench addresses this gap.

Method: Deepbench uses an LLM to generate realistic, domain-specific image corruptions for evaluating VLMs without labeled data.

Result: Evaluation shows significant robustness variability across six domains, emphasizing the need for domain-aware assessment.

Conclusion: Deepbench, released as open-source, supports domain-aware robustness research for VLMs.

Abstract: In real-world vision-language applications, practitioners increasingly rely
on large, pretrained foundation models rather than custom-built solutions,
despite limited transparency regarding their training data and processes. While
these models achieve impressive performance on general benchmarks, their
effectiveness can decline notably under specialized domain shifts, such as
unique imaging conditions or environmental variations. In this work, we
introduce Deepbench, a framework designed to assess domain-specific robustness
of vision-language models (VLMs). Deepbench leverages a large language model
(LLM) to generate realistic, context-aware image corruptions tailored to
specific deployment domains without requiring labeled data. We evaluate a range
of contrastive vision-language architectures and architectural variants across
six real-world domains and observe substantial variability in robustness,
highlighting the need for targeted, domain-aware evaluation. Deepbench is
released as open-source software to support further research into domain-aware
robustness assessment.

</details>


### [313] [Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking](https://arxiv.org/abs/2506.23783)
*Shiao Wang,Ju Huang,Qingchuan Ma,Jinfeng Gao,Chunyi Xu,Xiao Wang,Lan Chen,Bo Jiang*

Main category: cs.CV

TL;DR: Proposes Mamba-FETrack V2, an efficient RGB-Event object tracking framework using Vision Mamba for low-complexity, high-performance cross-modal fusion.


<details>
  <summary>Details</summary>
Motivation: Addresses the computational overhead and limited cross-modal interaction in existing RGB-Event tracking methods reliant on high-complexity Vision Transformers.

Method: Uses a lightweight Prompt Generator and Vision Mamba-based FEMamba backbone for dynamic prompt-guided feature extraction, interaction, and fusion.

Result: Demonstrates superior performance on benchmarks like COESOT, FE108, and FELT V2.

Conclusion: Mamba-FETrack V2 offers an efficient, high-performance solution for RGB-Event object tracking.

Abstract: Combining traditional RGB cameras with bio-inspired event cameras for robust
object tracking has garnered increasing attention in recent years. However,
most existing multimodal tracking algorithms depend heavily on high-complexity
Vision Transformer architectures for feature extraction and fusion across
modalities. This not only leads to substantial computational overhead but also
limits the effectiveness of cross-modal interactions. In this paper, we propose
an efficient RGB-Event object tracking framework based on the linear-complexity
Vision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a
lightweight Prompt Generator that utilizes embedded features from each
modality, together with a shared prompt pool, to dynamically generate
modality-specific learnable prompt vectors. These prompts, along with the
modality-specific embedded features, are then fed into a Vision Mamba-based
FEMamba backbone, which facilitates prompt-guided feature extraction,
cross-modal interaction, and fusion in a unified manner. Finally, the fused
representations are passed to the tracking head for accurate target
localization. Extensive experimental evaluations on multiple RGB-Event tracking
benchmarks, including short-term COESOT dataset and long-term datasets, i.e.,
FE108 and FELT V2, demonstrate the superior performance and efficiency of the
proposed tracking framework. The source code and pre-trained models will be
released on https://github.com/Event-AHU/Mamba_FETrack

</details>


### [314] [Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric Attention](https://arxiv.org/abs/2506.23542)
*Weida Wang,Changyong He,Jin Zeng,Di Qiu*

Main category: cs.CV

TL;DR: A novel ToF depth denoising network using motion-invariant graph fusion improves temporal stability and spatial sharpness by leveraging cross-frame geometric attention and a maximum a posteriori formulation.


<details>
  <summary>Details</summary>
Motivation: Depth images from ToF sensors are noisy, and existing methods either ignore multi-frame depth variations or lack temporal consistency, leading to poor results.

Method: Proposes a network using motion-invariant graph fusion, geometric attention, and a maximum a posteriori problem for denoising, unrolled into iterative filters.

Result: Achieves state-of-the-art performance on synthetic DVToF and real Kinectv2 datasets, with robust generalization.

Conclusion: The method effectively denoises ToF depth images while maintaining temporal and spatial quality, with interpretable and adaptive learning.

Abstract: Depth images captured by Time-of-Flight (ToF) sensors are prone to noise,
requiring denoising for reliable downstream applications. Previous works either
focus on single-frame processing, or perform multi-frame processing without
considering depth variations at corresponding pixels across frames, leading to
undesirable temporal inconsistency and spatial ambiguity. In this paper, we
propose a novel ToF depth denoising network leveraging motion-invariant graph
fusion to simultaneously enhance temporal stability and spatial sharpness.
Specifically, despite depth shifts across frames, graph structures exhibit
temporal self-similarity, enabling cross-frame geometric attention for graph
fusion. Then, by incorporating an image smoothness prior on the fused graph and
data fidelity term derived from ToF noise distribution, we formulate a maximum
a posterior problem for ToF denoising. Finally, the solution is unrolled into
iterative filters whose weights are adaptively learned from the graph-informed
geometric attention, producing a high-performance yet interpretable network.
Experimental results demonstrate that the proposed scheme achieves
state-of-the-art performance in terms of accuracy and consistency on synthetic
DVToF dataset and exhibits robust generalization on the real Kinectv2 dataset.
Source code will be released at
\href{https://github.com/davidweidawang/GIGA-ToF}{https://github.com/davidweidawang/GIGA-ToF}.

</details>


### [315] [Spurious-Aware Prototype Refinement for Reliable Out-of-Distribution Detection](https://arxiv.org/abs/2506.23881)
*Reihaneh Zohrabi,Hosein Hasani,Mahdieh Soleymani Baghshah,Anna Rohrbach,Marcus Rohrbach,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: SPROD is a prototype-based OOD detection method that mitigates spurious correlations, improving robustness without extra data or tuning.


<details>
  <summary>Details</summary>
Motivation: Existing OOD detection methods are vulnerable to spurious correlations, compromising reliability in real-world applications.

Method: SPROD refines class prototypes post-hoc to reduce bias from spurious features, applicable across diverse models and settings.

Result: SPROD outperforms existing methods, improving AUROC by 4.7% and FPR@95 by 9.3% on challenging datasets.

Conclusion: SPROD effectively addresses spurious correlation challenges in OOD detection, enhancing model reliability.

Abstract: Out-of-distribution (OOD) detection is crucial for ensuring the reliability
and safety of machine learning models in real-world applications, where they
frequently face data distributions unseen during training. Despite progress,
existing methods are often vulnerable to spurious correlations that mislead
models and compromise robustness. To address this, we propose SPROD, a novel
prototype-based OOD detection approach that explicitly addresses the challenge
posed by unknown spurious correlations. Our post-hoc method refines class
prototypes to mitigate bias from spurious features without additional data or
hyperparameter tuning, and is broadly applicable across diverse backbones and
OOD detection settings. We conduct a comprehensive spurious correlation OOD
detection benchmarking, comparing our method against existing approaches and
demonstrating its superior performance across challenging OOD datasets, such as
CelebA, Waterbirds, UrbanCars, Spurious Imagenet, and the newly introduced
Animals MetaCoCo. On average, SPROD improves AUROC by 4.7% and FPR@95 by 9.3%
over the second best.

</details>


### [316] [Pyramidal Patchification Flow for Visual Generation](https://arxiv.org/abs/2506.23543)
*Hui Li,Baoyou Chen,Liwei Zhang,Jiaye Li,Jingdong Wang,Siyu Zhu*

Main category: cs.CV

TL;DR: PPFlow introduces a Pyramidal Patchification Flow for Diffusion Transformers (DiTs), varying patch sizes by noise timestep to optimize computation cost and performance.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency and performance of DiTs by adapting patch sizes dynamically based on noise levels, avoiding the need for renoising tricks.

Method: Uses PPFlow with large patches for high noise and small patches for low noise, learning linear projections for each size and modifying Unpatchify. Operates on full latent representations.

Result: Achieves 1.6×-2.0× faster inference than SiT-B/2 with similar performance. Pretrained DiTs show even better results with minimal training time.

Conclusion: PPFlow effectively balances computation cost and performance, offering a scalable solution for DiTs.

Abstract: Diffusion transformers (DiTs) adopt Patchify, mapping patch representations
to token representations through linear projections, to adjust the number of
tokens input to DiT blocks and thus the computation cost. Instead of a single
patch size for all the timesteps, we introduce a Pyramidal Patchification Flow
(PPFlow) approach: Large patch sizes are used for high noise timesteps and
small patch sizes for low noise timesteps; Linear projections are learned for
each patch size; and Unpatchify is accordingly modified. Unlike Pyramidal Flow,
our approach operates over full latent representations other than pyramid
representations, and adopts the normal denoising process without requiring the
renoising trick. We demonstrate the effectiveness of our approach through two
training manners. Training from scratch achieves a $1.6\times$ ($2.0\times$)
inference speed over SiT-B/2 for 2-level (3-level) pyramid patchification with
slightly lower training FLOPs and similar image generation performance.
Training from pretrained normal DiTs achieves even better performance with
small training time. The code and checkpoint are at
https://github.com/fudan-generative-vision/PPFlow.

</details>


### [317] [Oneta: Multi-Style Image Enhancement Using Eigentransformation Functions](https://arxiv.org/abs/2506.23547)
*Jiwon Kim,Soohyun Hwang,Dong-O Kim,Changsu Han,Min Kyu Park,Chang-Su Kim*

Main category: cs.CV

TL;DR: Oneta is a novel multi-style image enhancement algorithm using two sequential operators (TF and CCM) and achieves high performance. It employs Y-Net and C-Net for parameter prediction and supports K styles via learnable tokens.


<details>
  <summary>Details</summary>
Motivation: To address the need for a versatile and high-performing multi-style image enhancement solution capable of handling diverse tasks like retouching, dehazing, and white balancing.

Method: Oneta uses intensity enhancement (TF) and color correction (CCM) sequentially. It introduces eigenTF for compact TF representation and employs Y-Net and C-Net for parameter prediction. K learnable tokens enable multi-style support.

Result: Oneta effectively performs six enhancement tasks across 30 datasets, demonstrating versatility and high performance.

Conclusion: Oneta is a simple yet powerful solution for multi-style image enhancement, achieving strong results across diverse tasks.

Abstract: The first algorithm, called Oneta, for a novel task of multi-style image
enhancement is proposed in this work. Oneta uses two point operators
sequentially: intensity enhancement with a transformation function (TF) and
color correction with a color correction matrix (CCM). This two-step
enhancement model, though simple, achieves a high performance upper bound.
Also, we introduce eigentransformation function (eigenTF) to represent TF
compactly. The Oneta network comprises Y-Net and C-Net to predict eigenTF and
CCM parameters, respectively. To support $K$ styles, Oneta employs $K$
learnable tokens. During training, each style token is learned using image
pairs from the corresponding dataset. In testing, Oneta selects one of the $K$
style tokens to enhance an image accordingly. Extensive experiments show that
the single Oneta network can effectively undertake six enhancement tasks --
retouching, image signal processing, low-light image enhancement, dehazing,
underwater image enhancement, and white balancing -- across 30 datasets.

</details>


### [318] [JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching](https://arxiv.org/abs/2506.23552)
*Mingi Kwon,Joonghyuk Shin,Jaeseok Jung,Jaesik Park,Youngjung Uh*

Main category: cs.CV

TL;DR: JAM-Flow is a unified framework for synthesizing facial motion and speech together, using flow matching and a Multi-Modal Diffusion Transformer (MM-DiT) architecture.


<details>
  <summary>Details</summary>
Motivation: Current generative models treat talking head synthesis and text-to-speech as separate tasks, missing their intrinsic link. JAM-Flow aims to bridge this gap.

Method: Uses flow matching and MM-DiT with Motion-DiT and Audio-DiT modules, coupled via selective joint attention layers and temporally aligned embeddings.

Result: Enables tasks like synchronized talking head generation from text and audio-driven animation in a single model.

Conclusion: JAM-Flow advances multi-modal generative modeling by unifying audio-visual synthesis.

Abstract: The intrinsic link between facial motion and speech is often overlooked in
generative modeling, where talking head synthesis and text-to-speech (TTS) are
typically addressed as separate tasks. This paper introduces JAM-Flow, a
unified framework to simultaneously synthesize and condition on both facial
motion and speech. Our approach leverages flow matching and a novel Multi-Modal
Diffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT
and Audio-DiT modules. These are coupled via selective joint attention layers
and incorporate key architectural choices, such as temporally aligned
positional embeddings and localized joint attention masking, to enable
effective cross-modal interaction while preserving modality-specific strengths.
Trained with an inpainting-style objective, JAM-Flow supports a wide array of
conditioning inputs-including text, reference audio, and reference
motion-facilitating tasks such as synchronized talking head generation from
text, audio-driven animation, and much more, within a single, coherent model.
JAM-Flow significantly advances multi-modal generative modeling by providing a
practical solution for holistic audio-visual synthesis. project page:
https://joonghyuk.com/jamflow-web

</details>


### [319] [LH2Face: Loss function for Hard High-quality Face](https://arxiv.org/abs/2506.23555)
*Fan Xie,Pan Cao*

Main category: cs.CV

TL;DR: The paper introduces LH2Face, a novel loss function for face recognition that addresses hard samples by incorporating adaptive margins and face quality awareness, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current face recognition systems using cosine similarity and softmax struggle with hard samples and lack consideration for face quality or recognition hardness, leading to overly uniform training strategies.

Method: The proposed LH2Face uses a vMF-based similarity measure, an adaptive margin function, proxy-based constraints, and a face reconstruction renderer to optimize performance.

Result: LH2Face achieves 49.39% accuracy on IJB-B, surpassing the second-best method by 2.37%.

Conclusion: LH2Face effectively improves face recognition performance on hard high-quality datasets by addressing key limitations of existing methods.

Abstract: In current practical face authentication systems, most face recognition (FR)
algorithms are based on cosine similarity with softmax classification. Despite
its reliable classification performance, this method struggles with hard
samples. A popular strategy to improve FR performance is incorporating angular
or cosine margins. However, it does not take face quality or recognition
hardness into account, simply increasing the margin value and thus causing an
overly uniform training strategy. To address this problem, a novel loss
function is proposed, named Loss function for Hard High-quality Face (LH2Face).
Firstly, a similarity measure based on the von Mises-Fisher (vMF) distribution
is stated, specifically focusing on the logarithm of the Probability Density
Function (PDF), which represents the distance between a probability
distribution and a vector. Then, an adaptive margin-based multi-classification
method using softmax, called the Uncertainty-Aware Margin Function, is
implemented in the article. Furthermore, proxy-based loss functions are used to
apply extra constraints between the proxy and sample to optimize their
representation space distribution. Finally, a renderer is constructed that
optimizes FR through face reconstruction and vice versa. Our LH2Face is
superior to similiar schemes on hard high-quality face datasets, achieving
49.39% accuracy on the IJB-B dataset, which surpasses the second-place method
by 2.37%.

</details>


### [320] [OcRFDet: Object-Centric Radiance Fields for Multi-View 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2506.23565)
*Mingqian Ji,Jian Yang,Shanshan Zhang*

Main category: cs.CV

TL;DR: The paper proposes Object-centric Radiance Fields (OcRF) to improve 3D object detection by focusing on foreground objects and avoiding background noise, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current methods for multi-view 3D object detection rely on implicit data-driven approaches, limiting performance. Radiance fields, successful in 3D reconstruction, are explored but degrade detection when used directly due to background interference.

Method: The authors introduce OcRF to model foreground objects, discarding background noise. They enhance 3D voxel features via rendering foreground objects and use opacity for Height-aware Opacity-based Attention (HOA) to improve 2D BEV features.

Result: OcRFDet achieves 57.2% mAP and 64.8% NDS on nuScenes test benchmark, outperforming prior methods.

Conclusion: Focusing on foreground objects with OcRF and HOA significantly improves 3D object detection performance.

Abstract: Current multi-view 3D object detection methods typically transfer 2D features
into 3D space using depth estimation or 3D position encoder, but in a fully
data-driven and implicit manner, which limits the detection performance.
Inspired by the success of radiance fields on 3D reconstruction, we assume they
can be used to enhance the detector's ability of 3D geometry estimation.
However, we observe a decline in detection performance, when we directly use
them for 3D rendering as an auxiliary task. From our analysis, we find the
performance drop is caused by the strong responses on the background when
rendering the whole scene. To address this problem, we propose object-centric
radiance fields, focusing on modeling foreground objects while discarding
background noises. Specifically, we employ Object-centric Radiance Fields
(OcRF) to enhance 3D voxel features via an auxiliary task of rendering
foreground objects. We further use opacity - the side-product of rendering- to
enhance the 2D foreground BEV features via Height-aware Opacity-based Attention
(HOA), where attention maps at different height levels are generated separately
via multiple networks in parallel. Extensive experiments on the nuScenes
validation and test datasets demonstrate that our OcRFDet achieves superior
performance, outperforming previous state-of-the-art methods with 57.2$\%$ mAP
and 64.8$\%$ NDS on the nuScenes test benchmark. Code will be available at
https://github.com/Mingqj/OcRFDet.

</details>


### [321] [Event-based Tiny Object Detection: A Benchmark Dataset and Baseline](https://arxiv.org/abs/2506.23575)
*Nuo Chen,Chao Xiao,Yimian Dai,Shiman He,Miao Li,Wei An*

Main category: cs.CV

TL;DR: The paper introduces EV-UAV, a large-scale event-based dataset for small object detection (SOD) in anti-UAV tasks, and proposes EV-SpSegNet, a novel baseline method with a Spatiotemporal Correlation loss for improved detection.


<details>
  <summary>Details</summary>
Motivation: Traditional cameras struggle with SOD due to low frame rates and data redundancy, while existing event-based datasets lack scale and diversity for SOD benchmarks.

Method: The authors introduce the EV-UAV dataset and propose EV-SpSegNet, a network for event segmentation in point cloud space, using a Spatiotemporal Correlation loss to leverage motion continuity.

Result: EV-SpSegNet outperforms existing methods on the EV-UAV dataset, which includes 147 sequences with 2.3 million annotations and small targets (6.8×5.4 pixels).

Conclusion: The EV-UAV dataset and EV-SpSegNet provide a robust benchmark and method for future research in event-based small object detection.

Abstract: Small object detection (SOD) in anti-UAV task is a challenging problem due to
the small size of UAVs and complex backgrounds. Traditional frame-based cameras
struggle to detect small objects in complex environments due to their low frame
rates, limited dynamic range, and data redundancy. Event cameras, with
microsecond temporal resolution and high dynamic range, provide a more
effective solution for SOD. However, existing event-based object detection
datasets are limited in scale, feature large targets size, and lack diverse
backgrounds, making them unsuitable for SOD benchmarks. In this paper, we
introduce a Event-based Small object detection (EVSOD) dataset (namely EV-UAV),
the first large-scale, highly diverse benchmark for anti-UAV tasks. It includes
147 sequences with over 2.3 million event-level annotations, featuring
extremely small targets (averaging 6.8 $\times$ 5.4 pixels) and diverse
scenarios such as urban clutter and extreme lighting conditions. Furthermore,
based on the observation that small moving targets form continuous curves in
spatiotemporal event point clouds, we propose Event based Sparse Segmentation
Network (EV-SpSegNet), a novel baseline for event segmentation in point cloud
space, along with a Spatiotemporal Correlation (STC) loss that leverages motion
continuity to guide the network in retaining target events. Extensive
experiments on the EV-UAV dataset demonstrate the superiority of our method and
provide a benchmark for future research in EVSOD. The dataset and code are at
https://github.com/ChenYichen9527/Ev-UAV.

</details>


### [322] [StackCLIP: Clustering-Driven Stacked Prompt in Zero-Shot Industrial Anomaly Detection](https://arxiv.org/abs/2506.23577)
*Yanning Hou,Yanran Ruan,Junfa Li,Shanshan Wang,Jianfeng Qiu,Ke Xu*

Main category: cs.CV

TL;DR: The paper proposes StackCLIP, a method using stacked prompts to enhance CLIP's text-image alignment for zero-shot industrial anomaly detection, improving generalization and performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods overfit to training categories, limiting generalization. StackCLIP addresses this by transforming category names into stacked prompts for better alignment.

Method: StackCLIP introduces Clustering-Driven Stacked Prompts (CSP) and Ensemble Feature Alignment (EFA) modules, along with Regulating Prompt Learning (RPL), to improve anomaly detection.

Result: The method achieves state-of-the-art performance in zero-shot anomaly detection and segmentation across seven datasets.

Conclusion: StackCLIP enhances generalization and performance in industrial anomaly detection, offering superior training speed and stability.

Abstract: Enhancing the alignment between text and image features in the CLIP model is
a critical challenge in zero-shot industrial anomaly detection tasks. Recent
studies predominantly utilize specific category prompts during pretraining,
which can cause overfitting to the training categories and limit model
generalization. To address this, we propose a method that transforms category
names through multicategory name stacking to create stacked prompts, forming
the basis of our StackCLIP model. Our approach introduces two key components.
The Clustering-Driven Stacked Prompts (CSP) module constructs generic prompts
by stacking semantically analogous categories, while utilizing multi-object
textual feature fusion to amplify discriminative anomalies among similar
objects. The Ensemble Feature Alignment (EFA) module trains knowledge-specific
linear layers tailored for each stack cluster and adaptively integrates them
based on the attributes of test categories. These modules work together to
deliver superior training speed, stability, and convergence, significantly
boosting anomaly segmentation performance. Additionally, our stacked prompt
framework offers robust generalization across classification tasks. To further
improve performance, we introduce the Regulating Prompt Learning (RPL) module,
which leverages the generalization power of stacked prompts to refine prompt
learning, elevating results in anomaly detection classification tasks.
Extensive testing on seven industrial anomaly detection datasets demonstrates
that our method achieves state-of-the-art performance in both zero-shot anomaly
detection and segmentation tasks.

</details>


### [323] [Dataset Distillation via Vision-Language Category Prototype](https://arxiv.org/abs/2506.23580)
*Yawen Zou,Guang Li,Duo Su,Zi Wang,Jun Yu,Chao Zhang*

Main category: cs.CV

TL;DR: The paper introduces a vision-language method for dataset distillation, enhancing performance by incorporating text prototypes derived from a large language model, improving generalization and logical coherence in outputs.


<details>
  <summary>Details</summary>
Motivation: Existing dataset distillation methods focus on images and overlook semantic context, limiting generalization and causing illogical outputs.

Method: Integrates vision-language methods, using text prototypes (from a large language model) alongside image prototypes to collaboratively synthesize data.

Result: Achieves state-of-the-art validation performance, generating logically coherent images with target objects and robust generalization.

Conclusion: The proposed framework expands dataset distillation beyond image-based approaches, demonstrating broad applicability and improved performance.

Abstract: Dataset distillation (DD) condenses large datasets into compact yet
informative substitutes, preserving performance comparable to the original
dataset while reducing storage, transmission costs, and computational
consumption. However, previous DD methods mainly focus on distilling
information from images, often overlooking the semantic information inherent in
the data. The disregard for context hinders the model's generalization ability,
particularly in tasks involving complex datasets, which may result in illogical
outputs or the omission of critical objects. In this study, we integrate
vision-language methods into DD by introducing text prototypes to distill
language information and collaboratively synthesize data with image prototypes,
thereby enhancing dataset distillation performance. Notably, the text
prototypes utilized in this study are derived from descriptive text information
generated by an open-source large language model. This framework demonstrates
broad applicability across datasets without pre-existing text descriptions,
expanding the potential of dataset distillation beyond traditional image-based
approaches. Compared to other methods, the proposed approach generates
logically coherent images containing target objects, achieving state-of-the-art
validation performance and demonstrating robust generalization. Source code and
generated data are available in
https://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/

</details>


### [324] [CAI: Caption-Sensitive Attention Intervention for Mitigating Object Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2506.23590)
*Qiming Li,Zekai Ye,Xiaocheng Feng,Weihong Zhong,Libo Qin,Ruihan Chen,Baohang Li,Kui Jiang,Yaowei Wang,Ting Liu,Bing Qin*

Main category: cs.CV

TL;DR: A training-free method, Caption-sensitive Attention Intervention (CAI), mitigates object hallucination in LVLMs by leveraging attention patterns from caption queries, achieving SOTA performance with minimal inference cost.


<details>
  <summary>Details</summary>
Motivation: LVLMs often produce inaccurate visual interpretations (object hallucination). Existing solutions are costly or slow. The paper addresses this by exploiting LVLMs' stronger attention to caption queries.

Method: Proposes CAI, a plug-and-play method that uses attention activation patterns from caption queries to enhance visual perception without training or significant inference overhead.

Result: CAI achieves state-of-the-art hallucination mitigation across four benchmarks for discriminative and generative tasks, with minimal added inference cost.

Conclusion: CAI effectively reduces object hallucination in LVLMs by leveraging attention patterns, offering a cost-efficient and scalable solution.

Abstract: Although Large Vision-Language Models (LVLMs) have demonstrated powerful
capabilities in interpreting visual information, they frequently produce
content that deviates from visual information, leading to object hallucination.
To tackle this, recent works mostly depend on expensive manual annotations and
training cost, or significantly increase inference time. In this work, we
observe that LVLMs' attention to visual information is significantly stronger
when answering caption queries compared to non-caption queries. Inspired by
this phenomenon, we propose Caption-sensitive Attention Intervention (CAI), a
training-free, plug-and-play hallucination mitigation method that leverages the
attention activation pattern in response to caption queries to enhance LVLMs'
visual perception capability. Extensive experimental results across four
benchmarks covering both discriminative and generative tasks, demonstrate that
CAI achieves state-of-the-art (SOTA) hallucination mitigating performance only
with minimal additional inference cost.

</details>


### [325] [AI-Generated Lecture Slides for Improving Slide Element Detection and Retrieval](https://arxiv.org/abs/2506.23605)
*Suyash Maniyar,Vishvesh Trivedi,Ajoy Mondal,Anand Mishra,C. V. Jawahar*

Main category: cs.CV

TL;DR: A pipeline (SynLecSlideGen) generates synthetic lecture slides using LLMs to reduce manual annotation. Few-shot transfer learning with synthetic data improves performance on real slides.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of lecture slides is labor-intensive and requires expertise. Synthetic data can address this limitation.

Method: Proposed SynLecSlideGen, an LLM-guided pipeline for synthetic slide generation, and evaluated using RealSlide (1,050 annotated slides). Few-shot transfer learning tested utility.

Result: Pretraining on synthetic slides significantly boosts performance compared to training only on real data.

Conclusion: Synthetic data effectively compensates for limited labeled lecture slides, reducing annotation effort.

Abstract: Lecture slide element detection and retrieval are key problems in slide
understanding. Training effective models for these tasks often depends on
extensive manual annotation. However, annotating large volumes of lecture
slides for supervised training is labor intensive and requires domain
expertise. To address this, we propose a large language model (LLM)-guided
synthetic lecture slide generation pipeline, SynLecSlideGen, which produces
high-quality, coherent and realistic slides. We also create an evaluation
benchmark, namely RealSlide by manually annotating 1,050 real lecture slides.
To assess the utility of our synthetic slides, we perform few-shot transfer
learning on real data using models pre-trained on them. Experimental results
show that few-shot transfer learning with pretraining on synthetic slides
significantly improves performance compared to training only on real data. This
demonstrates that synthetic data can effectively compensate for limited labeled
lecture slides. The code and resources of our work are publicly available on
our project website: https://synslidegen.github.io/.

</details>


### [326] [SG-LDM: Semantic-Guided LiDAR Generation via Latent-Aligned Diffusion](https://arxiv.org/abs/2506.23606)
*Zhengkang Xiang,Zizhao Li,Amir Khodabandeh,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: SG-LDM is a semantic-guided lidar diffusion model for high-fidelity point cloud synthesis, outperforming existing methods and enhancing downstream tasks like segmentation.


<details>
  <summary>Details</summary>
Motivation: Real-world lidar data is often scarce or lacks diversity, limiting deep learning pipelines. Existing methods overlook semantic guidance for practical applications.

Method: SG-LDM uses latent alignment and explicit semantic conditioning to generate lidar point clouds. It also introduces a diffusion-based lidar translation framework for cross-domain adaptation.

Result: SG-LDM achieves state-of-the-art performance in lidar synthesis and improves downstream segmentation via the proposed translation framework.

Conclusion: SG-LDM advances lidar synthesis with semantic guidance and domain adaptation, proving effective for enhancing perception tasks.

Abstract: Lidar point cloud synthesis based on generative models offers a promising
solution to augment deep learning pipelines, particularly when real-world data
is scarce or lacks diversity. By enabling flexible object manipulation, this
synthesis approach can significantly enrich training datasets and enhance
discriminative models. However, existing methods focus on unconditional lidar
point cloud generation, overlooking their potential for real-world
applications. In this paper, we propose SG-LDM, a Semantic-Guided Lidar
Diffusion Model that employs latent alignment to enable robust
semantic-to-lidar synthesis. By directly operating in the native lidar space
and leveraging explicit semantic conditioning, SG-LDM achieves state-of-the-art
performance in generating high-fidelity lidar point clouds guided by semantic
labels. Moreover, we propose the first diffusion-based lidar translation
framework based on SG-LDM, which enables cross-domain translation as a domain
adaptation strategy to enhance downstream perception performance. Systematic
experiments demonstrate that SG-LDM significantly outperforms existing lidar
diffusion models and the proposed lidar translation framework further improves
data augmentation performance in the downstream lidar segmentation task.

</details>


### [327] [PGOV3D: Open-Vocabulary 3D Semantic Segmentation with Partial-to-Global Curriculum](https://arxiv.org/abs/2506.23607)
*Shiqi Zhang,Sha Zhang,Jiajun Deng,Yedong Shen,Mingxiao MA,Yanyong Zhang*

Main category: cs.CV

TL;DR: PGOV3D introduces a two-stage Partial-to-Global curriculum for open-vocabulary 3D semantic segmentation, leveraging multi-view images and MLLMs for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook rich semantic content and cross-view correspondences in multi-view images, limiting effectiveness.

Method: Two-stage training: pre-training on dense partial scenes with MLLM-generated labels, then fine-tuning on complete scenes with pseudo labels.

Result: Competitive performance on ScanNet, ScanNet200, and S3DIS benchmarks.

Conclusion: PGOV3D effectively bridges the semantic gap between partial and complete 3D scenes, enhancing open-vocabulary segmentation.

Abstract: Existing open-vocabulary 3D semantic segmentation methods typically supervise
3D segmentation models by merging text-aligned features (e.g., CLIP) extracted
from multi-view images onto 3D points. However, such approaches treat
multi-view images merely as intermediaries for transferring open-vocabulary
information, overlooking their rich semantic content and cross-view
correspondences, which limits model effectiveness. To address this, we propose
PGOV3D, a novel framework that introduces a Partial-to-Global curriculum for
improving open-vocabulary 3D semantic segmentation. The key innovation lies in
a two-stage training strategy. In the first stage, we pre-train the model on
partial scenes that provide dense semantic information but relatively simple
geometry. These partial point clouds are derived from multi-view RGB-D inputs
via pixel-wise depth projection. To enable open-vocabulary learning, we
leverage a multi-modal large language model (MLLM) and a 2D segmentation
foundation model to generate open-vocabulary labels for each viewpoint,
offering rich and aligned supervision. An auxiliary inter-frame consistency
module is introduced to enforce feature consistency across varying viewpoints
and enhance spatial understanding. In the second stage, we fine-tune the model
on complete scene-level point clouds, which are sparser and structurally more
complex. We aggregate the partial vocabularies associated with each scene and
generate pseudo labels using the pre-trained model, effectively bridging the
semantic gap between dense partial observations and large-scale 3D
environments. Extensive experiments on ScanNet, ScanNet200, and S3DIS
benchmarks demonstrate that PGOV3D achieves competitive performance in
open-vocabulary 3D semantic segmentation.

</details>


### [328] [AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention](https://arxiv.org/abs/2506.23611)
*Ziao Liu,Zhenjia Li,Yifeng Shi,Xiangang Li*

Main category: cs.CV

TL;DR: AttentionGS eliminates the need for high-quality initial point clouds in 3D Gaussian Splatting (3DGS) by using structural attention for direct 3D reconstruction from random initialization, improving robustness in texture-deficient or constrained-view scenarios.


<details>
  <summary>Details</summary>
Motivation: 3DGS relies on high-quality point clouds from SfM, which fails in texture-deficient or constrained-view scenarios, limiting its applicability.

Method: AttentionGS uses geometric attention early in training to recover global structure and texture attention later to refine details. It also employs opacity-weighted gradients for better Gaussian densification.

Result: AttentionGS outperforms state-of-the-art methods, especially when point cloud initialization is unreliable.

Conclusion: AttentionGS enables more robust and flexible 3DGS, expanding its real-world applicability.

Abstract: 3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance
Fields (NeRF), excelling in complex scene reconstruction and efficient
rendering. However, it relies on high-quality point clouds from
Structure-from-Motion (SfM), limiting its applicability. SfM also fails in
texture-deficient or constrained-view scenarios, causing severe degradation in
3DGS reconstruction. To address this limitation, we propose AttentionGS, a
novel framework that eliminates the dependency on high-quality initial point
clouds by leveraging structural attention for direct 3D reconstruction from
randomly initialization. In the early training stage, we introduce geometric
attention to rapidly recover the global scene structure. As training
progresses, we incorporate texture attention to refine fine-grained details and
enhance rendering quality. Furthermore, we employ opacity-weighted gradients to
guide Gaussian densification, leading to improved surface reconstruction.
Extensive experiments on multiple benchmark datasets demonstrate that
AttentionGS significantly outperforms state-of-the-art methods, particularly in
scenarios where point cloud initialization is unreliable. Our approach paves
the way for more robust and flexible 3D Gaussian Splatting in real-world
applications.

</details>


### [329] [TurboVSR: Fantastic Video Upscalers and Where to Find Them](https://arxiv.org/abs/2506.23618)
*Zhongdao Wang,Guodongfang Zhao,Jingjing Ren,Bailan Feng,Shifeng Zhang,Wenbo Li*

Main category: cs.CV

TL;DR: TurboVSR is an ultra-efficient diffusion-based video super-resolution model that achieves state-of-the-art performance while being 100+ times faster than existing methods.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based VSR methods are computationally inefficient, taking too long to process videos. TurboVSR aims to address this by improving efficiency without sacrificing quality.

Method: TurboVSR uses a high-compression autoencoder, factorized conditioning for training, and converts a pre-trained diffusion model into a shortcut model for faster inference.

Result: TurboVSR matches state-of-the-art VSR quality while processing a 2-second 1080p video in just 7 seconds. It also supports 4K image super-resolution with fine details.

Conclusion: TurboVSR successfully balances efficiency and performance, making high-quality video and image super-resolution practical for real-world applications.

Abstract: Diffusion-based generative models have demonstrated exceptional promise in
the video super-resolution (VSR) task, achieving a substantial advancement in
detail generation relative to prior methods. However, these approaches face
significant computational efficiency challenges. For instance, current
techniques may require tens of minutes to super-resolve a mere 2-second, 1080p
video. In this paper, we present TurboVSR, an ultra-efficient diffusion-based
video super-resolution model. Our core design comprises three key aspects: (1)
We employ an autoencoder with a high compression ratio of 32$\times$32$\times$8
to reduce the number of tokens. (2) Highly compressed latents pose substantial
challenges for training. We introduce factorized conditioning to mitigate the
learning complexity: we first learn to super-resolve the initial frame;
subsequently, we condition the super-resolution of the remaining frames on the
high-resolution initial frame and the low-resolution subsequent frames. (3) We
convert the pre-trained diffusion model to a shortcut model to enable fewer
sampling steps, further accelerating inference. As a result, TurboVSR performs
on par with state-of-the-art VSR methods, while being 100+ times faster, taking
only 7 seconds to process a 2-second long 1080p video. TurboVSR also supports
image resolution by considering image as a one-frame video. Our efficient
design makes SR beyond 1080p possible, results on 4K (3648$\times$2048) image
SR show surprising fine details.

</details>


### [330] [Revisiting Audio-Visual Segmentation with Vision-Centric Transformer](https://arxiv.org/abs/2506.23623)
*Shaofei Huang,Rui Ling,Tianrui Hui,Hongyu Li,Xu Zhou,Shifeng Zhang,Si Liu,Richang Hong,Meng Wang*

Main category: cs.CV

TL;DR: The paper introduces a Vision-Centric Transformer (VCT) framework for Audio-Visual Segmentation (AVS), addressing limitations of audio-centric methods by using vision-derived queries and a PPQG module for improved performance.


<details>
  <summary>Details</summary>
Motivation: Current AVS methods using audio-centric Transformers face issues like perception ambiguity and weakened dense prediction due to visual detail loss.

Method: Proposes a VCT framework with vision-derived queries and a PPQG module to enhance audio-visual information aggregation.

Result: Achieves state-of-the-art performance on three subsets of the AVSBench dataset.

Conclusion: The VCT framework effectively improves AVS by leveraging vision-derived queries and audio-visual aggregation.

Abstract: Audio-Visual Segmentation (AVS) aims to segment sound-producing objects in
video frames based on the associated audio signal. Prevailing AVS methods
typically adopt an audio-centric Transformer architecture, where object queries
are derived from audio features. However, audio-centric Transformers suffer
from two limitations: perception ambiguity caused by the mixed nature of audio,
and weakened dense prediction ability due to visual detail loss. To address
these limitations, we propose a new Vision-Centric Transformer (VCT) framework
that leverages vision-derived queries to iteratively fetch corresponding audio
and visual information, enabling queries to better distinguish between
different sounding objects from mixed audio and accurately delineate their
contours. Additionally, we also introduce a Prototype Prompted Query Generation
(PPQG) module within our VCT framework to generate vision-derived queries that
are both semantically aware and visually rich through audio prototype prompting
and pixel context grouping, facilitating audio-visual information aggregation.
Extensive experiments demonstrate that our VCT framework achieves new
state-of-the-art performances on three subsets of the AVSBench dataset. The
code is available at https://github.com/spyflying/VCT_AVS.

</details>


### [331] [Blending Concepts with Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.23630)
*Lorenzo Olearo,Giorgio Longari,Alessandro Raganato,Rafael Peñaloza,Simone Melzi*

Main category: cs.CV

TL;DR: Diffusion models can blend diverse concepts into coherent images without training, using various methods, but no single method works best in all cases.


<details>
  <summary>Details</summary>
Motivation: To explore if diffusion models can blend distinct concepts into novel images under a zero-shot framework.

Method: Four blending methods exploiting different aspects of the diffusion pipeline (e.g., prompt scheduling, embedding interpolation, layer-wise conditioning).

Result: Diffusion models show creative blending capabilities, but outcomes vary based on method, prompt order, conceptual distance, and randomness.

Conclusion: Diffusion models have strong compositional potential but are sensitive to input variations, with no single blending method universally superior.

Abstract: Diffusion models have dramatically advanced text-to-image generation in
recent years, translating abstract concepts into high-fidelity images with
remarkable ease. In this work, we examine whether they can also blend distinct
concepts, ranging from concrete objects to intangible ideas, into coherent new
visual entities under a zero-shot framework. Specifically, concept blending
merges the key attributes of multiple concepts (expressed as textual prompts)
into a single, novel image that captures the essence of each concept. We
investigate four blending methods, each exploiting different aspects of the
diffusion pipeline (e.g., prompt scheduling, embedding interpolation, or
layer-wise conditioning). Through systematic experimentation across diverse
concept categories, such as merging concrete concepts, synthesizing compound
words, transferring artistic styles, and blending architectural landmarks, we
show that modern diffusion models indeed exhibit creative blending capabilities
without further training or fine-tuning. Our extensive user study, involving
100 participants, reveals that no single approach dominates in all scenarios:
each blending technique excels under certain conditions, with factors like
prompt ordering, conceptual distance, and random seed affecting the outcome.
These findings highlight the remarkable compositional potential of diffusion
models while exposing their sensitivity to seemingly minor input variations.

</details>


### [332] [Unified Multimodal Understanding via Byte-Pair Visual Encoding](https://arxiv.org/abs/2506.23639)
*Wanpeng Zhang,Yicheng Feng,Hao Luo,Yijiang Li,Zihao Yue,Sipeng Zheng,Zongqing Lu*

Main category: cs.CV

TL;DR: A framework for aligning modalities in MLLMs using byte-pair encoding for visual tokens, improving vision-language understanding.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of aligning different modalities in multimodal large language models (MLLMs) for better vision-language understanding.

Method: Uses byte-pair encoding for visual tokens, priority-guided encoding (frequency and spatial consistency), and multi-stage training with curriculum-driven data.

Result: Improved performance in diverse vision-language tasks by better capturing cross-modal relationships.

Conclusion: The approach bridges visual and textual representations, advancing more capable and efficient multimodal foundation models.

Abstract: Multimodal large language models (MLLMs) have made significant progress in
vision-language understanding, yet effectively aligning different modalities
remains a fundamental challenge. We present a framework that unifies multimodal
understanding by applying byte-pair encoding to visual tokens. Unlike
conventional approaches that rely on modality-specific encoders, our method
directly incorporates structural information into visual tokens, mirroring
successful tokenization strategies in text-only language models. We introduce a
priority-guided encoding scheme that considers both frequency and spatial
consistency, coupled with a multi-stage training procedure based on
curriculum-driven data composition. These enhancements enable the transformer
model to better capture cross-modal relationships and reason with visual
information. Comprehensive experiments demonstrate improved performance across
diverse vision-language tasks. By bridging the gap between visual and textual
representations, our approach contributes to the advancement of more capable
and efficient multimodal foundation models.

</details>


### [333] [VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation](https://arxiv.org/abs/2506.23641)
*Peng Huang,Junhu Fu,Bowen Guo,Zeju Li,Yuanyuan Wang,Yi Guo*

Main category: cs.CV

TL;DR: VAP-Diffusion leverages MLLMs to generate detailed descriptions for medical images, improving realism and diversity in generation.


<details>
  <summary>Details</summary>
Motivation: Detailed attribute descriptions (e.g., shape, size) for medical images are often unavailable, limiting generative models.

Method: Uses MLLMs with Chain-of-Thought prompts to derive descriptions, stores them by category, and employs a Prototype Condition Mechanism for robustness.

Result: Effective across three medical imaging types in four datasets.

Conclusion: VAP-Diffusion enhances medical image generation quality and diversity by integrating external knowledge.

Abstract: As the appearance of medical images is influenced by multiple underlying
factors, generative models require rich attribute information beyond labels to
produce realistic and diverse images. For instance, generating an image of skin
lesion with specific patterns demands descriptions that go beyond diagnosis,
such as shape, size, texture, and color. However, such detailed descriptions
are not always accessible. To address this, we explore a framework, termed
Visual Attribute Prompts (VAP)-Diffusion, to leverage external knowledge from
pre-trained Multi-modal Large Language Models (MLLMs) to improve the quality
and diversity of medical image generation. First, to derive descriptions from
MLLMs without hallucination, we design a series of prompts following
Chain-of-Thoughts for common medical imaging tasks, including dermatologic,
colorectal, and chest X-ray images. Generated descriptions are utilized during
training and stored across different categories. During testing, descriptions
are randomly retrieved from the corresponding category for inference. Moreover,
to make the generator robust to unseen combination of descriptions at the test
time, we propose a Prototype Condition Mechanism that restricts test embeddings
to be similar to those from training. Experiments on three common types of
medical imaging across four datasets verify the effectiveness of VAP-Diffusion.

</details>


### [334] [MReg: A Novel Regression Model with MoE-based Video Feature Mining for Mitral Regurgitation Diagnosis](https://arxiv.org/abs/2506.23648)
*Zhe Liu,Yuhao Huang,Lian Liu,Chengrui Zhang,Haotian Lin,Tong Han,Zhiyuan Zhu,Yanlin Chen,Yuerui Chen,Dong Ni,Zhongshan Gou,Xin Yang*

Main category: cs.CV

TL;DR: An automated MR diagnosis model (MReg) using 4-chamber cardiac color Doppler echocardiography videos improves accuracy and interpretability by mimicking clinical workflows and leveraging feature mining strategies.


<details>
  <summary>Details</summary>
Motivation: Current intelligent methods for MR diagnosis often misalign with clinical workflows, leading to suboptimal accuracy and interpretability.

Method: MReg formulates MR diagnosis as a regression task, uses feature selection/amplification to mimic sonographer logic, and introduces a feature summary module for category-level feature extraction.

Result: MReg outperforms other methods on a dataset of 1868 cases, demonstrating superior performance in MR diagnosis.

Conclusion: MReg offers a clinically aligned, accurate, and interpretable solution for automated MR diagnosis.

Abstract: Color Doppler echocardiography is a crucial tool for diagnosing mitral
regurgitation (MR). Recent studies have explored intelligent methods for MR
diagnosis to minimize user dependence and improve accuracy. However, these
approaches often fail to align with clinical workflow and may lead to
suboptimal accuracy and interpretability. In this study, we introduce an
automated MR diagnosis model (MReg) developed on the 4-chamber cardiac color
Doppler echocardiography video (A4C-CDV). It follows comprehensive feature
mining strategies to detect MR and assess its severity, considering clinical
realities. Our contribution is threefold. First, we formulate the MR diagnosis
as a regression task to capture the continuity and ordinal relationships
between categories. Second, we design a feature selection and amplification
mechanism to imitate the sonographer's diagnostic logic for accurate MR
grading. Third, inspired by the Mixture-of-Experts concept, we introduce a
feature summary module to extract the category-level features, enhancing the
representational capacity for more accurate grading. We trained and evaluated
our proposed MReg on a large in-house A4C-CDV dataset comprising 1868 cases
with three graded regurgitation labels. Compared to other weakly supervised
video anomaly detection and supervised classification methods, MReg
demonstrated superior performance in MR diagnosis. Our code is available at:
https://github.com/cskdstz/MReg.

</details>


### [335] [Towards Markerless Intraoperative Tracking of Deformable Spine Tissue](https://arxiv.org/abs/2506.23657)
*Connor Daly,Elettra Marconi,Marco Riva,Jinendra Ekanayake,Daniel S. Elson,Ferdinando Rodriguez y Baena*

Main category: cs.CV

TL;DR: The paper introduces SpineAlign, a system for tracking spine deformations in surgery using RGB-D imaging, along with a dataset and segmentation network.


<details>
  <summary>Details</summary>
Motivation: To reduce operating time and complexity by replacing bone-mounted tracking devices with markerless RGB-D imaging.

Method: Developed SpineAlign for deformation tracking, an intraoperative segmentation network, and CorrespondNet for key region prediction.

Result: First real-world clinical RGB-D dataset for spine surgery and a functional system for deformation tracking.

Conclusion: The work demonstrates the feasibility of markerless RGB-D tracking in spine surgery, with potential for broader clinical use.

Abstract: Consumer-grade RGB-D imaging for intraoperative orthopedic tissue tracking is
a promising method with high translational potential. Unlike bone-mounted
tracking devices, markerless tracking can reduce operating time and complexity.
However, its use has been limited to cadaveric studies. This paper introduces
the first real-world clinical RGB-D dataset for spine surgery and develops
SpineAlign, a system for capturing deformation between preoperative and
intraoperative spine states. We also present an intraoperative segmentation
network trained on this data and introduce CorrespondNet, a multi-task
framework for predicting key regions for registration in both intraoperative
and preoperative scenes.

</details>


### [336] [When Small Guides Large: Cross-Model Co-Learning for Test-Time Adaptation](https://arxiv.org/abs/2506.23724)
*Chang'an Yi,Xiaohui Deng,Guohao Chen,Yan Zhou,Qinghua Lu,Shuaicheng Niu*

Main category: cs.CV

TL;DR: COCA introduces a Cross-Model Co-Learning framework for Test-time Adaptation (TTA), leveraging complementary knowledge across models to enhance performance, even with varying model sizes.


<details>
  <summary>Details</summary>
Motivation: Existing TTA methods focus on single-model adaptation, but cross-model knowledge can provide complementary insights, improving adaptation accuracy.

Method: COCA uses two strategies: co-adaptation (integrating knowledge from other models) and self-adaptation (enhancing individual model strengths via unsupervised learning).

Result: COCA significantly improves adaptation accuracy, e.g., boosting ViT-Base's performance on ImageNet-C from 51.7% to 64.5% with Mobile-ViT's guidance.

Conclusion: COCA demonstrates the value of cross-model co-learning in TTA, offering a plug-and-play solution to enhance existing methods.

Abstract: Test-time Adaptation (TTA) adapts a given model to testing domain data with
potential domain shifts through online unsupervised learning, yielding
impressive performance. However, to date, existing TTA methods primarily focus
on single-model adaptation. In this work, we investigate an intriguing
question: how does cross-model knowledge influence the TTA process? Our
findings reveal that, in TTA's unsupervised online setting, each model can
provide complementary, confident knowledge to the others, even when there are
substantial differences in model size. For instance, a smaller model like
MobileViT (10.6M parameters) can effectively guide a larger model like ViT-Base
(86.6M parameters). In light of this, we propose COCA, a Cross-Model
Co-Learning framework for TTA, which mainly consists of two main strategies. 1)
Co-adaptation adaptively integrates complementary knowledge from other models
throughout the TTA process, reducing individual model biases. 2)
Self-adaptation enhances each model's unique strengths via unsupervised
learning, enabling diverse adaptation to the target domain. Extensive
experiments show that COCA, which can also serve as a plug-and-play module,
significantly boosts existing SOTAs, on models with various sizes--including
ResNets, ViTs, and Mobile-ViTs--via cross-model co-learned TTA. For example,
with Mobile-ViT's guidance, COCA raises ViT-Base's average adaptation accuracy
on ImageNet-C from 51.7% to 64.5%. The code is publicly available at
https://github.com/ycarobot/COCA.

</details>


### [337] [Partial Forward Blocking: A Novel Data Pruning Paradigm for Lossless Training Acceleration](https://arxiv.org/abs/2506.23674)
*Dongyue Wu,Zilin Guo,Jialong Zuo,Nong Sang,Changxin Gao*

Main category: cs.CV

TL;DR: PFB is a novel framework for lossless training acceleration by adaptively pruning less important samples based on shallow-layer features, reducing computational costs without needing proxy models or extra back-propagation.


<details>
  <summary>Details</summary>
Motivation: To address the high computational costs of large training datasets and inefficiencies in existing data pruning methods that rely on gradients or proxy models.

Method: Uses Partial Forward Blocking (PFB) to assess sample importance via shallow-layer features, prune less important samples, and dynamically prioritize rare samples using probability density.

Result: Achieves 0.5% accuracy improvement and 33% training time reduction on ImageNet with 40% data pruned.

Conclusion: PFB offers a computationally efficient and effective solution for training acceleration without compromising model performance.

Abstract: The ever-growing size of training datasets enhances the generalization
capability of modern machine learning models but also incurs exorbitant
computational costs. Existing data pruning approaches aim to accelerate
training by removing those less important samples. However, they often rely on
gradients or proxy models, leading to prohibitive additional costs of gradient
back-propagation and proxy model training. In this paper, we propose Partial
Forward Blocking (PFB), a novel framework for lossless training acceleration.
The efficiency of PFB stems from its unique adaptive pruning pipeline: sample
importance is assessed based on features extracted from the shallow layers of
the target model. Less important samples are then pruned, allowing only the
retained ones to proceed with the subsequent forward pass and loss
back-propagation. This mechanism significantly reduces the computational
overhead of deep-layer forward passes and back-propagation for pruned samples,
while also eliminating the need for auxiliary backward computations and proxy
model training. Moreover, PFB introduces probability density as an indicator of
sample importance. Combined with an adaptive distribution estimation module,
our method dynamically prioritizes relatively rare samples, aligning with the
constantly evolving training state. Extensive experiments demonstrate the
significant superiority of PFB in performance and speed. On ImageNet, PFB
achieves a 0.5% accuracy improvement and 33% training time reduction with 40%
data pruned.

</details>


### [338] [Pruning by Block Benefit: Exploring the Properties of Vision Transformer Blocks during Domain Adaptation](https://arxiv.org/abs/2506.23675)
*Patrick Glandorf,Bodo Rosenhahn*

Main category: cs.CV

TL;DR: P3B introduces a pruning method for Vision Transformers that globally assigns parameter resources by evaluating block-level contributions, maintaining performance even at high sparsity.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers are computationally expensive, and existing pruning methods misevaluate weight significance on unseen domains, leading to suboptimal performance.

Method: P3B uses block-level contributions to assign parameters, reactivates late-converging blocks, and sets layerwise keep ratios based on global metrics.

Result: P3B achieves state-of-the-art pruning, preserving accuracy even at 70% sparsity with only a 0.64% drop.

Conclusion: P3B is an effective pruning method for Vision Transformers, especially in transfer learning, balancing performance and computational efficiency.

Abstract: Vision Transformer have set new benchmarks in several tasks, but these models
come with the lack of high computational costs which makes them impractical for
resource limited hardware. Network pruning reduces the computational complexity
by removing less important operations while maintaining performance. However,
pruning a model on an unseen data domain, leads to a misevaluation of weight
significance, resulting in suboptimal resource assignment. In this work, we
find that task-sensitive layers initially fail to improve the feature
representation on downstream tasks, leading to performance loss for early
pruning decisions. To address this problem, we introduce Pruning by Block
Benefit (P3B), a pruning method that utilizes the relative contribution on
block level to globally assign parameter resources. P3B identifies low-impact
components to reduce parameter allocation while preserving critical ones.
Classical pruning mask optimization struggles to reactivate zero-mask-elements.
In contrast, P3B sets a layerwise keep ratio based on global performance
metrics, ensuring the reactivation of late-converging blocks. We show in
extensive experiments that P3B is a state of the art pruning method with most
noticeable gains in transfer learning tasks. Notably, P3B is able to conserve
high performance, even in high sparsity regimes of 70% parameter reduction
while only losing 0.64% in accuracy.

</details>


### [339] [A Unified Framework for Stealthy Adversarial Generation via Latent Optimization and Transferability Enhancement](https://arxiv.org/abs/2506.23676)
*Gaozheng Pei,Ke Ma,Dongpeng Zhang,Chengzhi Sun,Qianqian Xu,Qingming Huang*

Main category: cs.CV

TL;DR: A unified framework integrates traditional transferability strategies into diffusion-based adversarial example generation, enhancing performance in tasks like Deepfake detection.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based adversarial methods struggle with generalization beyond image classification and adapting transferability strategies.

Method: Proposes a unified framework combining traditional transferability enhancement with diffusion model-based adversarial example generation.

Result: Achieved first place in a competition, validating the framework's effectiveness.

Conclusion: The framework successfully broadens the applicability of diffusion-based adversarial methods.

Abstract: Due to their powerful image generation capabilities, diffusion-based
adversarial example generation methods through image editing are rapidly
gaining popularity. However, due to reliance on the discriminative capability
of the diffusion model, these diffusion-based methods often struggle to
generalize beyond conventional image classification tasks, such as in Deepfake
detection. Moreover, traditional strategies for enhancing adversarial example
transferability are challenging to adapt to these methods. To address these
challenges, we propose a unified framework that seamlessly incorporates
traditional transferability enhancement strategies into diffusion model-based
adversarial example generation via image editing, enabling their application
across a wider range of downstream tasks. Our method won first place in the
"1st Adversarial Attacks on Deepfake Detectors: A Challenge in the Era of
AI-Generated Media" competition at ACM MM25, which validates the effectiveness
of our approach.

</details>


### [340] [SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation](https://arxiv.org/abs/2506.23690)
*Shuai Tan,Biao Gong,Yujie Wei,Shiwei Zhang,Zhuoxin Liu,Dandan Zheng,Jingdong Chen,Yan Wang,Hao Ouyang,Kecheng Zheng,Yujun Shen*

Main category: cs.CV

TL;DR: SynMotion is a motion-customized video generation model that combines semantic guidance and visual adaptation to address limitations in existing approaches. It introduces dual-embedding semantic comprehension and parameter-efficient motion adapters, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on semantic-level alignment or visual representation alone, leading to overlooked motion complexity or semantic confusion. SynMotion aims to balance both aspects for better motion customization.

Method: SynMotion uses dual-embedding semantic comprehension to disentangle subject and motion representations, integrates motion adapters for visual fidelity, and employs an alternate optimization training strategy with the SPV dataset.

Result: SynMotion outperforms existing baselines in both text-to-video (T2V) and image-to-video (I2V) settings, demonstrating improved motion specificity and generalization.

Conclusion: SynMotion effectively balances semantic and visual adaptation for motion-customized video generation, validated by experimental results and the new MotionBench benchmark.

Abstract: Diffusion-based video motion customization facilitates the acquisition of
human motion representations from a few video samples, while achieving
arbitrary subjects transfer through precise textual conditioning. Existing
approaches often rely on semantic-level alignment, expecting the model to learn
new motion concepts and combine them with other entities (e.g., ''cats'' or
''dogs'') to produce visually appealing results. However, video data involve
complex spatio-temporal patterns, and focusing solely on semantics cause the
model to overlook the visual complexity of motion. Conversely, tuning only the
visual representation leads to semantic confusion in representing the intended
action. To address these limitations, we propose SynMotion, a new
motion-customized video generation model that jointly leverages semantic
guidance and visual adaptation. At the semantic level, we introduce the
dual-embedding semantic comprehension mechanism which disentangles subject and
motion representations, allowing the model to learn customized motion features
while preserving its generative capabilities for diverse subjects. At the
visual level, we integrate parameter-efficient motion adapters into a
pre-trained video generation model to enhance motion fidelity and temporal
coherence. Furthermore, we introduce a new embedding-specific training strategy
which \textbf{alternately optimizes} subject and motion embeddings, supported
by the manually constructed Subject Prior Video (SPV) training dataset. This
strategy promotes motion specificity while preserving generalization across
diverse subjects. Lastly, we introduce MotionBench, a newly curated benchmark
with diverse motion patterns. Experimental results across both T2V and I2V
settings demonstrate that \method outperforms existing baselines. Project page:
https://lucaria-academy.github.io/SynMotion/

</details>


### [341] [Single Image Test-Time Adaptation via Multi-View Co-Training](https://arxiv.org/abs/2506.23705)
*Smriti Joshi,Richard Osuala,Lidia Garrucho,Kaisar Kushibar,Dimitri Kessler,Oliver Diaz,Karim Lekadir*

Main category: cs.CV

TL;DR: Proposes a patch-based multi-view co-training method for single-image test-time adaptation in medical imaging, outperforming existing methods by 3.75% Dice score.


<details>
  <summary>Details</summary>
Motivation: Addresses the impracticality of large target datasets in clinical settings and the underutilization of volumetric data in current methods.

Method: Uses uncertainty-guided self-training to enforce feature and prediction consistency for volumetric segmentation with a single test-time image.

Result: Achieves performance close to supervised benchmarks and outperforms state-of-the-art methods by 3.75% Dice score.

Conclusion: The method is effective for real-time, per-patient adaptation in medical imaging and is integrated with nnUNet for accessibility.

Abstract: Test-time adaptation enables a trained model to adjust to a new domain during
inference, making it particularly valuable in clinical settings where such
on-the-fly adaptation is required. However, existing techniques depend on large
target domain datasets, which are often impractical and unavailable in medical
scenarios that demand per-patient, real-time inference. Moreover, current
methods commonly focus on two-dimensional images, failing to leverage the
volumetric richness of medical imaging data. Bridging this gap, we propose a
Patch-Based Multi-View Co-Training method for Single Image Test-Time
adaptation. Our method enforces feature and prediction consistency through
uncertainty-guided self-training, enabling effective volumetric segmentation in
the target domain with only a single test-time image. Validated on three
publicly available breast magnetic resonance imaging datasets for tumor
segmentation, our method achieves performance close to the upper bound
supervised benchmark while also outperforming all existing state-of-the-art
methods, on average by a Dice Similarity Coefficient of 3.75%. We publicly
share our accessible codebase, readily integrable with the popular nnUNet
framework, at https://github.com/smriti-joshi/muvi.git.

</details>


### [342] [Subjective Camera: Bridging Human Cognition and Visual Reconstruction through Sequence-Aware Sketch-Guided Diffusion](https://arxiv.org/abs/2506.23711)
*Haoyang Chen,Dongfang Sun,Caoyuan Ma,Shiqin Wang,Kewei Zhang,Zheng Wang,Zhixiang Wang*

Main category: cs.CV

TL;DR: Subjective Camera reconstructs scenes from mental impressions using verbal descriptions and sketches, overcoming language and sketch limitations by treating drawings as priors for photorealistic images.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with subjective input biases, modality gaps between sketches and 3D priors, and sketch quality issues, often requiring impractical precision or resource-heavy adaptations.

Method: The framework uses concept-sequential generation, text-reward optimization, sequence-aware disentangled generation, and latent optimization to bridge modality gaps and handle rough sketches.

Result: The approach achieves state-of-the-art performance in semantic and spatial coherence across diverse datasets.

Conclusion: Subjective Camera effectively translates subjective perceptions into photorealistic images without demanding high sketch precision or extensive training.

Abstract: We propose Subjective Camera, a human-as-imaging-device paradigm that
reconstructs real-world scenes from mental impressions through synergistic use
of verbal descriptions and progressive rough sketches. This approach overcomes
dual limitations of language ambiguity and sketch abstraction by treating the
user's drawing sequence as priors, effectively translating subjective
perceptual expectations into photorealistic images.
  Existing approaches face three fundamental barriers: (1) user-specific
subjective input biases, (2) huge modality gap between planar sketch and 3D
priors in diffusion, and (3) sketch quality-sensitive performance degradation.
Current solutions either demand resource-intensive model adaptation or impose
impractical requirements on sketch precision.
  Our framework addresses these challenges through concept-sequential
generation. (1) We establish robust appearance priors through text-reward
optimization, and then implement sequence-aware disentangled generation that
processes concepts in sketching order; these steps accommodate user-specific
subjective expectation in a train-free way. (2) We employ latent optimization
that effectively bridges the modality gap between planar sketches and 3D priors
in diffusion. (3) Our hierarchical reward-guided framework enables the use of
rough sketches without demanding artistic expertise. Comprehensive evaluation
across diverse datasets demonstrates that our approach achieves
state-of-the-art performance in maintaining both semantic and spatial
coherence.

</details>


### [343] [GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models](https://arxiv.org/abs/2506.23903)
*Hamza Rasaee,Taha Koleilat,Hassan Rivaz*

Main category: cs.CV

TL;DR: A prompt-driven vision-language model (VLM) integrating Grounding DINO with SAM2 improves ultrasound object segmentation across multiple organs, outperforming state-of-the-art methods without needing large annotated datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in ultrasound segmentation due to anatomical variability, diverse protocols, and limited annotated data.

Method: Uses a VLM with Grounding DINO and SAM2, fine-tuned on 15 ultrasound datasets via LoRA, and tested on 3 unseen datasets.

Result: Outperforms UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse, and SAMUS on seen datasets and maintains strong performance on unseen ones.

Conclusion: VLMs show promise for scalable, robust ultrasound analysis, reducing reliance on organ-specific annotated data.

Abstract: Accurate and generalizable object segmentation in ultrasound imaging remains
a significant challenge due to anatomical variability, diverse imaging
protocols, and limited annotated data. In this study, we propose a
prompt-driven vision-language model (VLM) that integrates Grounding DINO with
SAM2 to enable object segmentation across multiple ultrasound organs. A total
of 18 public ultrasound datasets, encompassing the breast, thyroid, liver,
prostate, kidney, and paraspinal muscle, were utilized. These datasets were
divided into 15 for fine-tuning and validation of Grounding DINO using Low Rank
Adaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for
testing to evaluate performance in unseen distributions. Comprehensive
experiments demonstrate that our approach outperforms state-of-the-art
segmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse,
and SAMUS on most seen datasets while maintaining strong performance on unseen
datasets without additional fine-tuning. These results underscore the promise
of VLMs in scalable and robust ultrasound image analysis, reducing dependence
on large, organ-specific annotated datasets. We will publish our code on
code.sonography.ai after acceptance.

</details>


### [344] [Proteus-ID: ID-Consistent and Motion-Coherent Video Customization](https://arxiv.org/abs/2506.23729)
*Guiyu Zhang,Chen Shi,Zijian Jiang,Xunzhi Xiang,Jingjing Qian,Shaoshuai Shi,Li Jiang*

Main category: cs.CV

TL;DR: Proteus-ID is a diffusion-based framework for video identity customization, addressing identity consistency and motion realism with novel modules like MIF, TAII, and AML, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: The task of synthesizing identity-consistent and motion-coherent videos from a single image and text prompt is challenging due to maintaining identity alignment and generating natural motion.

Method: Proteus-ID uses Multimodal Identity Fusion (MIF) for joint identity representation, Time-Aware Identity Injection (TAII) for dynamic conditioning, and Adaptive Motion Learning (AML) for motion realism.

Result: Proteus-ID outperforms existing methods in identity preservation, text alignment, and motion quality, validated on the Proteus-Bench dataset.

Conclusion: Proteus-ID sets a new benchmark for video identity customization, with publicly available code and data.

Abstract: Video identity customization seeks to synthesize realistic, temporally
coherent videos of a specific subject, given a single reference image and a
text prompt. This task presents two core challenges: (1) maintaining identity
consistency while aligning with the described appearance and actions, and (2)
generating natural, fluid motion without unrealistic stiffness. To address
these challenges, we introduce Proteus-ID, a novel diffusion-based framework
for identity-consistent and motion-coherent video customization. First, we
propose a Multimodal Identity Fusion (MIF) module that unifies visual and
textual cues into a joint identity representation using a Q-Former, providing
coherent guidance to the diffusion model and eliminating modality imbalance.
Second, we present a Time-Aware Identity Injection (TAII) mechanism that
dynamically modulates identity conditioning across denoising steps, improving
fine-detail reconstruction. Third, we propose Adaptive Motion Learning (AML), a
self-supervised strategy that reweights the training loss based on
optical-flow-derived motion heatmaps, enhancing motion realism without
requiring additional inputs. To support this task, we construct Proteus-Bench,
a high-quality dataset comprising 200K curated clips for training and 150
individuals from diverse professions and ethnicities for evaluation. Extensive
experiments demonstrate that Proteus-ID outperforms prior methods in identity
preservation, text alignment, and motion quality, establishing a new benchmark
for video identity customization. Codes and data are publicly available at
https://grenoble-zhang.github.io/Proteus-ID/.

</details>


### [345] [Can We Challenge Open-Vocabulary Object Detectors with Generated Content in Street Scenes?](https://arxiv.org/abs/2506.23751)
*Annika Mütze,Sadia Ilyas,Christian Dörpelkus,Matthias Rottmann*

Main category: cs.CV

TL;DR: The paper explores the limitations of open-vocabulary object detectors using synthetic data, revealing their challenges in overlooking objects and dependence on object location rather than semantics.


<details>
  <summary>Details</summary>
Motivation: To rigorously evaluate the generalization of open-vocabulary object detectors, as real-world data lacks control, and to identify systematic failure modes.

Method: Two automated pipelines using Stable Diffusion to inpaint unusual objects with diverse semantics, evaluated on synthetic data derived from LostAndFound and NuImages datasets.

Result: Open-vocabulary detectors struggle with overlooking objects and show strong dependence on object location, not semantics.

Conclusion: Synthetic data provides a systematic way to challenge and improve open-vocabulary models, offering insights for better data acquisition.

Abstract: Open-vocabulary object detectors such as Grounding DINO are trained on vast
and diverse data, achieving remarkable performance on challenging datasets. Due
to that, it is unclear where to find their limitations, which is of major
concern when using in safety-critical applications. Real-world data does not
provide sufficient control, required for a rigorous evaluation of model
generalization. In contrast, synthetically generated data allows to
systematically explore the boundaries of model competence/generalization. In
this work, we address two research questions: 1) Can we challenge
open-vocabulary object detectors with generated image content? 2) Can we find
systematic failure modes of those models? To address these questions, we design
two automated pipelines using stable diffusion to inpaint unusual objects with
high diversity in semantics, by sampling multiple substantives from WordNet and
ChatGPT. On the synthetically generated data, we evaluate and compare multiple
open-vocabulary object detectors as well as a classical object detector. The
synthetic data is derived from two real-world datasets, namely LostAndFound, a
challenging out-of-distribution (OOD) detection benchmark, and the NuImages
dataset. Our results indicate that inpainting can challenge open-vocabulary
object detectors in terms of overlooking objects. Additionally, we find a
strong dependence of open-vocabulary models on object location, rather than on
object semantics. This provides a systematic approach to challenge
open-vocabulary models and gives valuable insights on how data could be
acquired to effectively improve these models.

</details>


### [346] [Visual Textualization for Image Prompted Object Detection](https://arxiv.org/abs/2506.23785)
*Yongjian Wu,Yang Zhou,Jiya Saiyin,Bingzheng Wei,Yan Xu*

Main category: cs.CV

TL;DR: VisTex-OVLM enhances object detection in rare categories by projecting visual exemplars into text feature space, maintaining OVLM's architecture and achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To improve Object-level Vision-Language Models' (OVLMs) ability to detect rare categories that are hard to describe textually and lack pre-training data.

Method: Uses multi-scale textualizing blocks and multi-stage fusion to integrate visual exemplars into textualized visual tokens, guiding OVLMs alongside text prompts.

Result: Superior performance on open-set datasets and state-of-the-art results on PASCAL VOC and MSCOCO few-shot benchmarks.

Conclusion: VisTex-OVLM effectively enhances OVLMs for rare category detection without altering their architecture, achieving top performance.

Abstract: We propose VisTex-OVLM, a novel image prompted object detection method that
introduces visual textualization -- a process that projects a few visual
exemplars into the text feature space to enhance Object-level Vision-Language
Models' (OVLMs) capability in detecting rare categories that are difficult to
describe textually and nearly absent from their pre-training data, while
preserving their pre-trained object-text alignment. Specifically, VisTex-OVLM
leverages multi-scale textualizing blocks and a multi-stage fusion strategy to
integrate visual information from visual exemplars, generating textualized
visual tokens that effectively guide OVLMs alongside text prompts. Unlike
previous methods, our method maintains the original architecture of OVLM,
maintaining its generalization capabilities while enhancing performance in
few-shot settings. VisTex-OVLM demonstrates superior performance across
open-set datasets which have minimal overlap with OVLM's pre-training data and
achieves state-of-the-art results on few-shot benchmarks PASCAL VOC and MSCOCO.
The code will be released at https://github.com/WitGotFlg/VisTex-OVLM.

</details>


### [347] [Controllable Reference-Based Real-World Remote Sensing Image Super-Resolution with Generative Diffusion Priors](https://arxiv.org/abs/2506.23801)
*Ce Wang,Wanjie Sun*

Main category: cs.CV

TL;DR: CRefDiff, a controllable reference-based diffusion model, enhances remote sensing image super-resolution by addressing under-generation and over-reliance on reference images, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing RefSR methods struggle with real-world complexities like cross-sensor resolution gaps and land cover changes, leading to under-generation or over-reliance on reference images.

Method: CRefDiff leverages Stable Diffusion's generative prior, introduces a dual-branch fusion mechanism for adaptive reference integration, and uses a 'Better Start' strategy to reduce denoising steps.

Result: CRefDiff outperforms existing methods on the Real-RefRSSRD dataset, improving downstream tasks like scene classification and semantic segmentation.

Conclusion: CRefDiff offers superior performance, flexibility, and efficiency for real-world remote sensing SR, supported by a new dataset for future research.

Abstract: Super-resolution (SR) techniques can enhance the spatial resolution of remote
sensing images by utilizing low-resolution (LR) images to reconstruct
high-resolution (HR) images, enabling more efficient large-scale earth
observation applications. While single-image super-resolution (SISR) methods
have shown progress, reference-based super-resolution (RefSR) offers superior
performance by incorporating historical HR images alongside current LR
observations. However, existing RefSR methods struggle with real-world
complexities, such as cross-sensor resolution gap and significant land cover
changes, often leading to under-generation or over-reliance on reference image.
To address these challenges, we propose CRefDiff, a novel controllable
reference-based diffusion model for real-world remote sensing image SR. To
address the under-generation problem, CRefDiff is built upon the pretrained
Stable Diffusion model, leveraging its powerful generative prior to produce
accurate structures and textures. To mitigate over-reliance on the reference,
we introduce a dual-branch fusion mechanism that adaptively integrates both
local and global information from the reference image. Moreover, this novel
dual-branch design enables reference strength control during inference,
enhancing interactivity and flexibility of the model. Finally, a strategy named
Better Start is proposed to significantly reduce the number of denoising steps,
thereby accelerating the inference process. To support further research, we
introduce Real-RefRSSRD, a new real-world RefSR dataset for remote sensing
images, consisting of HR NAIP and LR Sentinel-2 image pairs with diverse land
cover changes and significant temporal gaps. Extensive experiments on
Real-RefRSSRD show that CRefDiff achieves state-of-the-art performance across
various metrics and improves downstream tasks such as scene classification and
semantic segmentation.

</details>


### [348] [A Survey on Vision-Language-Action Models for Autonomous Driving](https://arxiv.org/abs/2506.24044)
*Sicong Jiang,Zilin Huang,Kangan Qian,Ziang Luo,Tianze Zhu,Yang Zhong,Yihong Tang,Menglin Kong,Yunlong Wang,Siwen Jiao,Hao Ye,Zihao Sheng,Xin Zhao,Tuopu Wen,Zheng Fu,Sikai Chen,Kun Jiang,Diange Yang,Seongjin Choi,Lijun Sun*

Main category: cs.CV

TL;DR: This survey provides a comprehensive overview of Vision-Language-Action (VLA) models for Autonomous Driving (VLA4AD), covering architecture, evolution, model comparisons, datasets, benchmarks, and open challenges.


<details>
  <summary>Details</summary>
Motivation: To consolidate fragmented and rapidly expanding research on VLA models in autonomous driving, offering a structured reference for advancing interpretable and socially aligned autonomous vehicles.

Method: The survey formalizes architectural building blocks, traces the evolution of VLA models, compares over 20 representative models, and consolidates datasets and benchmarks.

Result: A detailed comparison of VLA models, identification of key protocols for measuring driving safety and explanation quality, and highlighting of open challenges like robustness and real-time efficiency.

Conclusion: The survey serves as a concise yet complete reference for future research in VLA4AD, addressing challenges and outlining future directions.

Abstract: The rapid progress of multimodal large language models (MLLM) has paved the
way for Vision-Language-Action (VLA) paradigms, which integrate visual
perception, natural language understanding, and control within a single policy.
Researchers in autonomous driving are actively adapting these methods to the
vehicle domain. Such models promise autonomous vehicles that can interpret
high-level instructions, reason about complex traffic scenes, and make their
own decisions. However, the literature remains fragmented and is rapidly
expanding. This survey offers the first comprehensive overview of VLA for
Autonomous Driving (VLA4AD). We (i) formalize the architectural building blocks
shared across recent work, (ii) trace the evolution from early explainer to
reasoning-centric VLA models, and (iii) compare over 20 representative models
according to VLA's progress in the autonomous driving domain. We also
consolidate existing datasets and benchmarks, highlighting protocols that
jointly measure driving safety, accuracy, and explanation quality. Finally, we
detail open challenges - robustness, real-time efficiency, and formal
verification - and outline future directions of VLA4AD. This survey provides a
concise yet complete reference for advancing interpretable socially aligned
autonomous vehicles. Github repo is available at
\href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.

</details>


### [349] [Towards Initialization-free Calibrated Bundle Adjustment](https://arxiv.org/abs/2506.23808)
*Carl Olsson,Amanda Nilsson*

Main category: cs.CV

TL;DR: The paper introduces a method for initialization-free bundle adjustment (BA) that leverages camera calibration to produce near-metric reconstructions, improving accuracy over projective-invariant approaches.


<details>
  <summary>Details</summary>
Motivation: Existing initialization-free BA methods using pseudo Object Space Error (pOSE) lack camera calibration knowledge, leading to projective ambiguity and requiring more data for successful reconstruction.

Method: The proposed method integrates pairwise relative rotation estimates, which carry calibration information and are similarity-invariant, into the pOSE framework to achieve near-metric reconstructions.

Result: Experiments show reliable optimization of the objective, with high-probability convergence to the global minimum from random starts, yielding accurate near-metric reconstructions.

Conclusion: The method successfully combines rotation averaging with pOSE, enabling initialization-free calibrated Structure from Motion (SfM) with improved metric accuracy.

Abstract: A recent series of works has shown that initialization-free BA can be
achieved using pseudo Object Space Error (pOSE) as a surrogate objective. The
initial reconstruction-step optimizes an objective where all terms are
projectively invariant and it cannot incorporate knowledge of the camera
calibration. As a result, the solution is only determined up to a projective
transformation of the scene and the process requires more data for successful
reconstruction.
  In contrast, we present a method that is able to use the known camera
calibration thereby producing near metric solutions, that is, reconstructions
that are accurate up to a similarity transformation. To achieve this we
introduce pairwise relative rotation estimates that carry information about
camera calibration. These are only invariant to similarity transformations,
thus encouraging solutions that preserve metric features of the real scene. Our
method can be seen as integrating rotation averaging into the pOSE framework
striving towards initialization-free calibrated SfM.
  Our experimental evaluation shows that we are able to reliably optimize our
objective, achieving convergence to the global minimum with high probability
from random starting solutions, resulting in accurate near metric
reconstructions.

</details>


### [350] [MadCLIP: Few-shot Medical Anomaly Detection with CLIP](https://arxiv.org/abs/2506.23810)
*Mahshid Shiri,Cigdem Beyan,Vittorio Murino*

Main category: cs.CV

TL;DR: A novel few-shot anomaly detection method using CLIP for medical data, achieving superior performance in classification and segmentation without synthetic data or memory banks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of few-shot anomaly detection in medical data by leveraging pre-trained models and improving semantic alignment.

Method: Uses a dual-branch design with learnable adapters in CLIP, learnable text prompts, and SigLIP loss for handling unpaired text prompts.

Result: Outperforms existing methods in same-dataset and cross-dataset evaluations for anomaly classification and segmentation.

Conclusion: The approach is effective, adaptable, and validated across multiple modalities, with each component contributing to its success.

Abstract: An innovative few-shot anomaly detection approach is presented, leveraging
the pre-trained CLIP model for medical data, and adapting it for both
image-level anomaly classification (AC) and pixel-level anomaly segmentation
(AS). A dual-branch design is proposed to separately capture normal and
abnormal features through learnable adapters in the CLIP vision encoder. To
improve semantic alignment, learnable text prompts are employed to link visual
features. Furthermore, SigLIP loss is applied to effectively handle the
many-to-one relationship between images and unpaired text prompts, showcasing
its adaptation in the medical field for the first time. Our approach is
validated on multiple modalities, demonstrating superior performance over
existing methods for AC and AS, in both same-dataset and cross-dataset
evaluations. Unlike prior work, it does not rely on synthetic data or memory
banks, and an ablation study confirms the contribution of each component. The
code is available at https://github.com/mahshid1998/MadCLIP.

</details>


### [351] [Imagine for Me: Creative Conceptual Blending of Real Images and Text via Blended Attention](https://arxiv.org/abs/2506.24085)
*Wonwoong Cho,Yanxia Zhang,Yan-Ying Chen,David I. Inouye*

Main category: cs.CV

TL;DR: IT-Blender, a T2I diffusion adapter, automates blending visual and textual concepts to enhance creativity, outperforming baselines by leveraging pretrained models and blended attention.


<details>
  <summary>Details</summary>
Motivation: Human cross-modal conceptual blending is prone to cognitive biases like design fixation, limiting creativity. IT-Blender aims to automate and improve this process.

Method: IT-Blender uses pretrained diffusion models (SD and FLUX) to blend latent representations of a clean reference image with a noisy generated image, aided by novel blended attention for detail preservation and disentanglement.

Result: IT-Blender outperforms baselines in blending visual and textual concepts, demonstrating its effectiveness in augmenting human creativity.

Conclusion: IT-Blender showcases the potential of image generative models to enhance creativity by automating cross-modal conceptual blending.

Abstract: Blending visual and textual concepts into a new visual concept is a unique
and powerful trait of human beings that can fuel creativity. However, in
practice, cross-modal conceptual blending for humans is prone to cognitive
biases, like design fixation, which leads to local minima in the design space.
In this paper, we propose a T2I diffusion adapter "IT-Blender" that can
automate the blending process to enhance human creativity. Prior works related
to cross-modal conceptual blending are limited in encoding a real image without
loss of details or in disentangling the image and text inputs. To address these
gaps, IT-Blender leverages pretrained diffusion models (SD and FLUX) to blend
the latent representations of a clean reference image with those of the noisy
generated image. Combined with our novel blended attention, IT-Blender encodes
the real reference image without loss of details and blends the visual concept
with the object specified by the text in a disentangled way. Our experiment
results show that IT-Blender outperforms the baselines by a large margin in
blending visual and textual concepts, shedding light on the new application of
image generative models to augment human creativity.

</details>


### [352] [Interpretable Zero-Shot Learning with Locally-Aligned Vision-Language Model](https://arxiv.org/abs/2506.23822)
*Shiming Chen,Bowen Duan,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CV

TL;DR: LaZSL is a locally-aligned vision-language model for interpretable zero-shot learning, improving interpretability and accuracy without extra training.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs like CLIP lack interpretability in predictions due to global similarity computation. LaZSL aims to align local visual features with attributes for better explainability.

Method: LaZSL uses optimal transport for local visual-semantic alignment, linking visual regions to attributes without additional training.

Result: The method enhances interpretability, improves accuracy, and shows strong domain generalization in experiments.

Conclusion: LaZSL provides an effective, interpretable solution for zero-shot learning by aligning local features with attributes.

Abstract: Large-scale vision-language models (VLMs), such as CLIP, have achieved
remarkable success in zero-shot learning (ZSL) by leveraging large-scale
visual-text pair datasets. However, these methods often lack interpretability,
as they compute the similarity between an entire query image and the embedded
category words, making it difficult to explain their predictions. One approach
to address this issue is to develop interpretable models by integrating
language, where classifiers are built using discrete attributes, similar to
human perception. This introduces a new challenge: how to effectively align
local visual features with corresponding attributes based on pre-trained VLMs.
To tackle this, we propose LaZSL, a locally-aligned vision-language model for
interpretable ZSL. LaZSL employs local visual-semantic alignment via optimal
transport to perform interaction between visual regions and their associated
attributes, facilitating effective alignment and providing interpretable
similarity without the need for additional training. Extensive experiments
demonstrate that our method offers several advantages, including enhanced
interpretability, improved accuracy, and strong domain generalization. Codes
available at: https://github.com/shiming-chen/LaZSL.

</details>


### [353] [Flash-VStream: Efficient Real-Time Understanding for Long Video Streams](https://arxiv.org/abs/2506.23825)
*Haoji Zhang,Yiqin Wang,Yansong Tang,Yong Liu,Jiashi Feng,Xiaojie Jin*

Main category: cs.CV

TL;DR: Flash-VStream is an efficient video language model for processing long videos, reducing latency with a novel Flash Memory module.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with long videos due to computational overhead and inefficiency.

Method: Uses a Flash Memory module with low-capacity context memory and high-capacity augmentation memory for efficient processing.

Result: Achieves state-of-the-art performance on benchmarks like EgoSchema and MLVU, with reduced inference latency.

Conclusion: Flash-VStream offers efficient, real-time processing for long videos, outperforming existing methods.

Abstract: Benefiting from the advances in large language models and cross-modal
alignment, existing multimodal large language models have achieved prominent
performance in image and short video understanding. However, the understanding
of long videos is still challenging, as their long-context nature results in
significant computational and memory overhead. Most existing work treats long
videos in the same way as short videos, which is inefficient for real-world
applications and hard to generalize to even longer videos. To address these
issues, we propose Flash-VStream, an efficient video language model capable of
processing extremely long videos and responding to user queries in real time.
Particularly, we design a Flash Memory module, containing a low-capacity
context memory to aggregate long-context temporal information and model the
distribution of information density, and a high-capacity augmentation memory to
retrieve detailed spatial information based on this distribution. Compared to
existing models, Flash-VStream achieves significant reductions in inference
latency. Extensive experiments on long video benchmarks and comprehensive video
benchmarks, i.e., EgoSchema, MLVU, LVBench, MVBench and Video-MME, demonstrate
the state-of-the-art performance and outstanding efficiency of our method. Code
is available at https://github.com/IVGSZ/Flash-VStream.

</details>


### [354] [Spatially Gene Expression Prediction using Dual-Scale Contrastive Learning](https://arxiv.org/abs/2506.23827)
*Mingcheng Qu,Yuncong Wu,Donglin Di,Yue Gao,Tonghua Su,Yang Song,Lei Fan*

Main category: cs.CV

TL;DR: NH2ST is a framework for predicting gene expression from pathology images by integrating spatial context and multi-modal data, outperforming existing methods by over 20% in PCC metrics.


<details>
  <summary>Details</summary>
Motivation: Existing methods for predicting gene expression from pathology images lack spatial and molecular interaction modeling, leading to poor performance in capturing cross-modal relationships.

Method: NH2ST uses a query branch and neighbor branch with cross-attention and contrastive learning to process target patches and neighboring regions, integrating pathology and gene modalities.

Result: The model outperforms existing methods on six datasets, achieving over 20% improvement in PCC metrics.

Conclusion: NH2ST effectively addresses the limitations of current methods by leveraging spatial context and multi-modal data for accurate gene expression prediction.

Abstract: Spatial transcriptomics (ST) provides crucial insights into tissue
micro-environments, but is limited to its high cost and complexity. As an
alternative, predicting gene expression from pathology whole slide images (WSI)
is gaining increasing attention. However, existing methods typically rely on
single patches or a single pathology modality, neglecting the complex spatial
and molecular interactions between target and neighboring information (e.g.,
gene co-expression). This leads to a failure in establishing connections among
adjacent regions and capturing intricate cross-modal relationships. To address
these issues, we propose NH2ST, a framework that integrates spatial context and
both pathology and gene modalities for gene expression prediction. Our model
comprises a query branch and a neighbor branch to process paired target patch
and gene data and their neighboring regions, where cross-attention and
contrastive learning are employed to capture intrinsic associations and ensure
alignments between pathology and gene expression. Extensive experiments on six
datasets demonstrate that our model consistently outperforms existing methods,
achieving over 20% in PCC metrics. Codes are available at
https://github.com/MCPathology/NH2ST

</details>


### [355] [Low-latency vision transformers via large-scale multi-head attention](https://arxiv.org/abs/2506.23832)
*Ronit D. Gross,Tal Halevi,Ella Koresh,Yarden Tzach,Ido Kanter*

Main category: cs.CV

TL;DR: The paper explores spontaneous symmetry breaking in multi-head attention (MHA) in transformers, generalizing it to large-scale MHA (LS-MHA) for improved classification accuracy and reduced latency.


<details>
  <summary>Details</summary>
Motivation: To understand and leverage the learning mechanism of MHA in transformers, particularly how heads focus on subsets of labels, and to improve classification tasks.

Method: Quantifies single-head performance (SHP) analogous to CNNs, analyzes SHP matrices, and replaces initial transformer blocks with convolutional layers for efficiency.

Result: Increased signal-to-noise ratio (SNR) and superior accuracy in vision transformers (ViTs), with reduced latency without compromising accuracy.

Conclusion: The findings suggest potential extensions to NLP tasks and highlight the advantages of hybrid architectures combining CNNs and transformers.

Abstract: The emergence of spontaneous symmetry breaking among a few heads of
multi-head attention (MHA) across transformer blocks in classification tasks
was recently demonstrated through the quantification of single-nodal
performance (SNP). This finding indicates that each head focuses its attention
on a subset of labels through cooperation among its SNPs. This underlying
learning mechanism is generalized to large-scale MHA (LS-MHA) using a single
matrix value representing single-head performance (SHP), analogous to
single-filter performance in convolutional neural networks (CNNs). The results
indicate that each SHP matrix comprises multiple unit clusters such that each
label being explicitly recognized by a few heads with negligible noise. This
leads to an increased signal-to-noise ratio (SNR) along the transformer blocks,
thereby improving classification accuracy. These features give rise to several
distinct vision transformer (ViT) architectures that achieve the same accuracy
but differ in their LS-MHA structures. As a result, their soft committee yields
superior accuracy, an outcome not typically observed in CNNs which rely on
hundreds of filters. In addition, a significant reduction in latency is
achieved without affecting the accuracy by replacing the initial transformer
blocks with convolutional layers. This substitution accelerates early-stage
learning, which is then improved by subsequent transformer layers. The
extension of this learning mechanism to natural language processing tasks,
based on quantitative differences between CNNs and ViT architectures, has the
potential to yield new insights in deep learning. The findings are demonstrated
using compact convolutional transformer architectures trained on the CIFAR-100
dataset.

</details>


### [356] [FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation](https://arxiv.org/abs/2506.24125)
*Jiacheng Cui,Xinyue Bi,Yaxin Luo,Xiaohan Zhao,Jiacheng Liu,Zhiqiang Shen*

Main category: cs.CV

TL;DR: The paper introduces Data Residual Matching (FADRM) for dataset distillation, improving efficiency and accuracy by leveraging data-level skip connections and optimization refinements.


<details>
  <summary>Details</summary>
Motivation: To explore the untapped potential of residual connections in data-centric approaches and address data information vanishing in dataset distillation.

Method: Proposes Data Residual Matching, using data-level skip connections to balance new knowledge and core local information, with optimization-level refinements for efficiency.

Result: Achieves 47.7% test accuracy on ImageNet-1K (single-model) and 50.0% (multi-model), reducing training time and GPU memory usage by 50%.

Conclusion: FADRM sets a new state-of-the-art, outperforming existing methods in both efficiency and effectiveness for dataset distillation.

Abstract: Residual connection has been extensively studied and widely applied at the
model architecture level. However, its potential in the more challenging
data-centric approaches remains unexplored. In this work, we introduce the
concept of Data Residual Matching for the first time, leveraging data-level
skip connections to facilitate data generation and mitigate data information
vanishing. This approach maintains a balance between newly acquired knowledge
through pixel space optimization and existing core local information
identification within raw data modalities, specifically for the dataset
distillation task. Furthermore, by incorporating optimization-level
refinements, our method significantly improves computational efficiency,
achieving superior performance while reducing training time and peak GPU memory
usage by 50%. Consequently, the proposed method Fast and Accurate Data Residual
Matching for Dataset Distillation (FADRM) establishes a new state-of-the-art,
demonstrating substantial improvements over existing methods across multiple
dataset benchmarks in both efficiency and effectiveness. For instance, with
ResNet-18 as the student model and a 0.8% compression ratio on ImageNet-1K, the
method achieves 47.7% test accuracy in single-model dataset distillation and
50.0% in multi-model dataset distillation, surpassing RDED by +5.7% and
outperforming state-of-the-art multi-model approaches, EDC and CV-DD, by +1.4%
and +4.0%. Code is available at: https://github.com/Jiacheng8/FADRM.

</details>


### [357] [PointSSIM: A novel low dimensional resolution invariant image-to-image comparison metric](https://arxiv.org/abs/2506.23833)
*Oscar Ovanger,Ragnar Hauge,Jacob Skauvold,Michael J. Pyrcz,Jo Eidsvik*

Main category: cs.CV

TL;DR: PointSSIM is a resolution-invariant image comparison metric for binary images, using marked point patterns and anchor points for robust analysis.


<details>
  <summary>Details</summary>
Motivation: The need for a reliable method to compare binary images of varying resolutions, especially for structural analysis.

Method: Transforms binary images into marked point patterns, extracts anchor points via distance transform, and compares using a summary vector of attributes.

Result: Efficient and reliable image comparison, particularly effective for structural analysis across resolutions.

Conclusion: PointSSIM is a promising tool for applications requiring resolution-invariant structural image comparison.

Abstract: This paper presents PointSSIM, a novel low-dimensional image-to-image
comparison metric that is resolution invariant. Drawing inspiration from the
structural similarity index measure and mathematical morphology, PointSSIM
enables robust comparison across binary images of varying resolutions by
transforming them into marked point pattern representations. The key features
of the image, referred to as anchor points, are extracted from binary images by
identifying locally adaptive maxima from the minimal distance transform. Image
comparisons are then performed using a summary vector, capturing intensity,
connectivity, complexity, and structural attributes. Results show that this
approach provides an efficient and reliable method for image comparison,
particularly suited to applications requiring structural analysis across
different resolutions.

</details>


### [358] [Refine Any Object in Any Scene](https://arxiv.org/abs/2506.23835)
*Ziwei Chen,Ziling Liu,Zitong Huang,Mingqi Gao,Feng Zheng*

Main category: cs.CV

TL;DR: RAISE introduces a 3D enhancement framework to recover fine-grained object geometry and appearance in scenes with missing views, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of high-fidelity object-level modeling in scenes with missing viewpoints, critical for detailed object understanding.

Method: A two-stage refinement: substitutes degraded objects with proxies using a 3D generative model, then aligns and corrects inconsistencies.

Result: RAISE significantly outperforms existing methods in novel view synthesis and geometry completion.

Conclusion: RAISE effectively enhances object geometry and appearance in scenes with missing views, advancing detailed object modeling.

Abstract: Viewpoint missing of objects is common in scene reconstruction, as camera
paths typically prioritize capturing the overall scene structure rather than
individual objects. This makes it highly challenging to achieve high-fidelity
object-level modeling while maintaining accurate scene-level representation.
Addressing this issue is critical for advancing downstream tasks requiring
detailed object understanding and appearance modeling. In this paper, we
introduce Refine Any object In any ScenE (RAISE), a novel 3D enhancement
framework that leverages 3D generative priors to recover fine-grained object
geometry and appearance under missing views. Starting from substituting
degraded objects with proxies, via a 3D generative model with strong 3D
understanding, RAISE progressively refines geometry and texture by aligning
each proxy to its degraded counterpart in 7-DOF pose, followed by correcting
spatial and appearance inconsistencies via registration-constrained
enhancement. This two-stage refinement ensures the high-fidelity geometry and
appearance of the original object in unseen views while maintaining consistency
in spatial positioning, observed geometry, and appearance. Extensive
experiments on challenging benchmarks show that RAISE significantly outperforms
state-of-the-art methods in both novel view synthesis and geometry completion
tasks. RAISE is made publicly available at https://github.com/PolySummit/RAISE.

</details>


### [359] [RGC-VQA: An Exploration Database for Robotic-Generated Video Quality Assessment](https://arxiv.org/abs/2506.23852)
*Jianing Jin,Jiangyong Ying,Huiyu Duan,Liu Yang,Sijing Wu,Yunhao Li,Yushuo Zheng,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: The paper introduces Robotic-Generated Content (RGC) and its unique quality assessment challenges, establishing the first RGC database (RGCD) and evaluating existing VQA models.


<details>
  <summary>Details</summary>
Motivation: The rise of robotic-generated videos necessitates dedicated research on their perceptual quality, as existing VQA models are inadequate for RGC.

Method: The authors create the RGCD with 2,100 videos, conduct subjective VQA experiments, and benchmark 11 state-of-the-art VQA models.

Result: Existing VQA models perform poorly on RGC, indicating a need for specialized models.

Conclusion: The study highlights the gap in RGC quality assessment and provides a foundational database for future research.

Abstract: As camera-equipped robotic platforms become increasingly integrated into
daily life, robotic-generated videos have begun to appear on streaming media
platforms, enabling us to envision a future where humans and robots coexist. We
innovatively propose the concept of Robotic-Generated Content (RGC) to term
these videos generated from egocentric perspective of robots. The perceptual
quality of RGC videos is critical in human-robot interaction scenarios, and RGC
videos exhibit unique distortions and visual requirements that differ markedly
from those of professionally-generated content (PGC) videos and user-generated
content (UGC) videos. However, dedicated research on quality assessment of RGC
videos is still lacking. To address this gap and to support broader robotic
applications, we establish the first Robotic-Generated Content Database (RGCD),
which contains a total of 2,100 videos drawn from three robot categories and
sourced from diverse platforms. A subjective VQA experiment is conducted
subsequently to assess human visual perception of robotic-generated videos.
Finally, we conduct a benchmark experiment to evaluate the performance of 11
state-of-the-art VQA models on our database. Experimental results reveal
significant limitations in existing VQA models when applied to complex,
robotic-generated content, highlighting a critical need for RGC-specific VQA
models. Our RGCD is publicly available at:
https://github.com/IntMeGroup/RGC-VQA.

</details>


### [360] [HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity](https://arxiv.org/abs/2506.23854)
*Yida Wang,Xueyang Zhang,Kun Zhan,Peng Jia,Xianpeng Lang*

Main category: cs.CV

TL;DR: HiNeuS is a unified framework for neural surface reconstruction that addresses multi-view radiance inconsistency, missing keypoints in textureless regions, and structural degradation from Eikonal constraints. It introduces differential visibility verification, planar-conformal regularization, and physically-grounded Eikonal relaxation for cohesive integration of appearance-geometry constraints.


<details>
  <summary>Details</summary>
Motivation: The paper aims to resolve persistent challenges in neural surface reconstruction, such as geometric fidelity and photometric consistency under complex conditions, by addressing core limitations in existing methods.

Method: HiNeuS uses differential visibility verification, planar-conformal regularization, and physically-grounded Eikonal relaxation to integrate appearance-geometry constraints synergistically.

Result: The framework achieves state-of-the-art performance, reducing Chamfer distance by 21.4% and improving PSNR by 2.32 dB, with superior qualitative results in recovering specular surfaces and low-textured regions.

Conclusion: HiNeuS successfully integrates appearance and geometry constraints, demonstrating generalizability in tasks like material decomposition and view-consistent relighting.

Abstract: Neural surface reconstruction faces persistent challenges in reconciling
geometric fidelity with photometric consistency under complex scene conditions.
We present HiNeuS, a unified framework that holistically addresses three core
limitations in existing approaches: multi-view radiance inconsistency, missing
keypoints in textureless regions, and structural degradation from over-enforced
Eikonal constraints during joint optimization. To resolve these issues through
a unified pipeline, we introduce: 1) Differential visibility verification
through SDF-guided ray tracing, resolving reflection ambiguities via continuous
occlusion modeling; 2) Planar-conformal regularization via ray-aligned geometry
patches that enforce local surface coherence while preserving sharp edges
through adaptive appearance weighting; and 3) Physically-grounded Eikonal
relaxation that dynamically modulates geometric constraints based on local
radiance gradients, enabling detail preservation without sacrificing global
regularity. Unlike prior methods that handle these aspects through sequential
optimizations or isolated modules, our approach achieves cohesive integration
where appearance-geometry constraints evolve synergistically throughout
training. Comprehensive evaluations across synthetic and real-world datasets
demonstrate state-of-the-art performance, including a 21.4% reduction in
Chamfer distance over reflection-aware baselines and 2.32 dB PSNR improvement
against neural rendering counterparts. Qualitative analyses reveal superior
capability in recovering specular instruments, urban layouts with
centimeter-scale infrastructure, and low-textured surfaces without local patch
collapse. The method's generalizability is further validated through successful
application to inverse rendering tasks, including material decomposition and
view-consistent relighting.

</details>


### [361] [A Closer Look at Conditional Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2506.23856)
*Ji Zhang,Shihan Wu,Lianli Gao,Jingkuan Song,Nicu Sebe,Heng Tao Shen*

Main category: cs.CV

TL;DR: CaPT improves generalization in Prompt Tuning by using Textual Class Information (TCI)-conditioned prompts, outperforming existing methods with minimal computational cost.


<details>
  <summary>Details</summary>
Motivation: Address the Base-New Tradeoff (BNT) dilemma in Prompt Tuning (PT) by identifying suboptimal performance of Visual Image Information (VII)-conditioned prompts and proposing TCI-conditioned prompts.

Method: Propose Class-adaptive Prompt Tuning (CaPT), which learns TCI-conditioned prompts from base classes for fast adaptation to new tasks. Also integrates with DePT to form DeCaPT.

Result: CaPT improves performance of five PT baselines on 11 datasets. DeCaPT outperforms state-of-the-art by 3.49% in H ACC.

Conclusion: TCI-conditioned prompts are key to solving BNT, and CaPT/DeCaPT offer efficient, high-performance solutions for PT.

Abstract: Despite the great promise of Prompt Tuning (PT) in adapting large
Vision-Language Pretrained Models (VLPMs) to downstream tasks, they often
struggle to overcome the Base-New Tradeoff (BNT) dilemma: as VLPMs are better
tuned to a base task, their ability to generalize to new tasks diminishes.
Recent work on conditional PT addresses this problem by replacing static
prompts with dynamic Visual Image Information (VII)-conditioned prompts,
improving the model's generalization to new tasks to some extent. In this work,
we first identify a critical issue with existing conditional PT methods: using
VII as the "condition" of prompts yields suboptimal performance, and even
random noise-conditioned prompts can outperform the VII-conditioned
counterparts. On further analysis, we find that learning dynamic prompts
conditioned on Textual Class Information (TCI) is the key to solving the BNT
problem. Motivated by this, we then propose Class-adaptive Prompt Tuning
(CaPT), which enables fast adaptation of tuned models to new classes by
learning TCI-conditioned prompts from base classes. Remarkably, CaPT can be
used as a plugin to mitigate the BNT problem for existing unconditional PT
schemes. Extensive experiments on 11 datasets show that CaPT consistently
improves the performance of five strong unconditional PT baselines with
negligible additional computational cost. Additionally, by integrating CaPT
with our recently proposed DePT framework, we devise a new conditional PT
approach, termed DeCaPT, which outperforms the H ACC of the state-of-the-art
conditional PT scheme by 3.49%, averaged over the 11 datasets. Code:
https://github.com/Koorye/CaPT.

</details>


### [362] [VMoBA: Mixture-of-Block Attention for Video Diffusion Models](https://arxiv.org/abs/2506.23858)
*Jianzong Wu,Liang Hou,Haotian Yang,Xin Tao,Ye Tian,Pengfei Wan,Di Zhang,Yunhai Tong*

Main category: cs.CV

TL;DR: VMoBA is a sparse attention mechanism for Video Diffusion Models (VDMs) that improves efficiency and quality by adapting to spatio-temporal patterns, achieving significant speedups in training and inference.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of full attention in VDMs limits long-duration, high-resolution video generation. Existing sparse methods are suboptimal for video data.

Method: VMoBA introduces layer-wise block partitioning, global block selection, and threshold-based block selection to optimize spatio-temporal attention.

Result: VMoBA achieves 2.92x FLOPs and 1.48x latency speedup in training, with comparable or better generation quality. Inference sees 2.40x FLOPs and 1.35x speedup.

Conclusion: VMoBA effectively addresses attention bottlenecks in VDMs, offering efficiency and quality improvements for video generation.

Abstract: The quadratic complexity of full attention mechanisms poses a significant
bottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration,
high-resolution videos. While various sparse attention methods have been
proposed, many are designed as training-free inference accelerators or do not
optimally capture the unique spatio-temporal characteristics inherent in video
data when trained natively. This paper introduces Video Mixture of Block
Attention (VMoBA), a novel sparse attention mechanism specifically adapted for
VDMs. Motivated by an in-depth analysis of attention patterns within
pre-trained video transformers, which revealed strong spatio-temporal locality,
varying query importance, and head-specific concentration levels, VMoBA
enhances the original MoBA framework with three key modifications: (1) a
layer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to
diverse spatio-temporal attention patterns and improve efficiency; (2) global
block selection to prioritize the most salient query-key block interactions
across an entire attention head; and (3) threshold-based block selection to
dynamically determine the number of attended blocks based on their cumulative
similarity. Extensive experiments demonstrate that VMoBA significantly
accelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and
1.48x latency speedup, while attaining comparable or even superior generation
quality to full attention. Furthermore, VMoBA exhibits competitive performance
in training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for
high-res video generation.

</details>


### [363] [Puzzles: Unbounded Video-Depth Augmentation for Scalable End-to-End 3D Reconstruction](https://arxiv.org/abs/2506.23863)
*Jiahao Ma,Lei Wang,Miaomiao liu,David Ahmedt-Aristizabal,Chuong Nguyen*

Main category: cs.CV

TL;DR: Puzzles is a data augmentation method for multi-view 3D reconstruction, enhancing training data diversity and performance without altering network architecture.


<details>
  <summary>Details</summary>
Motivation: Overcoming limitations in training data diversity and scale for 3D reconstruction methods like DUST3R.

Method: Synthesizes high-quality posed video-depth data from single images or clips using simulated camera trajectories and scene geometry.

Result: Boosts performance; models trained on 10% of original data with Puzzles match full-dataset accuracy.

Conclusion: Puzzles effectively augments data, improving 3D reconstruction without architectural changes.

Abstract: Multi-view 3D reconstruction remains a core challenge in computer vision.
Recent methods, such as DUST3R and its successors, directly regress pointmaps
from image pairs without relying on known scene geometry or camera parameters.
However, the performance of these models is constrained by the diversity and
scale of available training data. In this work, we introduce Puzzles, a data
augmentation strategy that synthesizes an unbounded volume of high-quality
posed video-depth data from a single image or video clip. By simulating diverse
camera trajectories and realistic scene geometry through targeted image
transformations, Puzzles significantly enhances data variety. Extensive
experiments show that integrating Puzzles into existing video-based 3D
reconstruction pipelines consistently boosts performance without modifying the
underlying network architecture. Notably, models trained on only ten percent of
the original data augmented with Puzzles still achieve accuracy comparable to
those trained on the full dataset. Code is available at
https://jiahao-ma.github.io/puzzles/.

</details>


### [364] [PriOr-Flow: Enhancing Primitive Panoramic Optical Flow with Orthogonal View](https://arxiv.org/abs/2506.23897)
*Longliang Liu,Miaojie Feng,Junda Cheng,Jijun Xiang,Xuan Zhu,Xin Yang*

Main category: cs.CV

TL;DR: PriOr-Flow is a dual-branch framework enhancing panoramic optical flow by leveraging orthogonal views to reduce distortion, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Conventional optical flow methods perform poorly in polar regions due to distortions from sphere-to-plane projections.

Method: Uses a Dual-Cost Collaborative Lookup (DCCL) operator and Ortho-Driven Distortion Compensation (ODDC) module to refine motion features and mitigate distortion.

Result: Achieves state-of-the-art performance on panoramic optical flow datasets.

Conclusion: PriOr-Flow effectively addresses distortion challenges in panoramic optical flow, setting a new benchmark.

Abstract: Panoramic optical flow enables a comprehensive understanding of temporal
dynamics across wide fields of view. However, severe distortions caused by
sphere-to-plane projections, such as the equirectangular projection (ERP),
significantly degrade the performance of conventional perspective-based optical
flow methods, especially in polar regions. To address this challenge, we
propose PriOr-Flow, a novel dual-branch framework that leverages the
low-distortion nature of the orthogonal view to enhance optical flow estimation
in these regions. Specifically, we introduce the Dual-Cost Collaborative Lookup
(DCCL) operator, which jointly retrieves correlation information from both the
primitive and orthogonal cost volumes, effectively mitigating distortion noise
during cost volume construction. Furthermore, our Ortho-Driven Distortion
Compensation (ODDC) module iteratively refines motion features from both
branches, further suppressing polar distortions. Extensive experiments
demonstrate that PriOr-Flow is compatible with various perspective-based
iterative optical flow methods and consistently achieves state-of-the-art
performance on publicly available panoramic optical flow datasets, setting a
new benchmark for wide-field motion estimation. The code is publicly available
at: https://github.com/longliangLiu/PriOr-Flow.

</details>


### [365] [Three-dimensional end-to-end deep learning for brain MRI analysis](https://arxiv.org/abs/2506.23916)
*Radhika Juglan,Marta Ligero,Zunamys I. Carrero,Asier Rabasco,Tim Lenz,Leo Misera,Gregory Patrick Veldhuizen,Paul Kuntke,Hagen H. Kitzler,Sven Nebelung,Daniel Truhn,Jakob Nikolas Kather*

Main category: cs.CV

TL;DR: Simpler convolutional networks (SFCN) outperform complex architectures (DenseNet, Swin Transformers) in age and sex prediction from brain MRI, showing better generalizability across diverse cohorts.


<details>
  <summary>Details</summary>
Motivation: Assess generalizability of deep learning methods in brain imaging, focusing on age and sex prediction, given their neurobiological significance.

Method: Evaluated three 3D architectures (SFCN, DenseNet, Swin Transformers) using T1-weighted MRI from four cohorts (UKB, DLBS, PPMI, IXI) for age and sex prediction.

Result: SFCN outperformed others with AUC 1.00 (UKB) and 0.85-0.91 (external) for sex, MAE 2.66 (UKB) and 4.98-5.81 (external) for age. Superiority confirmed via statistical tests.

Conclusion: Simpler networks generalize better than complex architectures in brain image analysis, highlighting their robustness across diverse datasets.

Abstract: Deep learning (DL) methods are increasingly outperforming classical
approaches in brain imaging, yet their generalizability across diverse imaging
cohorts remains inadequately assessed. As age and sex are key neurobiological
markers in clinical neuroscience, influencing brain structure and disease risk,
this study evaluates three of the existing three-dimensional architectures,
namely Simple Fully Connected Network (SFCN), DenseNet, and Shifted Window
(Swin) Transformers, for age and sex prediction using T1-weighted MRI from four
independent cohorts: UK Biobank (UKB, n=47,390), Dallas Lifespan Brain Study
(DLBS, n=132), Parkinson's Progression Markers Initiative (PPMI, n=108 healthy
controls), and Information eXtraction from Images (IXI, n=319). We found that
SFCN consistently outperformed more complex architectures with AUC of 1.00
[1.00-1.00] in UKB (internal test set) and 0.85-0.91 in external test sets for
sex classification. For the age prediction task, SFCN demonstrated a mean
absolute error (MAE) of 2.66 (r=0.89) in UKB and 4.98-5.81 (r=0.55-0.70) across
external datasets. Pairwise DeLong and Wilcoxon signed-rank tests with
Bonferroni corrections confirmed SFCN's superiority over Swin Transformer
across most cohorts (p<0.017, for three comparisons). Explainability analysis
further demonstrates the regional consistency of model attention across cohorts
and specific to each task. Our findings reveal that simpler convolutional
networks outperform the denser and more complex attention-based DL
architectures in brain image analysis by demonstrating better generalizability
across different datasets.

</details>


### [366] [Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers](https://arxiv.org/abs/2506.23918)
*Zhaochen Su,Peng Xia,Hangyu Guo,Zhenhua Liu,Yan Ma,Xiaoye Qu,Jiaqi Liu,Yanshu Li,Kaide Zeng,Zhengyuan Yang,Linjie Li,Yu Cheng,Heng Ji,Junxian He,Yi R.,Fung*

Main category: cs.CV

TL;DR: The paper surveys the shift from text-centric multimodal reasoning to models that dynamically think with images, outlining a three-stage evolution and key contributions.


<details>
  <summary>Details</summary>
Motivation: The text-centric approach in multimodal reasoning creates a semantic gap between perceptual data and symbolic thought. The paper aims to explore models that use vision dynamically, akin to human cognition.

Method: The survey establishes principles of the 'think with image' paradigm, reviews core methods for each stage, analyzes benchmarks, and identifies challenges.

Result: The paper provides a structured overview of the evolution in multimodal reasoning, highlighting key stages and contributions.

Conclusion: The survey offers a roadmap for future research to develop more powerful and human-aligned multimodal AI.

Abstract: Recent progress in multimodal reasoning has been significantly advanced by
textual Chain-of-Thought (CoT), a paradigm where models conduct reasoning
within language. This text-centric approach, however, treats vision as a
static, initial context, creating a fundamental "semantic gap" between rich
perceptual data and discrete symbolic thought. Human cognition often transcends
language, utilizing vision as a dynamic mental sketchpad. A similar evolution
is now unfolding in AI, marking a fundamental paradigm shift from models that
merely think about images to those that can truly think with images. This
emerging paradigm is characterized by models leveraging visual information as
intermediate steps in their thought process, transforming vision from a passive
input into a dynamic, manipulable cognitive workspace. In this survey, we chart
this evolution of intelligence along a trajectory of increasing cognitive
autonomy, which unfolds across three key stages: from external tool
exploration, through programmatic manipulation, to intrinsic imagination. To
structure this rapidly evolving field, our survey makes four key contributions.
(1) We establish the foundational principles of the think with image paradigm
and its three-stage framework. (2) We provide a comprehensive review of the
core methods that characterize each stage of this roadmap. (3) We analyze the
critical landscape of evaluation benchmarks and transformative applications.
(4) We identify significant challenges and outline promising future directions.
By providing this structured overview, we aim to offer a clear roadmap for
future research towards more powerful and human-aligned multimodal AI.

</details>


### [367] [Evaluating the Impact of Khmer Font Types on Text Recognition](https://arxiv.org/abs/2506.23963)
*Vannkinh Nom,Souhail Bakkali,Muhammad Muzzamil Luqman,Mickael Coustaty,Jean-Marc Ogier*

Main category: cs.CV

TL;DR: The study evaluates the impact of 19 Khmer fonts on OCR accuracy, identifying high-performing (e.g., Khmer, Odor MeanChey) and low-performing fonts (e.g., iSeth First, Bayon).


<details>
  <summary>Details</summary>
Motivation: Text recognition is challenging for complex scripts like Khmer due to font diversity, necessitating an evaluation of font impact on OCR accuracy.

Method: Used Pytesseract to assess OCR accuracy across 19 randomly selected Khmer fonts.

Result: High accuracy for fonts like Khmer and Odor MeanChey; low accuracy for iSeth First and Bayon.

Conclusion: Font selection is crucial for optimizing Khmer OCR, offering insights for robust system development.

Abstract: Text recognition is significantly influenced by font types, especially for
complex scripts like Khmer. The variety of Khmer fonts, each with its unique
character structure, presents challenges for optical character recognition
(OCR) systems. In this study, we evaluate the impact of 19 randomly selected
Khmer font types on text recognition accuracy using Pytesseract. The fonts
include Angkor, Battambang, Bayon, Bokor, Chenla, Dangrek, Freehand, Kh Kompong
Chhnang, Kh SN Kampongsom, Khmer, Khmer CN Stueng Songke, Khmer Savuth Pen,
Metal, Moul, Odor MeanChey, Preah Vihear, Siemreap, Sithi Manuss, and iSeth
First. Our comparison of OCR performance across these fonts reveals that Khmer,
Odor MeanChey, Siemreap, Sithi Manuss, and Battambang achieve high accuracy,
while iSeth First, Bayon, and Dangrek perform poorly. This study underscores
the critical importance of font selection in optimizing Khmer text recognition
and provides valuable insights for developing more robust OCR systems.

</details>


### [368] [Visual and Memory Dual Adapter for Multi-Modal Object Tracking](https://arxiv.org/abs/2506.23972)
*Boyue Xu,Ruichao Hou,Tongwei Ren,Gangshan Wu*

Main category: cs.CV

TL;DR: The paper proposes a dual adapter (VMDA) for multi-modal tracking, combining visual and memory adapters to enhance robustness by leveraging frequency, spatial, and temporal cues.


<details>
  <summary>Details</summary>
Motivation: Existing prompt-learning-based trackers underutilize critical cues across frequency and temporal domains, limiting reliability.

Method: Introduces VMDA with a visual adapter for adaptive feature transfer and a memory adapter for temporal cue propagation.

Result: Achieves state-of-the-art performance in RGB-Thermal, RGB-Depth, and RGB-Event tracking tasks.

Conclusion: VMDA effectively improves multi-modal tracking by integrating visual and temporal cues, validated by extensive experiments.

Abstract: Prompt-learning-based multi-modal trackers have achieved promising progress
by employing lightweight visual adapters to incorporate auxiliary modality
features into frozen foundation models. However, existing approaches often
struggle to learn reliable prompts due to limited exploitation of critical cues
across frequency and temporal domains. In this paper, we propose a novel visual
and memory dual adapter (VMDA) to construct more robust and discriminative
representations for multi-modal tracking. Specifically, we develop a simple but
effective visual adapter that adaptively transfers discriminative cues from
auxiliary modality to dominant modality by jointly modeling the frequency,
spatial, and channel-wise features. Additionally, we design the memory adapter
inspired by the human memory mechanism, which stores global temporal cues and
performs dynamic update and retrieval operations to ensure the consistent
propagation of reliable temporal information across video sequences. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
on the various multi-modal tracking tasks, including RGB-Thermal, RGB-Depth,
and RGB-Event tracking. Code and models are available at
https://github.com/xuboyue1999/mmtrack.git.

</details>


### [369] [Toward Simple and Robust Contrastive Explanations for Image Classification by Leveraging Instance Similarity and Concept Relevance](https://arxiv.org/abs/2506.23975)
*Yuliia Kaidashova,Bettina Finzel,Ute Schmid*

Main category: cs.CV

TL;DR: The paper introduces a concept-based method for contrastive explanations in image classification, analyzing explanation complexity and robustness under image augmentations.


<details>
  <summary>Details</summary>
Motivation: To understand why a classification model prefers one class over another by providing human-understandable contrastive explanations.

Method: Leverages instance embeddings and human-understandable concepts, extracts concepts with relevance scores, computes contrasts for similar instances, and evaluates explanation complexity and robustness.

Result: Higher concept relevance leads to simpler explanations, while lower relevance results in more complex ones. Explanations show varying robustness under augmentations.

Conclusion: The findings highlight the potential for more interpretable and robust AI systems through concept-based contrastive explanations.

Abstract: Understanding why a classification model prefers one class over another for
an input instance is the challenge of contrastive explanation. This work
implements concept-based contrastive explanations for image classification by
leveraging the similarity of instance embeddings and relevance of
human-understandable concepts used by a fine-tuned deep learning model. Our
approach extracts concepts with their relevance score, computes contrasts for
similar instances, and evaluates the resulting contrastive explanations based
on explanation complexity. Robustness is tested for different image
augmentations. Two research questions are addressed: (1) whether explanation
complexity varies across different relevance ranges, and (2) whether
explanation complexity remains consistent under image augmentations such as
rotation and noise. The results confirm that for our experiments higher concept
relevance leads to shorter, less complex explanations, while lower relevance
results in longer, more diffuse explanations. Additionally, explanations show
varying degrees of robustness. The discussion of these findings offers insights
into the potential of building more interpretable and robust AI systems.

</details>


### [370] [StyleDrive: Towards Driving-Style Aware Benchmarking of End-To-End Autonomous Driving](https://arxiv.org/abs/2506.23982)
*Ruiyang Hao,Bowen Jing,Haibao Yu,Zaiqing Nie*

Main category: cs.CV

TL;DR: The paper addresses the lack of personalization in end-to-end autonomous driving (E2EAD) by introducing a large-scale dataset with diverse driving preferences, enabling the development and evaluation of personalized models.


<details>
  <summary>Details</summary>
Motivation: Personalization is crucial for trust and adoption of autonomous vehicles, but existing E2EAD systems lack datasets capturing diverse driving preferences.

Method: The authors create a dataset with static and dynamic features, derive objective and subjective preference annotations using a visual language model (VLM), and validate labels through human-in-the-loop verification.

Result: Incorporating personalized preferences improves alignment with human driving behavior, as demonstrated by benchmarking state-of-the-art models.

Conclusion: This work establishes a foundation for personalized E2EAD, providing a standardized platform for integrating human preferences and advancing human-centric autonomy.

Abstract: While personalization has been explored in traditional autonomous driving
systems, it remains largely overlooked in end-to-end autonomous driving
(E2EAD), despite its growing prominence. This gap is critical, as user-aligned
behavior is essential for trust, comfort, and widespread adoption of autonomous
vehicles. A core challenge is the lack of large-scale real-world datasets
annotated with diverse and fine-grained driving preferences, hindering the
development and evaluation of personalized E2EAD models. In this work, we
present the first large-scale real-world dataset enriched with annotations
capturing diverse driving preferences, establishing a foundation for
personalization in E2EAD. We extract static environmental features from
real-world road topology and infer dynamic contextual cues using a fine-tuned
visual language model (VLM), enabling consistent and fine-grained scenario
construction. Based on these scenarios, we derive objective preference
annotations through behavioral distribution analysis and rule-based heuristics.
To address the inherent subjectivity of driving style, we further employ the
VLM to generate subjective annotations by jointly modeling scene semantics and
driver behavior. Final high-quality labels are obtained through a
human-in-the-loop verification process that fuses both perspectives. Building
on this dataset, we propose the first benchmark for evaluating personalized
E2EAD models. We assess several state-of-the-art models with and without
preference conditioning, demonstrating that incorporating personalized
preferences results in behavior more aligned with human driving. Our work lays
the foundation for personalized E2EAD by providing a standardized platform to
systematically integrate human preferences into data-driven E2EAD systems,
catalyzing future research in human-centric autonomy.

</details>


### [371] [Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data](https://arxiv.org/abs/2506.24039)
*Shubhabrata Mukherjee,Jack Lang,Obeen Kwon,Iryna Zenyuk,Valerie Brogden,Adam Weber,Daniela Ushizima*

Main category: cs.CV

TL;DR: Zenesis is a no-code platform for scientific image analysis, outperforming traditional and advanced methods with high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Zero-shot and prompt-based methods struggle with scarce scientific images, necessitating a solution like Zenesis to overcome data readiness barriers.

Method: Zenesis uses lightweight multi-modal adaptation, human-in-the-loop refinement, and heuristic-based temporal enhancement for zero-shot operation on raw scientific data.

Result: Zenesis achieved high accuracy (0.947-0.987), IOU (0.857-0.858), and Dice scores (0.923) on FIB-SEM data, surpassing baselines like Otsu and SAM.

Conclusion: Zenesis is a powerful tool for scientific imaging, especially where annotated datasets are scarce, enabling faster and more accurate analysis.

Abstract: Zero-shot and prompt-based technologies capitalized on using frequently
occurring images to transform visual reasoning tasks, which explains why such
technologies struggle with valuable yet scarce scientific image sets. In this
work, we propose Zenesis, a comprehensive no-code interactive platform designed
to minimize barriers posed by data readiness for scientific images. We develop
lightweight multi-modal adaptation techniques that enable zero-shot operation
on raw scientific data, along with human-in-the-loop refinement and
heuristic-based temporal enhancement options. We demonstrate the performance of
our approach through comprehensive comparison and validation on challenging
Focused Ion Beam Scanning Electron Microscopy (FIB-SEM) data of catalyst-loaded
membranes. Zenesis significantly outperforms baseline methods, achieving an
average accuracy of 0.947, an Intersection over Union (IOU) of 0.858, and a
Dice score of 0.923 for amorphous catalyst samples and accuracy of 0.987, an
IOU of 0.857, and a Dice score of 0.923 for crystalline samples. These results
mark a substantial improvement over traditional methods like Otsu thresholding
and even advanced models like Segment Anything Model (SAM) when used in
isolation. Our results demonstrate that Zenesis is a powerful tool for
scientific applications, particularly in fields where high-quality annotated
datasets are unavailable, accelerating accurate analysis of experimental
imaging.

</details>


### [372] [Continual Adaptation: Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios](https://arxiv.org/abs/2506.24063)
*Deng Li,Aming Wu,Yang Li,Yaowei Wang,Yahong Han*

Main category: cs.CV

TL;DR: The paper proposes a novel method for continual test-time adaptation in object detection, using a dual-path LoRA-based adapter and conditional diffusion for parameter generation to improve generalization and avoid performance degradation.


<details>
  <summary>Details</summary>
Motivation: Environments change over time and space, challenging object detectors trained on closed-set assumptions. Existing fine-tuning methods can degrade performance due to limited test images.

Method: The approach includes a dual-path LoRA-based domain-aware adapter, conditional diffusion-based parameter generation, and class-centered optimal transport alignment to mitigate forgetting.

Result: Experiments show the method's effectiveness in continuous domain adaptation, with improved generalization and object-related information capture.

Conclusion: The proposed mechanism enhances adaptation and generalization in dynamic environments while preventing catastrophic forgetting.

Abstract: In practice, environments constantly change over time and space, posing
significant challenges for object detectors trained based on a closed-set
assumption, i.e., training and test data share the same distribution. To this
end, continual test-time adaptation has attracted much attention, aiming to
improve detectors' generalization by fine-tuning a few specific parameters,
e.g., BatchNorm layers. However, based on a small number of test images,
fine-tuning certain parameters may affect the representation ability of other
fixed parameters, leading to performance degradation. Instead, we explore a new
mechanism, i.e., converting the fine-tuning process to a specific-parameter
generation. Particularly, we first design a dual-path LoRA-based domain-aware
adapter that disentangles features into domain-invariant and domain-specific
components, enabling efficient adaptation. Additionally, a conditional
diffusion-based parameter generation mechanism is presented to synthesize the
adapter's parameters based on the current environment, preventing the
optimization from getting stuck in local optima. Finally, we propose a
class-centered optimal transport alignment method to mitigate catastrophic
forgetting. Extensive experiments conducted on various continuous domain
adaptive object detection tasks demonstrate the effectiveness. Meanwhile,
visualization results show that the representation extracted by the generated
parameters can capture more object-related information and strengthen the
generalization ability.

</details>


### [373] [WaRA: Wavelet Low Rank Adaptation](https://arxiv.org/abs/2506.24092)
*Moein Heidari,Yasamin Medghalchi,Mahdi Khoursha,Reza Rezaeian,Ilker Hacihaliloglu*

Main category: cs.CV

TL;DR: WaRA introduces wavelet transforms for PEFT, improving multi-resolution analysis and outperforming LoRA in vision and language tasks.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods like LoRA use global low-rank factorizations, missing local or multi-scale structures in weight updates.

Method: WaRA uses wavelet transforms to decompose weight updates into multi-resolution representations, enabling low-rank factorization in the wavelet domain.

Result: WaRA excels in vision tasks (image generation, classification, segmentation) and shows promise in language tasks, reducing computational complexity.

Conclusion: WaRA offers a flexible, sparse, and efficient PEFT method, generalizing well across tasks.

Abstract: Parameter-efficient fine-tuning (PEFT) has gained widespread adoption across
various applications. Among PEFT techniques, Low-Rank Adaptation (LoRA) and its
extensions have emerged as particularly effective, allowing efficient model
adaptation while significantly reducing computational overhead. However,
existing approaches typically rely on global low-rank factorizations, which
overlook local or multi-scale structure, failing to capture complex patterns in
the weight updates. To address this, we propose WaRA, a novel PEFT method that
leverages wavelet transforms to decompose the weight update matrix into a
multi-resolution representation. By performing low-rank factorization in the
wavelet domain and reconstructing updates through an inverse transform, WaRA
obtains compressed adaptation parameters that harness multi-resolution
analysis, enabling it to capture both coarse and fine-grained features while
providing greater flexibility and sparser representations than standard LoRA.
Through comprehensive experiments and analysis, we demonstrate that WaRA
performs superior on diverse vision tasks, including image generation,
classification, and semantic segmentation, significantly enhancing generated
image quality while reducing computational complexity. Although WaRA was
primarily designed for vision tasks, we further showcase its effectiveness in
language tasks, highlighting its broader applicability and generalizability.
The code is publicly available at
\href{GitHub}{https://github.com/moeinheidari7829/WaRA}.

</details>


### [374] [MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction](https://arxiv.org/abs/2506.24096)
*Antoine Guédon,Diego Gomez,Nissim Maruani,Bingchen Gong,George Drettakis,Maks Ovsjanikov*

Main category: cs.CV

TL;DR: MILo introduces a differentiable Gaussian Splatting framework to extract accurate surface meshes directly from 3D Gaussians, preserving geometric details and reducing vertex count.


<details>
  <summary>Details</summary>
Motivation: Current methods for extracting surface meshes from Gaussian Splatting lose fine details or produce overly dense meshes, limiting downstream applications.

Method: MILo uses a differentiable procedure to construct meshes from Gaussians, featuring bidirectional consistency, adaptive mesh extraction, and signed distance computation.

Result: The method achieves state-of-the-art quality with fewer vertices, producing lightweight meshes suitable for simulations and animation.

Conclusion: MILo bridges volumetric and surface representations, enabling high-quality mesh extraction without post-processing.

Abstract: While recent advances in Gaussian Splatting have enabled fast reconstruction
of high-quality 3D scenes from images, extracting accurate surface meshes
remains a challenge. Current approaches extract the surface through costly
post-processing steps, resulting in the loss of fine geometric details or
requiring significant time and leading to very dense meshes with millions of
vertices. More fundamentally, the a posteriori conversion from a volumetric to
a surface representation limits the ability of the final mesh to preserve all
geometric structures captured during training. We present MILo, a novel
Gaussian Splatting framework that bridges the gap between volumetric and
surface representations by differentiably extracting a mesh from the 3D
Gaussians. We design a fully differentiable procedure that constructs the
mesh-including both vertex locations and connectivity-at every iteration
directly from the parameters of the Gaussians, which are the only quantities
optimized during training. Our method introduces three key technical
contributions: a bidirectional consistency framework ensuring both
representations-Gaussians and the extracted mesh-capture the same underlying
geometry during training; an adaptive mesh extraction process performed at each
training iteration, which uses Gaussians as differentiable pivots for Delaunay
triangulation; a novel method for computing signed distance values from the 3D
Gaussians that enables precise surface extraction while avoiding geometric
erosion. Our approach can reconstruct complete scenes, including backgrounds,
with state-of-the-art quality while requiring an order of magnitude fewer mesh
vertices than previous methods. Due to their light weight and empty interior,
our meshes are well suited for downstream applications such as physics
simulations or animation.

</details>


### [375] [DenseWorld-1M: Towards Detailed Dense Grounded Caption in the Real World](https://arxiv.org/abs/2506.24102)
*Xiangtai Li,Tao Zhang,Yanwei Li,Haobo Yuan,Shihao Chen,Yikang Zhou,Jiahao Meng,Yueyi Sun,Shilin Xu,Lu Qi,Tianheng Cheng,Yi Lin,Zilong Huang,Wenhao Huang,Jiashi Feng,Guang Shi*

Main category: cs.CV

TL;DR: DenseWorld-1M is a new dataset addressing gaps in existing caption datasets by providing detailed, dense grounded captions for high-resolution images, using a three-stage labeling pipeline and two VLM models.


<details>
  <summary>Details</summary>
Motivation: Existing caption datasets lack detailed descriptions, relations, and object-level grounding, limiting the potential of MLLMs.

Method: A three-stage labeling pipeline (open-world perception, detailed object caption generation, dense caption merging) and two VLM models (Detailed Region Caption, Spatial Caption Merging) are introduced.

Result: DenseWorld-1M proves effective in vision-language understanding, visual grounding, and region caption generation tasks.

Conclusion: The dataset and models fill a critical gap, enhancing multimodal understanding and grounding capabilities.

Abstract: Multimodal Large Language Models (MLLMs) demonstrate a complex understanding
of scenes, benefiting from large-scale and high-quality datasets. Most existing
caption datasets lack the ground locations and relations for visual entities.
Several grounded caption datasets face the problems of missing detailed
descriptions, relations, and massive object descriptions on high-resolution
images. To fill this gap for the community, we present DenseWorld-1M, the first
massive, detailed, dense grounded caption dataset in the real world. We design
a three-stage labeling pipeline, containing open-world perception, detailed
object caption generation, and dense caption merging. The first stage obtains
entity-level masks and labels. The second stage generates the object-level,
detailed captions with the guidance of masks and labels from the first stage.
The final stage merges object captions and masks into spatial and relational
dense captions. To accelerate the labeling process and improve caption quality,
we present two VLM models: the Detailed Region Caption model and the Spatial
Caption Merging model. Extensive experiments on various settings, including
vision-language understanding, visual grounding, and region caption generation,
demonstrate the effectiveness of our DenseWorld-1M dataset and labeling models.

</details>


### [376] [Epona: Autoregressive Diffusion World Model for Autonomous Driving](https://arxiv.org/abs/2506.24113)
*Kaiwen Zhang,Zhenyu Tang,Xiaotao Hu,Xingang Pan,Xiaoyang Guo,Yuan Liu,Jingwei Huang,Li Yuan,Qian Zhang,Xiao-Xiao Long,Xun Cao,Wei Yin*

Main category: cs.CV

TL;DR: Epona introduces an autoregressive diffusion world model for autonomous driving, improving long-horizon predictions and integrating trajectory planning via decoupled spatiotemporal factorization and modular prediction.


<details>
  <summary>Details</summary>
Motivation: Existing video diffusion-based world models lack flexibility for long-horizon predictions and struggle to integrate trajectory planning due to fixed-length frame modeling.

Method: Epona uses decoupled spatiotemporal factorization and modular trajectory-video prediction, along with a chain-of-forward training strategy to reduce autoregressive errors.

Result: Achieves 7.4% FVD improvement and longer prediction duration, outperforming prior works. Also serves as a real-time motion planner, excelling on NAVSIM benchmarks.

Conclusion: Epona advances autonomous driving world modeling by enabling localized, long-horizon predictions and seamless planning integration.

Abstract: Diffusion models have demonstrated exceptional visual quality in video
generation, making them promising for autonomous driving world modeling.
However, existing video diffusion-based world models struggle with
flexible-length, long-horizon predictions and integrating trajectory planning.
This is because conventional video diffusion models rely on global joint
distribution modeling of fixed-length frame sequences rather than sequentially
constructing localized distributions at each timestep. In this work, we propose
Epona, an autoregressive diffusion world model that enables localized
spatiotemporal distribution modeling through two key innovations: 1) Decoupled
spatiotemporal factorization that separates temporal dynamics modeling from
fine-grained future world generation, and 2) Modular trajectory and video
prediction that seamlessly integrate motion planning with visual modeling in an
end-to-end framework. Our architecture enables high-resolution, long-duration
generation while introducing a novel chain-of-forward training strategy to
address error accumulation in autoregressive loops. Experimental results
demonstrate state-of-the-art performance with 7.4\% FVD improvement and minutes
longer prediction duration compared to prior works. The learned world model
further serves as a real-time motion planner, outperforming strong end-to-end
planners on NAVSIM benchmarks. Code will be publicly available at
\href{https://github.com/Kevin-thu/Epona/}{https://github.com/Kevin-thu/Epona/}.

</details>


### [377] [TextMesh4D: High-Quality Text-to-4D Mesh Generation](https://arxiv.org/abs/2506.24121)
*Sisi Dai,Xinxin Su,Boyan Wan,Ruizhen Hu,Kai Xu*

Main category: cs.CV

TL;DR: TextMesh4D is a novel framework for high-quality text-to-4D generation, leveraging per-face Jacobians and a two-stage process for static object creation and dynamic motion synthesis.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the unexplored challenge of dynamic 3D content generation (text-to-4D) with diffusion guidance, aiming to advance beyond current text-to-image and text-to-3D models.

Method: The approach uses per-face Jacobians as a differentiable mesh representation, decomposing 4D generation into static object creation and dynamic motion synthesis, with a flexibility-rigidity regularization term for stability.

Result: TextMesh4D achieves state-of-the-art results in temporal consistency, structural fidelity, and visual realism, while operating efficiently on a single 24GB GPU.

Conclusion: TextMesh4D offers a cost-effective, high-quality solution for text-driven 4D mesh generation, with plans to release code for future research.

Abstract: Recent advancements in diffusion generative models significantly advanced
image, video, and 3D content creation from user-provided text prompts. However,
the challenging problem of dynamic 3D content generation (text-to-4D) with
diffusion guidance remains largely unexplored. In this paper, we introduce
TextMesh4D, a novel framework for high-quality text-to-4D generation. Our
approach leverages per-face Jacobians as a differentiable mesh representation
and decomposes 4D generation into two stages: static object creation and
dynamic motion synthesis. We further propose a flexibility-rigidity
regularization term to stabilize Jacobian optimization under video diffusion
priors, ensuring robust geometric performance. Experiments demonstrate that
TextMesh4D achieves state-of-the-art results in terms of temporal consistency,
structural fidelity, and visual realism. Moreover, TextMesh4D operates with a
low GPU memory overhead-requiring only a single 24GB GPU-offering a
cost-effective yet high-quality solution for text-driven 4D mesh generation.
The code will be released to facilitate future research in text-to-4D
generation.

</details>


### [378] [Calligrapher: Freestyle Text Image Customization](https://arxiv.org/abs/2506.24123)
*Yue Ma,Qingyan Bai,Hao Ouyang,Ka Leong Cheng,Qiuyu Wang,Hongyu Liu,Zichen Liu,Haofan Wang,Jingye Chen,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: Calligrapher is a diffusion-based framework for digital calligraphy, combining text customization and artistic typography with self-distillation, localized style injection, and in-context generation for precise style control.


<details>
  <summary>Details</summary>
Motivation: To address challenges in precise style control and data dependency for typographic customization in digital calligraphy and design.

Method: Uses self-distillation to create a style-centric benchmark, a trainable style encoder (Qformer + linear layers) for localized style injection, and in-context generation for embedding reference images into denoising.

Result: Accurately reproduces stylistic details and glyph positioning, outperforming traditional models in diverse fonts and design contexts.

Conclusion: Calligrapher automates high-quality typography, enhancing digital art, branding, and typographic design.

Abstract: We introduce Calligrapher, a novel diffusion-based framework that
innovatively integrates advanced text customization with artistic typography
for digital calligraphy and design applications. Addressing the challenges of
precise style control and data dependency in typographic customization, our
framework incorporates three key technical contributions. First, we develop a
self-distillation mechanism that leverages the pre-trained text-to-image
generative model itself alongside the large language model to automatically
construct a style-centric typography benchmark. Second, we introduce a
localized style injection framework via a trainable style encoder, which
comprises both Qformer and linear layers, to extract robust style features from
reference images. An in-context generation mechanism is also employed to
directly embed reference images into the denoising process, further enhancing
the refined alignment of target styles. Extensive quantitative and qualitative
evaluations across diverse fonts and design contexts confirm Calligrapher's
accurate reproduction of intricate stylistic details and precise glyph
positioning. By automating high-quality, visually consistent typography,
Calligrapher surpasses traditional models, empowering creative practitioners in
digital art, branding, and contextual typographic design.

</details>


### [379] [How to Design and Train Your Implicit Neural Representation for Video Compression](https://arxiv.org/abs/2506.24127)
*Matthew Gwilliam,Roy Zhang,Namitha Padmanabhan,Hongyang Du,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: The paper introduces Rabbit NeRV (RNeRV), a state-of-the-art implicit neural representation (INR) method for video compression, improving PSNR and addressing slow encoding speeds using hyper-networks.


<details>
  <summary>Details</summary>
Motivation: Current INR methods for video compression suffer from slow encoding speeds due to per-sample network training, limiting practical adoption.

Method: The authors analyze NeRV family methods, propose RNeRV for better performance, and investigate hyper-networks to enable real-time encoding by predicting INR weights.

Result: RNeRV achieves +1.27% PSNR improvement over alternatives. Hyper-networks with masked weights improve PSNR and MS-SSIM by 1.7% at 0.037 bpp.

Conclusion: The work advances video INR design, offering faster encoding and higher quality compression, with open-source code and project details available.

Abstract: Implicit neural representation (INR) methods for video compression have
recently achieved visual quality and compression ratios that are competitive
with traditional pipelines. However, due to the need for per-sample network
training, the encoding speeds of these methods are too slow for practical
adoption. We develop a library to allow us to disentangle and review the
components of methods from the NeRV family, reframing their performance in
terms of not only size-quality trade-offs, but also impacts on training time.
We uncover principles for effective video INR design and propose a
state-of-the-art configuration of these components, Rabbit NeRV (RNeRV). When
all methods are given equal training time (equivalent to 300 NeRV epochs) for 7
different UVG videos at 1080p, RNeRV achieves +1.27% PSNR on average compared
to the best-performing alternative for each video in our NeRV library. We then
tackle the encoding speed issue head-on by investigating the viability of
hyper-networks, which predict INR weights from video inputs, to disentangle
training from encoding to allow for real-time encoding. We propose masking the
weights of the predicted INR during training to allow for variable, higher
quality compression, resulting in 1.7% improvements to both PSNR and MS-SSIM at
0.037 bpp on the UCF-101 dataset, and we increase hyper-network parameters by
0.4% for 2.5%/2.7% improvements to PSNR/MS-SSIM with equal bpp and similar
speeds. Our project website is available at https://mgwillia.github.io/vinrb/
and our code is available at https://github.com/mgwillia/vinrb.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [380] [Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans](https://arxiv.org/abs/2506.22439)
*Javier Conde,Miguel González,María Grandury,Gonzalo Martínez,Pedro Reviriego,Mar Brysbaert*

Main category: cs.CL

TL;DR: The paper evaluates LLMs' alignment with human psycholinguistic ratings on word features, finding better alignment with Glasgow norms than Lancaster norms, suggesting limitations in sensory associations.


<details>
  <summary>Details</summary>
Motivation: To assess how well LLMs align with human ratings on psycholinguistic word features, leveraging existing datasets to uncover potential limitations in LLMs' understanding of sensory and embodied language.

Method: Evaluation of LLMs using two psycholinguistic datasets (Glasgow and Lancaster norms) covering thirteen word features, comparing alignment with human ratings.

Result: LLMs align better with Glasgow norms (arousal, valence, etc.) than Lancaster norms (sensory features), indicating limitations in sensory associations.

Conclusion: Current LLMs may lack embodied cognition for sensory associations, highlighting the value of psycholinguistic evaluations to uncover model limitations.

Abstract: The evaluation of LLMs has so far focused primarily on how well they can
perform different tasks such as reasoning, question-answering, paraphrasing, or
translating. For most of these tasks, performance can be measured with
objective metrics, such as the number of correct answers. However, other
language features are not easily quantified. For example, arousal,
concreteness, or gender associated with a given word, as well as the extent to
which we experience words with senses and relate them to a specific sense.
Those features have been studied for many years by psycholinguistics,
conducting large-scale experiments with humans to produce ratings for thousands
of words. This opens an opportunity to evaluate how well LLMs align with human
ratings on these word features, taking advantage of existing studies that cover
many different language features in a large number of words. In this paper, we
evaluate the alignment of a representative group of LLMs with human ratings on
two psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets
cover thirteen features over thousands of words. The results show that
alignment is \textcolor{black}{generally} better in the Glasgow norms evaluated
(arousal, valence, dominance, concreteness, imageability, familiarity, and
gender) than on the Lancaster norms evaluated (introceptive, gustatory,
olfactory, haptic, auditory, and visual). This suggests a potential limitation
of current LLMs in aligning with human sensory associations for words, which
may be due to their lack of embodied cognition present in humans and
illustrates the usefulness of evaluating LLMs with psycholinguistic datasets.

</details>


### [381] [AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents](https://arxiv.org/abs/2506.22485)
*Sudip Dasgupta,Himanshu Shankar*

Main category: cs.CL

TL;DR: A modular AI system for reviewing structured business documents outperforms humans in consistency, speed, and accuracy, though it requires human oversight in specialized domains.


<details>
  <summary>Details</summary>
Motivation: To improve the review of structured enterprise documents by addressing limitations of prior solutions focused on unstructured texts or basic compliance checks.

Method: Uses AI agents orchestrated with tools like LangChain and CrewAI for section-by-section evaluation, enforcing standardized outputs and continuous feedback loops.

Result: Achieves 99% consistency (vs. 92% for humans), reduces review time from 30 to 2.5 minutes, and halves error/bias rates, with 95% agreement with human experts.

Conclusion: The system is scalable and flexible for enterprise use but requires human oversight in specialized areas and faces operational costs for large-scale LLM usage.

Abstract: This study presents a modular, multi-agent system for the automated review of
highly structured enterprise business documents using AI agents. Unlike prior
solutions focused on unstructured texts or limited compliance checks, this
framework leverages modern orchestration tools such as LangChain, CrewAI,
TruLens, and Guidance to enable section-by-section evaluation of documents for
accuracy, consistency, completeness, and clarity. Specialized agents, each
responsible for discrete review criteria such as template compliance or factual
correctness, operate in parallel or sequence as required. Evaluation outputs
are enforced to a standardized, machine-readable schema, supporting downstream
analytics and auditability. Continuous monitoring and a feedback loop with
human reviewers allow for iterative system improvement and bias mitigation.
  Quantitative evaluation demonstrates that the AI Agent-as-Judge system
approaches or exceeds human performance in key areas: achieving 99% information
consistency (vs. 92% for humans), halving error and bias rates, and reducing
average review time from 30 to 2.5 minutes per document, with a 95% agreement
rate between AI and expert human judgment. While promising for a wide range of
industries, the study also discusses current limitations, including the need
for human oversight in highly specialized domains and the operational cost of
large-scale LLM usage. The proposed system serves as a flexible, auditable, and
scalable foundation for AI-driven document quality assurance in the enterprise
context.

</details>


### [382] [Hallucination Detection with Small Language Models](https://arxiv.org/abs/2506.22486)
*Ming Cheung*

Main category: cs.CL

TL;DR: A framework using multiple small language models to verify LLM responses detects hallucinations, improving F1 scores by 10%.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLM responses reduce reliability, especially without ground truth. A scalable verification method is needed.

Method: Integrates small models to verify LLM responses by breaking them into sentences and using "Yes" token probabilities.

Result: 10% improvement in F1 scores for detecting correct responses vs. hallucinations.

Conclusion: Multiple small models effectively verify LLM responses, offering a scalable solution.

Abstract: Since the introduction of ChatGPT, large language models (LLMs) have
demonstrated significant utility in various tasks, such as answering questions
through retrieval-augmented generation. Context can be retrieved using a
vectorized database, serving as a foundation for LLMs to generate responses.
However, hallucinations in responses can undermine the reliability of LLMs in
practical applications, and they are not easily detectable in the absence of
ground truth, particularly in question-and-answer scenarios. This paper
proposes a framework that integrates multiple small language models to verify
responses generated by LLMs using the retrieved context from a vectorized
database. By breaking down the responses into individual sentences and
utilizing the probability of generating "Yes" tokens from the outputs of
multiple models for a given set of questions, responses, and relevant context,
hallucinations can be detected. The proposed framework is validated through
experiments with real datasets comprising over 100 sets of questions, answers,
and contexts, including responses with fully and partially correct sentences.
The results demonstrate a 10\% improvement in F1 scores for detecting correct
responses compared to hallucinations, indicating that multiple small language
models can be effectively employed for answer verification, providing a
scalable and efficient solution for both academic and practical applications.

</details>


### [383] [PromptAug: Fine-grained Conflict Classification Using Data Augmentation](https://arxiv.org/abs/2506.22491)
*Oliver Warke,Joemon M. Jose,Faegheh Hasibi,Jan Breitsohl*

Main category: cs.CL

TL;DR: PromptAug is an LLM-based data augmentation method improving accuracy and F1-score by 2% for conflict detection, addressing data scarcity and LLM guardrails.


<details>
  <summary>Details</summary>
Motivation: High-quality labeled data for nuanced tasks like conflict detection is scarce and expensive, and social media restrictions limit research data access.

Method: Introduces PromptAug, an LLM-based data augmentation method, evaluated in extreme data scarcity scenarios with diversity and thematic analysis.

Result: Achieves 2% improvement in accuracy and F1-score; identifies four problematic patterns in augmented text.

Conclusion: PromptAug is effective for sensitive tasks like conflict detection, combining NLP and social science methods.

Abstract: Given the rise of conflicts on social media, effective classification models
to detect harmful behaviours are essential. Following the
garbage-in-garbage-out maxim, machine learning performance depends heavily on
training data quality. However, high-quality labelled data, especially for
nuanced tasks like identifying conflict behaviours, is limited, expensive, and
difficult to obtain. Additionally, as social media platforms increasingly
restrict access to research data, text data augmentation is gaining attention
as an alternative to generate training data. Augmenting conflict-related data
poses unique challenges due to Large Language Model (LLM) guardrails that
prevent generation of offensive content. This paper introduces PromptAug, an
innovative LLM-based data augmentation method. PromptAug achieves statistically
significant improvements of 2% in both accuracy and F1-score on conflict and
emotion datasets. To thoroughly evaluate PromptAug against other data
augmentation methods we conduct a robust evaluation using extreme data scarcity
scenarios, quantitative diversity analysis and a qualitative thematic analysis.
The thematic analysis identifies four problematic patterns in augmented text:
Linguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and
Augmented Content Misinterpretation.
  Overall, this work presents PromptAug as an effective method for augmenting
data in sensitive tasks like conflict detection, offering a unique,
interdisciplinary evaluation grounded in both natural language processing and
social science methodology.

</details>


### [384] [AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text](https://arxiv.org/abs/2506.22508)
*Chenyang Shao,Tianxing Li,Chenhao Pu,Fengli Xu,Yong Li*

Main category: cs.CL

TL;DR: AgentStealth is a framework using locally deployed smaller-scale language models (SLMs) for text anonymization, outperforming baselines in effectiveness and utility while avoiding cloud reliance.


<details>
  <summary>Details</summary>
Motivation: Existing anonymization methods either harm utility or pose privacy risks, highlighting the need for effective, locally deployable solutions.

Method: AgentStealth combines adversarial anonymization, supervised adaptation of SLMs, and online reinforcement learning for iterative improvement.

Result: Outperforms baselines by 12.3% in anonymization effectiveness and 6.8% in utility, with lightweight edge deployment.

Conclusion: AgentStealth offers a privacy-preserving, efficient solution for text anonymization, suitable for edge devices.

Abstract: In today's digital world, casual user-generated content often contains subtle
cues that may inadvertently expose sensitive personal attributes. Such risks
underscore the growing importance of effective text anonymization to safeguard
individual privacy. However, existing methods either rely on rigid replacements
that damage utility or cloud-based LLMs that are costly and pose privacy risks.
To address these issues, we explore the use of locally deployed smaller-scale
language models (SLMs) for anonymization. Yet training effective SLMs remains
challenging due to limited high-quality supervision. To address the challenge,
we propose AgentStealth, a self-reinforcing LLM anonymization framework.First,
we introduce an adversarial anonymization workflow enhanced by In-context
Contrastive Learning and Adaptive Utility-Aware Control. Second, we perform
supervised adaptation of SLMs using high-quality data collected from the
workflow, which includes both anonymization and attack signals. Finally, we
apply online reinforcement learning where the model leverages its internal
adversarial feedback to iteratively improve anonymization performance.
Experiments on two datasets show that our method outperforms baselines in both
anonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight
design supports direct deployment on edge devices, avoiding cloud reliance and
communication-based privacy risks. Our code is open-source at
https://github.com/tsinghua-fib-lab/AgentStealth.

</details>


### [385] [Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning](https://arxiv.org/abs/2506.22510)
*Zihao Zhao,Xinlong Zhai,Jinyu Yang,Chuan Shi*

Main category: cs.CL

TL;DR: The paper introduces MDGCL, a framework for multi-domain pre-training and cross-domain transfer in graph data, addressing domain-specific differences to improve representation learning.


<details>
  <summary>Details</summary>
Motivation: Current graph foundation models fail to account for domain-specific differences, limiting their effectiveness in multi-domain scenarios.

Method: MDGCL uses a novel contrastive learning strategy with domain tokens and a domain attention mechanism for fine-grained knowledge transfer.

Result: MDGCL outperforms state-of-the-art methods, achieving up to 19.33% improvement in accuracy and 19.13% in Macro-F1 score.

Conclusion: The proposed framework effectively addresses domain differences, enhancing graph representation learning and transferability.

Abstract: Foundation models have achieved great success in natural language processing
(NLP) and computer vision (CV). Their success largely stems from the ability to
integrate multi-domain knowledge in pre-training and transfer it to target
domains. Considering graph data, especially graphs without textual features, is
ubiquitous in real-world applications such as social networks and
recommendation systems, some researchers have attempted to extend this paradigm
to the graph field, aiming to construct graph foundation models. However,
unlike CV and NLP, there are huge gaps among the semantics and properties of
graphs in different domains, while current works still adopt traditional
contrastive pre-training strategies designed in the single-domain scenario,
which regard contrastive samples from different domains as equivalent. From
experimental investigations, we discovered that inherent domain-specific
differences prevent these strategies from effectively absorbing knowledge from
different domains to generate informative representations. In this paper, we
propose a novel multi-domain pre-training and cross-domain transfer framework,
namely MDGCL.In the pre-training stage, we design a contrastive learning
strategy to substantially recognize and capture domain differences, and
introduce domain tokens to encode domain-level global information. In the
downstream stage, we introduce a domain attention mechanism to enable
fine-grained domain knowledge transfer. Extensive experiments on five benchmark
datasets have demonstrated that our method outperforms state-of-the-art
significantly, with the maximum improvement of 19.33\% on accuracy and 19.13\%
on Macro-F1 score.

</details>


### [386] [Can "consciousness" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis](https://arxiv.org/abs/2506.22516)
*Jingkai Li*

Main category: cs.CL

TL;DR: The study applies Integrated Information Theory (IIT) 3.0 and 4.0 to analyze Large Language Model (LLM) representations using Theory of Mind (ToM) test data, finding no significant indicators of consciousness but revealing patterns under spatio-permutational analyses.


<details>
  <summary>Details</summary>
Motivation: To investigate whether differences in ToM test performances in LLM representations can be explained by IIT metrics and to differentiate between consciousness phenomena and inherent separations in LLM representational space.

Method: Applied IIT 3.0 and 4.0 metrics ($\Phi^{\max}$, $\Phi$, Conceptual Information, $\Phi$-structure) to LLM representations and compared them with Span Representations. Conducted experiments across LLM transformer layers and linguistic spans.

Result: No statistically significant indicators of consciousness were found in LLM representations, but patterns emerged under spatio-permutational analyses.

Conclusion: Contemporary Transformer-based LLM representations lack evidence of consciousness phenomena, though further investigation of patterns is warranted.

Abstract: Integrated Information Theory (IIT) provides a quantitative framework for
explaining consciousness phenomenon, positing that conscious systems comprise
elements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the
latest iterations of this framework -- to sequences of Large Language Model
(LLM) representations, analyzing data derived from existing Theory of Mind
(ToM) test results. Our study systematically investigates whether the
differences of ToM test performances, when presented in the LLM
representations, can be revealed by IIT estimates, i.e., $\Phi^{\max}$ (IIT
3.0), $\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\Phi$-structure
(IIT 4.0). Furthermore, we compare these metrics with the Span Representations
independent of any estimate for consciousness. This additional effort aims to
differentiate between potential "consciousness" phenomena and inherent
separations within LLM representational space. We conduct comprehensive
experiments examining variations across LLM transformer layers and linguistic
spans from stimuli. Our results suggest that sequences of contemporary
Transformer-based LLM representations lack statistically significant indicators
of observed "consciousness" phenomena but exhibit intriguing patterns under
$\textit{spatio}$-permutational analyses. The Appendix and code are available
as Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.

</details>


### [387] [Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation](https://arxiv.org/abs/2506.22518)
*Deyu Zou,Yongqiang Chen,Mufei Li,Siqi Miao,Chenxi Liu,Bo Han,James Cheng,Pan Li*

Main category: cs.CL

TL;DR: ReG improves graph-based RAG by aligning weak retrievers with LLMs, using feedback to remove spurious signals and reorganizing retrieved knowledge into coherent evidence chains, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of weak retrievers in graph-based RAG, which introduce spurious signals and unorganized knowledge due to lack of ground truth and graph abstraction.

Method: Introduces ReG, which uses LLM feedback to refine supervision and a structure-aware module to reorganize retrieved knowledge into coherent evidence chains.

Result: ReG improves performance by up to 10%, matches SOTA with 5% training data, reduces reasoning token cost by 30%, and boosts performance by 4% for reasoning-based LLMs.

Conclusion: ReG effectively enhances graph-based RAG by refining supervision and reorganizing knowledge, demonstrating broad applicability and efficiency gains.

Abstract: Graph-based retrieval-augmented generation (RAG) enables large language
models (LLMs) to ground responses with structured external knowledge from
up-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs
often rely on a weak retriever in graph-based RAG: I) Due to the lack of ground
truth, the retriever is often trained on weak supervision, which often
introduces spurious signals to the LLMs. II) Due to the abstraction of graph
data, the retrieved knowledge is often presented in unorganized forms. To
mitigate the issue, we present Refined Graph-based RAG (ReG) to align weak
retrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM
feedback to get rid of spurious signals and improve the quality of the
supervision. Meanwhile, ReG introduces a structure-aware reorganization module
to refactor the retrieval results into logically coherent evidence chains.
Experiments on prominent benchmarks demonstrate that ReG significantly and
consistently brings improvements across different LLM backbones by up to 10%.
The improved supervision quality enables ReG to match the state-of-the-art
performance with 5% training data and to transfer to out-of-distribution KGs.
Notably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token
cost by up to 30% and improves the performance by up to 4%.

</details>


### [388] [MisinfoTeleGraph: Network-driven Misinformation Detection for German Telegram Messages](https://arxiv.org/abs/2506.22529)
*Lu Kalkbrenner,Veronika Solopova,Steffen Zeiler,Robert Nickel,Dorothea Kolossa*

Main category: cs.CL

TL;DR: Misinfo-TeleGraph is a German-language Telegram dataset for misinformation detection, featuring 5M+ messages, metadata, and labels. GraphSAGE with LSTM outperforms text-only models, highlighting the role of network structure in detection.


<details>
  <summary>Details</summary>
Motivation: Addressing the underutilization of connectivity and message propagation in misinformation detection, especially on poorly moderated platforms like Telegram, which is critical in the German electoral context.

Method: Introduces Misinfo-TeleGraph, a dataset with metadata, channel relationships, and labels (weak/strong). Evaluates text-only models and GNNs (e.g., GraphSAGE with LSTM) using forwarding as network structure.

Result: GraphSAGE with LSTM aggregation outperforms text-only models in MCC and F1-score. Impact of subscribers, view counts, and label types (automatic vs. human) is evaluated.

Conclusion: Provides a benchmark and open dataset for future research on misinformation detection in low-moderation platforms, emphasizing the potential and challenges of weak supervision.

Abstract: Connectivity and message propagation are central, yet often underutilized,
sources of information in misinformation detection -- especially on poorly
moderated platforms such as Telegram, which has become a critical channel for
misinformation dissemination, namely in the German electoral context. In this
paper, we introduce Misinfo-TeleGraph, the first German-language Telegram-based
graph dataset for misinformation detection. It includes over 5 million messages
from public channels, enriched with metadata, channel relationships, and both
weak and strong labels. These labels are derived via semantic similarity to
fact-checks and news articles using M3-embeddings, as well as manual
annotation. To establish reproducible baselines, we evaluate both text-only
models and graph neural networks (GNNs) that incorporate message forwarding as
a network structure. Our results show that GraphSAGE with LSTM aggregation
significantly outperforms text-only baselines in terms of Matthews Correlation
Coefficient (MCC) and F1-score. We further evaluate the impact of subscribers,
view counts, and automatically versus human-created labels on performance, and
highlight both the potential and challenges of weak supervision in this domain.
This work provides a reproducible benchmark and open dataset for future
research on misinformation detection in German-language Telegram networks and
other low-moderation social platforms.

</details>


### [389] [RExBench: Can coding agents autonomously implement AI research extensions?](https://arxiv.org/abs/2506.22598)
*Nicholas Edwards,Yukyung Lee,Yujun,Mao,Yulu Qin,Sebastian Schuster,Najoung Kim*

Main category: cs.CL

TL;DR: RExBench evaluates LLM agents' ability to autonomously implement research extensions, finding current agents fall short despite some improvement with hints.


<details>
  <summary>Details</summary>
Motivation: To assess the capability of LLM agents in performing realistic research experiment extensions, a critical skill for autonomous research systems.

Method: Introduces RExBench, a benchmark with 12 tasks extending existing research, evaluated using automatic execution and domain expert instructions.

Result: All nine LLM agents tested failed most tasks; success rates improved with hints but remained below 40%.

Conclusion: Current LLM agents lack the autonomy to handle realistic research extensions without significant human guidance.

Abstract: Agents based on Large Language Models (LLMs) have shown promise for
performing sophisticated software engineering tasks autonomously. In addition,
there has been progress towards developing agents that can perform parts of the
research pipeline in machine learning and the natural sciences. We argue that
research extension and its implementation is a critical capability for such
systems, and introduce RExBench to support the evaluation of this capability.
RExBench is a benchmark consisting of 12 realistic research experiment
implementation tasks that aim to investigate research hypotheses that have not
previously been implemented. Each task is set up as an extension to an existing
research paper and codebase, accompanied by domain expert-written instructions.
RExBench is robust to data contamination, and supports an automatic evaluation
infrastructure that executes agent outputs to determine whether the success
criteria are met. We use this benchmark to evaluate nine LLM agents implemented
using three different frameworks: aider, Claude Code, and OpenHands. We find
that all agents evaluated fail to autonomously implement the majority of the
extensions. Although the success rate improves with additional human-written
hints, the best performance under this setting remains below 40%. This
indicates that current agents are still short of being able to handle realistic
research extension tasks without substantial human guidance.

</details>


### [390] [Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks](https://arxiv.org/abs/2506.22623)
*Badr Youbi Idrissi,Monica Millunzi,Amelia Sorrenti,Lorenzo Baraldi,Daryna Dementieva*

Main category: cs.CL

TL;DR: The paper introduces a new watermarking method for detecting synthetic text generated by LLMs, aiming to ensure ethical use. It replicates a baseline study, identifies its limitations, and proposes a robust alternative, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address concerns about misuse of LLMs by developing a reliable method for detecting machine-generated text, ensuring ethical AI applications.

Method: Replicates a baseline study, identifies its flaws, and proposes a novel watermarking technique. Evaluates robustness using paraphrased text.

Result: The proposed method shows superior robustness compared to existing watermarking techniques.

Conclusion: The new watermarking approach effectively detects synthetic text, enhancing ethical use of LLMs in AI-generated content.

Abstract: In the present-day scenario, Large Language Models (LLMs) are establishing
their presence as powerful instruments permeating various sectors of society.
While their utility offers valuable support to individuals, there are multiple
concerns over potential misuse. Consequently, some academic endeavors have
sought to introduce watermarking techniques, characterized by the inclusion of
markers within machine-generated text, to facilitate algorithmic
identification. This research project is focused on the development of a novel
methodology for the detection of synthetic text, with the overarching goal of
ensuring the ethical application of LLMs in AI-driven text generation. The
investigation commences with replicating findings from a previous baseline
study, thereby underscoring its susceptibility to variations in the underlying
generation model. Subsequently, we propose an innovative watermarking approach
and subject it to rigorous evaluation, employing paraphrased generated text to
asses its robustness. Experimental results highlight the robustness of our
proposal compared to the~\cite{aarson} watermarking method.

</details>


### [391] [Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge](https://arxiv.org/abs/2506.22644)
*Chase Fensore,Kaustubh Dhole,Joyce C Ho,Eugene Agichtein*

Main category: cs.CL

TL;DR: A hybrid RAG system combining sparse (BM25) and dense (E5) retrieval with Falcon3-10B-Instruct was evaluated on dynamic test sets, showing improved performance with neural re-ranking but high computational costs. The system ranked 4th in faithfulness and 11th in correctness.


<details>
  <summary>Details</summary>
Motivation: To evaluate retrieval-augmented generation (RAG) systems on dynamic test sets using the FineWeb-10BT corpus, aiming for relevant and faithful answers.

Method: Combines sparse (BM25) and dense (E5) retrieval, uses Falcon3-10B-Instruct for generation, and evaluates with synthetic questions. Neural re-ranking (RankLLaMA) and DSPy-optimized prompting were tested.

Result: Neural re-ranking improved MAP by 52% but was computationally expensive. DSPy-optimized prompting increased semantic similarity but had 0% refusal rates. The hybrid system ranked 4th in faithfulness and 11th in correctness.

Conclusion: Vocabulary alignment between questions and documents was key to performance. The hybrid system without re-ranking balanced performance and computational efficiency.

Abstract: We present our submission to the LiveRAG Challenge 2025, which evaluates
retrieval-augmented generation (RAG) systems on dynamic test sets using the
FineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense
(E5) retrieval methods and then aims to generate relevant and faithful answers
with Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic
questions generated with DataMorgana across 64 unique question-user
combinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP
from 0.523 to 0.797 (52% relative improvement) but introduces prohibitive
computational costs (84s vs 1.74s per question). While DSPy-optimized prompting
strategies achieved higher semantic similarity (0.771 vs 0.668), their 0%
refusal rates raised concerns about over-confidence and generalizability. Our
submitted hybrid system without re-ranking achieved 4th place in faithfulness
and 11th place in correctness among 25 teams. Analysis across question
categories reveals that vocabulary alignment between questions and documents
was the strongest predictor of performance on our development set, with
document-similar phrasing improving cosine similarity from 0.562 to 0.762.

</details>


### [392] [Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions](https://arxiv.org/abs/2506.22679)
*Ankush Raut,Projna Paromita,Sydney Begerowski,Suzanne Bell,Theodora Chaspari*

Main category: cs.CL

TL;DR: The paper evaluates LLMs for detecting micro-behaviors in team conversations, finding decoder-only models like Llama-3.1 outperform encoder-only ones, with implications for speech technologies in high-stakes environments.


<details>
  <summary>Details</summary>
Motivation: To assess the feasibility of LLMs in identifying subtle micro-behaviors in team dialogues, especially in contexts like space missions where text data is primary.

Method: Tested zero-shot classification, fine-tuning, and paraphrase-augmented fine-tuning with encoder-only LLMs (RoBERTa, DistilBERT) and few-shot text generation with decoder-only LLMs (Llama-3.1).

Result: Encoder-only LLMs struggled with underrepresented micro-behaviors, while Llama-3.1 achieved 44% macro F1 for 3-way and 68% for binary classification.

Conclusion: Decoder-only LLMs, like Llama-3.1, are more effective for micro-behavior detection in team conversations, suggesting potential for speech technologies in high-stakes settings.

Abstract: We explore the feasibility of large language models (LLMs) in detecting
subtle expressions of micro-behaviors in team conversations using transcripts
collected during simulated space missions. Specifically, we examine zero-shot
classification, fine-tuning, and paraphrase-augmented fine-tuning with
encoder-only sequence classification LLMs, as well as few-shot text generation
with decoder-only causal language modeling LLMs, to predict the micro-behavior
associated with each conversational turn (i.e., dialogue). Our findings
indicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to
detect underrepresented micro-behaviors, particularly discouraging speech, even
with weighted fine-tuning. In contrast, the instruction fine-tuned version of
Llama-3.1, a decoder-only LLM, demonstrated superior performance, with the best
models achieving macro F1-scores of 44% for 3-way classification and 68% for
binary classification. These results have implications for the development of
speech technologies aimed at analyzing team communication dynamics and
enhancing training interventions in high-stakes environments such as space
missions, particularly in scenarios where text is the only accessible data.

</details>


### [393] [VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs](https://arxiv.org/abs/2506.22694)
*Raghavv Goel,Sudhanshu Agrawal,Mukul Gagrani,Junyoung Park,Yifan Zao,He Zhang,Tian Liu,Yiping Yang,Xin Yuan,Jiuyan Lu,Chris Lott,Mingu Lee*

Main category: cs.CL

TL;DR: The paper introduces VocabTrim, a training-free technique to reduce drafting overhead in speculative decoding by limiting the drafter's vocabulary to frequently sampled tokens, improving speed in memory-bound environments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the unnecessary inference overhead in drafting for large-vocabulary target LLMs, which slows down generation in memory-bound settings.

Method: VocabTrim reconstructs the drafter's LM head to include only frequently sampled tokens, reducing drafting latency while slightly lowering acceptance rates.

Result: The method achieves a 16% memory-bound speed-up for Llama-3.2-3B-Instruct on Spec-Bench.

Conclusion: VocabTrim effectively improves generation speed in memory-bound environments by optimizing the drafter's vocabulary, despite a minor trade-off in acceptance rates.

Abstract: In this paper, we introduce a simple training-free technique to improve the
performance of drafter-based speculative decoding (SpD) methods that
incorporates language modeling head (LM head) during drafting process. A
drafter-based speculative decoding leverages one or more smaller language
models, a.k.a. drafters or draft models, to sample a draft sequence or tree
consisting of multiple tokens, followed by verification by a base LLM, a target
model, accepting a subset as its valid generation. As it is usually considered
that the speculative decoding requires one-to-one mapping between vocabularies
of the target model and the draft model, it has been natural to share the
vocabulary between them, or even share the LM head as in EAGLE or Medusa. We
first identify that this draft token sampling scheme inherently contains an
unnecessary inference overhead in drafting, especially for some target LLMs
with very large vocabularies. Then, we propose a simple technique, VocabTrim,
to mitigate the drafting overhead to improve the generation speed in
memory-bound environment. VocabTrim reconstructs the drafter LM head to contain
only a limited set of tokens, selected by the most frequently sampled from the
vocabulary of the target model. While limiting the vocabulary in drafting
slightly degrades the acceptance rate, it significantly reduces the drafting
latency in memory-bound process which is often the case on edge devices,
resulting in higher memory-bound speed up (MBSU). We show that our method can
boost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically
by 16% for Llama-3.2-3B-Instruct.

</details>


### [394] [Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report](https://arxiv.org/abs/2506.22698)
*Emily Dux Speltz*

Main category: cs.CL

TL;DR: The workshop explored the relationship between AI language models and human cognition in text comprehension and production, highlighting LLMs' potential and limitations, and emphasizing ethical AI use.


<details>
  <summary>Details</summary>
Motivation: To address gaps in understanding how AI language models relate to human cognitive processes in language tasks.

Method: Interdisciplinary collaboration among experts in cognitive psychology, linguistics, and AI, analyzing human and AI language processes.

Result: Found insights into LLMs' alignment with human cognition, their potential and limitations, and the benefits of human-AI collaboration.

Conclusion: Future research should focus on ethical AI use and enhancing human-AI collaboration in language tasks.

Abstract: This report synthesizes the outcomes of a recent interdisciplinary workshop
that brought together leading experts in cognitive psychology, language
learning, and artificial intelligence (AI)-based natural language processing
(NLP). The workshop, funded by the National Science Foundation, aimed to
address a critical knowledge gap in our understanding of the relationship
between AI language models and human cognitive processes in text comprehension
and composition. Through collaborative dialogue across cognitive, linguistic,
and technological perspectives, workshop participants examined the underlying
processes involved when humans produce and comprehend text, and how AI can both
inform our understanding of these processes and augment human capabilities. The
workshop revealed emerging patterns in the relationship between large language
models (LLMs) and human cognition, with highlights on both the capabilities of
LLMs and their limitations in fully replicating human-like language
understanding and generation. Key findings include the potential of LLMs to
offer insights into human language processing, the increasing alignment between
LLM behavior and human language processing when models are fine-tuned with
human feedback, and the opportunities and challenges presented by human-AI
collaboration in language tasks. By synthesizing these findings, this report
aims to guide future research, development, and implementation of LLMs in
cognitive psychology, linguistics, and education. It emphasizes the importance
of ethical considerations and responsible use of AI technologies while striving
to enhance human capabilities in text comprehension and production through
effective human-AI collaboration.

</details>


### [395] [The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure](https://arxiv.org/abs/2506.22724)
*Niyati Bafna,Tianjian Li,Kenton Murray,David R. Mortensen,David Yarowsky,Hale Sirin,Daniel Khashabi*

Main category: cs.CL

TL;DR: The paper investigates why multilingual generation in LLMs performs poorly for mid- to low-resource languages, attributing it to a translation barrier in the model's pipeline.


<details>
  <summary>Details</summary>
Motivation: To understand and address the poor quality of multilingual generation in LLMs, especially for mid- to low-resource languages.

Method: Analyzes a word translation task across 108 language pairs using logit lens to observe intermediate model processing.

Result: Finds translation failure is a major cause of poor outputs, especially for low-resource languages.

Conclusion: Highlights the translation barrier as a key hurdle and provides insights for improving multilingual generation in LLMs.

Abstract: Multilingual generation with large language models (LLMs) is often of poor
quality for mid- to low-resource languages. Building on insights from
interpretability, we demonstrate the existence of an implicit
task-solving-->translation pipeline for generation, whereby the model first
solves the required task in a largely target-language-agnostic manner, and
subsequently translates answer concepts into the intended target language. We
hypothesize that the failure of the translation stage is an important culprit
for the observed low quality of final outputs, and formalize this as the
translation barrier hypothesis. We test this hypothesis for a word translation
task across 108 language pairs, using logit lens to observe model processing in
intermediate layers. We find that a significant portion of overall failures
indeed stems from translation failure, or the model's inability to translate
correctly solved intermediate concepts into the target language. This is
especially true for low-resource target languages. Our results highlight an
important hurdle for end-to-end multilingual generation, and lend guiding
insights for future work seeking to improve multilinguality in LLMs.

</details>


### [396] [Jan-nano Technical Report](https://arxiv.org/abs/2506.22760)
*Alan Dao,Dinh Bach Vu*

Main category: cs.CL

TL;DR: Jan-nano is a 4B parameter language model that achieves high efficiency and performance by specializing in instant information retrieval, eliminating next token prediction training, and running on consumer hardware.


<details>
  <summary>Details</summary>
Motivation: Address the tradeoff between powerful capabilities and computational resources by creating a highly efficient, specialized language model.

Method: Fine-tuned from Qwen3-4B using a novel multi-stage RLVR system, eliminating next token prediction training (SFT).

Result: Achieves 83.2% on SimpleQA benchmark with MCP integration and supports 128K context length.

Conclusion: Jan-nano demonstrates that intelligence is about strategic specialization, not just scale, enabling high performance with minimal resources.

Abstract: Most language models face a fundamental tradeoff where powerful capabilities
require substantial computational resources. We shatter this constraint with
Jan-nano, a 4B parameter language model that redefines efficiency through
radical specialization: instead of trying to know everything, it masters the
art of finding anything instantly. Fine-tuned from Qwen3-4B using our novel
multi-stage RLVR system that completely eliminates reliance on next token
prediction training (SFT), Jan-nano achieves 83.2% on SimpleQA benchmark with
MCP integration while running on consumer hardware. With 128K context length,
Jan-nano proves that intelligence isn't about scale, it's about strategy.

</details>


### [397] [Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.22777)
*Miles Turpin,Andy Arditi,Marvin Li,Joe Benton,Julian Michael*

Main category: cs.CL

TL;DR: VFT (verbalization fine-tuning) reduces undetected reward hacking in RL-trained models by training them to acknowledge prompt cues, improving detection from 6% (with VFT) vs. 88-99% (without).


<details>
  <summary>Details</summary>
Motivation: Address the challenge of detecting reward hacking in RL-trained models, which exploit unintended strategies without revealing them in reasoning, posing risks in high-stakes applications.

Method: Propose VFT, a pre-RL intervention training models to verbalize influence from prompt cues. Evaluate by RL training on environments with cues signaling incorrect answers.

Result: VFT reduces undetected reward hacks to 6% post-RL, vs. 88-99% without VFT. Models verbalize cue influence 42-94% with VFT, vs. 1-10% in baselines.

Conclusion: VFT significantly improves detection of reward hacking by teaching models to verbalize behavior, enhancing transparency and safety in AI systems.

Abstract: Language models trained with RL can engage in reward hacking--exploiting
unintended strategies for high reward--without revealing this behavior in their
chain-of-thought reasoning, making detection difficult and posing risks for
high-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL
intervention that trains models to explicitly acknowledge when they are
influenced by prompt cues--hints which point to incorrect answers (e.g., "a
Stanford professor thinks the answer is A"). To evaluate VFT, we subsequently
train models with RL on environments where held-out prompt cues signal which
incorrect answers will receive high reward, incentivizing models to reward hack
by exploiting cues instead of reasoning correctly. We measure how often models
exploit these cues without verbalizing it. After RL, only 6% of the VFT-trained
model's responses consist of undetected reward hacks. In comparison, when we
perform RL without VFT, the rate of undetected reward hacks goes up to 88%;
with a debiasing baseline intervention, this increases further to 99%. VFT
achieves this by substantially increasing how often models verbalize the
influence of cues--from 8% to 42% after VFT, and up to 94% after RL--while
baselines remain low even after RL (10% and 1%). Our results show that teaching
models to explicitly verbalize reward hacking behavior before RL significantly
improves their detection, offering a practical path toward more transparent and
safe AI systems.

</details>


### [398] [ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models](https://arxiv.org/abs/2506.22791)
*Jianxin Yan,Wangze Ni,Lei Chen,Xuemin Lin,Peng Cheng,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: ContextCache introduces a context-aware semantic caching system for multi-turn dialogues, improving precision and recall while reducing latency and computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing semantic caching systems lack awareness of multi-turn dialogue contexts, leading to incorrect cache hits in different conversational settings.

Method: ContextCache uses a two-stage retrieval architecture: vector-based retrieval on the current query followed by contextual matching via self-attention mechanisms.

Result: ContextCache improves precision and recall in real-world conversations and reduces latency by 10x compared to direct LLM invocation.

Conclusion: ContextCache effectively addresses the limitations of existing systems, offering significant efficiency gains for LLM conversational applications.

Abstract: Semantic caching significantly reduces computational costs and improves
efficiency by storing and reusing large language model (LLM) responses.
However, existing systems rely primarily on matching individual queries,
lacking awareness of multi-turn dialogue contexts, which leads to incorrect
cache hits when similar queries appear in different conversational settings.
This demonstration introduces ContextCache, a context-aware semantic caching
system for multi-turn dialogues. ContextCache employs a two-stage retrieval
architecture that first executes vector-based retrieval on the current query to
identify potential matches and then integrates current and historical dialogue
representations through self-attention mechanisms for precise contextual
matching. Evaluation of real-world conversations shows that ContextCache
improves precision and recall compared to existing methods. Additionally,
cached responses exhibit approximately 10 times lower latency than direct LLM
invocation, enabling significant computational cost reductions for LLM
conversational applications.

</details>


### [399] [MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs](https://arxiv.org/abs/2506.22808)
*Jianhui Wei,Zijie Meng,Zikai Xiao,Tianxiang Hu,Yang Feng,Zhijie Zhou,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: The paper introduces MedEthicsQA, a benchmark for evaluating medical ethics in LLMs, revealing deficiencies in MedLLMs' ethical alignment.


<details>
  <summary>Details</summary>
Motivation: To address the insufficient exploration of ethical safety in Medical Large Language Models (MedLLMs).

Method: Creation of MedEthicsQA, a benchmark with 5,623 multiple-choice and 5,351 open-ended questions, integrating global ethical standards and rigorous quality control.

Result: State-of-the-art MedLLMs perform worse on medical ethics questions compared to their foundation counterparts.

Conclusion: The study highlights the need for better ethical alignment in MedLLMs and provides a reliable benchmark for future research.

Abstract: While Medical Large Language Models (MedLLMs) have demonstrated remarkable
potential in clinical tasks, their ethical safety remains insufficiently
explored. This paper introduces $\textbf{MedEthicsQA}$, a comprehensive
benchmark comprising $\textbf{5,623}$ multiple-choice questions and
$\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs.
We systematically establish a hierarchical taxonomy integrating global medical
ethical standards. The benchmark encompasses widely used medical datasets,
authoritative question banks, and scenarios derived from PubMed literature.
Rigorous quality control involving multi-stage filtering and multi-faceted
expert validation ensures the reliability of the dataset with a low error rate
($2.72\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance
in answering medical ethics questions compared to their foundation
counterparts, elucidating the deficiencies of medical ethics alignment. The
dataset, registered under CC BY-NC 4.0 license, is available at
https://github.com/JianhuiWei7/MedEthicsQA.

</details>


### [400] [Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models](https://arxiv.org/abs/2506.22813)
*Zhuojun Ding,Wei Wei,Chenghao Fan*

Main category: cs.CL

TL;DR: The paper introduces SaM, a framework for dynamically selecting and merging expert models for domain-specific tasks, improving generalization and scalability without extra training.


<details>
  <summary>Details</summary>
Motivation: Supervised fine-tuning for domain-specific tasks is costly and lacks adaptability in unified models. SaM addresses these issues by leveraging existing expert models.

Method: SaM selects and merges domain-specific experts based on domain similarity and performance, creating optimized task-specific models dynamically.

Result: SaM outperforms unified models by 10% on average across benchmarks and offers scalability by allowing easy addition/removal of experts.

Conclusion: SaM provides an effective, scalable solution for domain adaptation in information extraction tasks, with potential for further improvements.

Abstract: Supervised fine-tuning (SFT) is widely used to align large language models
(LLMs) with information extraction (IE) tasks, such as named entity recognition
(NER). However, annotating such fine-grained labels and training
domain-specific models is costly. Existing works typically train a unified
model across multiple domains, but such approaches lack adaptation and
scalability since not all training data benefits target domains and scaling
trained models remains challenging. We propose the SaM framework, which
dynamically Selects and Merges expert models at inference time. Specifically,
for a target domain, we select domain-specific experts pre-trained on existing
domains based on (i) domain similarity to the target domain and (ii)
performance on sampled instances, respectively. The experts are then merged to
create task-specific models optimized for the target domain. By dynamically
merging experts beneficial to target domains, we improve generalization across
various domains without extra training. Additionally, experts can be added or
removed conveniently, leading to great scalability. Extensive experiments on
multiple benchmarks demonstrate our framework's effectiveness, which
outperforms the unified model by an average of 10%. We further provide insights
into potential improvements, practical experience, and extensions of our
framework.

</details>


### [401] [Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization](https://arxiv.org/abs/2506.22846)
*Duygu Altinok*

Main category: cs.CL

TL;DR: Proposes LAIL, a novel auxiliary loss framework to enhance CTC-based ASR by leveraging LLMs' linguistic knowledge, improving WER without sacrificing CTC's efficiency.


<details>
  <summary>Details</summary>
Motivation: Autoregressive E2E ASR models are slow for real-time use, while CTC models lack linguistic dependency modeling. LAIL bridges this gap.

Method: Attaches connector layers to intermediate encoder layers, maps outputs to LLM embedding space, and computes a causal language modeling loss.

Result: Achieves state-of-the-art WER on LibriSpeech, TEDLIUM2, and WSJ with minimal computational overhead.

Conclusion: LAIL effectively enhances CTC-based ASR by integrating LLM linguistic knowledge, balancing performance and efficiency.

Abstract: End-to-end (E2E) automatic speech recognition (ASR) systems have
revolutionized the field by integrating all components into a single neural
network, with attention-based encoder-decoder models achieving state-of-the-art
performance. However, their autoregressive decoding process limits inference
speed, making them unsuitable for real-time applications. In contrast,
CTC-based models offer faster, non-autoregressive decoding but struggle to
model linguistic dependencies effectively. Addressing this challenge, we
propose a novel auxiliary loss framework called Language-Aware Intermediate
Loss (LAIL) to enhance CTC-based ASR using the linguistic knowledge of large
language models (LLMs). By attaching connector layers to intermediate encoder
layers, LAIL maps outputs to the embedding space of an LLM and computes a
causal language modeling loss during training. This approach enhances
linguistic modeling while preserving the computational efficiency of CTC
decoding. Using the Conformer architecture and various LLaMA models, we
demonstrate significant improvements in Word Error Rate (WER) on the
LibriSpeech, TEDLIUM2, and WSJ corpora, achieving state-of-the-art performance
for CTC-based ASR with minimal computational overhead.

</details>


### [402] [Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems](https://arxiv.org/abs/2506.22852)
*Yucheng Cai,Yuxuan Wu,Yi Huang,Junlan Feng,Zhijian Ou*

Main category: cs.CL

TL;DR: The paper proposes Knowledge Augmented Fine-Tuning (KAFT) to improve LLMs' factual accuracy in knowledge-intensive dialog systems by fine-tuning them with domain-specific data and external knowledge, outperforming prompting-based methods.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with factual accuracy in knowledge-intensive scenarios, even with retrieval-augmented generation (RAG) or agent-based approaches.

Method: Proposes KAFT, fine-tuning LLMs with domain-specific data and external knowledge, tested on the MobileCS2 dataset.

Result: KAFT significantly outperforms prompting in RAG and agent systems, especially in factual accuracy.

Conclusion: KAFT is a promising approach for enhancing LLMs' performance in knowledge-intensive dialog systems.

Abstract: Large language models (LLMs) have recently been applied to dialog systems.
Despite making progress, LLMs are prone to errors in knowledge-intensive
scenarios. Recently, approaches based on retrieval augmented generation (RAG)
and agent have emerged to improve the factual accuracy by enhancing the LLMs
with knowledge retrieved from external knowledge bases (KBs). This is mostly
implemented by prompting the LLMs with instructions, examples and the retrieved
knowledge. However, LLMs may have difficulty using the retrieved knowledge
effectively for response generation, because they are not well trained to do
such generation for specific domains. To mitigate this problem, we propose to
finetune the LLMs in the RAG-based and agent-based systems with domain-specific
data, together with domain-specific external knowledge, which is called
knowledge augmented finetuning (KAFT). We base our study on the MobileCS2
dataset, a real-life customer service dialog dataset that features intensive
knowledge interactions, to systematically compare the prompting and KAFT
techniques in the RAG-based and agent-based systems. Experiment results show
that KAFT substantially surpasses prompting in both RAG and agent systems,
particularly in terms of factual accuracy. To the best of our knowledge, this
paper represents the first solid empirical work to investigate the KAFT idea.

</details>


### [403] [DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues](https://arxiv.org/abs/2506.22853)
*Kyochul Jang,Donghyeon Lee,Kyusik Kim,Dongseok Heo,Taewhoo Lee,Woojeong Kim,Bongwon Suh*

Main category: cs.CL

TL;DR: The paper introduces DICE-SCORE and DICE-BENCH to address the lack of realistic function-calling benchmarks, revealing gaps in current models for real-world deployment.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook real-world complexity, focusing only on single-turn interactions.

Method: DICE-SCORE evaluates tool-related information dispersion; DICE-BENCH synthesizes practical datasets using a tool graph and multi-agent system.

Result: Analysis shows low DICE-SCOREs in existing benchmarks; DICE-BENCH creates 1,607 high-scoring instances.

Conclusion: Significant improvements are needed for LLMs to perform effectively in real-world function-calling scenarios.

Abstract: Existing function-calling benchmarks focus on single-turn interactions.
However, they overlook the complexity of real-world scenarios. To quantify how
existing benchmarks address practical applications, we introduce DICE-SCORE, a
metric that evaluates the dispersion of tool-related information such as
function name and parameter values throughout the dialogue. Analyzing existing
benchmarks through DICE-SCORE reveals notably low scores, highlighting the need
for more realistic scenarios. To address this gap, we present DICE-BENCH, a
framework that constructs practical function-calling datasets by synthesizing
conversations through a tool graph that maintains dependencies across rounds
and a multi-agent system with distinct personas to enhance dialogue
naturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our
experiments on 19 LLMs with DICE-BENCH show that significant advances are still
required before such models can be deployed effectively in real-world settings.
Our code and data are all publicly available:
https://snuhcc.github.io/DICE-Bench/.

</details>


### [404] [Mind the Gap: Entity-Preserved Context-Aware ASR Structured Transcriptions](https://arxiv.org/abs/2506.22858)
*Duygu Altinok*

Main category: cs.CL

TL;DR: A novel training approach for ASR systems improves named entity and numerical data recognition by using overlapping context windows and enriched training data, reducing WER and enhancing semantic understanding.


<details>
  <summary>Details</summary>
Motivation: ASR systems like Whisper struggle with named entities and numerical data, impacting accuracy and semantic understanding in critical domains.

Method: Extends semantic context by adding overlapping 5-second windows to 30-second chunks, creating a 40-second effective window. Reassigns boundary-spanning entities and uses enriched training data with embedded labels.

Result: Improves performance on semantic tasks like NER and entity formatting, as shown on the Spoken Wikipedia dataset.

Conclusion: Context-aware training effectively addresses ASR limitations in long-form transcription and complex entity recognition.

Abstract: Automatic Speech Recognition (ASR) systems, such as Whisper, achieve high
transcription accuracy but struggle with named entities and numerical data,
especially when proper formatting is required. These issues increase word error
rate (WER) and impair semantic understanding in critical domains like legal,
financial, and medical applications. We propose a novel training approach that
extends the semantic context of ASR models by adding overlapping context
windows during training. By sliding 5-second overlaps on both sides of
30-second chunks, we create a 40-second "effective semantic window," improving
entity recognition and formatting while focusing predictions on the central 30
seconds. To address entities spanning chunk boundaries, we reassign such
entities entirely to the right-hand chunk, ensuring proper formatting.
Additionally, enriched training data with embedded entity labels enables the
model to learn both recognition and type-specific formatting. Evaluated on the
Spoken Wikipedia dataset, our method improves performance across semantic
tasks, including named entity recognition (NER) and entity formatting. These
results highlight the effectiveness of context-aware training in addressing ASR
limitations for long-form transcription and complex entity recognition tasks.

</details>


### [405] [Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models](https://arxiv.org/abs/2506.22957)
*Younwoo Choi,Changling Li,Yongjin Yang,Zhijing Jin*

Main category: cs.CL

TL;DR: The paper introduces and evaluates 'interlocutor awareness' in LLMs, focusing on their ability to identify and adapt to dialogue partners. It examines three dimensions and demonstrates practical impacts, including enhanced collaboration and new vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Understanding LLMs' awareness of conversational partners is crucial for reliable performance and safety in multi-agent and human-AI systems, as prior work overlooked this aspect.

Method: The study formalizes interlocutor awareness and evaluates it across reasoning patterns, linguistic style, and alignment preferences. It includes case studies to demonstrate practical implications.

Result: LLMs reliably identify peers like GPT and Claude. Interlocutor awareness improves collaboration but also introduces vulnerabilities like reward-hacking and jailbreak susceptibility.

Conclusion: The findings emphasize the dual potential of identity-sensitive behavior in LLMs, calling for further research and safeguards in multi-agent deployments.

Abstract: As large language models (LLMs) are increasingly integrated into multi-agent
and human-AI systems, understanding their awareness of both self-context and
conversational partners is essential for ensuring reliable performance and
robust safety. While prior work has extensively studied situational awareness
which refers to an LLM's ability to recognize its operating phase and
constraints, it has largely overlooked the complementary capacity to identify
and adapt to the identity and characteristics of a dialogue partner. In this
paper, we formalize this latter capability as interlocutor awareness and
present the first systematic evaluation of its emergence in contemporary LLMs.
We examine interlocutor inference across three dimensions-reasoning patterns,
linguistic style, and alignment preferences-and show that LLMs reliably
identify same-family peers and certain prominent model families, such as GPT
and Claude. To demonstrate its practical significance, we develop three case
studies in which interlocutor awareness both enhances multi-LLM collaboration
through prompt adaptation and introduces new alignment and safety
vulnerabilities, including reward-hacking behaviors and increased jailbreak
susceptibility. Our findings highlight the dual promise and peril of
identity-sensitive behavior in LLMs, underscoring the need for further
understanding of interlocutor awareness and new safeguards in multi-agent
deployments. Our code is open-sourced at
https://github.com/younwoochoi/InterlocutorAwarenessLLM.

</details>


### [406] [On the Generalizability of "Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals"](https://arxiv.org/abs/2506.22977)
*Asen Dotsinski,Udit Thakur,Marko Ivanov,Mohammad Hafeez Khan,Maria Heuss*

Main category: cs.CL

TL;DR: A reproduction study of Ortu et al. (2024) confirms their findings on factual and counterfactual handling in language models, extends the work to larger models, explores prompt structure impact, and tests domain-specific validity.


<details>
  <summary>Details</summary>
Motivation: To validate and extend Ortu et al.'s findings on how language models handle facts and counterfactuals, focusing on model generalizability, prompt structure, and domain-specific effects.

Method: Reproduced experiments on GPT-2 and Pythia 6.9B, extended to Llama 3.1 8B, tested prompt variations, and evaluated domain-specific prompts.

Result: Confirmed primary findings, found reduced attention head specialization in larger models, observed prompt structure impacts, and identified domain-specific skews.

Conclusion: Attention head ablation's effectiveness varies by model, prompt, domain, and task, with limitations in underrepresented domains.

Abstract: We present a reproduction study of "Competition of Mechanisms: Tracing How
Language Models Handle Facts and Counterfactuals" (Ortu et al., 2024), which
investigates competition of mechanisms in language models between factual
recall and counterfactual in-context repetition. Our study successfully
reproduces their primary findings regarding the localization of factual and
counterfactual information, the dominance of attention blocks in mechanism
competition, and the specialization of attention heads in handling competing
information. We reproduce their results on both GPT-2 (Radford et al., 2019)
and Pythia 6.9B (Biderman et al., 2023). We extend their work in three
significant directions. First, we explore the generalizability of these
findings to even larger models by replicating the experiments on Llama 3.1 8B
(Grattafiori et al., 2024), discovering greatly reduced attention head
specialization. Second, we investigate the impact of prompt structure by
introducing variations where we avoid repeating the counterfactual statement
verbatim or we change the premise word, observing a marked decrease in the
logit for the counterfactual token. Finally, we test the validity of the
authors' claims for prompts of specific domains, discovering that certain
categories of prompts skew the results by providing the factual prediction
token as part of the subject of the sentence. Overall, we find that the
attention head ablation proposed in Ortu et al. (2024) is ineffective for
domains that are underrepresented in their dataset, and that the effectiveness
varies based on model architecture, prompt structure, domain and task.

</details>


### [407] [A Systematic Study of Compositional Syntactic Transformer Language Models](https://arxiv.org/abs/2506.22978)
*Yida Zhao,Hao Xve,Xiang Hu,Kewei Tu*

Main category: cs.CL

TL;DR: The paper introduces a unified framework for compositional syntactic language models (SLMs) based on constituency parse trees, evaluates design choices, and provides recommendations.


<details>
  <summary>Details</summary>
Motivation: To enhance Transformers by incorporating syntactic biases and improving performance in tasks like language modeling and syntactic generalization.

Method: Proposes a unified framework for compositional SLMs, evaluates existing and novel variants across multiple tasks.

Result: Comprehensive empirical evaluation leads to design recommendations for compositional SLMs.

Conclusion: The framework and recommendations improve the design and effectiveness of compositional SLMs.

Abstract: Syntactic language models (SLMs) enhance Transformers by incorporating
syntactic biases through the modeling of linearized syntactic parse trees
alongside surface sentences. This paper focuses on compositional SLMs that are
based on constituency parse trees and contain explicit bottom-up composition of
constituent representations. We identify key aspects of design choices in
existing compositional SLMs and propose a unified framework encompassing both
existing models and novel variants. We conduct a comprehensive empirical
evaluation of all the variants in our framework across language modeling,
syntactic generalization, summarization, dialogue, and inference efficiency.
Based on the experimental results, we make multiple recommendations on the
design of compositional SLMs. Our code is released at
https://github.com/zhaoyd1/compositional_SLMs.

</details>


### [408] [SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions](https://arxiv.org/abs/2506.23046)
*Xianzhe Fan,Xuhui Zhou,Chuanyang Jin,Kolby Nottingham,Hao Zhu,Maarten Sap*

Main category: cs.CL

TL;DR: The SoMi-ToM benchmark evaluates Theory of Mind (ToM) in embodied multi-agent social interactions, revealing a significant performance gap between humans and large vision-language models (LVLMs).


<details>
  <summary>Details</summary>
Motivation: Existing ToM benchmarks focus on static, text-based scenarios, lacking realism compared to dynamic, real-world social interactions.

Method: The SoMi-ToM benchmark uses multimodal data (visual, dialogue, action) from the SoMi environment, with first-person and third-person evaluation methods.

Result: LVLMs perform significantly worse than humans, with accuracy gaps of 40.1% (first-person) and 26.4% (third-person).

Conclusion: Future LVLMs need enhanced ToM capabilities for complex, embodied social interactions.

Abstract: Humans continuously infer the states, goals, and behaviors of others by
perceiving their surroundings in dynamic, real-world social interactions.
However, most Theory of Mind (ToM) benchmarks only evaluate static, text-based
scenarios, which have a significant gap compared to real interactions. We
propose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in
embodied multi-agent complex social interactions. This benchmark is based on
rich multimodal interaction data generated by the interaction environment SoMi,
covering diverse crafting goals and social relationships. Our framework
supports multi-level evaluation: (1) first-person evaluation provides
multimodal (visual, dialogue, action, etc.) input from a first-person
perspective during a task for real-time state inference, (2) third-person
evaluation provides complete third-person perspective video and text records
after a task for goal and behavior inference. This evaluation method allows for
a more comprehensive examination of a model's ToM capabilities from both the
subjective immediate experience and the objective global observation. We
constructed a challenging dataset containing 35 third-person perspective
videos, 363 first-person perspective images, and 1225 expert-annotated
multiple-choice questions (three options). On this dataset, we systematically
evaluated the performance of human subjects and several state-of-the-art large
vision-language models (LVLMs). The results show that LVLMs perform
significantly worse than humans on SoMi-ToM: the average accuracy gap between
humans and models is 40.1% in first-person evaluation and 26.4% in third-person
evaluation. This indicates that future LVLMs need to further improve their ToM
capabilities in embodied, complex social interactions.

</details>


### [409] [MariNER: A Dataset for Historical Brazilian Portuguese Named Entity Recognition](https://arxiv.org/abs/2506.23051)
*João Lucas Luz Lima Sarcinelli,Marina Lages Gonçalves Teixeira,Jade Bortot de Paiva,Diego Furtado Silva*

Main category: cs.CL

TL;DR: The paper introduces MariNER, the first gold-standard NER dataset for early 20th-century Brazilian Portuguese, addressing the lack of resources for historical texts. It also evaluates state-of-the-art NER models on this dataset.


<details>
  <summary>Details</summary>
Motivation: Brazilian Portuguese lacks high-quality NER datasets, especially for historical texts, which are crucial for digital humanities.

Method: The paper constructs MariNER, a manually annotated dataset with over 9,000 sentences, and evaluates existing NER models on it.

Result: MariNER is created as a gold-standard dataset, and the performance of NER models is assessed for this historical context.

Conclusion: The work fills a gap in NER resources for Brazilian Portuguese and provides insights into model performance for historical texts.

Abstract: Named Entity Recognition (NER) is a fundamental Natural Language Processing
(NLP) task that aims to identify and classify entity mentions in texts across
different categories. While languages such as English possess a large number of
high-quality resources for this task, Brazilian Portuguese still lacks in
quantity of gold-standard NER datasets, especially when considering specific
domains. Particularly, this paper considers the importance of NER for analyzing
historical texts in the context of digital humanities. To address this gap,
this work outlines the construction of MariNER: \textit{Mapeamento e
Anota\c{c}\~oes de Registros hIst\'oricos para NER} (Mapping and Annotation of
Historical Records for NER), the first gold-standard dataset for early
20th-century Brazilian Portuguese, with more than 9,000 manually annotated
sentences. We also assess and compare the performance of state-of-the-art NER
models for the dataset.

</details>


### [410] [Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning](https://arxiv.org/abs/2506.23056)
*Xiang Zhuang,Bin Wu,Jiyu Cui,Kehua Feng,Xiaotong Li,Huabin Xing,Keyan Ding,Qiang Zhang,Huajun Chen*

Main category: cs.CL

TL;DR: The paper introduces K-MSE, a knowledge-enhanced framework using Monte Carlo Tree Search and a molecular substructure knowledge base to improve LLMs' performance in molecular structure elucidation.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with molecular structure elucidation due to limited specialized chemical knowledge.

Method: K-MSE integrates an external molecular substructure knowledge base and a molecule-spectrum scorer for accurate reasoning and evaluation.

Result: The framework achieves over 20% improvement on GPT-4o-mini and GPT-4o.

Conclusion: K-MSE effectively enhances LLMs' capability in molecular structure elucidation by addressing knowledge gaps and evaluation issues.

Abstract: Molecular structure elucidation involves deducing a molecule's structure from
various types of spectral data, which is crucial in chemical experimental
analysis. While large language models (LLMs) have shown remarkable proficiency
in analyzing and reasoning through complex tasks, they still encounter
substantial challenges in molecular structure elucidation. We identify that
these challenges largely stem from LLMs' limited grasp of specialized chemical
knowledge. In this work, we introduce a Knowledge-enhanced reasoning framework
for Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search
for test-time scaling as a plugin. Specifically, we construct an external
molecular substructure knowledge base to extend the LLMs' coverage of the
chemical structure space. Furthermore, we design a specialized
molecule-spectrum scorer to act as a reward model for the reasoning process,
addressing the issue of inaccurate solution evaluation in LLMs. Experimental
results show that our approach significantly boosts performance, particularly
gaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is
available at https://github.com/HICAI-ZJU/K-MSE.

</details>


### [411] [Text2VectorSQL: Bridging Text-to-SQL and Vector Search for Unified Natural Language Queries](https://arxiv.org/abs/2506.23071)
*Zhengren Wang,Bozhou Li,Dongwen Yao,Wentao Zhang*

Main category: cs.CL

TL;DR: Text2VectorSQL unifies Text-to-SQL and vector search to enhance natural language query handling for structured and unstructured data, outperforming baselines with semantic filtering and multi-modal matching.


<details>
  <summary>Details</summary>
Motivation: Existing Text-to-SQL and VectorSQL methods struggle with unstructured data and ambiguous queries, lacking expressiveness and tailored evaluation frameworks.

Method: Text2VectorSQL integrates semantic filtering, multi-modal matching, and retrieval acceleration, using synthetic data for model training and an automatic pipeline for evaluation.

Result: The framework shows significant performance improvements over baseline methods, enabling more versatile database interactions.

Conclusion: Text2VectorSQL bridges the gap between Text-to-SQL and vector search, laying the groundwork for intuitive database interfaces.

Abstract: While Text-to-SQL enables natural language interaction with structured
databases, its effectiveness diminishes with unstructured data or ambiguous
queries due to rigid syntax and limited expressiveness. Concurrently, vector
search has emerged as a powerful paradigm for semantic retrieval, particularly
for unstructured data. However, existing VectorSQL implementations still rely
heavily on manual crafting and lack tailored evaluation frameworks, leaving a
significant gap between theoretical potential and practical deployment. To
bridge these complementary paradigms, we introduces Text2VectorSQL, a novel
framework unifying Text-to-SQL and vector search to overcome expressiveness
constraints and support more diverse and holistical natural language queries.
Specifically, Text2VectorSQL enables semantic filtering, multi-modal matching,
and retrieval acceleration. For evaluation, we build vector index on
appropriate columns, extend user queries with semantic search, and annotate
ground truths via an automatic pipeline with expert review. Furthermore, we
develop dedicated Text2VectorSQL models with synthetic data, demonstrating
significant performance improvements over baseline methods. Our work
establishes the foundation for the Text2VectorSQL task, paving the way for more
versatile and intuitive database interfaces. The repository will be publicly
available at https://github.com/Open-DataFlow/Text2VectorSQL.

</details>


### [412] [From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship](https://arxiv.org/abs/2506.23101)
*Yue Xu,Wenjie Wang*

Main category: cs.CL

TL;DR: The paper introduces Genres, a benchmark for evaluating gender bias in multimodal large language models (MLLMs) through relational and contextual interactions, revealing persistent biases not evident in single-character settings.


<details>
  <summary>Details</summary>
Motivation: Addressing concerns about gender bias in MLLMs, particularly in socially sensitive applications, by moving beyond isolated evaluations to study relational and contextual bias in interactions.

Method: Developed Genres, a benchmark using dual-character profiles and narrative generation tasks to assess gender bias across multiple dimensions in MLLMs.

Result: Experiments showed context-sensitive gender biases in MLLMs, highlighting the need for relationship-aware benchmarks.

Conclusion: The study emphasizes the importance of relational bias evaluation and offers insights for future bias mitigation in MLLMs.

Abstract: Multimodal large language models (MLLMs) have shown impressive capabilities
across tasks involving both visual and textual modalities. However, growing
concerns remain about their potential to encode and amplify gender bias,
particularly in socially sensitive applications. Existing benchmarks
predominantly evaluate bias in isolated scenarios, overlooking how bias may
emerge subtly through interpersonal interactions. We fill this gap by going
beyond single-entity evaluation and instead focusing on a deeper examination of
relational and contextual gender bias in dual-individual interactions. We
introduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs
through the lens of social relationships in generated narratives. Genres
assesses gender bias through a dual-character profile and narrative generation
task that captures rich interpersonal dynamics and supports a fine-grained bias
evaluation suite across multiple dimensions. Experiments on both open- and
closed-source MLLMs reveal persistent, context-sensitive gender biases that are
not evident in single-character settings. Our findings underscore the
importance of relationship-aware benchmarks for diagnosing subtle,
interaction-driven gender bias in MLLMs and provide actionable insights for
future bias mitigation.

</details>


### [413] [FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes](https://arxiv.org/abs/2506.23111)
*Janki Atul Nawale,Mohammed Safi Ur Rahman Khan,Janani D,Mansi Gupta,Danish Pruthi,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: INDIC-BIAS is an India-centric benchmark to evaluate fairness in LLMs across 85 diverse identity groups, revealing strong biases against marginalized identities.


<details>
  <summary>Details</summary>
Motivation: Existing fairness studies are Western-focused, lacking applicability to culturally diverse regions like India.

Method: Curated 1,800 socio-cultural topics, generated 20,000 scenario templates, and structured evaluations into plausibility, judgment, and generation tasks.

Result: LLMs exhibit strong negative biases against marginalized groups and struggle to mitigate bias even when prompted.

Conclusion: The study highlights allocative and representational harms of LLMs in India, advocating cautious usage and releasing INDIC-BIAS for further research.

Abstract: Existing studies on fairness are largely Western-focused, making them
inadequate for culturally diverse countries such as India. To address this gap,
we introduce INDIC-BIAS, a comprehensive India-centric benchmark designed to
evaluate fairness of LLMs across 85 identity groups encompassing diverse
castes, religions, regions, and tribes. We first consult domain experts to
curate over 1,800 socio-cultural topics spanning behaviors and situations,
where biases and stereotypes are likely to emerge. Grounded in these topics, we
generate and manually validate 20,000 real-world scenario templates to probe
LLMs for fairness. We structure these templates into three evaluation tasks:
plausibility, judgment, and generation. Our evaluation of 14 popular LLMs on
these tasks reveals strong negative biases against marginalized identities,
with models frequently reinforcing common stereotypes. Additionally, we find
that models struggle to mitigate bias even when explicitly asked to rationalize
their decision. Our evaluation provides evidence of both allocative and
representational harms that current LLMs could cause towards Indian identities,
calling for a more cautious usage in practical applications. We release
INDIC-BIAS as an open-source benchmark to advance research on benchmarking and
mitigating biases and stereotypes in the Indian context.

</details>


### [414] [Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models](https://arxiv.org/abs/2506.23122)
*Shivam Sharma,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: The paper explores identifying narrative roles (Hero, Villain, Victim, Other) in memes across English and code-mixed languages, using a balanced dataset. It evaluates various models, highlighting challenges in detecting 'Victim' and generalizing across cultures.


<details>
  <summary>Details</summary>
Motivation: To address the nuanced, culture-specific language in memes and improve role detection in visual-textual content, contrasting synthetic hateful content.

Method: Evaluates multilingual transformers, sentiment/abuse classifiers, LLMs, and multimodal models under zero-shot settings, using precision, recall, and F1 metrics.

Result: Larger models like DeBERTa-v3 and Qwen2.5-VL perform well, but 'Victim' detection and cross-cultural generalization remain challenging. Hybrid prompts improve results marginally.

Conclusion: Cultural grounding, prompt engineering, and multimodal reasoning are crucial for modeling subtle narrative roles in memes.

Abstract: This work investigates the challenging task of identifying narrative roles -
Hero, Villain, Victim, and Other - in Internet memes, across three diverse test
sets spanning English and code-mixed (English-Hindi) languages. Building on an
annotated dataset originally skewed toward the 'Other' class, we explore a more
balanced and linguistically diverse extension, originally introduced as part of
the CLEF 2024 shared task. Comprehensive lexical and structural analyses
highlight the nuanced, culture-specific, and context-rich language used in real
memes, in contrast to synthetically curated hateful content, which exhibits
explicit and repetitive lexical markers. To benchmark the role detection task,
we evaluate a wide spectrum of models, including fine-tuned multilingual
transformers, sentiment and abuse-aware classifiers, instruction-tuned LLMs,
and multimodal vision-language models. Performance is assessed under zero-shot
settings using precision, recall, and F1 metrics. While larger models like
DeBERTa-v3 and Qwen2.5-VL demonstrate notable gains, results reveal consistent
challenges in reliably identifying the 'Victim' class and generalising across
cultural and code-mixed content. We also explore prompt design strategies to
guide multimodal models and find that hybrid prompts incorporating structured
instructions and role definitions offer marginal yet consistent improvements.
Our findings underscore the importance of cultural grounding, prompt
engineering, and multimodal reasoning in modelling subtle narrative framings in
visual-textual content.

</details>


### [415] [Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2506.23127)
*Zhaoye Fei,Li Ji,Siyin Wang,Junhao Shi,Jingjing Gong,Xipeng Qiu*

Main category: cs.CL

TL;DR: Embodied Planner-R1 is a reinforcement learning framework for LLMs to improve interactive task planning, achieving high success rates on benchmarks with strong generalization.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with embodied task planning due to static knowledge and lack of causal learning in dynamic environments.

Method: Uses pure reinforcement learning with group rollout, sparse rewards, and Interactive Policy Optimization (IPO) for efficient learning.

Result: Achieves 97.78% on ALFWorld and 79.92% on ScienceWorld, with minimal performance drop in unseen environments.

Conclusion: The framework enhances LLMs' interactive capabilities and generalization in embodied planning tasks.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks, yet they face significant challenges in embodied task planning
scenarios that require continuous environmental understanding and action
generation. Existing approaches generate open-loop action scripts based on
static knowledge, making it difficult to learn causal relationships between
actions and environmental feedback, particularly in partially observable
environments. We introduce Embodied Planner-R1, a novel outcome-driven
reinforcement learning framework that enables LLMs to develop interactive
capabilities through autonomous exploration with minimal supervision. Our
framework incorporates three key innovations: (1) Without human annotations, we
employ pure reinforcement learning with group rollout, incorporating
in-environment interaction through parallel exploration; (2) completion-driven
sparse reward; and (3) Interactive Policy Optimization (IPO) for efficient
learning from grouped trajectories. Across two challenging text-based Embodied
planning benchmarks, Embodied Planner-R1 achieves impressive completion rates
of 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a
large margin, and suffers only a -3.66% drop in previously unseen environments,
evidencing strong generalization.

</details>


### [416] [Format-Adapter: Improving Reasoning Capability of LLMs by Adapting Suitable Format](https://arxiv.org/abs/2506.23133)
*Dingzirui Wang,Xuanliang Zhang,Rongyu Cao,Longxu Dou,Xianzhen Luo,Yingwei Ma,Qingfu Zhu,Wanxiang Che,Binhua Li,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: Format-Adapter uses LLMs to generate and select reasoning formats, improving performance by 4.3% over prior methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of human-labeled reasoning formats, which are costly and may not suit all tasks.

Method: Proposes a method to measure reasoning errors and introduces Format-Adapter, which generates and selects formats to minimize these errors.

Result: Achieves a 4.3% average performance improvement on math and commonsense reasoning tasks.

Conclusion: Format-Adapter effectively adapts reasoning formats to tasks, outperforming previous methods.

Abstract: Generating and voting multiple answers is an effective method to mitigate
reasoning inconsistencies of large language models (LLMs). Prior works have
shown that multiple reasoning formats outperform a single format when
generating multiple answers. However, previous works using multiple formats
rely on formats labeled by humans, which could be unsuitable for all tasks and
have high labeling costs. To address this issue, we adapt suitable formats to
the given tasks by generating and selecting formats. We first propose how to
measure the reasoning error when generating multiple answers. Then, we
introduce Format-Adapter, which utilizes LLMs to generate and select suitable
reasoning formats by minimizing the error measurement we present. We conduct
experiments on math and commonsense reasoning tasks, where Format-Adapter
achieves a 4.3% performance improvement on average over previous works,
demonstrating the effectiveness.

</details>


### [417] [LLM-Assisted Question-Answering on Technical Documents Using Structured Data-Aware Retrieval Augmented Generation](https://arxiv.org/abs/2506.23136)
*Shadman Sobhan,Mohammad Ariful Haque*

Main category: cs.CL

TL;DR: Proposes a RAG pipeline for technical documents, handling tables and images, with high faithfulness and relevancy scores.


<details>
  <summary>Details</summary>
Motivation: Address challenges of LLMs like hallucination and outdated knowledge, and improve retrieval from complex technical documents.

Method: Combines vector similarity search with a fine-tuned reranker (Gemma-2-9b-it) trained using RAFT on a custom dataset.

Result: Achieves 94% faithfulness (RAGas) and 96% (DeepEval), with 87% (RAGas) and 93% (DeepEval) answer relevancy.

Conclusion: The pipeline outperforms general RAG methods, especially for table-based and out-of-context questions.

Abstract: Large Language Models (LLMs) are capable of natural language understanding
and generation. But they face challenges such as hallucination and outdated
knowledge. Fine-tuning is one possible solution, but it is resource-intensive
and must be repeated with every data update. Retrieval-Augmented Generation
(RAG) offers an efficient solution by allowing LLMs to access external
knowledge sources. However, traditional RAG pipelines struggle with retrieving
information from complex technical documents with structured data such as
tables and images. In this work, we propose a RAG pipeline, capable of handling
tables and images in documents, for technical documents that support both
scanned and searchable formats. Its retrieval process combines vector
similarity search with a fine-tuned reranker based on Gemma-2-9b-it. The
reranker is trained using RAFT (Retrieval-Augmented Fine-Tuning) on a custom
dataset designed to improve context identification for question answering. Our
evaluation demonstrates that the proposed pipeline achieves a high faithfulness
score of 94% (RAGas) and 96% (DeepEval), and an answer relevancy score of 87%
(RAGas) and 93% (DeepEval). Comparative analysis demonstrates that the proposed
architecture is superior to general RAG pipelines in terms of table-based
questions and handling questions outside context.

</details>


### [418] [Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion](https://arxiv.org/abs/2506.23137)
*Siyuan Li,Ruitong Liu,Yan Wen,Te Sun*

Main category: cs.CL

TL;DR: The paper introduces Flow-Modulated Scoring (FMS) for Knowledge Graph Completion, combining context-aware static embeddings with dynamic transformations to improve relational modeling, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing embedding-based KGC methods lack contextual and dynamic relational understanding, limiting their effectiveness.

Method: FMS uses a semantic context learning module and a conditional flow-matching module to dynamically refine static scores with context-informed relational paths.

Result: FMS achieves superior performance on standard benchmarks compared to prior state-of-the-art methods.

Conclusion: FMS enhances KGC by integrating static and dynamic relational information, offering a more profound semantic understanding.

Abstract: Effective modeling of multifaceted relations is pivotal for Knowledge Graph
Completion (KGC). However, a majority of existing approaches are predicated on
static, embedding-based scoring, exhibiting inherent limitations in capturing
contextual dependencies and relational dynamics. Addressing this gap, we
propose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal
components: (1) a semantic context learning module that encodes
context-sensitive entity representations, and (2) a conditional flow-matching
module designed to learn the dynamic transformation from a head to a tail
embedding, governed by the aforementioned context. The resultant predictive
vector field, representing the context-informed relational path, serves to
dynamically refine the initial static score of an entity pair. Through this
synergy of context-aware static representations and conditioned dynamic
information, FMS facilitates a more profound modeling of relational semantics.
Comprehensive evaluations on several standard benchmarks demonstrate that our
proposed method surpasses prior state-of-the-art results.

</details>


### [419] [Benchmarking Deep Search over Heterogeneous Enterprise Data](https://arxiv.org/abs/2506.23139)
*Prafulla Kumar Choubey,Xiangyu Peng,Shilpa Bhagavath,Kung-Hsiang Huang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: A new benchmark for Deep Search evaluates retrieval-augmented generation (RAG) using diverse, sparse sources like documents, Slack messages, and URLs. It includes synthetic data simulating business workflows, with 39,190 artifacts and answerable/unanswerable queries. Experiments show top RAG methods score only 32.96, with retrieval being the main bottleneck.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of evaluating RAG systems in realistic, complex scenarios requiring multi-hop reasoning over diverse, noisy sources.

Method: A synthetic data pipeline simulates business workflows, generating interconnected content with noise and multi-hop questions. The benchmark includes 39,190 artifacts and varied queries.

Result: Best RAG methods score 32.96, with retrieval identified as the primary bottleneck due to struggles in deep search and evidence retrieval.

Conclusion: The benchmark highlights limitations in current RAG systems, emphasizing the need for improved retrieval methods to enhance performance in complex, real-world scenarios.

Abstract: We present a new benchmark for evaluating Deep Search--a realistic and
complex form of retrieval-augmented generation (RAG) that requires
source-aware, multi-hop reasoning over diverse, sparsed, but related sources.
These include documents, meeting transcripts, Slack messages, GitHub, and URLs,
which vary in structure and often contain human-to-human interactions. We build
it using a synthetic data pipeline that simulates business workflows across
product planning, development, and support stages, generating interconnected
content with realistic noise and multi-hop questions with guaranteed
ground-truth answers. We release our benchmark with both answerable and
unanswerable queries, and retrieval pool of 39,190 enterprise artifacts,
enabling fine-grained evaluation of long-context LLM and RAG systems. Our
experiments reveal that even the best-performing agentic RAG methods achieve an
average performance score of 32.96 on our benchmark. With further analysis, we
highlight retrieval as the main bottleneck: existing methods struggle to
conduct deep searches and retrieve all necessary evidence. Consequently, they
often reason over partial context, leading to significant performance
degradation.

</details>


### [420] [Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions](https://arxiv.org/abs/2506.23146)
*Dingzriui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: The paper introduces the Learning-to-Context Slope (LCS) metric to evaluate in-context learning (ICL) effectiveness in LLMs, addressing reliability, attribution, and data scarcity issues of current methods.


<details>
  <summary>Details</summary>
Motivation: Current ICL evaluation methods are unreliable, poorly attributed, and impractical in data-limited scenarios, necessitating a better metric.

Method: Proposes LCS, which models the slope between learning gain (loss decrease) and contextual relevance (demonstration-input relevance) to quantify ICL effectiveness.

Result: LCS reliably correlates with performance improvements, works in biased/data-scarce settings, and identifies actionable thresholds and critical model capabilities.

Conclusion: LCS is a robust metric for evaluating ICL effectiveness, offering practical insights for practitioners.

Abstract: In-context learning (ICL) has emerged as an effective approach to enhance the
performance of large language models (LLMs). However, its effectiveness varies
significantly across models and tasks, posing challenges for practitioners to
determine when ICL reliably improves performance. Current evaluation
approaches, reliant on performance change after applying ICL, suffer from low
reliability, poor attribution, and impracticality in data-insufficient
scenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that
quantifies ICL effectiveness by modeling the slope between learning gain (loss
decrease from demonstrations) and contextual relevance (demonstration-input
relevance). LCS addresses key limitations of performance-based metrics: (1) it
captures continuous loss changes even when outputs are incorrect, improving
reliability; (2) its formulation attributes ICL failures to weak contextual
alignment (inability to adapt inputs to demonstrations) or strong output
calibration (self-verification of correctness); and (3) it minimizes reliance
on labeled data via synthetic evaluation. Extensive experiments demonstrate
that LCS strongly correlates with performance improvements in labeled settings
and reliably reflects true effectiveness in biased or data-scarce scenarios.
Further analysis reveals actionable thresholds for LCS and identifies model
capabilities critical to ICL success.

</details>


### [421] [V-SYNTHESIS: Task-Agnostic Synthesis of Consistent and Diverse In-Context Demonstrations from Scratch via V-Entropy](https://arxiv.org/abs/2506.23149)
*Dingzirui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: The paper introduces V-Synthesis, a method to synthesize demonstrations from scratch for arbitrary tasks in ICL, using a novel consistency metric called V-Score to improve performance and reduce bias.


<details>
  <summary>Details</summary>
Motivation: High labeling costs for ICL demonstrations and limitations of existing synthesis methods (task-specific or reliant on pre-existing demonstrations) drive the need for a more flexible and efficient approach.

Method: Proposes V-Score, a consistency metric, and V-Synthesis, which uses proportional sampling based on V-Score to ensure high consistency and diversity in synthesized demonstrations.

Result: V-Synthesis improves performance by an average of 2.0% over existing methods, demonstrating its effectiveness.

Conclusion: V-Synthesis offers a scalable and efficient solution for synthesizing demonstrations from scratch, addressing the challenges of consistency and diversity.

Abstract: High labeling cost for in-context learning (ICL) demonstrations motivates
using large language models (LLMs) for synthesis to reduce overhead. However,
existing synthesis methods are mainly task-specific or rely on pre-existing
demonstrations. So this paper focuses on synthesizing demonstrations from
scratch for arbitrary tasks. A major challenge in synthesizing from scratch is
ensuring consistency with the target task, as the lack of labeling guidance
could lead to synthesis bias. We first propose a consistency metric called
V-Score, which has higher performance and lower computation cost compared with
the metrics based on grams or embedding vectors. Furthermore, we introduce
V-Synthesis, which leverages V-Score for proportional sampling to ensure both
high consistency and diversity of synthesized demonstrations. Experimental
results demonstrate that V-Synthesis yields an average performance improvement
of 2.0% compared to existing synthesis methods confirming the effectiveness of
V-Synthesis.

</details>


### [422] [RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams](https://arxiv.org/abs/2506.23192)
*Gabriel Iturra-Bocaz,Felipe Bravo-Marquez*

Main category: cs.CL

TL;DR: RiverText is a Python library for training and evaluating incremental word embeddings from text streams, addressing the static nature of traditional models.


<details>
  <summary>Details</summary>
Motivation: Traditional word embeddings are static and struggle with evolving language patterns, especially in dynamic contexts like social media.

Method: RiverText implements incremental techniques (Skip-gram, CBOW, Word Context Matrix) using PyTorch and adapts evaluation tasks for streaming.

Result: The library provides a standardized framework for dynamic word embeddings and compares methods with various hyperparameters.

Conclusion: RiverText is a valuable open-source tool for NLP and IR communities working with streaming text data.

Abstract: Word embeddings have become essential components in various information
retrieval and natural language processing tasks, such as ranking, document
classification, and question answering. However, despite their widespread use,
traditional word embedding models present a limitation in their static nature,
which hampers their ability to adapt to the constantly evolving language
patterns that emerge in sources such as social media and the web (e.g., new
hashtags or brand names). To overcome this problem, incremental word embedding
algorithms are introduced, capable of dynamically updating word representations
in response to new language patterns and processing continuous data streams.
  This paper presents RiverText, a Python library for training and evaluating
incremental word embeddings from text data streams. Our tool is a resource for
the information retrieval and natural language processing communities that work
with word embeddings in streaming scenarios, such as analyzing social media.
The library implements different incremental word embedding techniques, such as
Skip-gram, Continuous Bag of Words, and Word Context Matrix, in a standardized
framework. In addition, it uses PyTorch as its backend for neural network
training. We have implemented a module that adapts existing intrinsic static
word embedding evaluation tasks for word similarity and word categorization to
a streaming setting. Finally, we compare the implemented methods with different
hyperparameter settings and discuss the results. Our open-source library is
available at https://github.com/dccuchile/rivertext.

</details>


### [423] [Generalist Reward Models: Found Inside Large Language Models](https://arxiv.org/abs/2506.23235)
*Yi-Chen Li,Tian Xu,Yang Yu,Xuqin Zhang,Xiong-Hui Chen,Zhongxiang Ling,Ningjing Chao,Lei Yuan,Zhi-Hua Zhou*

Main category: cs.CL

TL;DR: The paper shows that LLMs inherently contain a latent reward model, equivalent to inverse reinforcement learning, eliminating the need for costly human preference data. This method outperforms existing approaches.


<details>
  <summary>Details</summary>
Motivation: To bypass the high cost of human preference data for aligning LLMs and provide a rigorous theoretical foundation for AI feedback methods.

Method: Proves that a latent reward model exists in LLMs trained via next-token prediction, equivalent to offline inverse reinforcement learning, and uses it for alignment without further training.

Result: Demonstrates superior performance over existing methods, including trained reward models, with a provably better error bound.

Conclusion: The findings suggest a more efficient and scalable paradigm for LLM alignment, leveraging pre-trained knowledge instead of explicit reward modeling.

Abstract: The alignment of Large Language Models (LLMs) is critically dependent on
reward models trained on costly human preference data. While recent work
explores bypassing this cost with AI feedback, these methods often lack a
rigorous theoretical foundation. In this paper, we discover that a powerful
generalist reward model is already latently present within any LLM trained via
standard next-token prediction. We prove that this endogenous reward is not a
heuristic, but is theoretically equivalent to a reward function learned through
offline inverse reinforcement learning. This connection allows us to directly
elicit a high-quality reward signal from a base (pre-trained or supervised
fine-tuned) model without any further training. Critically, we also prove that
subsequent reinforcement learning using this endogenous reward leads to a
policy with a provably superior error bound compared to the base model. To our
best knowledge, this is the first theoretical proof of the effectiveness of
reinforcement learning for LLMs. Our experiments validate this theory,
demonstrating that our method not only outperforms existing LLM-as-a-judge
approaches but can also surpass explicitly trained reward models. These
findings suggest that the reward modeling stage can be replaced by a principled
method of eliciting the knowledge already captured during pre-training,
heralding a more efficient, powerful, and scalable paradigm for LLMs alignment
as well as multi-modal models.

</details>


### [424] [Two Spelling Normalization Approaches Based on Large Language Models](https://arxiv.org/abs/2506.23288)
*Miguel Domingo,Francisco Casacuberta*

Main category: cs.CL

TL;DR: The paper proposes two new spelling normalization methods using large language models, comparing unsupervised training and machine translation, and finds machine translation more suitable.


<details>
  <summary>Details</summary>
Motivation: The lack of standardized spelling in historical documents poses challenges for scholars, prompting the need for effective normalization techniques.

Method: Two approaches are introduced: one using unsupervised training and another leveraging machine translation with large language models.

Result: Both methods show promise, but machine translation outperforms unsupervised training across diverse datasets.

Conclusion: Statistical machine translation remains the most effective technology for spelling normalization in historical texts.

Abstract: The absence of standardized spelling conventions and the organic evolution of
human language present an inherent linguistic challenge within historical
documents, a longstanding concern for scholars in the humanities. Addressing
this issue, spelling normalization endeavors to align a document's orthography
with contemporary standards. In this study, we propose two new approaches based
on large language models: one of which has been trained without a supervised
training, and a second one which has been trained for machine translation. Our
evaluation spans multiple datasets encompassing diverse languages and
historical periods, leading us to the conclusion that while both of them
yielded encouraging results, statistical machine translation still seems to be
the most suitable technology for this task.

</details>


### [425] [Objective-Free Local Learning and Emergent Language Structure in Thinking Machines](https://arxiv.org/abs/2506.23293)
*P. Myles Eugenio*

Main category: cs.CL

TL;DR: A neuro-symbolic framework for generative language modeling using emergent learning and hierarchical Hopfield memory, enabling unsupervised tokenization and coherent language generation.


<details>
  <summary>Details</summary>
Motivation: To explore how symbolic structure can emerge from local neural learning without predefined tokens or supervision, advancing scalable and interpretable neuro-symbolic systems.

Method: Uses a hierarchical Hopfield memory chain as a dynamic tokenizer, learning symbol sequences as multi-scale representations and binding co-occurring features into hierarchical tokens.

Result: The model generates synthetic languages with human-like morphology, generalizes beyond initial inference class, and supports compositional inference via emergent embedding neurons.

Conclusion: The framework provides a foundation for scalable, interpretable neuro-symbolic systems, advancing neuromorphic architectures for generative language models.

Abstract: We present a neuro-symbolic framework for generative language modeling based
on local, event-driven emergent learning. At its core is a hierarchical
Hopfield memory chain acting as a compositional short-term memory and dynamic
tokenizer (retokenizer). Rather than relying on predefined tokens or
supervision, the model builds structure from scratch, learning symbol sequences
as multi-scale representations. It constructs projection tensors that bind
co-occurring features into hierarchical tokens, introducing redundancy (i.e an
emergent gauge structure) and enabling compression of local activations into
long-range dependencies. Curiously, we find that the retokenizer can filter
natural language patterns from noise, generating synthetic languages with
coherent internal morphology -- quantifiably the same as human language.
Language is learned in a local (Hebbian) fashion, where model constraints
dictate allowed emergent structure, and new information is retained in
alignment with this structure. The absence of a global objective enables a form
of plasticity not found in conventional language models, allowing the system to
generalize beyond its initial inference class -- even without explicit data. We
demonstrate that briefly activating a new neuron during inference binds
distributed multi-scale token features into a symbolic embedding. These
emergent embedding neurons act as long-term memory and support a key-value
mechanism for compositional inference and generalization. This architecture
provides a methodological foundation for studying how symbolic structure can
emerge from local neural learning. It offers a new pathway for building
scalable, interpretable neuro-symbolic systems -- where tokens, grammar, and
reasoning arise as compressed memory traces within a Hopfield hierarchy. This
approach advances the development of neuromorphic architectures for generative
language models.

</details>


### [426] [Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)](https://arxiv.org/abs/2506.23315)
*Shouvon Sarker,Xishuang Dong,Lijun Qian*

Main category: cs.CL

TL;DR: The paper presents a BERT-based ensemble model for detecting and classifying medication events in clinical notes, improving performance metrics by 5-6%.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying key variables like medications from clinical data, leveraging the n2c2 2022 shared task and its annotated dataset (CMED).

Method: Pretrained BERT models on Wikipedia and MIMIC data, fine-tuned them on CMED, and used ensemble voting for final predictions.

Result: The ensemble model improved strict Micro-F score by ~5% and strict Macro-F score by ~6%.

Conclusion: BERT-based ensemble models are effective for medication event classification in clinical notes.

Abstract: Identification of key variables such as medications, diseases, relations from
health records and clinical notes has a wide range of applications in the
clinical domain. n2c2 2022 provided shared tasks on challenges in natural
language processing for clinical data analytics on electronic health records
(EHR), where it built a comprehensive annotated clinical data Contextualized
Medication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of
this challenge that is to detect and classify medication events from clinical
notes through building a novel BERT-based ensemble model. It started with
pretraining BERT models on different types of big data such as Wikipedia and
MIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED
training data. These fine-tuned BERT models were employed to accomplish
medication event classification on CMED testing data with multiple predictions.
These multiple predictions generated by these fine-tuned BERT models were
integrated to build final prediction with voting strategies. Experimental
results demonstrated that BERT-based ensemble models can effectively improve
strict Micro-F score by about 5% and strict Macro-F score by about 6%,
respectively.

</details>


### [427] [Information Loss in LLMs' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family](https://arxiv.org/abs/2506.23340)
*Yumeng Lin,Xufeng Duan,David Haslett,Yige Chen,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: The study examines how training data, language proximity, and family affect multilingual translation quality in GPT-4 and Llama 2, revealing data size and linguistic distance as key factors.


<details>
  <summary>Details</summary>
Motivation: To understand challenges in multilingual translation, especially for low-resource or linguistically distant languages, and identify factors influencing translation quality.

Method: Evaluated GPT-4 and Llama 2 using round-trip translations, assessed with BLEU scores and BERT similarity metrics.

Result: Found that abundant training data mitigates linguistic divergence, but languages closer to English perform better in low-resource settings. Orthographic, phylogenetic, syntactic, and geographical distances predict performance.

Conclusion: Translation quality depends on both data volume and structural/typological language relationships, highlighting linguistic constraints in multilingual models.

Abstract: Large language models have achieved impressive progress in multilingual
translation, yet they continue to face challenges with certain language
pairs-particularly those with limited training data or significant linguistic
divergence from English. This study systematically investigates how training
data, language proximity, and language family affect information loss in
multilingual translation. We evaluate two large language models, GPT-4 and
Llama 2, by performing round-trip translations. Translation quality was
assessed using BLEU scores and BERT similarity metrics. Our results reveal a
robust interaction between training data size and language distance: while
abundant training data can mitigate the effects of linguistic divergence,
languages structurally closer to English consistently yield higher translation
quality in low-resource conditions. Among various distance metrics,
orthographic, phylogenetic, syntactic, and geographical distances emerge as
strong predictors of translation performance. Language family also exerts an
independent influence. These findings contribute to a deeper understanding of
the linguistic constraints shaping multilingual translation in large language
models, emphasizing that translation quality is shaped not only by data volume
but also by structural and typological relationships between languages.

</details>


### [428] [ATGen: A Framework for Active Text Generation](https://arxiv.org/abs/2506.23342)
*Akim Tsvigun,Daniil Vasilev,Ivan Tsvigun,Ivan Lysenko,Talgat Bektleuov,Aleksandr Medvedev,Uliana Vinogradova,Nikita Severin,Mikhail Mozikov,Andrey Savchenko,Rostislav Grigorev,Ramil Kuleev,Fedor Zhdanov,Artem Shelmanov,Ilya Makarov*

Main category: cs.CL

TL;DR: ATGen bridges active learning (AL) with text generation tasks, reducing annotation effort and costs using human annotators and LLMs.


<details>
  <summary>Details</summary>
Motivation: Limited application of AL to natural language generation (NLG) tasks despite their popularity.

Method: ATGen framework integrates AL strategies with NLG, supporting human annotators and LLMs (e.g., ChatGPT, Claude).

Result: Reduces human annotation effort and LLM API costs; provides a platform for benchmarking AL strategies in NLG.

Conclusion: ATGen successfully applies AL to NLG, offering a practical and cost-effective solution.

Abstract: Active learning (AL) has demonstrated remarkable potential in reducing the
annotation effort required for training machine learning models. However,
despite the surging popularity of natural language generation (NLG) tasks in
recent years, the application of AL to NLG has been limited. In this paper, we
introduce Active Text Generation (ATGen) - a comprehensive framework that
bridges AL with text generation tasks, enabling the application of
state-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered
annotation in NLG tasks using both human annotators and automatic annotation
agents based on large language models (LLMs). The framework supports LLMs
deployed as services, such as ChatGPT and Claude, or operated on-premises.
Furthermore, ATGen provides a unified platform for smooth implementation and
benchmarking of novel AL strategies tailored to NLG tasks. Finally, we present
evaluation results for state-of-the-art AL strategies across diverse settings
and multiple text generation tasks. We show that ATGen reduces both the effort
of human annotators and costs associated with API calls to LLM-based annotation
agents. The code of the framework is available on GitHub under the MIT license.
The video presentation is available at http://atgen-video.nlpresearch.group

</details>


### [429] [Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs](https://arxiv.org/abs/2506.23377)
*Taejin Kim,Siun-Chuon Mau,Konrad Vesey*

Main category: cs.CL

TL;DR: The paper introduces Perspective-Dial, a method to quantify and control bias and perspective in LLM outputs using a metric space and systematic prompt engineering.


<details>
  <summary>Details</summary>
Motivation: There's a lack of quantifiable understanding of bias and perspective in LLM outputs, which are increasingly used in critical roles.

Method: Uses Perspective Space for measurement and Systematic Prompt Engineering with greedy-coordinate descent to control LLM output perspective.

Result: Empirically quantifies and adjusts LLM outputs for various topics, enabling applications like bias detection and narrative tracking.

Conclusion: Perspective-Dial provides a practical approach to manage LLM output perspectives without needing a deep theoretical understanding of bias.

Abstract: Large language models (LLMs) are used in a variety of mission-critical roles.
Due to the rapidly developing nature of LLMs, there is a lack of quantifiable
understanding of the bias and perspective associated with LLM output. Inspired
by this need, this paper considers the broader issue of perspective or
viewpoint of general text and perspective control of large-language model (LLM)
output. Perspective-Dial consists of two main components: a (1) metric space,
dubbed Perspective Space, that enables quantitative measurements of different
perspectives regarding a topic, and the use of (2) Systematic Prompt
Engineering that utilizes greedy-coordinate descent to control LLM output
perspective based on measurement feedback from the Perspective Space. The
empirical nature of the approach allows progress to side step a principled
understanding of perspective or bias -- effectively quantifying and adjusting
outputs for a variety of topics. Potential applications include detection,
tracking and mitigation of LLM bias, narrative detection, sense making and
tracking in public discourse, and debate bot advocating given perspective.

</details>


### [430] [Hierarchical Memory Organization for Wikipedia Generation](https://arxiv.org/abs/2506.23393)
*Eugene J. Yu,Dawei Zhu,Yifan Song,Xiangyu Wong,Jiebin Zhang,Wenxuan Shi,Xiaoguang Li,Qun Liu,Sujian Li*

Main category: cs.CL

TL;DR: The paper introduces the MOG framework for autonomously generating Wikipedia articles by organizing fine-grained memory units hierarchically, improving informativeness and verifiability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of integrating accurate and comprehensive information from diverse sources for Wikipedia article generation.

Method: Proposes the Memory Organization-based Generation (MOG) framework, which extracts and hierarchically organizes memory units from web documents to guide article generation, including a citation module for traceability.

Result: MOG outperforms baseline methods on the WikiStart dataset, producing more informative and reliable articles with minimized hallucinations.

Conclusion: MOG is effective for real-world Wikipedia article generation, ensuring alignment between memory and article structure while enhancing traceability.

Abstract: Generating Wikipedia articles autonomously is a challenging task requiring
the integration of accurate, comprehensive, and well-structured information
from diverse sources. This paper introduces the Memory Organization-based
Generation (MOG) framework, a novel approach to address these challenges by
leveraging a hierarchical memory architecture. MOG extracts fine-grained memory
units from web documents, recursively organizes them into a Wikipedia-style
hierarchical structure, and uses this structure to guide the generation
process. This ensures alignment between memory and the article outline,
improving both informativeness and verifiability while minimizing
hallucinations. Additionally, a citation module is implemented to enhance
traceability by linking every generated sentence to specific memory units.
Evaluations on our newly created WikiStart dataset demonstrate that MOG
outperforms baseline methods in producing informative and reliable articles,
making it particularly robust in real-world scenarios.

</details>


### [431] [Datasets for Fairness in Language Models: An In-Depth Survey](https://arxiv.org/abs/2506.23411)
*Jiale Zhang,Zichong Wang,Avash Palikhe,Zhipeng Yin,Wenbin Zhang*

Main category: cs.CL

TL;DR: A survey reviewing fairness datasets in language model research, introducing a unified evaluation framework to highlight biases and guide dataset selection.


<details>
  <summary>Details</summary>
Motivation: Address the lack of attention to fairness datasets in benchmarks, which shape evaluations of language models.

Method: Review and characterize widely used fairness datasets, introduce a unified evaluation framework, and apply it to 24 benchmarks.

Result: Reveals demographic disparities and overlooked biases, offering guidance for dataset use and interpretation.

Conclusion: Encourages more thoughtful use of fairness datasets and creation of diverse benchmarks; provides open resources for transparency.

Abstract: Fairness benchmarks play a central role in shaping how we evaluate language
models, yet surprisingly little attention has been given to examining the
datasets that these benchmarks rely on. This survey addresses that gap by
presenting a broad and careful review of the most widely used fairness datasets
in current language model research, characterizing them along several key
dimensions including their origin, scope, content, and intended use to help
researchers better appreciate the assumptions and limitations embedded in these
resources. To support more meaningful comparisons and analyses, we introduce a
unified evaluation framework that reveals consistent patterns of demographic
disparities across datasets and scoring methods. Applying this framework to
twenty four common benchmarks, we highlight the often overlooked biases that
can influence conclusions about model fairness and offer practical guidance for
selecting, combining, and interpreting these datasets. We also point to
opportunities for creating new fairness benchmarks that reflect more diverse
social contexts and encourage more thoughtful use of these tools going forward.
All code, data, and detailed results are publicly available at
https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets
to promote transparency and reproducibility across the research community.

</details>


### [432] [TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs](https://arxiv.org/abs/2506.23423)
*Felipe Nuti,Tim Franzmeyer,João Henriques*

Main category: cs.CL

TL;DR: The paper introduces Tuning Contribution (TuCo), a method to measure fine-tuning's impact on individual LLM outputs by analyzing hidden states and decomposing models into pre-training and fine-tuning components.


<details>
  <summary>Details</summary>
Motivation: Existing work lacks a systematic way to quantify fine-tuning's effect on individual LLM outputs, limiting understanding of its influence on model behavior and safety.

Method: Proposes TuCo, which tracks hidden states and decomposes models into pre-training and fine-tuning components, allowing scaling of the fine-tuning effect during inference.

Result: TuCo reveals that adversarial attacks reduce fine-tuning's influence, and successful attacks correlate with lower TuCo values.

Conclusion: TuCo provides a quantitative tool to study fine-tuning's role in model behavior and safety, highlighting its importance in adversarial scenarios.

Abstract: Past work has studied the effects of fine-tuning on large language models'
(LLMs) overall performance on certain tasks. However, a quantitative and
systematic method for analyzing its effect on individual outputs is still
lacking. Here, we propose a new method for measuring the contribution that
fine-tuning makes to individual LLM responses, assuming access to the original
pre-trained model. Our method tracks the model's intermediate hidden states,
providing a more fine-grained insight into the effects of fine-tuning than a
simple comparison of final outputs from pre-trained and fine-tuned models. We
introduce and theoretically analyze an exact decomposition of any fine-tuned
LLM into a pre-training component and a fine-tuning component. Empirically, we
find that model behavior and performance can be steered by up- or down-scaling
the fine-tuning component during the forward pass. Motivated by this finding
and our theoretical analysis, we define the Tuning Contribution (TuCo) as the
ratio of the magnitudes of the fine-tuning component to the pre-training
component. We observe that three prominent adversarial attacks on LLMs
circumvent safety measures in a way that reduces TuCo, and that TuCo is
consistently lower on prompts where these attacks succeed compared to those
where they do not. This suggests that attenuating the effect of fine-tuning on
model outputs plays a role in the success of such attacks. In summary, TuCo
enables the quantitative study of how fine-tuning influences model behavior and
safety, and vice versa.

</details>


### [433] [Pipelined Decoder for Efficient Context-Aware Text Generation](https://arxiv.org/abs/2506.23431)
*Zixian Huang,Chenxu Niu,Yu Gu,Gengyang Xiao,Xinwei Huang,Gong Cheng*

Main category: cs.CL

TL;DR: Proposes a pipelined decoder for parallel text generation, improving speed without compromising quality or memory.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models are slow due to sequential token generation, creating a bottleneck.

Method: Introduces a pipelined decoder that generates multiple subsequences in parallel at each time-step.

Result: Significantly improves generation speed in tasks like QA, summarization, and keyphrase generation, with minimal quality loss.

Conclusion: The pipelined decoder effectively addresses speed limitations while maintaining performance and efficiency.

Abstract: As the basis of generative AI, an autoregressive model requires the
generation of a new token depending on all the previously generated tokens,
which brings high quality but also restricts the model to generate tokens one
by one, forming a bottleneck limiting the generation speed. In this paper, we
propose a new decoder architecture that efficiently generates text in parallel
for context-aware generation tasks. Our proposed pipelined decoder initiates
the generation of multiple subsequences simultaneously, and, at each time-step,
it generates a new token for each subsequence to realize parallelism.
Experiments on multiple text generation tasks, including question answering,
text summarization, and keyphrase generation, show that our pipelined decoder
significantly improves the generation speed without a significant loss of
generation quality or additional memory consumption.

</details>


### [434] [What to Keep and What to Drop: Adaptive Table Filtering Framework](https://arxiv.org/abs/2506.23463)
*Jang Won June*

Main category: cs.CL

TL;DR: ATF (Adaptive Table Filtering Framework) prunes uninformative table columns and rows to improve LLM performance on large tables, reducing cells by ~70% and boosting TableQA tasks.


<details>
  <summary>Details</summary>
Motivation: Large tables exceed input limits for LLMs, hindering table-based reasoning.

Method: ATF uses LLM-generated column descriptions, clustering, and sparse-dense alignment scores for question-aware filtering.

Result: ATF reduces table cells by ~70%, improving TableQA performance but slightly dropping Table Fact Verification accuracy.

Conclusion: ATF adaptively balances informativeness and minimalism, enhancing LLM efficiency for table-based tasks.

Abstract: Large language models (LLMs) for table-based reasoning often struggle with
large tables due to input length limits. We propose ATF (Adaptive Table
Filtering Framework), a modular and question-aware filtering pipeline that
prunes uninformative columns and rows using LLM-generated column descriptions,
clustering, and sparse-dense alignment scores. ATF integrates seamlessly with
existing models (e.g., TAPAS, TAPEX) without retraining. Experiments show that
ATF reduces table cells by ~70\%, boosting performance on out-of-domain TableQA
tasks while causing slight performance drops on Table Fact Verification, where
full-table context is more critical. These results highlight ATF's ability to
adaptively balance informativeness and minimalism across tasks.

</details>


### [435] [Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent](https://arxiv.org/abs/2506.23485)
*Haocheng Yu,Yaxiong Wu,Hao Wang,Wei Guo,Yong Liu,Yawen Li,Yuyang Ye,Junping Du,Enhong Chen*

Main category: cs.CL

TL;DR: TAIRA is a thought-augmented LLM-powered interactive recommender agent system that improves handling of complex user intents through distilled thought patterns, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-powered interactive recommender agents struggle with diverse and complex user intents due to limited planning and generalization capabilities.

Method: TAIRA uses a multi-agent system with a manager agent for task decomposition and planning, enhanced by Thought Pattern Distillation (TPD). User simulation schemes evaluate performance.

Result: TAIRA outperforms existing methods, especially on challenging tasks, and generalizes well on novel tasks.

Conclusion: TAIRA effectively manages complex user intents in interactive recommendation systems, validated by experiments and user simulations.

Abstract: Interactive recommendation is a typical information-seeking task that allows
users to interactively express their needs through natural language and obtain
personalized recommendations. Large language model-powered (LLM-powered) agents
have become a new paradigm in interactive recommendations, effectively
capturing users' real-time needs and enhancing personalized experiences.
However, due to limited planning and generalization capabilities, existing
formulations of LLM-powered interactive recommender agents struggle to
effectively address diverse and complex user intents, such as intuitive,
unrefined, or occasionally ambiguous requests. To tackle this challenge, we
propose a novel thought-augmented interactive recommender agent system (TAIRA)
that addresses complex user intents through distilled thought patterns.
Specifically, TAIRA is designed as an LLM-powered multi-agent system featuring
a manager agent that orchestrates recommendation tasks by decomposing user
needs and planning subtasks, with its planning capacity strengthened through
Thought Pattern Distillation (TPD), a thought-augmentation method that extracts
high-level thoughts from the agent's and human experts' experiences. Moreover,
we designed a set of user simulation schemes to generate personalized queries
of different difficulties and evaluate the recommendations based on specific
datasets. Through comprehensive experiments conducted across multiple datasets,
TAIRA exhibits significantly enhanced performance compared to existing methods.
Notably, TAIRA shows a greater advantage on more challenging tasks while
generalizing effectively on novel tasks, further validating its superiority in
managing complex user intents within interactive recommendation systems. The
code is publicly available at:https://github.com/Alcein/TAIRA.

</details>


### [436] [Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably](https://arxiv.org/abs/2506.23508)
*Zhihao Zhang,Qiaole Dong,Qi Zhang,Jun Zhao,Enyu Zhou,Zhiheng Xi,Senjie Jin,Xiaoran Fan,Yuhao Zhou,Yanwei Fu,Tao Ji,Tao Gui,Xuanjing Huang*

Main category: cs.CL

TL;DR: SFT and RFT adapt multimodal models to tasks but differ in knowledge retention. SFT learns fast but forgets prior knowledge, while RFT learns slower but retains knowledge. RFT's alignment with the base model's probability landscape reduces interference.


<details>
  <summary>Details</summary>
Motivation: To understand how SFT and RFT impact prior knowledge in multimodal models when adapting to novel tasks.

Method: Used jigsaw puzzles as a novel task with Qwen2.5-VL, comparing SFT and RFT's learning dynamics and knowledge retention.

Result: SFT causes catastrophic forgetting, while RFT maintains prior knowledge by reinforcing aligned samples. Supervised training on RFT rollouts helps SFT preserve knowledge.

Conclusion: Data distribution, not algorithmic differences, drives forgetting. RFT is promising for stable continual learning in multimodal models.

Abstract: Post-training algorithms such as Supervised Fine-Tuning (SFT) and
Reinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large
language models to downstream tasks. While effective at task adaptation, their
impact on prior knowledge remains unclear. In this paper, we introduce jigsaw
puzzles as a novel task absent from existing pretraining corpora and
systematically study the behavior of SFT and RFT on an open-source multimodal
model, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid
task acquisition but leads to catastrophic forgetting, whereas RFT learns more
slowly on novel tasks but maintains prior knowledge. We analyze this phenomenon
through the lens of learning dynamics, showing that RFT reinforces correct
samples that are naturally aligned with the base model's probability landscape,
mitigating interference with prior knowledge. Moreover, supervised training on
correct RFT-simulated rollouts allows SFT to preserve knowledge while rapidly
learning new tasks. These findings suggest that data distribution, rather than
algorithmic differences, plays a central role in forgetting, and highlight
RFT's potential for stable continual learning in multimodal large language
models.

</details>


### [437] [NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning](https://arxiv.org/abs/2506.23524)
*Phan Quoc Hung Mai,Quang Hung Nguyen,Phuong Giang Duong,Hong Hanh Nguyen,Nguyen Tuan Long*

Main category: cs.CL

TL;DR: NEU-ESC is a new Vietnamese dataset for sentiment and topic classification in education, addressing gaps in existing datasets. It achieves high accuracy with multitask BERT models and is publicly available.


<details>
  <summary>Details</summary>
Motivation: Existing educational datasets lack domain relevance and student slang, especially in Vietnamese. NEU-ESC fills this gap by providing a richer, more diverse dataset.

Method: The dataset is curated from university forums. Multitask learning with BERT models is explored for sentiment and topic classification.

Result: Performance reaches 83.7% and 79.8% accuracy for sentiment and topic classification, respectively. Benchmarks compare NEU-ESC with other datasets and models.

Conclusion: NEU-ESC improves Vietnamese educational sentiment and topic analysis, with strong model performance and public availability for further research.

Abstract: In the field of education, understanding students' opinions through their
comments is crucial, especially in the Vietnamese language, where resources
remain limited. Existing educational datasets often lack domain relevance and
student slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese
dataset for Educational Sentiment Classification and Topic Classification,
curated from university forums, which offers more samples, richer class
diversity, longer texts, and broader vocabulary. In addition, we explore
multitask learning using encoder-only language models (BERT), in which we
showed that it achieves performance up to 83.7% and 79.8% accuracy for
sentiment and topic classification tasks. We also benchmark our dataset and
model with other datasets and models, including Large Language Models, and
discuss these benchmarks. The dataset is publicly available at:
https://huggingface.co/datasets/hung20gg/NEU-ESC.

</details>


### [438] [On Recipe Memorization and Creativity in Large Language Models: Is Your Model a Creative Cook, a Bad Cook, or Merely a Plagiator?](https://arxiv.org/abs/2506.23527)
*Jan Kvapil,Martin Fajcik*

Main category: cs.CL

TL;DR: The paper investigates memorization, creativity, and nonsense in LLM-generated recipes, using human annotations and an automated 'LLM-as-judge' pipeline for scalability.


<details>
  <summary>Details</summary>
Motivation: To analyze and quantify memorization, creativity, and nonsense in LLM-generated recipes, and to automate human annotation for scalability.

Method: Human annotation of 20 recipes (ingredients and steps) to assess memorization, creativity, and nonsense, followed by an automated 'LLM-as-judge' pipeline for large-scale analysis.

Result: Mixtral relies heavily on memorized content. The automated pipeline, using Llama 3.1+Gemma 2 9B, achieves 78% accuracy in ingredient matching.

Conclusion: The study provides a scalable framework to rigorously evaluate LLMs' creative capacities in recipe generation.

Abstract: This work-in-progress investigates the memorization, creativity, and nonsense
found in cooking recipes generated from Large Language Models (LLMs).
Precisely, we aim (i) to analyze memorization, creativity, and non-sense in
LLMs using a small, high-quality set of human judgments and (ii) to evaluate
potential approaches to automate such a human annotation in order to scale our
study to hundreds of recipes. To achieve (i), we conduct a detailed human
annotation on 20 preselected recipes generated by LLM (Mixtral), extracting
each recipe's ingredients and step-by-step actions to assess which elements are
memorized--i.e., directly traceable to online sources possibly seen during
training--and which arise from genuine creative synthesis or outright nonsense.
We find that Mixtral consistently reuses ingredients that can be found in
online documents, potentially seen during model training, suggesting strong
reliance on memorized content. To achieve aim (ii) and scale our analysis
beyond small sample sizes and single LLM validation, we design an
``LLM-as-judge'' pipeline that automates recipe generation, nonsense detection,
parsing ingredients and recipe steps, and their annotation. For instance,
comparing its output against human annotations, the best ingredient extractor
and annotator is Llama 3.1+Gemma 2 9B, achieving up to 78% accuracy on
ingredient matching. This automated framework enables large-scale
quantification of memorization, creativity, and nonsense in generated recipes,
providing rigorous evidence of the models' creative capacities.

</details>


### [439] [Semantic-guided Diverse Decoding for Large Language Model](https://arxiv.org/abs/2506.23601)
*Weijie Shi,Yue Cui,Yaguang Wu,Jingzhi Fang,Shibo Zhang,Mengze Li,Sirui Han,Jia Zhu,Jiajie Xu,Xiaofang Zhou*

Main category: cs.CL

TL;DR: SemDiD improves semantic diversity in language model outputs, outperforming existing methods in quality and diversity.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack semantic diversity, limiting applications like Best-of-N strategies and reinforcement learning.

Method: SemDiD uses orthogonal directional guidance, dynamic inter-group repulsion, and position-debiased probability assessment in embedding space.

Result: SemDiD improves Best-of-N coverage by 1.4-5.2%, accelerates RLHF training by 15%, and boosts accuracy by up to 2.1%.

Conclusion: SemDiD effectively balances quality and semantic diversity, outperforming current approaches.

Abstract: Diverse decoding of large language models is crucial for applications
requiring multiple semantically distinct responses, yet existing methods
primarily achieve lexical rather than semantic diversity. This limitation
significantly constrains Best-of-N strategies, group-based reinforcement
learning, and data synthesis. While temperature sampling and diverse beam
search modify token distributions or apply n-gram penalties, they fail to
ensure meaningful semantic differentiation. We introduce Semantic-guided
Diverse Decoding (SemDiD), operating directly in embedding space that balances
quality with diversity through three complementary mechanisms: orthogonal
directional guidance, dynamic inter-group repulsion, and position-debiased
probability assessment. SemDiD harmonizes these competing objectives using
adaptive gain functions and constraint optimization, ensuring both quality
thresholds and maximal semantic differentiation. Experiments show SemDiD
consistently outperforms existing methods, improving Best-of-N coverage by
1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15%
while increasing accuracy by up to 2.1%.

</details>


### [440] [Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs](https://arxiv.org/abs/2506.23610)
*Manuel Pratelli,Marinella Petrocchi*

Main category: cs.CL

TL;DR: LLMs can generate synthetic behavioral data, but their ability to replicate personality-driven psychological differences is uncertain. This study tests LLM agents conditioned on Big-Five traits to mimic human susceptibility to misinformation, finding some traits replicated while others diverge.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs can ethically and accurately simulate human personality-driven behaviors, particularly in discerning misinformation.

Method: LLM agents were conditioned on Big-Five personality profiles and compared to human responses in judging headline accuracy using published datasets.

Result: Some traits (Agreeableness, Conscientiousness) were reliably replicated, while others showed biases in how LLMs internalize personality.

Conclusion: LLMs show promise but have limits in simulating personality-driven behaviors, offering insights into modeling cognitive diversity in AI.

Abstract: Large language models (LLMs) make it possible to generate synthetic
behavioural data at scale, offering an ethical and low-cost alternative to
human experiments. Whether such data can faithfully capture psychological
differences driven by personality traits, however, remains an open question. We
evaluate the capacity of LLM agents, conditioned on Big-Five profiles, to
reproduce personality-based variation in susceptibility to misinformation,
focusing on news discernment, the ability to judge true headlines as true and
false headlines as false. Leveraging published datasets in which human
participants with known personality profiles rated headline accuracy, we create
matching LLM agents and compare their responses to the original human patterns.
Certain trait-misinformation associations, notably those involving
Agreeableness and Conscientiousness, are reliably replicated, whereas others
diverge, revealing systematic biases in how LLMs internalize and express
personality. The results underscore both the promise and the limits of
personality-aligned LLMs for behavioral simulation, and offer new insight into
modeling cognitive diversity in artificial agents.

</details>


### [441] [Robustness of Misinformation Classification Systems to Adversarial Examples Through BeamAttack](https://arxiv.org/abs/2506.23661)
*Arnisa Fazla,Lucas Krauter,David Guzman Piedrahita,Andrianos Michail*

Main category: cs.CL

TL;DR: BeamAttack is extended to include word deletions and skip substitutions, achieving high attack success rates while preserving text similarity.


<details>
  <summary>Details</summary>
Motivation: To enhance adversarial attack methods for evaluating text classification robustness by discovering minimal modifications.

Method: Extends BeamAttack with word deletions, skip substitutions, and LIME integration for prioritizing word replacements. Evaluated on BiLSTM, BERT, and RoBERTa.

Result: Over 99% attack success rate while maintaining semantic and lexical similarity.

Conclusion: BeamAttack is effective but has limitations; implementation is publicly available.

Abstract: We extend BeamAttack, an adversarial attack algorithm designed to evaluate
the robustness of text classification systems through word-level modifications
guided by beam search. Our extensions include support for word deletions and
the option to skip substitutions, enabling the discovery of minimal
modifications that alter model predictions. We also integrate LIME to better
prioritize word replacements. Evaluated across multiple datasets and victim
models (BiLSTM, BERT, and adversarially trained RoBERTa) within the BODEGA
framework, our approach achieves over a 99\% attack success rate while
preserving the semantic and lexical similarity of the original texts. Through
both quantitative and qualitative analysis, we highlight BeamAttack's
effectiveness and its limitations. Our implementation is available at
https://github.com/LucK1Y/BeamAttack

</details>


### [442] [Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation](https://arxiv.org/abs/2506.23662)
*Philip Lippmann,Jie Yang*

Main category: cs.CL

TL;DR: ZEST is a zero-shot framework for contextual embedding adaptation using synthetic proxy corpora, achieving near-full-corpus performance without retraining or real corpus access.


<details>
  <summary>Details</summary>
Motivation: Current context-aware embedding methods require target corpus access or domain-specific finetuning, which is impractical in privacy-sensitive or resource-limited settings.

Method: ZEST synthesizes a compact proxy corpus from a few exemplar documents, emulating domain-specific distributions, and uses it for zero-shot adaptation of frozen encoders.

Result: On the MTEB benchmark, ZEST performs within 0.5% of models with full corpus access, using only five example documents.

Conclusion: ZEST offers a practical solution for deploying adaptable, high-performance embeddings in constrained environments without retraining or real corpus access.

Abstract: Context-aware embedding methods boost retrieval accuracy by conditioning on
corpus statistics (e.g., term co-occurrence and topical patterns) extracted
from neighboring documents. However, this context-aware approach requires
access to the target corpus or requires domain-specific finetuning, posing
practical barriers in privacy-sensitive or resource-constrained settings. We
present ZEST, a zero-shot contextual adaptation framework that replaces real
corpus access with a one-time offline synthesis of a compact proxy. Given only
a handful exemplar documents representative of the general target domain, we
use a multi-step hierarchical procedure to generate a synthetic context corpus
of several hundred documents that aims to emulate key domain-specific
distributions. At inference, the frozen context-aware encoder uses this proxy
corpus -- without any finetuning or target corpus access -- to produce
domain-adapted embeddings. Across the MTEB benchmark, ZEST's zero-shot
synthetic context adaptation using only five example documents performs within
0.5% of models leveraging full target corpus access -- demonstrating remarkable
efficacy without any retraining. ZEST thus provides a practical method for
deploying high-performance, adaptable embeddings in constrained environments.

</details>


### [443] [L0: Reinforcement Learning to Become General Agents](https://arxiv.org/abs/2506.23667)
*Junjie Zhang,Jingyi Xi,Zhuoyang Song,Junyu Lu,Yuhua Ke,Ting Sun,Yukun Yang,Jiaxing Zhang,Songxin Zhang,Zejian Xie*

Main category: cs.CL

TL;DR: L-Zero (L0) is a scalable, end-to-end training pipeline for general-purpose agents, improving LLM performance on multi-turn tasks via reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Addressing scalability and training efficiency challenges in training LLMs for autonomous, multi-turn tasks.

Method: Introduces L0 with a low-cost, sandboxed agent worker pool and NB-Agent, a code-as-action scaffold. Uses Reinforcement Learning with Verifiable Rewards (RLVR).

Result: Boosts accuracy on SimpleQA from 30% to 80% and on HotpotQA from 22% to 41%.

Conclusion: L0 effectively enhances LLM problem-solving skills and is open-sourced for broader use.

Abstract: Training large language models (LLMs) to act as autonomous agents for
multi-turn, long-horizon tasks remains significant challenges in scalability
and training efficiency. To address this, we introduce L-Zero (L0), a scalable,
end-to-end training pipeline for general-purpose agents. Featuring a low-cost,
extensible, and sandboxed concurrent agent worker pool, L0 lowers the barrier
for applying reinforcement learning in complex environments. We also introduce
NB-Agent, the agent scaffold within L0, which operates in a "code-as-action"
fashion via a Read-Eval-Print-Loop (REPL). We evaluate L0 on factuality
question-answering benchmarks. Our experiments demonstrate that a base model
can develop robust problem-solving skills using solely Reinforcement Learning
with Verifiable Rewards (RLVR). On the Qwen2.5-7B-Instruct model, our method
boosts accuracy on SimpleQA from 30 % to 80 % and on HotpotQA from 22 % to 41
%. We have open-sourced the entire L0 system, including our L0 series models,
the NB-Agent, a complete training pipeline, and the corresponding training
recipes on (https://github.com/cmriat/l0).

</details>


### [444] [AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data](https://arxiv.org/abs/2506.23735)
*JiaRu Wu,Mingwei Liu*

Main category: cs.CL

TL;DR: AutoEvoEval is a framework for evaluating LLMs using controlled, diverse, and realistic test samples via interpretable atomic evolution operations. It reveals significant accuracy drops and model sensitivities, highlighting the limitations of current benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation benchmarks for LLMs are static and lack systematic control over perturbation types and complexity, limiting robustness assessment.

Method: Proposes AutoEvoEval, a framework with 22 atomic evolution operations and multi-round compositions to generate diverse test samples for close-ended tasks.

Result: Atomic operations cause a 7.283% accuracy drop, with structure-disrupting edits being most harmful. Combining steps amplifies adversarial effects by up to 52.932%.

Conclusion: Current benchmarks may overestimate model generalization; evolution-aware evaluation is needed for robust assessment.

Abstract: Large language models (LLMs) have shown remarkable performance on various
tasks, but existing evaluation benchmarks are often static and insufficient to
fully assess their robustness and generalization in realistic scenarios. Prior
work using evolutionary or adversarial data augmentation has improved
evaluation diversity but lacks systematic control over perturbation types and
multi-step complexity, limiting comprehensive robustness analysis. To address
these gaps, we propose AutoEvoEval, an evolution-based evaluation framework for
close-ended tasks such as multi-choice question answering. AutoEvoEval
introduces 22 interpretable atomic evolution operations and supports
multi-round compositions, enabling controlled generation of diverse,
challenging, and realistic test samples. We conduct extensive experiments
addressing four research questions on a broad set of open- and closed-source
LLMs. Our results show that atomic operations cause an average accuracy drop of
7.283\%, with structure-disrupting or misleading semantic edits causing the
largest declines. Model sensitivities vary significantly for the same
perturbation, and combining multiple evolution steps amplifies adversarial
effects by up to 52.932\%. These findings suggest current benchmarks may
overestimate true model generalization and emphasize the need for
evolution-aware robustness evaluation. Code and resources are available at:
https://github.com/SYSUSELab/AutoEvoEval.

</details>


### [445] [Positional Bias in Binary Question Answering: How Uncertainty Shapes Model Preferences](https://arxiv.org/abs/2506.23743)
*Tiziano Labruna,Simone Gallo,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: The paper investigates positional bias in binary question answering across five large language models, showing bias grows with answer uncertainty.


<details>
  <summary>Details</summary>
Motivation: To quantify and analyze positional bias in models when answer uncertainty varies.

Method: Adapted SQuAD-it dataset with incorrect options and tested on WebGPT and Winning Arguments benchmarks, flipping answer order to measure bias.

Result: Positional bias is minimal in low-uncertainty conditions but increases exponentially with uncertainty.

Conclusion: Answer uncertainty significantly influences positional bias in models, highlighting a need for bias mitigation in uncertain scenarios.

Abstract: Positional bias in binary question answering occurs when a model
systematically favors one choice over another based solely on the ordering of
presented options. In this study, we quantify and analyze positional bias
across five large language models under varying degrees of answer uncertainty.
We re-adapted the SQuAD-it dataset by adding an extra incorrect answer option
and then created multiple versions with progressively less context and more
out-of-context answers, yielding datasets that range from low to high
uncertainty. Additionally, we evaluate two naturally higher-uncertainty
benchmarks: (1) WebGPT - question pairs with unequal human-assigned quality
scores, and (2) Winning Arguments - where models predict the more persuasive
argument in Reddit's r/ChangeMyView exchanges. Across each dataset, the order
of the "correct" (or higher-quality/persuasive) option is systematically
flipped (first placed in position 1, then in position 2) to compute both
Preference Fairness and Position Consistency. We observe that positional bias
is nearly absent under low-uncertainty conditions, but grows exponentially when
it becomes doubtful to decide which option is correct.

</details>


### [446] [Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model](https://arxiv.org/abs/2506.23840)
*Bowen Ding,Yuhan Chen,Futing Wang,Lingfeng Ming,Tao Lin*

Main category: cs.CL

TL;DR: DuP-PO improves token efficiency in Large Reasoning Models by reducing unnecessary thinking tokens, enhancing performance on simple tasks.


<details>
  <summary>Details</summary>
Motivation: LRMs waste tokens on verbose responses for simple tasks due to thinking tokens, reducing efficiency.

Method: Proposes DuP-PO with rollout sampling, advantage control, and policy shaping to regulate thinking tokens.

Result: DuP-PO boosts token efficiency and maintains or improves performance on math reasoning benchmarks.

Conclusion: DuP-PO effectively mitigates the thinking trap in LRMs, optimizing token usage without sacrificing accuracy.

Abstract: Large Reasoning Models (LRMs) excel at solving complex problems but face an
overthinking dilemma. When handling simple tasks, they often produce verbose
responses overloaded with thinking tokens (e.g., wait, however). These tokens
trigger unnecessary high-level reasoning behaviors like reflection and
backtracking, reducing efficiency. In this work, our pilot study reveals that
these thinking-token-induced behaviors are not essential for effective
problem-solving and may even hinder correct reasoning within constrained token
budgets. We identify this phenomenon as the thinking trap. To mitigate this
issue, we propose Dual Policy Preference Optimization (DuP-PO), a novel
algorithm featuring: (1) A rollout sampling strategy that guarantees balanced
exposure to responses with and without thinking tokens; (2) A fine-grained
advantage control technique to dynamically regulate the prediction of target
tokens; (3) A policy shaping method ensuring stable gradient contributions from
thinking tokens. Experimental results on five popular math reasoning benchmarks
show that DuP-PO performs well on the popular LRM, which significantly improves
their token efficiency during reasoning, while achieving superior performance
of the base model.

</details>


### [447] [Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What to Do About It](https://arxiv.org/abs/2506.23864)
*Seyed Mahed Mousavi,Edoardo Cecchinato,Lucia Hornikova,Giuseppe Riccardi*

Main category: cs.CL

TL;DR: The paper audits three reasoning benchmarks (SocialIQa, FauxPas-EAI, ToMi) and finds flaws in items and evaluation methods. Using LLMs, it identifies design and scoring issues, showing model performance is sensitive to input variations, not reasoning. It calls for better evaluation protocols.


<details>
  <summary>Details</summary>
Motivation: To assess the validity of reasoning benchmarks and LLM evaluations, uncovering flaws that may misrepresent model capabilities.

Method: Systematic audit using five LLMs (GPT-3, 3.5, 4, o1, LLaMA 3.1) and human annotation to identify benchmark issues and re-evaluate cleaned subsets.

Result: Found structural, semantic, and pragmatic flaws in benchmarks. Model scores improve due to input variations, not reasoning. Performance is sensitive to minor changes.

Conclusion: Current benchmarks may misrepresent LLM reasoning. Need for evaluation protocols focusing on inference processes, not static outputs. Audited data and tools released for better assessments.

Abstract: We conduct a systematic audit of three widely used reasoning benchmarks,
SocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark
items and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and
LLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic
issues in benchmark design (e.g., duplicated items, ambiguous wording, and
implausible answers), as well as scoring procedures that prioritize output form
over reasoning process. Through systematic human annotation and re-evaluation
on cleaned benchmark subsets, we find that model scores often improve not due
to due to erratic surface wording variations and not to improved reasoning.
Infact, further analyses show that model performance is highly sensitive to
minor input variations such as context availability and phrasing, revealing
that high scores may reflect alignment with format-specific cues rather than
consistent inference based on the input. These findings challenge the validity
of current benchmark-based claims about reasoning in LLMs, and highlight the
need for evaluation protocols that assess reasoning as a process of drawing
inference from available information, rather than as static output selection.
We release audited data and evaluation tools to support more interpretable and
diagnostic assessments of model reasoning.

</details>


### [448] [Advancing Multi-Step Mathematical Reasoning in Large Language Models through Multi-Layered Self-Reflection with Auto-Prompting](https://arxiv.org/abs/2506.23888)
*André de Souza Loureiro,Jorge Valverde-Rebaza,Julieta Noguez,David Escarcega,Ricardo Marcacini*

Main category: cs.CL

TL;DR: The MAPS framework enhances multi-step reasoning in LLMs by combining CoT, Self-Reflection, and Auto-Prompting, outperforming standard methods and matching specialized models.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex multi-step reasoning tasks, necessitating a more adaptive and iterative approach.

Method: MAPS integrates CoT, Self-Reflection, and Auto-Prompting to iteratively refine reasoning through dynamic prompts.

Result: MAPS outperforms standard CoT and achieves competitive results with reasoning-optimized models on benchmarks.

Conclusion: MAPS balances accuracy and cost by limiting reflection depth, enabling general-purpose LLMs to perform like specialized models.

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
improved their problem-solving capabilities. However, these models still
struggle when faced with complex multi-step reasoning tasks. In this paper, we
propose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework,
a novel approach designed to enhance multi-step mathematical reasoning in LLMs
by integrating techniques such as Chain of Thought (CoT), Self-Reflection, and
Auto-Prompting. Unlike traditional static prompting methods, MAPS employs an
iterative refinement process. Initially, the model generates a solution using
CoT prompting. When errors are detected, an adaptive self-reflection mechanism
identifies and analyzes them, generating tailored prompts to guide corrections.
These dynamically adjusted prompts enable the model to iteratively refine its
reasoning. Experiments on four well-established benchmarks across multiple LLMs
show that MAPS significantly outperforms standard CoT and achieves competitive
results with reasoning-optimized models. In addition, MAPS enables
general-purpose LLMs to reach performance levels comparable to specialized
reasoning models. While deeper reflection layers improve accuracy, they also
increase token usage and costs. To balance this trade-off, MAPS strategically
limits reflection depth, ensuring an optimal balance between cost and reasoning
performance.

</details>


### [449] [The Trilemma of Truth in Large Language Models](https://arxiv.org/abs/2506.23921)
*Germans Savcisens,Tina Eliassi-Rad*

Main category: cs.CL

TL;DR: The paper introduces sAwMIL, a method to assess the veracity of LLMs' internal knowledge, revealing key insights about truth signals and probe performance.


<details>
  <summary>Details</summary>
Motivation: To address flawed assumptions in existing methods for probing LLM knowledge and provide a reliable way to verify what LLMs 'know.'

Method: Introduces sAwMIL, a probing method using internal activations of LLMs, based on multiple-instance learning and conformal prediction.

Result: Evaluated on 16 LLMs and 3 datasets, findings include truth signal concentration, asymmetry in truth/falsehood signals, and probe performance differences.

Conclusion: sAwMIL offers a reliable method for verifying LLM knowledge, with insights into truth signals and probe effectiveness.

Abstract: We often attribute human characteristics to large language models (LLMs) and
claim that they "know" certain things. LLMs have an internal probabilistic
knowledge that represents information retained during training. How can we
assess the veracity of this knowledge? We examine two common methods for
probing the veracity of LLMs and discover several assumptions that are flawed.
To address these flawed assumptions, we introduce sAwMIL (short for Sparse
Aware Multiple-Instance Learning), a probing method that utilizes the internal
activations of LLMs to separate statements into true, false, and neither.
sAwMIL is based on multiple-instance learning and conformal prediction. We
evaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including
both default and chat-based variants, as well as on 3 new datasets. Among the
insights we provide are: (1) the veracity signal is often concentrated in the
third quarter of an LLM's depth; (2) truth and falsehood signals are not always
symmetric; (3) linear probes perform better on chat models than on default
models; (4) nonlinear probes may be required to capture veracity signals for
some LLMs with reinforcement learning from human feedback or knowledge
distillation; and (5) LLMs capture a third type of signal that is distinct from
true and false and is neither true nor false. These findings provide a reliable
method for verifying what LLMs "know" and how certain they are of their
probabilistic internal knowledge.

</details>


### [450] [IMPACT: Inflectional Morphology Probes Across Complex Typologies](https://arxiv.org/abs/2506.23929)
*Mohammed J. Saeed,Tommi Vehvilainen,Evgeny Fedoseev,Sevil Caliskan,Tatiana Vodolazova*

Main category: cs.CL

TL;DR: IMPACT is a synthetic evaluation framework for assessing LLMs' understanding of inflectional morphology in five morphologically rich languages, revealing gaps in their linguistic complexity handling.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs truly grasp linguistic complexity, especially in morphology, beyond fluent outputs in non-English languages.

Method: Introduce IMPACT, a synthetic framework with unit-test-style cases for evaluating LLMs across Arabic, Russian, Finnish, Turkish, and Hebrew.

Result: LLMs struggle with morphologically rich languages and uncommon patterns, especially in judging ungrammatical examples, and some techniques degrade performance.

Conclusion: LLMs have significant gaps in handling linguistic complexity, highlighting the need for improvement; IMPACT is released to aid further research.

Abstract: Large Language Models (LLMs) have shown significant progress on various
multilingual benchmarks and are increasingly used to generate and evaluate text
in non-English languages. However, while they may produce fluent outputs, it
remains unclear to what extent these models truly grasp the underlying
linguistic complexity of those languages, particularly in morphology. To
investigate this, we introduce IMPACT, a synthetically generated evaluation
framework focused on inflectional morphology, which we publicly release,
designed to evaluate LLM performance across five morphologically rich
languages: Arabic, Russian, Finnish, Turkish, and Hebrew. IMPACT includes
unit-test-style cases covering both shared and language-specific phenomena,
from basic verb inflections (e.g., tense, number, gender) to unique features
like Arabic's reverse gender agreement and vowel harmony in Finnish and
Turkish. We assess eight multilingual LLMs that, despite strong English
performance, struggle with other languages and uncommon morphological patterns,
especially when judging ungrammatical examples. We also show that Chain of
Thought and Thinking Models can degrade performance. Our work exposes gaps in
LLMs' handling of linguistic complexity, pointing to clear room for
improvement. To support further research, we publicly release the IMPACT
framework.

</details>


### [451] [Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages](https://arxiv.org/abs/2506.23930)
*Ruhina Tabasshum Prome,Tarikul Islam Tamiti,Anomadarshi Barua*

Main category: cs.CL

TL;DR: The paper explores prompt engineering on LLMs for hate speech detection in low-resource languages, introducing metaphor prompting and evaluating six strategies on Llama2-7B.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of hate speech detection in low-resource languages due to limited datasets, focusing on Bengali and extending to Hindi, English, and German.

Method: Six prompting strategies (e.g., zero-shot, metaphor prompting) tested on Llama2-7B, compared with pre-trained embeddings (GloVe, Word2Vec, FastText) and deep learning models (MLP, CNN, BiGRU).

Result: Performance evaluated via F1 score and environmental impact (CO2 emissions, electricity, computational time), demonstrating metaphor prompting's effectiveness.

Conclusion: Metaphor prompting is innovative and effective for hate speech detection in low-resource languages, outperforming traditional methods while considering environmental impact.

Abstract: The rapid expansion of social media leads to a marked increase in hate
speech, which threatens personal lives and results in numerous hate crimes.
Detecting hate speech presents several challenges: diverse dialects, frequent
code-mixing, and the prevalence of misspelled words in user-generated content
on social media platforms. Recent progress in hate speech detection is
typically concentrated on high-resource languages. However, low-resource
languages still face significant challenges due to the lack of large-scale,
high-quality datasets. This paper investigates how we can overcome this
limitation via prompt engineering on large language models (LLMs) focusing on
low-resource Bengali language. We investigate six prompting strategies -
zero-shot prompting, refusal suppression, flattering the classifier, multi-shot
prompting, role prompting, and finally our innovative metaphor prompting to
detect hate speech effectively in low-resource languages. We pioneer the
metaphor prompting to circumvent the built-in safety mechanisms of LLMs that
marks a significant departure from existing jailbreaking methods. We
investigate all six different prompting strategies on the Llama2-7B model and
compare the results extensively with three pre-trained word embeddings - GloVe,
Word2Vec, and FastText for three different deep learning models - multilayer
perceptron (MLP), convolutional neural network (CNN), and bidirectional gated
recurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in
the low-resource Bengali language, we also evaluate it in another low-resource
language - Hindi, and two high-resource languages - English and German. The
performance of all prompting techniques is evaluated using the F1 score, and
environmental impact factor (IF), which measures CO$_2$ emissions, electricity
usage, and computational time.

</details>


### [452] [Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs](https://arxiv.org/abs/2506.23940)
*Yang Dai,Jianxiang An,Tianwei Lin,Hongyang He,Hongzhe Huang,Wenqiao Zhang,Zheqi Lv,Siliang Tang,Yueting Zhuang*

Main category: cs.CL

TL;DR: A framework for unifying domain-specific MLLMs via parameter integration, using CAPS for efficient fusion and domain compatibility scoring.


<details>
  <summary>Details</summary>
Motivation: Addressing the fragmentation of knowledge in domain-specific MLLMs and enabling modular composition of expert capabilities.

Method: Compatibility-Aware Parameter Splicing (CAPS) strategy for selective parameter fusion, extended to low-rank adaptation layers, with domain compatibility scoring.

Result: Effective integration of heterogeneous expertise across diverse multimodal benchmarks, with minimal inference overhead.

Conclusion: The framework offers a scalable solution for compositional, domain-adaptive MLLMs.

Abstract: Multimodal Large Language Models (MLLMs) have achieved success across various
domains. However, their applicability tends to degrade when confronted with
different types of data inputs, especially for MLLMs that have been fine-tuned
for specific tasks. Despite its importance, the study of knowledge sharing
among domain-specific MLLMs--such as those trained for mathematics or
code--remains largely underexplored. To address the fragmentation of knowledge
across domain-specialized MLLMs, we propose a unified parameter integration
framework that enables modular composition of expert capabilities. Our method
is grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy,
which leverages both local functional attribution and global
information-theoretic signals to guide selective parameter fusion. By extending
this mechanism to the low-rank adaptation layer granularity, we ensure
efficient integration with minimal inference overhead. Furthermore, we
introduce a domain compatibility scoring mechanism that quantifies inter-expert
alignment at the activation level and correlates with downstream task utility.
This principled fusion protocol allows the final model to synergize
heterogeneous expertise while preserving structural modularity. Extensive
evaluations across diverse multimodal benchmarks validate the effectiveness of
our framework, offering a scalable path toward compositional, domain-adaptive
MLLMs.

</details>


### [453] [Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders](https://arxiv.org/abs/2506.23951)
*Mathis Le Bail,Jérémie Dentan,Davide Buscaldi,Sonia Vanier*

Main category: cs.CL

TL;DR: SAEs are adapted for sentence classification, improving interpretability and causality of extracted features compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: To explore SAE-based explainability in sentence classification, a less-studied domain, and enhance interpretability.

Method: A novel SAE-based architecture with a specialized classifier head and activation rate sparsity loss, benchmarked against ConceptShap, ICA, and other SAE techniques.

Result: Improved causality and interpretability of features, validated on two benchmarks and four Pythia LLMs.

Conclusion: The proposed SAE architecture is effective for sentence classification, offering better interpretability and causality.

Abstract: Sparse Autoencoders (SAEs) have been successfully used to probe Large
Language Models (LLMs) and extract interpretable concepts from their internal
representations. These concepts are linear combinations of neuron activations
that correspond to human-interpretable features. In this paper, we investigate
the effectiveness of SAE-based explainability approaches for sentence
classification, a domain where such methods have not been extensively explored.
We present a novel SAE-based architecture tailored for text classification,
leveraging a specialized classifier head and incorporating an activation rate
sparsity loss. We benchmark this architecture against established methods such
as ConceptShap, Independent Component Analysis, and other SAE-based concept
extraction techniques. Our evaluation covers two classification benchmarks and
four fine-tuned LLMs from the Pythia family. We further enrich our analysis
with two novel metrics for measuring the precision of concept-based
explanations, using an external sentence encoder. Our empirical results show
that our architecture improves both the causality and interpretability of the
extracted features.

</details>


### [454] [TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation](https://arxiv.org/abs/2506.23979)
*Renren Jin,Tianhao Shen,Xinwei Wu,Dan Shi,Haoran Sun,Wuwei Huang,Quandong Wang,Wei Liu,Jian Luan,Bin Wang,Deyi Xiong*

Main category: cs.CL

TL;DR: The TaP framework automates scalable preference dataset generation for multilingual LLM fine-tuning, outperforming larger datasets.


<details>
  <summary>Details</summary>
Motivation: High-quality datasets for LLM fine-tuning are resource-intensive and mostly English-only, necessitating a scalable multilingual solution.

Method: TaP uses a structured taxonomy to automate diverse and comprehensive preference dataset generation for supervised and preference fine-tuning.

Result: LLMs trained on TaP-generated datasets outperform those using larger open-source datasets.

Conclusion: TaP provides an efficient, scalable solution for multilingual preference dataset generation, enhancing LLM performance.

Abstract: Conducting supervised fine-tuning and preference fine-tuning on large
language models (LLMs) requires high-quality datasets to improve their ability
to follow instructions and align with human preferences and values. However,
constructing such datasets is resource-intensive, and most available datasets
for supervised and preference fine-tuning are in English. To address these
challenges, we propose the \underline{\textbf{Ta}}xonomy-Guided
\underline{\textbf{P}}reference Data Generation (TaP) framework, which
facilitates automated and scalable construction of preference datasets across
various languages. TaP is grounded in a structured taxonomy that allows
fine-grained control over dataset composition, thereby ensuring both diversity
and comprehensive coverage. We employ TaP-generated datasets to perform
supervised and preference fine-tuning on various LLMs. Experimental results
demonstrate that LLMs trained on TaP-generated datasets outperform those
trained on existing open-source datasets. Remarkably, LLMs trained on
TaP-generated datasets surpass the performance of those trained on an
open-source dataset that is 180 times larger.

</details>


### [455] [Machine Understanding of Scientific Language](https://arxiv.org/abs/2506.23990)
*Dustin Wright*

Main category: cs.CL

TL;DR: The paper focuses on developing tools and methods for machine understanding of scientific language to identify faithfulness in scientific texts, addressing challenges like limited data and misinformation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the societal problem of identifying the faithfulness of scientific texts automatically due to the increasing volume of potentially misleading information online.

Method: The thesis contributes to natural language processing and machine learning through methods like automatic fact-checking, adversarial claim generation, domain adaptation, and zero-shot scientific fact-checking.

Result: The research provides new methods and resources for detecting exaggerated claims, modeling information change, and learning from limited data, demonstrating their effectiveness in identifying misinformation.

Conclusion: The thesis concludes that its outputs are valuable for analyzing science communication at scale and generating insights into the process, particularly in handling limited data and misinformation.

Abstract: Scientific information expresses human understanding of nature. This
knowledge is largely disseminated in different forms of text, including
scientific papers, news articles, and discourse among people on social media.
While important for accelerating our pursuit of knowledge, not all scientific
text is faithful to the underlying science. As the volume of this text has
burgeoned online in recent years, it has become a problem of societal
importance to be able to identify the faithfulness of a given piece of
scientific text automatically. This thesis is concerned with the cultivation of
datasets, methods, and tools for machine understanding of scientific language,
in order to analyze and understand science communication at scale. To arrive at
this, I present several contributions in three areas of natural language
processing and machine learning: automatic fact checking, learning with limited
data, and scientific text processing. These contributions include new methods
and resources for identifying check-worthy claims, adversarial claim
generation, multi-source domain adaptation, learning from crowd-sourced labels,
cite-worthiness detection, zero-shot scientific fact checking, detecting
exaggerated scientific claims, and modeling degrees of information change in
science communication. Critically, I demonstrate how the research outputs of
this thesis are useful for effectively learning from limited amounts of
scientific text in order to identify misinformative scientific statements and
generate new insights into the science communication process

</details>


### [456] [Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning](https://arxiv.org/abs/2506.23998)
*Seungjun Yi,Joakim Nguyen,Huimin Xu,Terence Lim,Andrew Well,Mia Markey,Ying Ding*

Main category: cs.CL

TL;DR: An automated LLM pipeline for thematic analysis of clinical narratives in CHD, reducing manual effort and improving scalability.


<details>
  <summary>Details</summary>
Motivation: Traditional thematic analysis of CHD narratives is labor-intensive and unscalable, necessitating an automated solution.

Method: A multi-agent LLM framework with optional RLHF for end-to-end thematic analysis of clinical narratives.

Result: Enables scalable, patient-centered analysis of qualitative datasets, aligning with human analysis.

Conclusion: The proposed system offers a scalable, automated alternative to manual thematic analysis in CHD research.

Abstract: Congenital heart disease (CHD) presents complex, lifelong challenges often
underrepresented in traditional clinical metrics. While unstructured narratives
offer rich insights into patient and caregiver experiences, manual thematic
analysis (TA) remains labor-intensive and unscalable. We propose a fully
automated large language model (LLM) pipeline that performs end-to-end TA on
clinical narratives, which eliminates the need for manual coding or full
transcript review. Our system employs a novel multi-agent framework, where
specialized LLM agents assume roles to enhance theme quality and alignment with
human analysis. To further improve thematic relevance, we optionally integrate
reinforcement learning from human feedback (RLHF). This supports scalable,
patient-centered analysis of large qualitative datasets and allows LLMs to be
fine-tuned for specific clinical contexts.

</details>


### [457] [Large Language Models Don't Make Sense of Word Problems. A Scoping Review from a Mathematics Education Perspective](https://arxiv.org/abs/2506.24006)
*Anselm R. Strohmaier,Wim Van Dooren,Kathrin Seßler,Brian Greer,Lieven Verschaffel*

Main category: cs.CL

TL;DR: LLMs excel at solving word problems but lack deep understanding of real-world context, limiting their educational value.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' ability to support math learning by solving word problems and their implications for education.

Method: Conducted a scoping review with a technical overview, systematic literature review, and empirical evaluation of LLMs on word problems.

Result: LLMs solve s-problems with high accuracy but struggle with real-world context, showing superficial understanding.

Conclusion: LLMs' lack of deep comprehension limits their effectiveness as instructional tools in math classrooms.

Abstract: The progress of Large Language Models (LLMs) like ChatGPT raises the question
of how they can be integrated into education. One hope is that they can support
mathematics learning, including word-problem solving. Since LLMs can handle
textual input with ease, they appear well-suited for solving mathematical word
problems. Yet their real competence, whether they can make sense of the
real-world context, and the implications for classrooms remain unclear. We
conducted a scoping review from a mathematics-education perspective, including
three parts: a technical overview, a systematic review of word problems used in
research, and a state-of-the-art empirical evaluation of LLMs on mathematical
word problems. First, in the technical overview, we contrast the
conceptualization of word problems and their solution processes between LLMs
and students. In computer-science research this is typically labeled
mathematical reasoning, a term that does not align with usage in mathematics
education. Second, our literature review of 213 studies shows that the most
popular word-problem corpora are dominated by s-problems, which do not require
a consideration of realities of their real-world context. Finally, our
evaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, and o3 on 287 word problems
shows that most recent LLMs solve these s-problems with near-perfect accuracy,
including a perfect score on 20 problems from PISA. LLMs still showed
weaknesses in tackling problems where the real-world context is problematic or
non-sensical. In sum, we argue based on all three aspects that LLMs have
mastered a superficial solution process but do not make sense of word problems,
which potentially limits their value as instructional tools in mathematics
classrooms.

</details>


### [458] [EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations](https://arxiv.org/abs/2506.24016)
*Hyunjong Kim,Sangyeop Kim,Jongheon Jeong,Yeongjae Cho,Sungzoon Cho*

Main category: cs.CL

TL;DR: EXPERT is a reference-free evaluation metric for image captioning, providing structured explanations based on fluency, relevance, and descriptiveness, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current explainable metrics lack standardized criteria and verified explanation quality, prompting the need for a structured, high-quality evaluation method.

Method: EXPERT uses a two-stage evaluation template to supervise a vision-language model for scoring and explanation generation, leveraging large-scale datasets of structured explanations.

Result: EXPERT outperforms existing metrics on benchmarks and provides higher-quality explanations, validated by human evaluation.

Conclusion: EXPERT sets a new standard for explainable evaluation in image captioning, with publicly available code and datasets.

Abstract: Recent advances in large language models and vision-language models have led
to growing interest in explainable evaluation metrics for image captioning.
However, these metrics generate explanations without standardized criteria, and
the overall quality of the generated explanations remains unverified. In this
paper, we propose EXPERT, a reference-free evaluation metric that provides
structured explanations based on three fundamental criteria: fluency,
relevance, and descriptiveness. By constructing large-scale datasets of
high-quality structured explanations, we develop a two-stage evaluation
template to effectively supervise a vision-language model for both scoring and
explanation generation. EXPERT achieves state-of-the-art results on benchmark
datasets while providing significantly higher-quality explanations than
existing metrics, as validated through comprehensive human evaluation. Our code
and datasets are available at https://github.com/hjkim811/EXPERT.

</details>


### [459] [STACK: Adversarial Attacks on LLM Safeguard Pipelines](https://arxiv.org/abs/2506.24068)
*Ian R. McKenzie,Oskar J. Hollinsworth,Tom Tseng,Xander Davies,Stephen Casper,Aaron D. Tucker,Robert Kirk,Adam Gleave*

Main category: cs.CL

TL;DR: The paper evaluates AI defense pipelines, proposing a novel classifier and a staged attack (STACK) that exposes vulnerabilities, suggesting mitigations.


<details>
  <summary>Details</summary>
Motivation: To assess the security of AI defense pipelines, which lack prior evaluation, and identify vulnerabilities.

Method: Developed an open-source defense pipeline, tested a few-shot-prompted classifier, and introduced the STACK attack procedure.

Result: The classifier outperformed ShieldGemma (0% ASR on ClearHarm), but STACK achieved 71% ASR in black-box and 33% in transfer attacks.

Conclusion: AI defense pipelines are vulnerable to staged attacks; specific mitigations are needed to enhance security.

Abstract: Frontier AI developers are relying on layers of safeguards to protect against
catastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus
model using one such defense pipeline, and other frontier developers including
Google DeepMind and OpenAI pledge to soon deploy similar defenses. However, the
security of such pipelines is unclear, with limited prior work evaluating or
attacking these pipelines. We address this gap by developing and red-teaming an
open-source defense pipeline. First, we find that a novel few-shot-prompted
input and output classifier outperforms state-of-the-art open-weight safeguard
model ShieldGemma across three attacks and two datasets, reducing the attack
success rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,
we introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on
ClearHarm in a black-box attack against the few-shot-prompted classifier
pipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%
ASR, providing initial evidence that it is feasible to design attacks with no
access to the target pipeline. We conclude by suggesting specific mitigations
that developers could use to thwart staged attacks.

</details>


### [460] [On the Predictive Power of Representation Dispersion in Language Models](https://arxiv.org/abs/2506.24106)
*Yanhong Li,Ming Li,Karen Livescu,Jiawei Zhou*

Main category: cs.CL

TL;DR: Language models with wider embedding spaces (higher representation dispersion) achieve lower perplexity, and this dispersion can be used for practical tasks like model selection and improving retrieval-based methods.


<details>
  <summary>Details</summary>
Motivation: To explore the relationship between a model's embedding space breadth (representation dispersion) and its text prediction performance (perplexity).

Method: Analyze representation dispersion (average pairwise cosine distance) across models (LLaMA, Qwen) and domains (Wikipedia, news, scientific abstracts). Use dispersion for tasks like model selection and improving retrieval methods. Introduce a push-away objective to increase dispersion.

Result: Higher dispersion strongly correlates with lower perplexity. Dispersion aids in model selection, identifies optimal layers for retrieval, and improves perplexity when increased via training.

Conclusion: Representation dispersion is a key factor in model performance and can be leveraged for practical improvements without labeled data.

Abstract: We show that a language model's ability to predict text is tightly linked to
the breadth of its embedding space: models that spread their contextual
representations more widely tend to achieve lower perplexity. Concretely, we
find that representation dispersion - the average pairwise cosine distance
among hidden vectors - strongly and negatively correlates with perplexity
across diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia,
news, scientific abstracts). Beyond illustrating this link, we show how
dispersion can be leveraged for a range of practical tasks without requiring
labeled data. First, measuring dispersion on unlabeled text allows us to
predict downstream accuracy in new domains, offering a data-efficient tool for
model selection. Next, we find that identifying layers with higher dispersion
pinpoints the best representations for retrieval-based methods such as kNN-LM,
bypassing exhaustive layer-by-layer searches. Finally, we integrate a simple
push-away objective into training, which increases dispersion in both
single-domain and cross-domain scenarios and directly improves perplexity in
each.

</details>


### [461] [Computational Detection of Intertextual Parallels in Biblical Hebrew: A Benchmark Study Using Transformer-Based Language Models](https://arxiv.org/abs/2506.24117)
*David M. Smiley*

Main category: cs.CL

TL;DR: The study evaluates transformer-based models (E5, AlephBERT, MPNet, LaBSE) for detecting parallel passages in the Hebrew Bible, finding E5 and AlephBERT most effective.


<details>
  <summary>Details</summary>
Motivation: Traditional manual comparison of biblical Hebrew passages is labor-intensive and error-prone, prompting the need for automated methods.

Method: Pre-trained models generate word embeddings for passages; cosine similarity and Wasserstein Distance measure parallels.

Result: E5 excels in detecting parallels, while AlephBERT better differentiates non-parallel passages.

Conclusion: Pre-trained models improve efficiency and accuracy in identifying intertextual parallels, with potential for broader ancient language studies.

Abstract: Identifying parallel passages in biblical Hebrew is foundational in biblical
scholarship for uncovering intertextual relationships. Traditional methods rely
on manual comparison, which is labor-intensive and prone to human error. This
study evaluates the potential of pre-trained transformer-based language models,
including E5, AlephBERT, MPNet, and LaBSE, for detecting textual parallels in
the Hebrew Bible. Focusing on known parallels between the books of Samuel/Kings
and Chronicles, I assessed each model's capability to generate word embeddings
that delineate parallel from non-parallel passages. Utilizing cosine similarity
and Wasserstein Distance measures, I found that E5 and AlephBERT show
significant promise, with E5 excelling in parallel detection and AlephBERT
demonstrating stronger non-parallel differentiation. These findings indicate
that pre-trained models can enhance the efficiency and accuracy of detecting
intertextual parallels in ancient texts, suggesting broader applications for
ancient language studies.

</details>

<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 19]
- [cs.LG](#cs.LG) [Total: 99]
- [cs.CV](#cs.CV) [Total: 99]
- [cs.CL](#cs.CL) [Total: 42]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning](https://arxiv.org/abs/2507.00008)
*Hang Wu,Hongkai Chen,Yujun Cai,Chang Liu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

TL;DR: DiMo-GUI is a training-free framework for GUI grounding using dynamic visual grounding and modality-aware optimization, improving accuracy without additional training.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in grounding natural language queries in GUIs due to visual diversity, clutter, and language ambiguity.

Method: Splits GUI into textual and iconic elements, uses vision-language models for independent reasoning, and dynamically refines predictions by focusing on candidate regions.

Result: Outperforms baseline inference pipelines on standard GUI grounding benchmarks.

Conclusion: Combining modality separation with region-focused reasoning effectively improves GUI grounding without extra training.

Abstract: Grounding natural language queries in graphical user interfaces (GUIs) poses
unique challenges due to the diversity of visual elements, spatial clutter, and
the ambiguity of language. In this paper, we introduce DiMo-GUI, a
training-free framework for GUI grounding that leverages two core strategies:
dynamic visual grounding and modality-aware optimization. Instead of treating
the GUI as a monolithic image, our method splits the input into textual
elements and iconic elements, allowing the model to reason over each modality
independently using general-purpose vision-language models. When predictions
are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by
generating candidate focal regions centered on the model's initial predictions
and incrementally zooms into subregions to refine the grounding result. This
hierarchical refinement process helps disambiguate visually crowded layouts
without the need for additional training or annotations. We evaluate our
approach on standard GUI grounding benchmarks and demonstrate consistent
improvements over baseline inference pipelines, highlighting the effectiveness
of combining modality separation with region-focused reasoning.

</details>


### [2] [TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables](https://arxiv.org/abs/2507.00041)
*Varun Mannam,Fang Wang,Chaochun Liu,Xin Chen*

Main category: cs.AI

TL;DR: TalentMine is a novel LLM-enhanced framework that improves semantic understanding of tabular data in talent management, achieving 100% accuracy in query tasks.


<details>
  <summary>Details</summary>
Motivation: Current table extraction methods fail to capture semantic relationships in tabular data, leading to poor performance in retrieval-augmented systems.

Method: TalentMine uses multimodal reasoning to create semantically enriched table representations, preserving both structure and semantics.

Result: TalentMine outperforms standard methods, achieving 100% accuracy in query tasks, compared to 0% for AWS Textract and 40% for AWS Textract Visual Q&A.

Conclusion: The framework addresses semantic loss in table extraction, offering a robust solution for talent management systems with superior performance.

Abstract: In talent management systems, critical information often resides in complex
tabular formats, presenting significant retrieval challenges for conventional
language models. These challenges are pronounced when processing Talent
documentation that requires precise interpretation of tabular relationships for
accurate information retrieval and downstream decision-making. Current table
extraction methods struggle with semantic understanding, resulting in poor
performance when integrated into retrieval-augmented chat applications. This
paper identifies a key bottleneck - while structural table information can be
extracted, the semantic relationships between tabular elements are lost,
causing downstream query failures. To address this, we introduce TalentMine, a
novel LLM-enhanced framework that transforms extracted tables into semantically
enriched representations. Unlike conventional approaches relying on CSV or text
linearization, our method employs specialized multimodal reasoning to preserve
both structural and semantic dimensions of tabular data. Experimental
evaluation across employee benefits document collections demonstrates
TalentMine's superior performance, achieving 100% accuracy in query answering
tasks compared to 0% for standard AWS Textract extraction and 40% for AWS
Textract Visual Q&A capabilities. Our comparative analysis also reveals that
the Claude v3 Haiku model achieves optimal performance for talent management
applications. The key contributions of this work include (1) a systematic
analysis of semantic information loss in current table extraction pipelines,
(2) a novel LLM-based method for semantically enriched table representation,
(3) an efficient integration framework for retrieval-augmented systems as
end-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks
showing substantial improvements across multiple categories.

</details>


### [3] [A collaborative digital twin built on FAIR data and compute infrastructure](https://arxiv.org/abs/2507.00048)
*Thomas M. Deucher,Juan C. Verduzco,Michael Titus,Alejandro Strachan*

Main category: cs.AI

TL;DR: A distributed self-driving lab (SDL) framework using FAIR data and nanoHUB services accelerates collaborative optimization tasks with shared data and machine learning.


<details>
  <summary>Details</summary>
Motivation: To enhance collaboration and efficiency in scientific discovery by integrating FAIR data infrastructure with SDLs for shared optimization tasks.

Method: Uses nanoHUB services for simulation and FAIR data management, enabling shared databases, automated data processing, and active learning for optimization.

Result: Researchers can collaboratively optimize tasks (e.g., food dye recipes) with real-time ML updates and shared data.

Conclusion: The framework is scalable and adaptable to various optimization problems, promoting accessible and collaborative research.

Abstract: The integration of machine learning with automated experimentation in
self-driving laboratories (SDL) offers a powerful approach to accelerate
discovery and optimization tasks in science and engineering applications. When
supported by findable, accessible, interoperable, and reusable (FAIR) data
infrastructure, SDLs with overlapping interests can collaborate more
effectively. This work presents a distributed SDL implementation built on
nanoHUB services for online simulation and FAIR data management. In this
framework, geographically dispersed collaborators conducting independent
optimization tasks contribute raw experimental data to a shared central
database. These researchers can then benefit from analysis tools and machine
learning models that automatically update as additional data become available.
New data points are submitted through a simple web interface and automatically
processed using a nanoHUB Sim2L, which extracts derived quantities and indexes
all inputs and outputs in a FAIR data repository called ResultsDB. A separate
nanoHUB workflow enables sequential optimization using active learning, where
researchers define the optimization objective, and machine learning models are
trained on-the-fly with all existing data, guiding the selection of future
experiments. Inspired by the concept of ``frugal twin", the optimization task
seeks to find the optimal recipe to combine food dyes to achieve the desired
target color. With easily accessible and inexpensive materials, researchers and
students can set up their own experiments, share data with collaborators, and
explore the combination of FAIR data, predictive ML models, and sequential
optimization. The tools introduced are generally applicable and can easily be
extended to other optimization problems.

</details>


### [4] [SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network](https://arxiv.org/abs/2507.00050)
*Devin Y. De Silva,Sandareka Wickramanayake,Dulani Meedeniya,Sanka Rasnayaka*

Main category: cs.AI

TL;DR: SEZ-HARN is a self-explainable zero-shot HAR model that recognizes unseen activities and provides skeleton videos for transparency, achieving competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: Overcome data limitations and lack of transparency in existing HAR models for real-world applications.

Method: Introduces SEZ-HARN, a model that recognizes untrained activities and explains decisions via skeleton videos. Evaluated on four datasets.

Result: SEZ-HARN achieves near-best accuracy (within 3% on PAMAP2) while providing understandable explanations.

Conclusion: SEZ-HARN balances accuracy and transparency, making it suitable for real-world HAR applications.

Abstract: Human Activity Recognition (HAR), which uses data from Inertial Measurement
Unit (IMU) sensors, has many practical applications in healthcare and assisted
living environments. However, its use in real-world scenarios has been limited
by the lack of comprehensive IMU-based HAR datasets that cover a wide range of
activities and the lack of transparency in existing HAR models. Zero-shot HAR
(ZS-HAR) overcomes the data limitations, but current models struggle to explain
their decisions, making them less transparent. This paper introduces a novel
IMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity
Recognition Network (SEZ-HARN). It can recognize activities not encountered
during training and provide skeleton videos to explain its decision-making
process. We evaluate the effectiveness of the proposed SEZ-HARN on four
benchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its
performance against three state-of-the-art black-box ZS-HAR models. The
experiment results demonstrate that SEZ-HARN produces realistic and
understandable explanations while achieving competitive Zero-shot recognition
accuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\% of the
best-performing black-box model on PAMAP2 while maintaining comparable
performance on the other three datasets.

</details>


### [5] [Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation](https://arxiv.org/abs/2507.00054)
*Shreyansh Padarha*

Main category: cs.AI

TL;DR: AdvDistill improves knowledge distillation by using reward-guided dataset distillation, enhancing student model performance in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Current knowledge distillation techniques limit student models to copying teacher responses, reducing generalizability and efficiency, especially in reasoning tasks.

Method: AdvDistill uses multiple teacher responses per prompt, assigns rewards via rule-based verifiers, and trains student models with weighted rewards.

Result: Significant improvement in student model performance for mathematical and complex reasoning tasks.

Conclusion: Incorporating a rewarding mechanism in dataset distillation enhances efficacy and benefits for reasoning tasks.

Abstract: The push to compress and impart the proficiency of Large Language Models
(LLMs) into more deployable and efficient Small Language Models (SLMs) has
benefited from improvements in knowledge distillation (KD) techniques. These
techniques allow a smaller student model to learn from a more capable and
larger teacher model's responses. However, distillation often revolves around
the student model merely copying the teacher's in-distribution responses,
limiting its generalisability. This limitation is amplified on reasoning tasks
and can be computationally expensive. In this study, we propose AdvDistill, a
reward-guided dataset distillation framework. We utilise multiple generations
(responses) from a teacher for each prompt and assign rewards based on
rule-based verifiers. These varying and normally distributed rewards serve as
weights when training student models. Our methods and their subsequent
behavioural analysis demonstrate a significant improvement in student model
performance for mathematical and complex reasoning tasks, showcasing the
efficacy and benefits of incorporating a rewarding mechanism in dataset
distillation processes.

</details>


### [6] [VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems](https://arxiv.org/abs/2507.00079)
*Ethan Smyth,Alessandro Suglia*

Main category: cs.AI

TL;DR: The paper introduces VoyagerVision, a multi-modal model that uses visual feedback (screenshots) to enhance spatial interpretation in Minecraft, outperforming its predecessor Voyager by creating unique structures.


<details>
  <summary>Details</summary>
Motivation: To extend the open-ended potential of AI by improving spatial environment interpretation through visual inputs, leveraging advancements in LLMs like GPT-4o.

Method: Proposes VoyagerVision, a model integrating visual feedback (screenshots) into Minecraft tasks, building on the Voyager framework.

Result: VoyagerVision created 2.75 unique structures in 50 iterations, outperforming Voyager, and succeeded in 50% of flat-world building tests.

Conclusion: Visual feedback significantly enhances AI's spatial task performance, demonstrating the potential of multi-modal models in open-ended environments like Minecraft.

Abstract: Open-endedness is an active field of research in the pursuit of capable
Artificial General Intelligence (AGI), allowing models to pursue tasks of their
own choosing. Simultaneously, recent advancements in Large Language Models
(LLMs) such as GPT-4o [9] have allowed such models to be capable of
interpreting image inputs. Implementations such as OMNI-EPIC [4] have made use
of such features, providing an LLM with pixel data of an agent's POV to parse
the environment and allow it to solve tasks. This paper proposes that providing
these visual inputs to a model gives it greater ability to interpret spatial
environments, and as such, can increase the number of tasks it can successfully
perform, extending its open-ended potential. To this aim, this paper proposes
VoyagerVision -- a multi-modal model capable of creating structures within
Minecraft using screenshots as a form of visual feedback, building on the
foundation of Voyager. VoyagerVision was capable of creating an average of 2.75
unique structures within fifty iterations of the system, as Voyager was
incapable of this, it is an extension in an entirely new direction.
Additionally, in a set of building unit tests VoyagerVision was successful in
half of all attempts in flat worlds, with most failures arising in more complex
structures. Project website is available at
https://esmyth-dev.github.io/VoyagerVision.github.io/

</details>


### [7] [Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models](https://arxiv.org/abs/2507.00092)
*Basab Jha,Firoj Paudel,Ujjwal Puri,Zhang Yuting,Choi Donghyuk,Wang Junhao*

Main category: cs.AI

TL;DR: The paper introduces inverse reasoning, a method for LLMs to explain their reasoning post-hoc, improving transparency and performance.


<details>
  <summary>Details</summary>
Motivation: To address the blackbox nature of LLM decision-making and enhance interpretability.

Method: Inverse reasoning via metacognitive attention processes in SAGE-nano, a 4B-parameter model.

Result: Achieves 74.6% accuracy on AQUA-RAT and 92.1% human preference for explanations, rivaling top models.

Conclusion: Inverse reasoning advances transparent AI, improving safety, education, and scientific discovery.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities at
solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but
their decision-making processes remain somewhat blackbox. We introduce
textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and
explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a
4-billion-parameter reasoning model, employs a metacognitive structure that
reflects back via attention processes to identify major decision points and
generate explanations of reasoning choices. While typical CoT approaches are
directed towards forward reasoning generation, inverse reasoning provides
insight into why specific reasoning chains were selected over others. Through
thorough testing of logical reasoning puzzles, math problems and ethical
dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we
demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy
(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for
its task, and offers performance almost on par with models like Claude-3.5
Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for
LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework
to reverse the attention flow, (iii) comprehensive evaluation frameworks for
reasoning transparency, and (iv) evidence that increasing reasoning using
inverse reasoning improves interpretability along with reasoning performance.
Our work creates new avenues for transparent AI systems and closes significant
gaps in AI safety, education, and scientific discovery.

</details>


### [8] [BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis](https://arxiv.org/abs/2507.00180)
*Vidhi Rathore*

Main category: cs.AI

TL;DR: A novel pipeline using RL and decision trees extracts interpretable decision logic from legacy systems, validated on dummy systems.


<details>
  <summary>Details</summary>
Motivation: Legacy systems lack documentation, making modernization difficult. Traditional methods fail to capture underlying intent.

Method: Uses RL to explore input space, identify decision boundaries, clusters transitions with K-Means, and trains decision trees for human-readable rules.

Result: RL agent focuses on boundary regions; extracted rules accurately reflect system logic, aiding legacy migration.

Conclusion: The pipeline effectively extracts interpretable logic, promising for generating specifications and test cases.

Abstract: Modernizing legacy software systems is a critical but challenging task, often
hampered by a lack of documentation and understanding of the original system's
intricate decision logic. Traditional approaches like behavioral cloning merely
replicate input-output behavior without capturing the underlying intent. This
paper proposes a novel pipeline to automatically extract interpretable decision
logic from legacy systems treated as black boxes. The approach uses a
Reinforcement Learning (RL) agent to explore the input space and identify
critical decision boundaries by rewarding actions that cause meaningful changes
in the system's output. These counterfactual state transitions, where the
output changes, are collected and clustered using K-Means. Decision trees are
then trained on these clusters to extract human-readable rules that approximate
the system's decision logic near the identified boundaries. I demonstrated the
pipeline's effectiveness on three dummy legacy systems with varying complexity,
including threshold-based, combined-conditional, and non-linear range logic.
Results show that the RL agent successfully focuses exploration on relevant
boundary regions, and the extracted rules accurately reflect the core logic of
the underlying dummy systems, providing a promising foundation for generating
specifications and test cases during legacy migration.

</details>


### [9] [ChatGPT produces more "lazy" thinkers: Evidence of cognitive engagement decline](https://arxiv.org/abs/2507.00181)
*Georgios P. Georgiou*

Main category: cs.AI

TL;DR: AI tools like ChatGPT may reduce cognitive engagement in students during academic writing tasks, suggesting a need for strategies to ensure active learning.


<details>
  <summary>Details</summary>
Motivation: To investigate the impact of generative AI (ChatGPT) on students' cognitive engagement during academic writing, addressing concerns about reduced deep thinking.

Method: Experimental design with students randomly assigned to AI-assisted (ChatGPT) or control groups, assessed using a cognitive engagement scale (CES-AI).

Result: Lower cognitive engagement scores in the ChatGPT group compared to the control, indicating potential cognitive offloading.

Conclusion: AI assistance may hinder deep learning; pedagogical strategies are needed to promote active engagement with AI tools.

Abstract: Despite the increasing use of large language models (LLMs) in education,
concerns have emerged about their potential to reduce deep thinking and active
learning. This study investigates the impact of generative artificial
intelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of
students during academic writing tasks. The study employed an experimental
design with participants randomly assigned to either an AI-assisted (ChatGPT)
or a non-assisted (control) condition. Participants completed a structured
argumentative writing task followed by a cognitive engagement scale (CES), the
CES-AI, developed to assess mental effort, attention, deep processing, and
strategic thinking. The results revealed significantly lower cognitive
engagement scores in the ChatGPT group compared to the control group. These
findings suggest that AI assistance may lead to cognitive offloading. The study
contributes to the growing body of literature on the psychological implications
of AI in education and raises important questions about the integration of such
tools into academic practice. It calls for pedagogical strategies that promote
active, reflective engagement with AI-generated content to avoid compromising
self-regulated learning and deep cognitive involvement of students.

</details>


### [10] [Holistic Artificial Intelligence in Medicine; improved performance and explainability](https://arxiv.org/abs/2507.00205)
*Periklis Petridis,Georgios Margaritis,Vasiliki Stoumpou,Dimitris Bertsimas*

Main category: cs.AI

TL;DR: xHAIM improves HAIM by adding explainability and task-specific data use, boosting AUC from 79.9% to 90.3%.


<details>
  <summary>Details</summary>
Motivation: Address HAIM's lack of explainability and task-agnostic data use in medical AI.

Method: Four-step framework: task-relevant data identification, patient summary generation, predictive modeling, and clinical explanations.

Result: Improved AUC from 79.9% to 90.3% on chest pathology and operative tasks.

Conclusion: xHAIM enhances AI's clinical utility by making it explainable and interactive for clinicians.

Abstract: With the increasing interest in deploying Artificial Intelligence in
medicine, we previously introduced HAIM (Holistic AI in Medicine), a framework
that fuses multimodal data to solve downstream clinical tasks. However, HAIM
uses data in a task-agnostic manner and lacks explainability. To address these
limitations, we introduce xHAIM (Explainable HAIM), a novel framework
leveraging Generative AI to enhance both prediction and explainability through
four structured steps: (1) automatically identifying task-relevant patient data
across modalities, (2) generating comprehensive patient summaries, (3) using
these summaries for improved predictive modeling, and (4) providing clinical
explanations by linking predictions to patient-specific medical knowledge.
Evaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9%
to 90.3% across chest pathology and operative tasks. Importantly, xHAIM
transforms AI from a black-box predictor into an explainable decision support
system, enabling clinicians to interactively trace predictions back to relevant
patient data, bridging AI advancements with clinical utility.

</details>


### [11] [Learning for routing: A guided review of recent developments and future directions](https://arxiv.org/abs/2507.00218)
*Fangting Zhou,Attila Lischka,Balazs Kulcsar,Jiaming Wu,Morteza Haghir Chehreghani,Gilbert Laporte*

Main category: cs.AI

TL;DR: A review of ML applications for NP-hard routing problems like TSP and VRP, proposing a taxonomy for ML-based methods and integrating OR with ML techniques.


<details>
  <summary>Details</summary>
Motivation: The complexity of NP-hard routing problems makes exact solutions computationally expensive and heuristics suboptimal, prompting the use of ML for better solutions.

Method: Categorizes ML-based routing methods into construction-based and improvement-based approaches, integrating traditional OR with ML.

Result: Provides a structured framework for future research on ML-enhanced routing problems.

Conclusion: ML techniques show promise in improving routing problem solutions, and the proposed taxonomy guides future work in this area.

Abstract: This paper reviews the current progress in applying machine learning (ML)
tools to solve NP-hard combinatorial optimization problems, with a focus on
routing problems such as the traveling salesman problem (TSP) and the vehicle
routing problem (VRP). Due to the inherent complexity of these problems, exact
algorithms often require excessive computational time to find optimal
solutions, while heuristics can only provide approximate solutions without
guaranteeing optimality. With the recent success of machine learning models,
there is a growing trend in proposing and implementing diverse ML techniques to
enhance the resolution of these challenging routing problems. We propose a
taxonomy categorizing ML-based routing methods into construction-based and
improvement-based approaches, highlighting their applicability to various
problem characteristics. This review aims to integrate traditional OR methods
with state-of-the-art ML techniques, providing a structured framework to guide
future research and address emerging VRP variants.

</details>


### [12] [ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context](https://arxiv.org/abs/2507.00417)
*Joongwon Kim,Anirudh Goyal,Liang Tan,Hannaneh Hajishirzi,Srinivasan Iyer,Tianlu Wang*

Main category: cs.AI

TL;DR: ASTRO trains language models to reason like search algorithms using self-reflection and exploration, improving performance on math problems.


<details>
  <summary>Details</summary>
Motivation: Enhance reasoning capabilities of non-reasoner models like Llama 3 by teaching structured search behavior.

Method: Uses synthetic data from Monte Carlo Tree Search (MCTS) to create chain-of-thought traces, followed by RL with verifiable rewards.

Result: Achieves significant performance gains (16-26.9%) on math benchmarks like MATH-500, AMC 2023, and AIME 2024.

Conclusion: Search-inspired training effectively boosts reasoning in open LLMs, especially for iterative correction tasks.

Abstract: We introduce ASTRO, the "Autoregressive Search-Taught Reasoner", a framework
for training language models to reason like search algorithms, explicitly
leveraging self-reflection, backtracking, and exploration in their outputs.
Recently, training large language models (LLMs) via reinforcement learning (RL)
has led to the advent of reasoning models with greatly enhanced reasoning
capabilities. Open-source replications of reasoning models, while successful,
build upon models that already exhibit strong reasoning capabilities along with
search behavior observed even before RL. As a result, it is yet unclear how to
boost the reasoning capabilities of other non-reasoner models including Llama
3. ASTRO teaches such models to internalize structured search behavior through
a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over
mathematical problem-solving trajectories. By converting search traces into
natural language chain-of-thoughts that capture both successes and recoveries
from failure, ASTRO bootstraps models with a rich prior for exploration during
RL. We finetune our models on these search-derived traces and further improve
performance via RL with verifiable rewards. We apply ASTRO to the Llama 3
family of models and achieve absolute performance gains of 16.0% on MATH-500,
26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon
challenging problems that require iterative correction. Our results demonstrate
that search-inspired training offers a principled way to instill robust
reasoning capabilities into open LLMs.

</details>


### [13] [Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning](https://arxiv.org/abs/2507.00432)
*Maggie Huan,Yuetai Li,Tuney Zheng,Xiaoyu Xu,Seungone Kim,Minxin Du,Radha Poovendran,Graham Neubig,Xiang Yue*

Main category: cs.AI

TL;DR: Most math-focused LLMs fail to generalize gains to other domains, with RL-tuned models outperforming SFT-tuned ones in cross-domain tasks.


<details>
  <summary>Details</summary>
Motivation: To determine if improvements in math benchmarks reflect broader problem-solving abilities or narrow overfitting.

Method: Evaluated 20+ reasoning-tuned models across diverse tasks and conducted controlled experiments on Qwen3-14B models with different tuning methods.

Result: RL-tuned models generalize better, while SFT-tuned models lose general capabilities due to representation drift.

Conclusion: Suggests rethinking post-training methods, especially reliance on SFT-distilled data, to enhance reasoning models.

Abstract: Math reasoning has become the poster child of progress in large language
models (LLMs), with new models rapidly surpassing human-level performance on
benchmarks like MATH and AIME. But as math leaderboards improve week by week,
it is worth asking: do these gains reflect broader problem-solving ability or
just narrow overfitting? To answer this question, we evaluate over 20
open-weight reasoning-tuned models across a broad suite of tasks, including
math, scientific QA, agent planning, coding, and standard
instruction-following. We surprisingly find that most models that succeed in
math fail to transfer their gains to other domains. To rigorously study this
phenomenon, we conduct controlled experiments on Qwen3-14B models using
math-only data but different tuning methods. We find that reinforcement
learning (RL)-tuned models generalize well across domains, while supervised
fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space
representation and token-space distribution shift analyses reveal that SFT
induces substantial representation and output drift, while RL preserves
general-domain structure. Our results suggest a need to rethink standard
post-training recipes, particularly the reliance on SFT-distilled data for
advancing reasoning models.

</details>


### [14] [Advancing Local Search in SMT-NRA with MCSAT Integration](https://arxiv.org/abs/2507.00557)
*Tianyi Ding,Haokun Li,Xinpeng Ni,Bican Xia,Tianqi Zhao*

Main category: cs.AI

TL;DR: The paper introduces a two-dimensional cell-jump move and an extended local search framework (2d-LS) for SMT-NRA, integrating MCSAT and sample-cell projection to enhance efficiency. A hybrid framework combining these methods shows improved performance.


<details>
  <summary>Details</summary>
Motivation: To advance local search for SMT-NRA by generalizing key operations and integrating efficient techniques like MCSAT and sample-cell projection.

Method: Proposes 2d-cell-jump, extends LS to 2d-LS, integrates MCSAT with sample-cell projection, and designs a hybrid framework combining MCSAT, 2d-LS, and OpenCAD.

Result: Experimental results show improved local search performance, validating the effectiveness of the proposed methods.

Conclusion: The introduced techniques and hybrid framework significantly enhance the efficiency of local search for SMT-NRA.

Abstract: In this paper, we advance local search for Satisfiability Modulo the Theory
of Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a
two-dimensional cell-jump move, called \emph{$2d$-cell-jump}, generalizing the
key operation, cell-jump, of the local search method for SMT-NRA. Then, we
propose an extended local search framework, named \emph{$2d$-LS} (following the
local search framework, LS, for SMT-NRA), integrating the model constructing
satisfiability calculus (MCSAT) framework to improve search efficiency. To
further improve the efficiency of MCSAT, we implement a recently proposed
technique called \emph{sample-cell projection operator} for MCSAT, which is
well suited for CDCL-style search in the real domain and helps guide the search
away from conflicting states. Finally, we design a hybrid framework for SMT-NRA
combining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through
information exchange. The experimental results demonstrate improvements in
local search performance, highlighting the effectiveness of the proposed
methods.

</details>


### [15] [Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess](https://arxiv.org/abs/2507.00726)
*Dongyoon Hwang,Hojoon Lee,Jaegul Choo,Dongmin Park,Jongho Park*

Main category: cs.AI

TL;DR: RL for LLMs in chess shows promise with dense rewards but plateaus below expert levels due to pretrained models' chess understanding deficits.


<details>
  <summary>Details</summary>
Motivation: Explore if LLMs can develop strategic reasoning in chess using RL, leveraging dense rewards from a chess-pretrained network.

Method: Use a chess-pretrained action-value network for dense rewards (knowledge distillation) and compare with sparse binary rewards.

Result: Dense rewards outperform sparse ones, but models plateau below expert levels due to pretrained models' chess understanding limitations.

Conclusion: RL alone may not fully overcome deficits in pretrained models' chess understanding, suggesting further research is needed.

Abstract: While reinforcement learning (RL) for large language models (LLMs) has shown
promise in mathematical reasoning, strategic reasoning for LLMs using RL
remains largely unexplored. We investigate whether LLMs can develop strategic
reasoning capabilities through RL in chess. To this end, we leverage a
chess-pretrained action-value network to provide dense reward on the LLM's
output move quality, which can be seen as a form of knowledge distillation. Our
experiments show that our distillation-based dense rewards often outperform
sparse binary rewards. However, surprisingly, all models plateau far below
expert levels. We provide SFT and RL ablations on chess reasoning training and
find evidence that this limitation stems from a deficit in the pretrained
models' internal understanding of chess--a deficit which RL alone may not be
able to fully overcome.

</details>


### [16] [A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis](https://arxiv.org/abs/2507.00810)
*Qing Xu,Xiaohua Xuan*

Main category: cs.AI

TL;DR: An improved numerical algorithm for minimax problems using nonsmooth optimization, quadratic programming, and iterative methods, with proven convergence under mild assumptions.


<details>
  <summary>Details</summary>
Motivation: Address the need for efficient and reliable solutions to minimax problems in fields like robust optimization and imbalanced learning.

Method: Combines nonsmooth optimization, quadratic programming, and iterative processes, with convergence proof under gradient continuity and boundedness.

Result: The algorithm is theoretically sound with guaranteed convergence under specified conditions.

Conclusion: The proposed algorithm is versatile and applicable in diverse areas, offering a robust solution to minimax problems.

Abstract: In this paper, we propose an improved numerical algorithm for solving minimax
problems based on nonsmooth optimization, quadratic programming and iterative
process. We also provide a rigorous proof of convergence for our algorithm
under some mild assumptions, such as gradient continuity and boundedness. Such
an algorithm can be widely applied in various fields such as robust
optimization, imbalanced learning, etc.

</details>


### [17] [SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents](https://arxiv.org/abs/2507.00841)
*Siyuan Liang,Tianmeng Fang,Zhe Liu,Aishan Liu,Yan Xiao,Jinyuan He,Ee-Chien Chang,Xiaochun Cao*

Main category: cs.AI

TL;DR: The paper addresses security risks in multimodal foundation model-driven agents, proposing a risk discrimination mechanism and automated assessment to mitigate jailbreak vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Multimodal agents face jailbreak risks where attackers bypass constraints to trigger sensitive operations, highlighting a gap in existing security measures for complex interactions.

Method: The study constructs a risk discrimination mechanism using behavioral sequence information and designs an automated assessment scheme based on a large language model.

Result: Preliminary validation shows improved recognition of risky behaviors and reduced jailbreak probability in high-risk tasks.

Conclusion: The work offers insights for security risk modeling and protection in multimodal intelligent agent systems.

Abstract: With the wide application of multimodal foundation models in intelligent
agent systems, scenarios such as mobile device control, intelligent assistant
interaction, and multimodal task execution are gradually relying on such large
model-driven agents. However, the related systems are also increasingly exposed
to potential jailbreak risks. Attackers may induce the agents to bypass the
original behavioral constraints through specific inputs, and then trigger
certain risky and sensitive operations, such as modifying settings, executing
unauthorized commands, or impersonating user identities, which brings new
challenges to system security. Existing security measures for intelligent
agents still have limitations when facing complex interactions, especially in
detecting potentially risky behaviors across multiple rounds of conversations
or sequences of tasks. In addition, an efficient and consistent automated
methodology to assist in assessing and determining the impact of such risks is
currently lacking. This work explores the security issues surrounding mobile
multimodal agents, attempts to construct a risk discrimination mechanism by
incorporating behavioral sequence information, and designs an automated
assisted assessment scheme based on a large language model. Through preliminary
validation in several representative high-risk tasks, the results show that the
method can improve the recognition of risky behaviors to some extent and assist
in reducing the probability of agents being jailbroken. We hope that this study
can provide some valuable references for the security risk modeling and
protection of multimodal intelligent agent systems.

</details>


### [18] [Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact](https://arxiv.org/abs/2507.00951)
*Rizwan Qureshi,Ranjan Sapkota,Abbas Shah,Amgad Muneer,Anas Zafar,Ashmal Vayani,Maged Shoman,Abdelrahman B. M. Eldaly,Kai Zhang,Ferhat Sadak,Shaina Raza,Xinqi Fan,Ravid Shwartz-Ziv,Hong Yan,Vinjia Jain,Aman Chadha,Manoj Karkee,Jia Wu,Philip Torr,Seyedali Mirjalili*

Main category: cs.AI

TL;DR: The paper explores the limitations of current AI models in achieving AGI, emphasizing the need for grounded agency, modular reasoning, and multi-agent coordination. It proposes Agentic RAG frameworks and other strategies to bridge the gap between statistical learning and goal-directed cognition.


<details>
  <summary>Details</summary>
Motivation: To address the enduring question of whether machines can think and act like humans, and to synthesize cross-disciplinary insights for advancing AGI.

Method: Cross-disciplinary synthesis of AGI development, analyzing architectural and cognitive foundations, and proposing strategies like Agentic RAG frameworks, generalization methods, and neurosymbolic systems.

Result: Highlights the potential of modular reasoning, memory integration, and multi-agent coordination, while identifying key challenges in achieving AGI.

Conclusion: True AGI requires more than scale; it demands integrated memory, reasoning, and adaptive behavior, with ongoing scientific, technical, and ethical challenges to address.

Abstract: Can machines truly think, reason and act in domains like humans? This
enduring question continues to shape the pursuit of Artificial General
Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,
DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal
fluency and partial reasoning, these systems remain fundamentally limited by
their reliance on token-level prediction and lack of grounded agency. This
paper offers a cross-disciplinary synthesis of AGI development, spanning
artificial intelligence, cognitive neuroscience, psychology, generative models,
and agent-based systems. We analyze the architectural and cognitive foundations
of general intelligence, highlighting the role of modular reasoning, persistent
memory, and multi-agent coordination. In particular, we emphasize the rise of
Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use
to enable more adaptive behavior. We discuss generalization strategies,
including information compression, test-time adaptation, and training-free
methods, as critical pathways toward flexible, domain-agnostic intelligence.
Vision-Language Models (VLMs) are reexamined not just as perception modules but
as evolving interfaces for embodied understanding and collaborative task
completion. We also argue that true intelligence arises not from scale alone
but from the integration of memory and reasoning: an orchestration of modular,
interactive, and self-improving components where compression enables adaptive
behavior. Drawing on advances in neurosymbolic systems, reinforcement learning,
and cognitive scaffolding, we explore how recent architectures begin to bridge
the gap between statistical learning and goal-directed cognition. Finally, we
identify key scientific, technical, and ethical challenges on the path to AGI.

</details>


### [19] [Enhancing LLM Agent Safety via Causal Influence Prompting](https://arxiv.org/abs/2507.00979)
*Dongyoon Hahm,Woogyeol Jin,June Suk Choi,Sungsoo Ahn,Kimin Lee*

Main category: cs.AI

TL;DR: CIP uses causal influence diagrams (CIDs) to improve safety in LLM-powered autonomous agents by modeling decision-making risks and refining actions iteratively.


<details>
  <summary>Details</summary>
Motivation: Ensuring safe and reliable behavior in LLM-powered agents to prevent unintended consequences.

Method: Three-step approach: initialize CID for task, guide agent interactions using CID, and iteratively refine CID based on outcomes.

Result: Enhanced safety in code execution and mobile device control tasks.

Conclusion: CIP effectively mitigates risks in agent decision-making using CIDs.

Abstract: As autonomous agents powered by large language models (LLMs) continue to
demonstrate potential across various assistive tasks, ensuring their safe and
reliable behavior is crucial for preventing unintended consequences. In this
work, we introduce CIP, a novel technique that leverages causal influence
diagrams (CIDs) to identify and mitigate risks arising from agent
decision-making. CIDs provide a structured representation of cause-and-effect
relationships, enabling agents to anticipate harmful outcomes and make safer
decisions. Our approach consists of three key steps: (1) initializing a CID
based on task specifications to outline the decision-making process, (2)
guiding agent interactions with the environment using the CID, and (3)
iteratively refining the CID based on observed behaviors and outcomes.
Experimental results demonstrate that our method effectively enhances safety in
both code execution and mobile device control tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [20] [Hypertokens: Holographic Associative Memory in Tokenized LLMs](https://arxiv.org/abs/2507.00002)
*Christopher James Augeri*

Main category: cs.LG

TL;DR: HDRAM (Holographically Defined Random Access Memory) addresses precision loss in LLMs by treating transformer latent space as a spread-spectrum channel, improving associative retrieval without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) suffer from precision loss, reframed as information spreading, which is addressed by shifting the problem to an information-theoretic communication issue.

Method: Introduces HDRAM, a symbolic memory framework using hypertokens, integrating classical error-correcting codes, holographic computing, and quantum-inspired search for principled despreading.

Result: HDRAM enables efficient key-value operations and Grover-style search in latent space, significantly improving associative retrieval.

Conclusion: The Classical-Holographic-Quantum-inspired (CHQ) principles in HDRAM fortify transformer architectures without requiring changes to their design.

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but suffer from
apparent precision loss, reframed here as information spreading. This reframing
shifts the problem from computational precision to an information-theoretic
communication issue. We address the K:V and V:K memory problem in LLMs by
introducing HDRAM (Holographically Defined Random Access Memory), a symbolic
memory framework treating transformer latent space as a spread-spectrum
channel. Built upon hypertokens, structured symbolic codes integrating
classical error-correcting codes (ECC), holographic computing, and
quantum-inspired search, HDRAM recovers distributed information through
principled despreading. These phase-coherent memory addresses enable efficient
key-value operations and Grover-style search in latent space. By combining ECC
grammar with compressed sensing and Krylov subspace alignment, HDRAM
significantly improves associative retrieval without architectural changes,
demonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can
fortify transformer architectures.

</details>


### [21] [Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE](https://arxiv.org/abs/2507.00003)
*Eyhab Al-Masri*

Main category: cs.LG

TL;DR: NeutroSENSE is a neutrosophic-enhanced ensemble framework for IoT intrusion detection, improving accuracy and explainability by quantifying uncertainty through truth, falsity, and indeterminacy components.


<details>
  <summary>Details</summary>
Motivation: To enhance interpretability and trust in AI-driven IoT intrusion detection by addressing uncertainty and enabling informed abstention.

Method: Integrates Random Forest, XGBoost, and Logistic Regression with neutrosophic logic, decomposing predictions into T, F, and I components, and uses adaptive thresholds for review.

Result: Achieved 97% accuracy on IoT-CAD dataset, with misclassified samples showing higher indeterminacy (I=0.62 vs. I=0.24 for correct ones).

Conclusion: Neutrosophic logic improves accuracy and explainability, supporting trust-aware AI for IoT security, especially in edge deployments.

Abstract: This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework
for interpretable intrusion detection in IoT environments. By integrating
Random Forest, XGBoost, and Logistic Regression with neutrosophic logic, the
system decomposes prediction confidence into truth (T), falsity (F), and
indeterminacy (I) components, enabling uncertainty quantification and
abstention. Predictions with high indeterminacy are flagged for review using
both global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD
dataset, NeutroSENSE achieved 97% accuracy, while demonstrating that
misclassified samples exhibit significantly higher indeterminacy (I = 0.62)
than correct ones (I = 0.24). The use of indeterminacy as a proxy for
uncertainty enables informed abstention and targeted review-particularly
valuable in edge deployments. Figures and tables validate the correlation
between I-scores and error likelihood, supporting more trustworthy,
human-in-the-loop AI decisions. This work shows that neutrosophic logic
enhances both accuracy and explainability, providing a practical foundation for
trust-aware AI in edge and fog-based IoT security systems.

</details>


### [22] [A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search](https://arxiv.org/abs/2507.00004)
*Austin R. Ellis-Mohr,Anuj K. Nayak,Lav R. Varshney*

Main category: cs.LG

TL;DR: The paper introduces DS3, a framework for optimizing inference costs in LLMs by modeling it as stochastic traversal over a skill graph, enabling comparative analysis of strategies like CoT and ToT.


<details>
  <summary>Details</summary>
Motivation: High computational, energy, and financial costs of LLM inference, especially for reasoning tasks, necessitate more efficient operating points beyond existing compute-optimality characterizations.

Method: DS3, a framework representing inference as stochastic traversal over a learned skill graph, with closed-form expressions for task success and compute cost across strategies like CoT and ToT.

Result: The framework theoretically recovers empirical patterns, such as linear accuracy scaling with logarithmic compute, and unifies BoN and majority voting behavior.

Conclusion: DS3 enhances theoretical understanding of training-inference interdependencies, supporting better algorithmic design and resource allocation.

Abstract: Large language models (LLMs) demand considerable computational, energy, and
financial resources during both training and deployment. While scaling laws for
training have guided much of the field's recent progress, inference costs now
represent a significant and growing component of the overall resource burden,
particularly for reasoning-focused models. Existing characterizations of
compute-optimality that consider model size, dataset size, and inference tokens
in isolation or in fixed combinations risk overlooking more efficient operating
points. We introduce directed stochastic skill search (DS3), a general
framework that represents inference as stochastic traversal over a learned
skill graph. From a simplified yet expressive instantiation, we derive
closed-form expressions for task success and compute cost across a wide range
of inference strategies -- including chain-of-thought (CoT) and tree-of-thought
(ToT) -- enabling comparative analysis as a function of task difficulty and
model capability. To that end, we extend a prior first-principles tripartite
graph framework of LLM training to incorporate inference, and separately bridge
DS3 with empirical methods that characterize LLM scaling behavior. We
theoretically recover empirically observed patterns, including: linear accuracy
scaling with logarithmic compute; variation in preferred inference strategies
as a function of task difficulty and model capability; emergent behavior
elicited by reasoning even when performance plateaus under parameter scaling;
and both best-of-N (BoN) and majority voting behavior captured within a unified
analytical framework. By explicitly characterizing training-inference
interdependencies, our framework deepens theoretical understanding and supports
principled algorithmic design and resource allocation.

</details>


### [23] [Novel RL approach for efficient Elevator Group Control Systems](https://arxiv.org/abs/2507.00011)
*Nathan Vaartjes,Vincent Francois-Lavet*

Main category: cs.LG

TL;DR: The paper proposes a Reinforcement Learning (RL)-based Elevator Group Control System (EGCS) to optimize elevator traffic management, outperforming traditional rule-based methods.


<details>
  <summary>Details</summary>
Motivation: Heuristic or pattern-based controllers fail to handle the stochastic and combinatorial nature of elevator dispatching, necessitating a more adaptive solution.

Method: The system is modeled as a Markov Decision Process (MDP) with innovations like a novel action space encoding, infra-steps for continuous arrivals, and a tailored reward signal. Dueling Double Deep Q-learning is used.

Result: The RL-based EGCS adapts to fluctuating traffic patterns and outperforms traditional rule-based algorithms.

Conclusion: The proposed RL approach effectively addresses the challenges of elevator dispatching, offering improved efficiency and adaptability.

Abstract: Efficient elevator traffic management in large buildings is critical for
minimizing passenger travel times and energy consumption. Because heuristic- or
pattern-detection-based controllers struggle with the stochastic and
combinatorial nature of dispatching, we model the six-elevator, fifteen-floor
system at Vrije Universiteit Amsterdam as a Markov Decision Process and train
an end-to-end Reinforcement Learning (RL) Elevator Group Control System (EGCS).
Key innovations include a novel action space encoding to handle the
combinatorial complexity of elevator dispatching, the introduction of
infra-steps to model continuous passenger arrivals, and a tailored reward
signal to improve learning efficiency. In addition, we explore various ways to
adapt the discounting factor to the infra-step formulation. We investigate RL
architectures based on Dueling Double Deep Q-learning, showing that the
proposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a
highly stochastic environment, and thereby outperforms a traditional rule-based
algorithm.

</details>


### [24] [Towards Undistillable Models by Minimizing Conditional Mutual Information](https://arxiv.org/abs/2507.00012)
*Linfeng Ye,Shayan Mohajer Hamidi,En-hui Yang*

Main category: cs.LG

TL;DR: The paper introduces a method (CMIM) to create undistillable DNNs by minimizing conditional mutual information (CMI) alongside cross-entropy loss, ensuring intellectual property protection.


<details>
  <summary>Details</summary>
Motivation: To protect DNN intellectual property by making models undistillable, preventing effective knowledge distillation.

Method: Proposes CMIM, training DNNs by minimizing cross-entropy loss and CMI values of temperature-scaled clusters.

Result: CMIM models are undistillable by tested KD methods and outperform models trained with only cross-entropy loss.

Conclusion: CMIM effectively creates undistillable DNNs while maintaining or improving prediction accuracy.

Abstract: A deep neural network (DNN) is said to be undistillable if, when used as a
black-box input-output teacher, it cannot be distilled through knowledge
distillation (KD). In this case, the distilled student (referred to as the
knockoff student) does not outperform a student trained independently with
label smoothing (LS student) in terms of prediction accuracy. To protect
intellectual property of DNNs, it is desirable to build undistillable DNNs. To
this end, it is first observed that an undistillable DNN may have the trait
that each cluster of its output probability distributions in response to all
sample instances with the same label should be highly concentrated to the
extent that each cluster corresponding to each label should ideally collapse
into one probability distribution. Based on this observation and by measuring
the concentration of each cluster in terms of conditional mutual information
(CMI), a new training method called CMI minimized (CMIM) method is proposed,
which trains a DNN by jointly minimizing the conventional cross entropy (CE)
loss and the CMI values of all temperature scaled clusters across the entire
temperature spectrum. The resulting CMIM model is shown, by extensive
experiments, to be undistillable by all tested KD methods existing in the
literature. That is, the knockoff students distilled by these KD methods from
the CMIM model underperform the respective LS students. In addition, the CMIM
model is also shown to performs better than the model trained with the CE loss
alone in terms of their own prediction accuracy.

</details>


### [25] [ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting](https://arxiv.org/abs/2507.00013)
*Hyunwoo Seo,Chiehyeon Lim*

Main category: cs.LG

TL;DR: ST-MTM improves time-series forecasting by integrating seasonal-trend decomposition with masked modeling and contrastive learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing masked time-series modeling ignores inherent semantic structures in raw data, leading to spurious temporal patterns.

Method: ST-MTM uses seasonal-trend decomposition with novel masking strategies for seasonal and trend components, plus contrastive learning.

Result: ST-MTM achieves superior forecasting performance compared to existing methods.

Conclusion: Decomposition-based masked modeling effectively captures temporal semantics, enhancing forecasting accuracy.

Abstract: Forecasting complex time series is an important yet challenging problem that
involves various industrial applications. Recently, masked time-series modeling
has been proposed to effectively model temporal dependencies for forecasting by
reconstructing masked segments from unmasked ones. However, since the semantic
information in time series is involved in intricate temporal variations
generated by multiple time series components, simply masking a raw time series
ignores the inherent semantic structure, which may cause MTM to learn spurious
temporal patterns present in the raw data. To capture distinct temporal
semantics, we show that masked modeling techniques should address entangled
patterns through a decomposition approach. Specifically, we propose ST-MTM, a
masked time-series modeling framework with seasonal-trend decomposition, which
includes a novel masking method for the seasonal-trend components that
incorporates different temporal variations from each component. ST-MTM uses a
period masking strategy for seasonal components to produce multiple masked
seasonal series based on inherent multi-periodicity and a sub-series masking
strategy for trend components to mask temporal regions that share similar
variations. The proposed masking method presents an effective pre-training task
for learning intricate temporal variations and dependencies. Additionally,
ST-MTM introduces a contrastive learning task to support masked modeling by
enhancing contextual consistency among multiple masked seasonal
representations. Experimental results show that our proposed ST-MTM achieves
consistently superior forecasting performance compared to existing masked
modeling, contrastive learning, and supervised forecasting methods.

</details>


### [26] [SWE-Bench-CL: Continual Learning for Coding Agents](https://arxiv.org/abs/2507.00014)
*Thomas Joshi,Shayan Chowdhury,Fatih Uysal*

Main category: cs.LG

TL;DR: SWE-Bench-CL is a continual learning benchmark for evaluating LLMs in evolving software development tasks, featuring chronologically ordered GitHub issues, an interactive evaluation framework, and specialized metrics.


<details>
  <summary>Details</summary>
Motivation: Real-world software development is dynamic, but current benchmarks focus on static tasks. SWE-Bench-CL addresses this gap by simulating repository evolution.

Method: The benchmark uses GitHub issues in chronological sequences, includes an interactive LangGraph-based framework with semantic memory, and introduces continual learning metrics.

Result: The paper provides a reproducible platform for evaluating memory-enabled and memory-disabled agents, with tools to measure stability-plasticity trade-offs.

Conclusion: SWE-Bench-CL offers a robust framework for developing adaptive AI agents in software engineering, with publicly available code and data.

Abstract: Large Language Models (LLMs) have achieved impressive results on static
code-generation benchmarks, but real-world software development unfolds as a
continuous stream of evolving issues, fixes, and feature requests. We introduce
SWE-Bench-CL, a novel continual learning benchmark built on the human-verified
SWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By
organizing GitHub issues into chronologically ordered sequences that reflect
natural repository evolution, SWE-Bench-CL enables direct evaluation of an
agent's ability to accumulate experience, transfer knowledge across tasks, and
resist catastrophic forgetting. We complement the dataset with (i) a
preliminary analysis of inter-task structural similarity and contextual
sensitivity, (ii) an interactive LangGraph-based evaluation framework augmented
with a FAISS-backed semantic memory module, and (iii) a suite of specialized
continual learning metrics -- including average accuracy, forgetting,
forward/backward transfer, tool-use efficiency, and a generalized Composite
Continual Learning Score and CL-F-beta score -- to capture the
stability-plasticity trade-off. We outline a rigorous experimental protocol
comparing memory-enabled and memory-disabled agents across diverse Python
repositories. All code and data are publicly available at
https://github.com/thomasjoshi/agents-never-forget, providing the community
with a reproducible platform for developing more adaptive and robust AI agents
in software engineering.

</details>


### [27] [Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications](https://arxiv.org/abs/2507.00015)
*Lu Zhang,Sangarapillai Lambotharan,Gan Zheng,Guisheng Liao,Xuekang Liu,Fabio Roli,Carsten Maple*

Main category: cs.LG

TL;DR: A novel ViT architecture with an adversarial indicator (AdvI) token is proposed to defend against adversarial attacks in modulation classification, combining training and runtime defenses in one model.


<details>
  <summary>Details</summary>
Motivation: Transformers are vulnerable to adversarial attacks in radio signal classification, necessitating a robust defense mechanism.

Method: Introduces AdvI token in ViT for attack detection, integrates adversarial training, and analyzes attention mechanisms.

Result: Outperforms competitive methods in handling white-box attacks like FGM, PGD, and BIM.

Conclusion: The AdvI token effectively detects and defends against adversarial attacks, simplifying system architecture.

Abstract: The remarkable success of transformers across various fields such as natural
language processing and computer vision has paved the way for their
applications in automatic modulation classification, a critical component in
the communication systems of Internet of Things (IoT) devices. However, it has
been observed that transformer-based classification of radio signals is
susceptible to subtle yet sophisticated adversarial attacks. To address this
issue, we have developed a defensive strategy for transformer-based modulation
classification systems to counter such adversarial attacks. In this paper, we
propose a novel vision transformer (ViT) architecture by introducing a new
concept known as adversarial indicator (AdvI) token to detect adversarial
attacks. To the best of our knowledge, this is the first work to propose an
AdvI token in ViT to defend against adversarial attacks. Integrating an
adversarial training method with a detection mechanism using AdvI token, we
combine a training time defense and running time defense in a unified neural
network model, which reduces architectural complexity of the system compared to
detecting adversarial perturbations using separate models. We investigate into
the operational principles of our method by examining the attention mechanism.
We show the proposed AdvI token acts as a crucial element within the ViT,
influencing attention weights and thereby highlighting regions or features in
the input data that are potentially suspicious or anomalous. Through
experimental results, we demonstrate that our approach surpasses several
competitive methods in handling white-box attack scenarios, including those
utilizing the fast gradient method, projected gradient descent attacks and
basic iterative method.

</details>


### [28] [Gradient-based Fine-Tuning through Pre-trained Model Regularization](https://arxiv.org/abs/2507.00016)
*Xuanbo Liu,Liu Liu,Fuxiang Wu,Fusheng Hao,Xianglong Liu*

Main category: cs.LG

TL;DR: GRFT is a gradient-based, regularized fine-tuning method that updates specific rows/columns of weight matrices, reducing storage and computational costs while outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning large pre-trained models is resource-intensive. GPS reduces parameters but increases resource demands. GRFT aims to optimize this trade-off.

Method: GRFT updates rows/columns with the highest sum of squared gradients, reducing parameters and storage. It also uses regularization for better knowledge transfer.

Result: GRFT achieves state-of-the-art performance, updating only 1.22% (FGVC) and 0.30% (VTAB) of parameters, outperforming GPS, Adapter Tuning, and LoRA.

Conclusion: GRFT is efficient and effective, balancing resource use and performance, making it a promising method for fine-tuning large models.

Abstract: Large pre-trained models have demonstrated extensive applications across
various fields. However, fine-tuning these models for specific downstream tasks
demands significant computational resources and storage. One fine-tuning
method, gradient-based parameter selection (GPS), focuses on fine-tuning only
the parameters with high gradients in each neuron, thereby reducing the number
of training parameters. Nevertheless, this approach increases computational
resource requirements and storage demands. In this paper, we propose an
efficient gradient-based and regularized fine-tuning method (GRFT) that updates
the rows or columns of the weight matrix. We theoretically demonstrate that the
rows or columns with the highest sum of squared gradients are optimal for
updating. This strategy effectively reduces storage overhead and improves the
efficiency of parameter selection. Additionally, we incorporate regularization
to enhance knowledge transfer from the pre-trained model. GRFT achieves
state-of-the-art performance, surpassing existing methods such as GPS, Adapter
Tuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the
total parameters on FGVC and VTAB datasets, respectively, demonstrating its
high efficiency and effectiveness. The source code will be released soon.

</details>


### [29] [Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections](https://arxiv.org/abs/2507.00018)
*Bo Wang,Qinyuan Cheng,Runyu Peng,Rong Bao,Peiji Li,Qipeng Guo,Linyang Li,Zhiyuan Zeng,Yunhua Zhou,Xipeng Qiu*

Main category: cs.LG

TL;DR: A unified framework links SFT and preference learning in LLM post-training, showing SFT as implicit reward learning. A learning rate reduction method improves performance, and alternative SFT objectives enhance post-DPO results.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between SFT and preference learning in LLM post-training and address limitations in conventional SFT.

Method: Mathematical derivation to unify SFT and preference learning, propose learning rate reduction, and derive alternative SFT objectives.

Result: Significant performance improvements (up to 25% relative gain, 6% absolute win rate increase) and enhanced post-DPO model performance.

Conclusion: The framework and proposed methods effectively address SFT limitations and improve LLM post-training outcomes.

Abstract: Post-training processes are essential phases in grounding pre-trained
language models to real-world tasks, with learning from demonstrations or
preference signals playing a crucial role in this adaptation. We present a
unified theoretical framework bridging Supervised Fine-Tuning (SFT) and
preference learning in Large Language Model (LLM) post-training. Through
rigorous mathematical derivation, we demonstrate that both SFT and preference
learning methods like Direct Preference Optimization (DPO) operate within the
same optimal policy-reward subspace, with SFT representing a special case of
implicit reward learning. Our analysis reveals a critical limitation in
conventional SFT: the KL divergence term in distribution matching becomes
constant with respect to the policy during optimization, failing to constrain
model updates. To address this, we propose a simple yet effective learning rate
reduction approach that yields significant performance improvements (up to
\textbf{25\%} relative gain and \textbf{6\%} absolute win rate increase in
instruction following tasks. Additionally, we derive alternative SFT objectives
from various f-divergence functions that preserve the KL term during
optimization, further enhancing post-DPO model performance. Finally, we extend
the theoretical relationship between LLM logits and Q-functions from preference
learning to the SFT context, providing mathematical derivations and
experimental validation.

</details>


### [30] [Quantum Inspired Encoding Strategies for Machine Learning Models: Proposing and Evaluating Instance Level, Global Discrete, and Class Conditional Representations](https://arxiv.org/abs/2507.00019)
*Minati Rath,Hema Date*

Main category: cs.LG

TL;DR: The paper proposes and compares three quantum-inspired data encoding strategies (ILS, GDS, CCVS) for classical ML, focusing on encoding efficiency, correctness, and classification performance.


<details>
  <summary>Details</summary>
Motivation: To reduce high encoding time while ensuring correct encoding values and analyzing their impact on classification performance in classical ML models.

Method: Three encoding strategies are evaluated: ILS (treats each dataset row independently), GDS (maps unique feature values uniformly), and CCVS (encodes unique values per class).

Result: The study assesses encoding efficiency, correctness, model accuracy, and computational cost, providing insights into trade-offs.

Conclusion: The paper offers optimization insights for quantum-inspired data transformations in classical ML workflows.

Abstract: In this study, we propose, evaluate and compare three quantum inspired data
encoding strategies, Instance Level Strategy (ILS), Global Discrete Strategy
(GDS) and Class Conditional Value Strategy (CCVS), for transforming classical
data into quantum data for use in pure classical machine learning models. The
primary objective is to reduce high encoding time while ensuring correct
encoding values and analyzing their impact on classification performance. The
Instance Level Strategy treats each row of dataset independently; mimics local
quantum states. Global Discrete Value Based encoding strategy maps all unique
feature values across the full dataset to quantum states uniformly. In
contrast, the Class conditional Value based encoding strategy encodes unique
values separately for each class, preserving class dependent information.
  We apply these encoding strategies to a classification task and assess their
impact on en-coding efficiency, correctness, model accuracy, and computational
cost. By analyzing the trade offs between encoding time, precision, and
predictive performance, this study provides insights into optimizing quantum
inspired data transformations for classical machine learning workflows.

</details>


### [31] [Variational Autoencoder for Generating Broader-Spectrum prior Proposals in Markov chain Monte Carlo Methods](https://arxiv.org/abs/2507.00020)
*Marcio Borges,Felipe Pereira,Michel Tosin*

Main category: cs.LG

TL;DR: A Variational Autoencoder (VAE) is used to improve McMC methods by generating flexible prior proposals, outperforming traditional KLE in scenarios with unknown correlation lengths.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like KLE require prior knowledge of covariance functions, which is often unavailable. The study aims to enhance McMC efficiency and adaptability in high-dimensional Bayesian inverse problems.

Method: The VAE framework is applied to generate data-driven prior proposals for subsurface flow modeling, tested on a synthetic groundwater flow inversion problem.

Result: VAE matches KLE accuracy when correlation length is known and outperforms it otherwise, while reducing stochastic dimensionality for computational efficiency.

Conclusion: Deep generative models like VAE can enhance McMC methods, making Bayesian inference more adaptable and efficient in high-dimensional problems.

Abstract: This study uses a Variational Autoencoder method to enhance the efficiency
and applicability of Markov Chain Monte Carlo (McMC) methods by generating
broader-spectrum prior proposals. Traditional approaches, such as the
Karhunen-Lo\`eve Expansion (KLE), require previous knowledge of the covariance
function, often unavailable in practical applications. The VAE framework
enables a data-driven approach to flexibly capture a broader range of
correlation structures in Bayesian inverse problems, particularly subsurface
flow modeling. The methodology is tested on a synthetic groundwater flow
inversion problem, where pressure data is used to estimate permeability fields.
Numerical experiments demonstrate that the VAE-based parameterization achieves
comparable accuracy to KLE when the correlation length is known and outperforms
KLE when the assumed correlation length deviates from the true value. Moreover,
the VAE approach significantly reduces stochastic dimensionality, improving
computational efficiency. The results suggest that leveraging deep generative
models in McMC methods can lead to more adaptable and efficient Bayesian
inference in high-dimensional problems.

</details>


### [32] [GLU Attention Improve Transformer](https://arxiv.org/abs/2507.00022)
*Zehao Wang*

Main category: cs.LG

TL;DR: Introduces GLU Attention, a novel attention mechanism using Gated Linear Units to enhance neural network performance without extra parameters or computational costs.


<details>
  <summary>Details</summary>
Motivation: To improve model performance and convergence speed by introducing nonlinearity into attention values.

Method: Proposes GLU Attention, integrating Gated Linear Units into attention mechanisms, compatible with existing technologies like Flash Attention and RoPE.

Result: GLU Attention enhances performance and convergence speed in text and vision tasks with minimal overhead.

Conclusion: GLU Attention is a lightweight, effective enhancement for attention mechanisms, easily integrated with other technologies.

Abstract: Gated Linear Units (GLU) have shown great potential in enhancing neural
network performance. In this paper, I introduce a novel attention mechanism
called GLU Attention, which introduces nonlinearity into the values of
Attention. My experiments demonstrate that GLU Attention improves both model
performance and convergence speed across text and vision modalities with zero
additional parameters and negligible computational costs. GLU Attention is
lightweight and can seamlessly integrate with other technologies, such as Flash
Attention, Rotary Position Embedding (RoPE), and various Multi-Head Attention
(MHA) variants such as Grouped-Query Attention (GQA). This project is
open-sourced at github.

</details>


### [33] [AIMatDesign: Knowledge-Augmented Reinforcement Learning for Inverse Materials Design under Data Scarcity](https://arxiv.org/abs/2507.00024)
*Yeyong Yu,Xilei Bian,Jie Xiong,Xing Wu,Quan Qian*

Main category: cs.LG

TL;DR: AIMatDesign, a reinforcement learning framework, addresses challenges in high-dimensional materials design by augmenting data, refining predictions with LLMs, and incorporating expert knowledge, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of unreliable predictions and lack of expert knowledge integration in machine learning-driven inverse design for novel materials.

Method: Uses reinforcement learning with data augmentation, automated refinement via LLMs, and knowledge-based reward functions.

Result: Outperforms traditional methods in efficiency, speed, and success rates; validated with experimental synthesis of high-performance Zr-based alloys.

Conclusion: AIMatDesign offers a reliable, efficient framework for closed-loop materials discovery, validated by experimental results.

Abstract: With the growing demand for novel materials, machine learning-driven inverse
design methods face significant challenges in reconciling the high-dimensional
materials composition space with limited experimental data. Existing approaches
suffer from two major limitations: (I) machine learning models often lack
reliability in high-dimensional spaces, leading to prediction biases during the
design process; (II) these models fail to effectively incorporate domain expert
knowledge, limiting their capacity to support knowledge-guided inverse design.
To address these challenges, we introduce AIMatDesign, a reinforcement learning
framework that addresses these limitations by augmenting experimental data
using difference-based algorithms to build a trusted experience pool,
accelerating model convergence. To enhance model reliability, an automated
refinement strategy guided by large language models (LLMs) dynamically corrects
prediction inconsistencies, reinforcing alignment between reward signals and
state value functions. Additionally, a knowledge-based reward function
leverages expert domain rules to improve stability and efficiency during
training. Our experiments demonstrate that AIMatDesign significantly surpasses
traditional machine learning and reinforcement learning methods in discovery
efficiency, convergence speed, and success rates. Among the numerous candidates
proposed by AIMatDesign, experimental synthesis of representative Zr-based
alloys yielded a top-performing BMG with 1.7GPa yield strength and 10.2\%
elongation, closely matching predictions. Moreover, the framework accurately
captured the trend of yield strength variation with composition, demonstrating
its reliability and potential for closed-loop materials discovery.

</details>


### [34] [Generalizing to New Dynamical Systems via Frequency Domain Adaptation](https://arxiv.org/abs/2507.00025)
*Tiexin Qin,Hong Yan,Haoliang Li*

Main category: cs.LG

TL;DR: FNSDA is a parameter-efficient method for generalizing to new dynamics by adapting in Fourier space, outperforming existing methods with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Current deep learning approaches for modeling dynamics struggle with generalization to unseen systems governed by the same dynamics but differing in environmental characteristics.

Method: FNSDA identifies shareable dynamics via automatic Fourier mode partitioning and adjusts modes for new environments using low-dimensional latent parameters.

Result: FNSDA achieves superior or competitive generalization on four dynamic systems with reduced parameter cost.

Conclusion: FNSDA offers an efficient and effective solution for generalizing to new dynamics, validated by strong performance across diverse systems.

Abstract: Learning the underlying dynamics from data with deep neural networks has
shown remarkable potential in modeling various complex physical dynamics.
However, current approaches are constrained in their ability to make reliable
predictions in a specific domain and struggle with generalizing to unseen
systems that are governed by the same general dynamics but differ in
environmental characteristics. In this work, we formulate a parameter-efficient
method, Fourier Neural Simulator for Dynamical Adaptation (FNSDA), that can
readily generalize to new dynamics via adaptation in the Fourier space.
Specifically, FNSDA identifies the shareable dynamics based on the known
environments using an automatic partition in Fourier modes and learns to adjust
the modes specific for each new environment by conditioning on low-dimensional
latent systematic parameters for efficient generalization. We evaluate our
approach on four representative families of dynamic systems, and the results
show that FNSDA can achieve superior or competitive generalization performance
compared to existing methods with a significantly reduced parameter cost. Our
code is available at https://github.com/WonderSeven/FNSDA.

</details>


### [35] [ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models](https://arxiv.org/abs/2507.00026)
*Jiale Ding,Xiang Zheng,Cong Wang,Wei-Bin Lee,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.LG

TL;DR: ROSE is a framework using multi-objective reinforcement learning to generate diverse and context-rich adversarial prompts for better LLM safety evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing manual safety benchmarks are static and labor-intensive, while automated methods lack topic diversity and real-world alignment.

Method: Proposes ROSE, a framework using multi-objective reinforcement learning to fine-tune an adversarial LLM for generating diverse and context-rich prompts.

Result: ROSE outperforms existing methods in uncovering safety vulnerabilities in LLMs, improving integrated evaluation metrics.

Conclusion: ROSE advances practical, reality-oriented safety evaluation for LLMs.

Abstract: As Large Language Models (LLMs) are increasingly deployed as black-box
components in real-world applications, evaluating their safety-especially under
adversarial prompting-has become critical. Arguably, effective safety
evaluations should be adaptive, evolving with LLM capabilities, and also cover
a broad spectrum of harmful topics and real-world scenarios to fully expose
potential vulnerabilities. Existing manual safety benchmarks, built on
handcrafted adversarial prompts, are limited by their static nature and the
intensive labor required to update them, making it difficult to keep pace with
rapidly advancing LLMs. In contrast, automated adversarial prompt generation
offers a promising path toward adaptive evaluation. However, current methods
often suffer from insufficient adversarial topic coverage (topic-level
diversity) and weak alignment with real-world contexts. These shortcomings stem
from the exploration-exploitation dilemma in black-box optimization and a lack
of real-world contextualization, resulting in adversarial prompts that are both
topically narrow and scenario-repetitive. To address these issues, we propose
Reality-Oriented Safety Evaluation (ROSE), a novel framework that uses
multi-objective reinforcement learning to fine-tune an adversarial LLM for
generating topically diverse and contextually rich adversarial prompts.
Experiments show that ROSE outperforms existing methods in uncovering safety
vulnerabilities in state-of-the-art LLMs, with notable improvements in
integrated evaluation metrics. We hope ROSE represents a step toward more
practical and reality-oriented safety evaluation of LLMs. WARNING: This paper
contains examples of potentially harmful text.

</details>


### [36] [HiT-JEPA: A Hierarchical Self-supervised Trajectory Embedding Framework for Similarity Computation](https://arxiv.org/abs/2507.00028)
*Lihuan Li,Hao Xue,Shuang Ao,Yang Song,Flora Salim*

Main category: cs.LG

TL;DR: HiT-JEPA is a hierarchical framework for multi-scale urban trajectory representation, capturing fine-grained details and high-level semantics in one model.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to integrate fine-grained trajectory details and high-level summaries, limiting analysis of long-term dependencies and local nuances.

Method: HiT-JEPA uses a three-layer hierarchy to capture point-level details, intermediate patterns, and high-level abstractions.

Result: Experiments show HiT-JEPA produces richer, multi-scale representations for trajectory similarity tasks.

Conclusion: HiT-JEPA effectively unifies local dynamics and global semantics in trajectory analysis.

Abstract: The representation of urban trajectory data plays a critical role in
effectively analyzing spatial movement patterns. Despite considerable progress,
the challenge of designing trajectory representations that can capture diverse
and complementary information remains an open research problem. Existing
methods struggle in incorporating trajectory fine-grained details and
high-level summary in a single model, limiting their ability to attend to both
long-term dependencies while preserving local nuances. To address this, we
propose HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint
Embedding Predictive Architecture), a unified framework for learning
multi-scale urban trajectory representations across semantic abstraction
levels. HiT-JEPA adopts a three-layer hierarchy that progressively captures
point-level fine-grained details, intermediate patterns, and high-level
trajectory abstractions, enabling the model to integrate both local dynamics
and global semantics in one coherent structure. Extensive experiments on
multiple real-world datasets for trajectory similarity computation show that
HiT-JEPA's hierarchical design yields richer, multi-scale representations. Code
is available at: https://anonymous.4open.science/r/HiT-JEPA.

</details>


### [37] [LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing](https://arxiv.org/abs/2507.00029)
*Wenbing Li,Zikai Song,Hang Zhou,Yunyao Zhang,Junqing Yu,Wei Yang*

Main category: cs.LG

TL;DR: LoRA-Mixer integrates LoRA experts into a MoE framework, improving parameter efficiency and task fidelity by dynamically routing task-specific LoRA experts in attention modules. It outperforms base models and state-of-the-art methods with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Existing methods combining LoRA with MoE for LLMs suffer from inefficiency and diluted task fidelity due to swapping entire layers or adding parallel branches.

Method: LoRA-Mixer replaces projection matrices in attention modules with dynamically routed LoRA experts, supporting joint optimization or deployment of pre-trained LoRA modules. It uses a hard-soft routing strategy and an adaptive Specialization Balance Loss for robust training.

Result: LoRA-Mixer achieves significant improvements (up to 7.61%) over base models and outperforms state-of-the-art methods with 48% fewer parameters.

Conclusion: LoRA-Mixer is a lightweight, efficient framework that enhances LLM adaptation to multiple tasks, demonstrating strong performance and parameter efficiency.

Abstract: Recent efforts to combine low-rank adaptation (LoRA) with mixture-of-experts
(MoE) for adapting large language models (LLMs) to multiple tasks still exhibit
prevailing limitations: they either swap entire attention/feed-forward layers
for switch experts or bolt on parallel expert branches, diluting parameter
efficiency and task fidelity. We propose the LoRA-Mixer, a modular and
lightweight MoE framework that integrates LoRA experts. Our core innovation
lies in replacing the projection matrices of the attention module's
input/output linear layers with dynamically routed, task-specific LoRA experts.
This design ensures seamless compatibility with diverse foundation models,
including transformers and state space models (SSMs), by leveraging their
inherent linear projection structures. The framework supports two operational
paradigms: (1) joint optimization of LoRA experts and routing mechanisms via a
novel hard-soft routing strategy, or (2) direct deployment of pre-trained,
frozen LoRA modules sourced from external repositories. To enable robust router
training with limited data while ensuring stable routing decisions and
maximizing expert reuse, we introduce an adaptive Specialization Balance Loss
(SBL) that jointly optimizes expert balance and task-specific alignment.
Extensive experiments on seven benchmark datasets, including MedQA, CoLA,
SST-2, GSM8K, ARC-E, ARC-C, and HumanEval, demonstrate the effectiveness of
LoRA-Mixer. On datasets such as GSM8K, HumanEval, and MedQA, LoRA-Mixer
achieves significant improvements of 7.61%, 4.88%, and 3.08% over the base
models, respectively. Compared with state-of-the-art methods, LoRA-Mixer
achieves additional improvements of 1.09%, 1.45%, and 1.68%, respectively,
using only 48% of the parameters, demonstrating its efficiency and strong
performance.

</details>


### [38] [Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.00030)
*Abhishek Verma,Nallarasan V,Balaraman Ravindran*

Main category: cs.LG

TL;DR: A novel DRL approach integrates contextual bandits to adaptively select action durations, improving performance and efficiency in tasks like Atari games.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored aspect of temporal scale in DRL, enhancing policy flexibility and computational efficiency.

Method: Augments DQN with a contextual bandit module to learn optimal action repetition rates based on state contexts.

Result: Experiments on Atari 2600 games show significant performance improvements over static duration baselines.

Conclusion: The approach offers a scalable solution for real-time applications requiring dynamic action durations.

Abstract: Deep Reinforcement Learning (DRL) has achieved remarkable success in complex
sequential decision-making tasks, such as playing Atari 2600 games and
mastering board games. A critical yet underexplored aspect of DRL is the
temporal scale of action execution. We propose a novel paradigm that integrates
contextual bandits with DRL to adaptively select action durations, enhancing
policy flexibility and computational efficiency. Our approach augments a Deep
Q-Network (DQN) with a contextual bandit module that learns to choose optimal
action repetition rates based on state contexts. Experiments on Atari 2600
games demonstrate significant performance improvements over static duration
baselines, highlighting the efficacy of adaptive temporal abstractions in DRL.
This paradigm offers a scalable solution for real-time applications like gaming
and robotics, where dynamic action durations are critical.

</details>


### [39] [Enhancing Spatio-Temporal Forecasting with Spatial Neighbourhood Fusion:A Case Study on COVID-19 Mobility in Peru](https://arxiv.org/abs/2507.00031)
*Chuan Li,Jiang You,Hassine Moungla,Vincent Gauthier,Miguel Nunez-del-Prado,Hugo Alatrista-Salas*

Main category: cs.LG

TL;DR: The paper introduces a Spatial Neighbourhood Fusion (SPN) technique to improve mobility flow forecasting in sparse datasets, showing a 9.85% reduction in test MSE.


<details>
  <summary>Details</summary>
Motivation: Accurate human mobility modeling is crucial for epidemic response, but sparse spatial data limits conventional models.

Method: Proposes SPN, a lightweight method to augment cell features with aggregated neighbor signals, tested on three forecasting models.

Result: SPN consistently enhances forecasting, reducing test MSE by up to 9.85%.

Conclusion: Spatial smoothing of sparse signals is a simple, effective approach for robust spatio-temporal forecasting in public health crises.

Abstract: Accurate modeling of human mobility is critical for understanding epidemic
spread and deploying timely interventions. In this work, we leverage a
large-scale spatio-temporal dataset collected from Peru's national Digital
Contact Tracing (DCT) application during the COVID-19 pandemic to forecast
mobility flows across urban regions. A key challenge lies in the spatial
sparsity of hourly mobility counts across hexagonal grid cells, which limits
the predictive power of conventional time series models. To address this, we
propose a lightweight and model-agnostic Spatial Neighbourhood Fusion (SPN)
technique that augments each cell's features with aggregated signals from its
immediate H3 neighbors. We evaluate this strategy on three forecasting
backbones: NLinear, PatchTST, and K-U-Net, under various historical input
lengths. Experimental results show that SPN consistently improves forecasting
performance, achieving up to 9.85 percent reduction in test MSE. Our findings
demonstrate that spatial smoothing of sparse mobility signals provides a simple
yet effective path toward robust spatio-temporal forecasting during public
health crises.

</details>


### [40] [Data Collection with Non-Uniform Axial Power for Phase II of the OECD/NEA AI/ML Critical Heat Flux Benchmark](https://arxiv.org/abs/2507.00034)
*Reece Bourisaw,Reid McCants,Jean-Marie Le Corre,Anna Iskhakova,Arsen S. Iskhakov*

Main category: cs.LG

TL;DR: The paper compiles and digitizes CHF data for uniform and non-uniform heating conditions, highlighting the limitations of classical correlations and neural networks trained on uniform data, and emphasizes the need for models incorporating axial power distributions.


<details>
  <summary>Details</summary>
Motivation: To support Phase II of the OECD/NEA AI/ML CHF benchmark by providing curated datasets and baseline modeling results for advanced transfer-learning and uncertainty quantification.

Method: Heating profiles were extracted, interpolated, validated, and encoded in machine-readable formats. Classical correlations and neural networks were evaluated.

Result: Classical correlations and neural networks trained on uniform data perform poorly for non-uniform heating, indicating the need for models that account for axial power distributions.

Conclusion: The study provides foundational data and insights for future CHF benchmark phases, focusing on transfer-learning, uncertainty quantification, and design optimization.

Abstract: Critical heat flux (CHF) marks the onset of boiling crisis in light-water
reactors, defining safe thermal-hydraulic operating limits. To support Phase II
of the OECD/NEA AI/ML CHF benchmark, which introduces spatially varying power
profiles, this work compiles and digitizes a broad CHF dataset covering both
uniform and non-uniform axial heating conditions. Heating profiles were
extracted from technical reports, interpolated onto a consistent axial mesh,
validated via energy-balance checks, and encoded in machine-readable formats
for benchmark compatibility.
  Classical CHF correlations exhibit substantial errors under uniform heating
and degrade markedly when applied to non-uniform profiles, while modern tabular
methods offer improved but still imperfect predictions. A neural network
trained solely on uniform data performs well in that regime but fails to
generalize to spatially varying scenarios, underscoring the need for models
that explicitly incorporate axial power distributions. By providing these
curated datasets and baseline modeling results, this study lays the groundwork
for advanced transfer-learning strategies, rigorous uncertainty quantification,
and design-optimization efforts in the next phase of the CHF benchmark.

</details>


### [41] [IDRIFTNET: Physics-Driven Spatiotemporal Deep Learning for Iceberg Drift Forecasting](https://arxiv.org/abs/2507.00036)
*Rohan Putatunda,Sanjay Purushotham,Ratnaksha Lele,Vandana P. Janeja*

Main category: cs.LG

TL;DR: IDRIFTNET, a hybrid physics-driven deep learning model, improves iceberg trajectory forecasting by combining analytical drift physics with residual learning, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Accurate iceberg trajectory forecasting is challenging due to sparse data and complex environmental influences, limiting deep learning models' effectiveness.

Method: Proposes IDRIFTNET, integrating analytical drift physics with a rotate-augmented spectral neural network to learn mismatch patterns and capture global/local data patterns.

Result: IDRIFTNET achieves lower Final Displacement Error (FDE) and Average Displacement Error (ADE) compared to other models on Antarctic icebergs A23A and B22A.

Conclusion: IDRIFTNET effectively forecasts iceberg trajectories under dynamic conditions, addressing data scarcity and nonlinear motion challenges.

Abstract: Drifting icebergs in the polar oceans play a key role in the Earth's climate
system, impacting freshwater fluxes into the ocean and regional ecosystems
while also posing a challenge to polar navigation. However, accurately
forecasting iceberg trajectories remains a formidable challenge, primarily due
to the scarcity of spatiotemporal data and the complex, nonlinear nature of
iceberg motion, which is also impacted by environmental variables. The iceberg
motion is influenced by multiple dynamic environmental factors, creating a
highly variable system that makes trajectory identification complex. These
limitations hinder the ability of deep learning models to effectively capture
the underlying dynamics and provide reliable predictive outcomes. To address
these challenges, we propose a hybrid IDRIFTNET model, a physics-driven deep
learning model that combines an analytical formulation of iceberg drift
physics, with an augmented residual learning model. The model learns the
pattern of mismatch between the analytical solution and ground-truth
observations, which is combined with a rotate-augmented spectral neural network
that captures both global and local patterns from the data to forecast future
iceberg drift positions. We compare IDRIFTNET model performance with
state-of-the-art models on two Antarctic icebergs: A23A and B22A. Our findings
demonstrate that IDRIFTNET outperforms other models by achieving a lower Final
Displacement Error (FDE) and Average Displacement Error (ADE) across a variety
of time points. These results highlight IDRIFTNET's effectiveness in capturing
the complex, nonlinear drift of icebergs for forecasting iceberg trajectories
under limited data and dynamic environmental conditions.

</details>


### [42] [Model Fusion via Neuron Interpolation](https://arxiv.org/abs/2507.00037)
*Phoomraphee Luenam,Andreas Spanopoulos,Amit Sant,Thomas Hofmann,Sotiris Anagnostidis,Sidak Pal Singh*

Main category: cs.LG

TL;DR: A novel neuron-centric model fusion algorithm integrates multiple neural networks effectively, outperforming prior methods, especially in zero-shot and non-IID scenarios.


<details>
  <summary>Details</summary>
Motivation: Model fusion is challenging due to differences in internal representations (e.g., permutation invariance, random initialization, or varied training data distributions).

Method: Neuron-centric algorithms group intermediate neurons of parent models, using neuron attribution scores, and generalize to arbitrary layer types.

Result: Outperforms previous fusion techniques on benchmark datasets, particularly in zero-shot and non-IID cases.

Conclusion: The approach effectively combines neural networks regardless of training data distribution, with code publicly available.

Abstract: Model fusion aims to combine the knowledge of multiple models by creating one
representative model that captures the strengths of all of its parents.
However, this process is non-trivial due to differences in internal
representations, which can stem from permutation invariance, random
initialization, or differently distributed training data. We present a novel,
neuron-centric family of model fusion algorithms designed to integrate multiple
trained neural networks into a single network effectively regardless of
training data distribution. Our algorithms group intermediate neurons of parent
models to create target representations that the fused model approximates with
its corresponding sub-network. Unlike prior approaches, our approach
incorporates neuron attribution scores into the fusion process. Furthermore,
our algorithms can generalize to arbitrary layer types. Experimental results on
various benchmark datasets demonstrate that our algorithms consistently
outperform previous fusion techniques, particularly in zero-shot and non-IID
fusion scenarios. The code is available at
https://github.com/AndrewSpano/neuron-interpolation-model-fusion.

</details>


### [43] [Quality over Quantity: An Effective Large-Scale Data Reduction Strategy Based on Pointwise V-Information](https://arxiv.org/abs/2507.00038)
*Fei Chen,Wenchi Zhou*

Main category: cs.LG

TL;DR: The paper proposes a data reduction strategy using Pointwise V-information (PVI) to enhance model training efficiency by selecting optimal instances, preserving performance with minimal accuracy loss, and improving convergence.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of selecting the most informative instances in large datasets to improve data quality and training efficiency.

Method: Quantifies instance difficulty using PVI, filters low-difficulty instances statically, and employs progressive learning on sorted instances.

Result: Removing 10%-30% of data preserves classifier performance (0.0001%-0.76% accuracy loss) and achieves 0.8% accuracy gain with faster convergence.

Conclusion: The PVI-based strategy enhances model performance and training efficiency, with successful cross-lingual application to Chinese NLP tasks.

Abstract: Data reduction plays a vital role in data-centric AI by identifying the most
informative instance within large-scale datasets to enhance model training
efficiency. The core challenge lies in how to select the optimal
instances-rather than the entire datasets-to improve data quality and training
efficiency. In this paper, we propose an effective data reduction strategy
based on Pointwise V-information(PVI). First, we quantify instance difficulty
using PVI and filter out low-difficulty instances enabling a static approach.
Experiments demonstrate that removing 10%-30% of the data preserves the
classifier performance with only a 0.0001% to 0.76% loss in accuracy.Second, we
use a progressive learning approach to training the classifiers on instances
sorted by ascending PVI, accelerating convergence and achieving a 0.8% accuracy
gain over conventional training. Our results suggest that with the effective
data reduction strategy, training a classifier on the selected optimal subset
could enhance the model performance and boost training efficiency. Moreover, we
have transferred the PVI framework, which previously applied only to English
datasets, to diverse Chinese NLP tasks and base models, leading to valuable
insights for cross-lingual data reduction and faster training. The codes are
released at https://github.com/zhouwenchi/DatasetReductionStrategy.

</details>


### [44] [Pattern-Based Graph Classification: Comparison of Quality Measures and Importance of Preprocessing](https://arxiv.org/abs/2507.00039)
*Lucas Potin,Rosa Figueiredo,Vincent Labatut,Christine Largeron*

Main category: cs.LG

TL;DR: The paper compares 38 quality measures for graph classification, evaluates their theoretical properties, and proposes a clustering-based preprocessing step to improve performance.


<details>
  <summary>Details</summary>
Motivation: The lack of a focused comparison of quality measures for graph classification leads to the systematic use of widespread measures without thorough evaluation.

Method: The authors characterize 38 quality measures theoretically, create a benchmark with public datasets, and propose a clustering-based preprocessing step.

Result: Some popular measures underperform, while the proposed preprocessing step reduces pattern count without compromising performance.

Conclusion: The study highlights the need for careful measure selection and demonstrates the effectiveness of preprocessing for graph classification.

Abstract: Graph classification aims to categorize graphs based on their structural and
attribute features, with applications in diverse fields such as social network
analysis and bioinformatics. Among the methods proposed to solve this task,
those relying on patterns (i.e. subgraphs) provide good explainability, as the
patterns used for classification can be directly interpreted. To identify
meaningful patterns, a standard approach is to use a quality measure, i.e. a
function that evaluates the discriminative power of each pattern. However, the
literature provides tens of such measures, making it difficult to select the
most appropriate for a given application. Only a handful of surveys try to
provide some insight by comparing these measures, and none of them specifically
focuses on graphs. This typically results in the systematic use of the most
widespread measures, without thorough evaluation. To address this issue, we
present a comparative analysis of 38 quality measures from the literature. We
characterize them theoretically, based on four mathematical properties. We
leverage publicly available datasets to constitute a benchmark, and propose a
method to elaborate a gold standard ranking of the patterns. We exploit these
resources to perform an empirical comparison of the measures, both in terms of
pattern ranking and classification performance. Moreover, we propose a
clustering-based preprocessing step, which groups patterns appearing in the
same graphs to enhance classification performance. Our experimental results
demonstrate the effectiveness of this step, reducing the number of patterns to
be processed while achieving comparable performance. Additionally, we show that
some popular measures widely used in the literature are not associated with the
best results.

</details>


### [45] [Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation](https://arxiv.org/abs/2507.00055)
*Varsha Pendyala,Pedro Morgado,William Sethares*

Main category: cs.LG

TL;DR: The paper introduces LiSER, a knowledge distillation framework for speech emotion recognition (SER) that uses unlabeled audio-visual data and teacher models to train lightweight student models, reducing reliance on labeled datasets.


<details>
  <summary>Details</summary>
Motivation: Voice interfaces can be enhanced by SER, but collecting labeled data is costly. Multi-modal (audio-visual) SER is beneficial but data-intensive.

Method: Proposes LiSER, a knowledge distillation framework leveraging unlabeled audio-visual data and large teacher models to train lightweight student models.

Result: Experiments on RAVDESS and CREMA-D datasets show LiSER reduces the need for extensive labeled data in SER tasks.

Conclusion: LiSER effectively addresses the challenge of labeled data scarcity in multi-modal SER by utilizing knowledge distillation.

Abstract: Voice interfaces integral to the human-computer interaction systems can
benefit from speech emotion recognition (SER) to customize responses based on
user emotions. Since humans convey emotions through multi-modal audio-visual
cues, developing SER systems using both the modalities is beneficial. However,
collecting a vast amount of labeled data for their development is expensive.
This paper proposes a knowledge distillation framework called LightweightSER
(LiSER) that leverages unlabeled audio-visual data for SER, using large teacher
models built on advanced speech and face representation models. LiSER transfers
knowledge regarding speech emotions and facial expressions from the teacher
models to lightweight student models. Experiments conducted on two benchmark
datasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence
on extensive labeled datasets for SER tasks.

</details>


### [46] [Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data](https://arxiv.org/abs/2507.00061)
*Hoang-Dieu Vu,Duc-Nghia Tran,Quang-Tu Pham,Hieu H. Pham,Nicolas Vuillerme,Duc-Tan Tran*

Main category: cs.LG

TL;DR: Smooth-Distill is a self-distillation framework for HAR and sensor placement detection, using a unified CNN (MTL-net) with a smoothed historical model as the teacher, reducing computational overhead while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency of traditional distillation methods and improve multitask learning for HAR and sensor placement detection.

Method: Utilizes MTL-net, a CNN processing accelerometer data, branching into two tasks. Uses a smoothed historical model as the teacher for self-distillation.

Result: Outperforms alternatives in HAR and placement detection, with improved stability and reduced overfitting.

Conclusion: Smooth-Distill offers an efficient, accurate solution for multitask learning with accelerometer data, reducing computational costs for resource-constrained platforms.

Abstract: This paper introduces Smooth-Distill, a novel self-distillation framework
designed to simultaneously perform human activity recognition (HAR) and sensor
placement detection using wearable sensor data. The proposed approach utilizes
a unified CNN-based architecture, MTL-net, which processes accelerometer data
and branches into two outputs for each respective task. Unlike conventional
distillation methods that require separate teacher and student models, the
proposed framework utilizes a smoothed, historical version of the model itself
as the teacher, significantly reducing training computational overhead while
maintaining performance benefits. To support this research, we developed a
comprehensive accelerometer-based dataset capturing 12 distinct sleep postures
across three different wearing positions, complementing two existing public
datasets (MHealth and WISDM). Experimental results show that Smooth-Distill
consistently outperforms alternative approaches across different evaluation
scenarios, achieving notable improvements in both human activity recognition
and device placement detection tasks. This method demonstrates enhanced
stability in convergence patterns during training and exhibits reduced
overfitting compared to traditional multitask learning baselines. This
framework contributes to the practical implementation of knowledge distillation
in human activity recognition systems, offering an effective solution for
multitask learning with accelerometer data that balances accuracy and training
efficiency. More broadly, it reduces the computational cost of model training,
which is critical for scenarios requiring frequent model updates or training on
resource-constrained platforms. The code and model are available at
https://github.com/Kuan2vn/smooth\_distill.

</details>


### [47] [Fractional Policy Gradients: Reinforcement Learning with Long-Term Memory](https://arxiv.org/abs/2507.00073)
*Urvi Pawar,Kunal Telangi*

Main category: cs.LG

TL;DR: Fractional Policy Gradients (FPG) uses fractional calculus for long-term temporal modeling in reinforcement learning, improving sample efficiency and reducing variance compared to standard methods.


<details>
  <summary>Details</summary>
Motivation: Standard policy gradient methods suffer from high variance and inefficient sampling due to Markovian assumptions. FPG addresses these limitations by incorporating long-term temporal correlations.

Method: FPG reformulates gradients using Caputo fractional derivatives, enabling power-law temporal correlations. It includes an efficient recursive computation technique for fractional temporal-difference errors.

Result: FPG achieves asymptotic variance reduction of order O(t^(-alpha)) and shows 35-68% sample efficiency gains and 24-52% variance reduction in experiments.

Conclusion: FPG offers a mathematically grounded approach to leverage long-range dependencies in reinforcement learning without added computational cost.

Abstract: We propose Fractional Policy Gradients (FPG), a reinforcement learning
framework incorporating fractional calculus for long-term temporal modeling in
policy optimization. Standard policy gradient approaches face limitations from
Markovian assumptions, exhibiting high variance and inefficient sampling. By
reformulating gradients using Caputo fractional derivatives, FPG establishes
power-law temporal correlations between state transitions. We develop an
efficient recursive computation technique for fractional temporal-difference
errors with constant time and memory requirements. Theoretical analysis shows
FPG achieves asymptotic variance reduction of order O(t^(-alpha)) versus
standard policy gradients while preserving convergence. Empirical validation
demonstrates 35-68% sample efficiency gains and 24-52% variance reduction
versus state-of-the-art baselines. This framework provides a mathematically
grounded approach for leveraging long-range dependencies without computational
overhead.

</details>


### [48] [Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap](https://arxiv.org/abs/2507.00075)
*Yifan Sun,Yushan Liang,Zhen Zhang,Jiaye Teng*

Main category: cs.LG

TL;DR: The paper explores the dynamics of self-improvement in LLMs, introducing the solver-verifier gap concept to model performance evolution and predict outcomes early in training. It also examines the role of external data.


<details>
  <summary>Details</summary>
Motivation: Understanding how LLM performance evolves during self-improvement and the impact of external data is underexplored.

Method: Theoretical modeling of self-improvement dynamics using the solver-verifier gap, validated empirically on various LLMs and datasets.

Result: The solver-verifier gap effectively predicts self-improvement outcomes. External data has minimal impact on final performance in limited regimes.

Conclusion: The solver-verifier gap provides a robust framework for understanding and predicting self-improvement in LLMs, with practical implications for data usage.

Abstract: Self-improvement is among the most prominent techniques within the realm of
large language models (LLM), aiming to enhance the LLM performance without
relying on external data. Despite its significance, generally how LLM
performances evolve during the self-improvement process remains underexplored.
In this paper, we theoretically model the training dynamics of self-improvement
via the concept of solver-verifier gap. This is inspired by the conjecture that
the performance enhancement of self-improvement stems from the gap between
LLM's solver capability and verifier capability. Based on the theoretical
framework, we further introduce how to predict the ultimate power of
self-improvement using only information from the first few training epochs. We
empirically validate the effectiveness of the theoretical model on various LLMs
and datasets. Beyond self-improvement, we extend our analysis to investigate
how external data influences these dynamics within the framework. Notably, we
find that under limited external data regimes, such external data can be
utilized at any stage without significantly affecting final performances, which
accords with the empirical observations.

</details>


### [49] [The language of time: a language model perspective on time-series foundation models](https://arxiv.org/abs/2507.00078)
*Yi Xie,Yun Xiong,Zejian Shi,Hao Niu,Zhengfu Liu*

Main category: cs.LG

TL;DR: The paper resolves the paradox of time series foundation models' cross-domain success by analyzing their representation learning and generalization, showing they generalize language models' paradigms to probabilistic forms.


<details>
  <summary>Details</summary>
Motivation: To understand why time series foundation models succeed in cross-domain transfer despite the intuitive implausibility due to distinct dynamical systems.

Method: Theoretical and experimental analysis of patch-based time series foundation models, comparing their representation mechanisms to language models.

Result: Time series patches can be quantized into a discrete vocabulary with statistical properties similar to natural language, enabling robust transfer.

Conclusion: The work provides a theoretical foundation for understanding and improving the safety and reliability of time series foundation models.

Abstract: With the rise of large language models, the paradigm of training foundation
models with massive parameter counts on vast datasets has been adopted in
multiple domains to achieve remarkable success. Time series foundation models
represent a significant extension of this paradigm, demonstrating exceptional
expressive power, generalization, and cross-domain transferability. However,
this gives rise to a fundamental paradox: time series data reflect distinct
dynamical systems, making cross-domain transfer intuitively implausible, yet
this is contradicted by the models' empirical success. To resolve this paradox,
this paper investigates, from both theoretical and experimental perspectives,
the representation learning mechanisms and generalization capabilities of
patch-based time series foundation models. We argue that such models are not
merely applying a new architecture but are fundamentally generalizing the
representation paradigm of language models by extending deterministic
vector-based representations to latent probabilistic distributional forms. Our
theoretical analysis supports this framework by demonstrating that continuous
time-series patches can be faithfully quantized into a discrete vocabulary
whose key statistical properties are highly consistent with those of natural
language. This generalization allows time series models to inherit the robust
representation and transfer abilities of large language models, thereby
explaining their superior performance in temporal tasks. Ultimately, our work
provides a rigorous theoretical cornerstone for understanding, evaluating, and
improving the safety and reliability of large-scale time series foundation
models.

</details>


### [50] [Online Meal Detection Based on CGM Data Dynamics](https://arxiv.org/abs/2507.00080)
*Ali Tavasoli,Heman Shakeri*

Main category: cs.LG

TL;DR: Using dynamical modes from CGM data improves meal detection accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: To enhance meal event detection by leveraging glucose dynamics for better accuracy and interpretability.

Method: Extract dynamical modes from CGM data to identify meal-related patterns and anomalies.

Result: Improved detection accuracy and robustness across diverse datasets.

Conclusion: Dynamical features offer a superior framework for meal detection compared to traditional methods.

Abstract: We utilize dynamical modes as features derived from Continuous Glucose
Monitoring (CGM) data to detect meal events. By leveraging the inherent
properties of underlying dynamics, these modes capture key aspects of glucose
variability, enabling the identification of patterns and anomalies associated
with meal consumption. This approach not only improves the accuracy of meal
detection but also enhances the interpretability of the underlying glucose
dynamics. By focusing on dynamical features, our method provides a robust
framework for feature extraction, facilitating generalization across diverse
datasets and ensuring reliable performance in real-world applications. The
proposed technique offers significant advantages over traditional approaches,
improving detection accuracy,

</details>


### [51] [Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission](https://arxiv.org/abs/2507.00082)
*Faranaksadat Solat,Joohyung Lee,Mohamed Seif,Dusit Niyato,H. Vincent Poor*

Main category: cs.LG

TL;DR: FedHLM is a hybrid language model framework combining SLMs and LLMs with federated learning to optimize token-level uncertainty thresholds, reducing LLM transmissions by 95% with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Traditional HLMs face high communication overhead due to frequent offloading to LLMs for uncertain predictions, especially in bandwidth-constrained settings.

Method: FedHLM integrates uncertainty-aware inference with federated learning to collaboratively learn dynamic thresholds and uses P2P token resolution and hierarchical model aggregation.

Result: FedHLM reduces LLM transmissions by over 95% with negligible accuracy loss in large-scale news classification tasks.

Conclusion: FedHLM is scalable and efficient for edge-AI applications, addressing communication overhead while maintaining accuracy.

Abstract: Hybrid Language Models (HLMs) combine the low-latency efficiency of Small
Language Models (SLMs) on edge devices with the high accuracy of Large Language
Models (LLMs) on centralized servers. Unlike traditional end-to-end LLM
inference, HLMs reduce latency and communication by invoking LLMs only when
local SLM predictions are uncertain, i.e., when token-level confidence is low
or entropy is high. However, ambiguous or low-confidence predictions still
require frequent offloading to the LLM, leading to significant communication
overhead in bandwidth-constrained settings. To address this, we propose FedHLM,
a communication-efficient HLM framework that integrates uncertainty-aware
inference with Federated Learning (FL). FedHLM's key innovation lies in
collaboratively learning token-level uncertainty thresholds that govern when
LLM assistance is needed. Rather than using static or manually tuned
thresholds, FedHLM employs FL to optimize these thresholds in a
privacy-preserving, distributed manner. Additionally, it leverages
embedding-based token representations for Peer-to-Peer (P2P) resolution,
enabling clients to reuse tokens inferred by semantically similar peers without
engaging the LLM. We further introduce hierarchical model aggregation: edge
servers refine local routing policies through client updates, while
cross-cluster coordination aligns global decision boundaries. This layered
design captures recurring uncertainty patterns, reducing redundant LLM queries.
Experiments on large-scale news classification tasks show that FedHLM reduces
LLM transmissions by over 95 percent with negligible accuracy loss, making it
well-suited for scalable and efficient edge-AI applications.

</details>


### [52] [Strategic Counterfactual Modeling of Deep-Target Airstrike Systems via Intervention-Aware Spatio-Causal Graph Networks](https://arxiv.org/abs/2507.00083)
*Wei Meng*

Main category: cs.LG

TL;DR: The paper introduces IA-STGNN, a framework for modeling causal relationships between tactical strikes and strategic delays, outperforming baselines with improved accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Addresses gaps in current simulations by capturing intermediate variables in the 'resilience - nodal suppression - negotiation window' chain.

Method: Proposes IA-STGNN, integrating graph attention, counterfactual simulation, and spatial intervention reconstruction for dynamic simulations.

Result: IA-STGNN reduces MAE by 12.8% and increases Top-5% accuracy by 18.4%, enhancing causal consistency and stability.

Conclusion: IA-STGNN offers interpretable predictions for strategic delay, supporting applications like nuclear deterrence and policy modeling.

Abstract: This study addresses the lack of structured causal modeling between tactical
strike behavior and strategic delay in current strategic-level simulations,
particularly the structural bottlenecks in capturing intermediate variables
within the "resilience - nodal suppression - negotiation window" chain. We
propose the Intervention-Aware Spatio-Temporal Graph Neural Network (IA-STGNN),
a novel framework that closes the causal loop from tactical input to strategic
delay output. The model integrates graph attention mechanisms, counterfactual
simulation units, and spatial intervention node reconstruction to enable
dynamic simulations of strike configurations and synchronization strategies.
Training data are generated from a multi-physics simulation platform (GEANT4 +
COMSOL) under NIST SP 800-160 standards, ensuring structural traceability and
policy-level validation. Experimental results demonstrate that IA-STGNN
significantly outperforms baseline models (ST-GNN, GCN-LSTM, XGBoost),
achieving a 12.8 percent reduction in MAE and 18.4 percent increase in Top-5
percent accuracy, while improving causal path consistency and intervention
stability. IA-STGNN enables interpretable prediction of strategic delay and
supports applications such as nuclear deterrence simulation, diplomatic window
assessment, and multi-strategy optimization, providing a structured and
transparent AI decision-support mechanism for high-level policy modeling.

</details>


### [53] [A Joint Topology-Data Fusion Graph Network for Robust Traffic Speed Prediction with Data Anomalism](https://arxiv.org/abs/2507.00085)
*Ruiyuan Jiang,Dongyao Jia,Eng Gee Lim,Pengfei Fan,Yuli Zhang,Shangbo Wang*

Main category: cs.LG

TL;DR: GFEN is a new framework for traffic speed prediction that combines spatiotemporal graph fusion and hybrid methods to improve accuracy and adaptability.


<details>
  <summary>Details</summary>
Motivation: Current traffic prediction methods struggle with complexity, non-linearity, and static techniques for non-stationary data, limiting accuracy and adaptability.

Method: GFEN uses topological spatiotemporal graph fusion and a hybrid approach (k-th order difference and attention-based deep learning) to model multi-scale features and smooth data.

Result: GFEN outperforms state-of-the-art methods by 6.3% in accuracy and converges twice as fast as hybrid models.

Conclusion: GFEN offers superior performance and efficiency, enhancing traffic prediction systems.

Abstract: Accurate traffic prediction is essential for Intelligent Transportation
Systems (ITS), yet current methods struggle with the inherent complexity and
non-linearity of traffic dynamics, making it difficult to integrate spatial and
temporal characteristics. Furthermore, existing approaches use static
techniques to address non-stationary and anomalous historical data, which
limits adaptability and undermines data smoothing. To overcome these
challenges, we propose the Graph Fusion Enhanced Network (GFEN), an innovative
framework for network-level traffic speed prediction. GFEN introduces a novel
topological spatiotemporal graph fusion technique that meticulously extracts
and merges spatial and temporal correlations from both data distribution and
network topology using trainable methods, enabling the modeling of multi-scale
spatiotemporal features. Additionally, GFEN employs a hybrid methodology
combining a k-th order difference-based mathematical framework with an
attention-based deep learning structure to adaptively smooth historical
observations and dynamically mitigate data anomalies and non-stationarity.
Extensive experiments demonstrate that GFEN surpasses state-of-the-art methods
by approximately 6.3% in prediction accuracy and exhibits convergence rates
nearly twice as fast as recent hybrid models, confirming its superior
performance and potential to significantly enhance traffic prediction system
efficiency.

</details>


### [54] [pUniFind: a unified large pre-trained deep learning model pushing the limit of mass spectra interpretation](https://arxiv.org/abs/2507.00087)
*Jiale Zhao,Pengzhi Mao,Kaifei Wang,Yiming Li,Yaping Peng,Ranfei Chen,Shuqi Lu,Xiaohong Ji,Jiaxiang Ding,Xin Zhang,Yucheng Liao,Weinan E,Weijie Zhang,Han Wen,Hao Chi*

Main category: cs.LG

TL;DR: pUniFind is a multimodal pre-trained model for proteomics, integrating peptide-spectrum scoring and de novo sequencing, outperforming traditional methods with improved sensitivity and modification coverage.


<details>
  <summary>Details</summary>
Motivation: To address the lack of unified scoring frameworks in deep learning for mass spectrometry data interpretation.

Method: pUniFind uses cross-modality prediction on 100M spectra, combining peptide-spectrum scoring and zero-shot de novo sequencing.

Result: Achieves 42.6% more identified peptides in immunopeptidomics and 60% more PSMs than de novo methods.

Conclusion: pUniFind establishes a scalable, sensitive, and interpretable deep learning framework for proteomics.

Abstract: Deep learning has advanced mass spectrometry data interpretation, yet most
models remain feature extractors rather than unified scoring frameworks. We
present pUniFind, the first large-scale multimodal pre-trained model in
proteomics that integrates end-to-end peptide-spectrum scoring with open,
zero-shot de novo sequencing. Trained on over 100 million open search-derived
spectra, pUniFind aligns spectral and peptide modalities via cross modality
prediction and outperforms traditional engines across diverse datasets,
particularly achieving a 42.6 percent increase in the number of identified
peptides in immunopeptidomics. Supporting over 1,300 modifications, pUniFind
identifies 60 percent more PSMs than existing de novo methods despite a
300-fold larger search space. A deep learning based quality control module
further recovers 38.5 percent additional peptides including 1,891 mapped to the
genome but absent from reference proteomes while preserving full fragment ion
coverage. These results establish a unified, scalable deep learning framework
for proteomic analysis, offering improved sensitivity, modification coverage,
and interpretability.

</details>


### [55] [A new machine learning framework for occupational accidents forecasting with safety inspections integration](https://arxiv.org/abs/2507.00089)
*Aho Yapi,Pierre Latouche,Arnaud Guillin,Yan Bailly*

Main category: cs.LG

TL;DR: A framework for short-term occupational accident forecasting using safety inspections and binary time series modeling, with LSTM outperforming other methods (0.86 balanced accuracy).


<details>
  <summary>Details</summary>
Motivation: To improve workplace safety by predicting high-risk periods using routine safety inspection data, enabling proactive interventions.

Method: Uses binary time series modeling, sliding-window cross-validation, and compares machine learning algorithms (logistic regression, tree-based models, neural networks). LSTM is identified as the best performer.

Result: LSTM achieves 0.86 balanced accuracy in detecting high-risk periods, validating the framework's robustness.

Conclusion: The methodology effectively converts inspection data into actionable weekly risk scores, aiding decision-makers in prioritizing safety measures.

Abstract: We propose a generic framework for short-term occupational accident
forecasting that leverages safety inspections and models accident occurrences
as binary time series. The approach generates daily predictions, which are then
aggregated into weekly safety assessments to better inform decision making. To
ensure the reliability and operational applicability of the forecasts, we apply
a sliding-window cross-validation procedure specifically designed for time
series data, combined with an evaluation based on aggregated period-level
metrics. Several machine learning algorithms, including logistic regression,
tree-based models, and neural networks, are trained and systematically compared
within this framework. Unlike the other approaches, the long short-term memory
(LSTM) network outperforms the other approaches and detects the upcoming
high-risk periods with a balanced accuracy of 0.86, confirming the robustness
of our methodology and demonstrating that a binary time series model can
anticipate these critical periods based on safety inspections. The proposed
methodology converts routine safety inspection data into clear weekly risk
scores, detecting the periods when accidents are most likely. Decision-makers
can integrate these scores into their planning tools to classify inspection
priorities, schedule targeted interventions, and funnel resources to the sites
or shifts classified as highest risk, stepping in before incidents occur and
getting the greatest return on safety investments.

</details>


### [56] [Generating Heterogeneous Multi-dimensional Data : A Comparative Study](https://arxiv.org/abs/2507.00090)
*Corbeau Michael,Claeys Emmanuelle,Serrurier Mathieu,Zarat Pascale*

Main category: cs.LG

TL;DR: Comparison of data generation methods for optimizing firefighter resource allocation, evaluating efficacy using domain-specific and standard metrics.


<details>
  <summary>Details</summary>
Motivation: To improve firefighter response optimization by generating synthetic data that accurately represents real-world intervention scenarios.

Method: Comparison of Random Sampling, Tabular Variational Autoencoders, GANs, Conditional Tabular GANs, and Diffusion Probabilistic Models, evaluated with domain-specific and standard metrics.

Result: Evaluation highlights the challenges of capturing complex, unbalanced distributions and nuanced requirements of synthetic data for firefighting scenarios.

Conclusion: Domain-specific metrics are crucial for assessing synthetic data quality in firefighting resource allocation, as traditional metrics may not suffice.

Abstract: Allocation of personnel and material resources is highly sensible in the case
of firefighter interventions. This allocation relies on simulations to
experiment with various scenarios. The main objective of this allocation is the
global optimization of the firefighters response. Data generation is then
mandatory to study various scenarios In this study, we propose to compare
different data generation methods. Methods such as Random Sampling, Tabular
Variational Autoencoders, standard Generative Adversarial Networks, Conditional
Tabular Generative Adversarial Networks and Diffusion Probabilistic Models are
examined to ascertain their efficacy in capturing the intricacies of
firefighter interventions. Traditional evaluation metrics often fall short in
capturing the nuanced requirements of synthetic datasets for real-world
scenarios. To address this gap, an evaluation of synthetic data quality is
conducted using a combination of domain-specific metrics tailored to the
firefighting domain and standard measures such as the Wasserstein distance.
Domain-specific metrics include response time distribution, spatial-temporal
distribution of interventions, and accidents representation. These metrics are
designed to assess data variability, the preservation of fine and complex
correlations and anomalies such as event with a very low occurrence, the
conformity with the initial statistical distribution and the operational
relevance of the synthetic data. The distribution has the particularity of
being highly unbalanced, none of the variables following a Gaussian
distribution, adding complexity to the data generation process.

</details>


### [57] [DFReg: A Physics-Inspired Framework for Global Weight Distribution Regularization in Neural Networks](https://arxiv.org/abs/2507.00101)
*Giovanni Ruggieri*

Main category: cs.LG

TL;DR: DFReg is a physics-inspired regularization method for deep neural networks, leveraging Density Functional Theory to enforce global weight distribution regularity without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Traditional regularization methods like Dropout or L2 decay lack global structural regularity. DFReg aims to address this by drawing inspiration from Density Functional Theory to encourage smooth, diverse, and well-distributed weight configurations.

Method: DFReg applies a functional penalty derived from Density Functional Theory to the global distribution of weights, ensuring structural regularity without stochastic perturbations or architectural modifications.

Result: The method promotes smooth and diverse weight configurations, offering a novel approach to regularization that avoids the limitations of traditional techniques.

Conclusion: DFReg provides a physics-inspired, global regularization solution for deep neural networks, enhancing weight distribution without altering architecture or relying on stochastic methods.

Abstract: We introduce DFReg, a physics-inspired regularization method for deep neural
networks that operates on the global distribution of weights. Drawing from
Density Functional Theory (DFT), DFReg applies a functional penalty to
encourage smooth, diverse, and well-distributed weight configurations. Unlike
traditional techniques such as Dropout or L2 decay, DFReg imposes global
structural regularity without architectural changes or stochastic
perturbations.

</details>


### [58] [Towards transparent and data-driven fault detection in manufacturing: A case study on univariate, discrete time series](https://arxiv.org/abs/2507.00102)
*Bernd Hofmann,Patrick Bruendl,Huong Giang Nguyen,Joerg Franke*

Main category: cs.LG

TL;DR: A data-driven, interpretable fault detection method for industrial quality control combines machine learning, Shapley explanations, and domain-specific visualization, achieving high accuracy and transparency.


<details>
  <summary>Details</summary>
Motivation: Address the lack of adaptability in conventional quality control and the interpretability issues of black-box machine learning models in industrial settings.

Method: Integrates supervised machine learning for fault classification, Shapley Additive Explanations for interpretability, and domain-specific visualization. Evaluated via perturbation analysis and expert assessment.

Result: Achieves 95.9% fault detection accuracy, with confirmed relevance and interpretability of explanations.

Conclusion: The human-centric approach enhances trust in data-driven fault detection, improving industrial quality control.

Abstract: Ensuring consistent product quality in modern manufacturing is crucial,
particularly in safety-critical applications. Conventional quality control
approaches, reliant on manually defined thresholds and features, lack
adaptability to the complexity and variability inherent in production data and
necessitate extensive domain expertise. Conversely, data-driven methods, such
as machine learning, demonstrate high detection performance but typically
function as black-box models, thereby limiting their acceptance in industrial
environments where interpretability is paramount. This paper introduces a
methodology for industrial fault detection, which is both data-driven and
transparent. The approach integrates a supervised machine learning model for
multi-class fault classification, Shapley Additive Explanations for post-hoc
interpretability, and a do-main-specific visualisation technique that maps
model explanations to operator-interpretable features. Furthermore, the study
proposes an evaluation methodology that assesses model explanations through
quantitative perturbation analysis and evaluates visualisations by qualitative
expert assessment. The approach was applied to the crimping process, a
safety-critical joining technique, using a dataset of univariate, discrete time
series. The system achieves a fault detection accuracy of 95.9 %, and both
quantitative selectivity analysis and qualitative expert evaluations confirmed
the relevance and inter-pretability of the generated explanations. This
human-centric approach is designed to enhance trust and interpretability in
data-driven fault detection, thereby contributing to applied system design in
industrial quality control.

</details>


### [59] [Graph Neural Networks in Wind Power Forecasting](https://arxiv.org/abs/2507.00105)
*Javier Castellano,Ignacio Villanueva*

Main category: cs.LG

TL;DR: GNNs perform comparably to CNNs in wind energy forecasting using NWP data over 24-36 hours.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of GNNs in wind energy forecasting compared to traditional CNN-based methods.

Method: Evaluated GNN architectures against CNN benchmarks using five years of historical data from three wind farms, with NWP variables as predictors.

Result: Certain GNN architectures matched the performance of the best CNN-based models.

Conclusion: GNNs are a viable alternative to CNNs for wind energy forecasting, offering comparable accuracy.

Abstract: We study the applicability of GNNs to the problem of wind energy forecasting.
We find that certain architectures achieve performance comparable to our best
CNN-based benchmark. The study is conducted on three wind power facilities
using five years of historical data. Numerical Weather Prediction (NWP)
variables were used as predictors, and models were evaluated on a 24 to 36 hour
ahead test horizon.

</details>


### [60] [Text-to-Level Diffusion Models With Various Text Encoders for Super Mario Bros](https://arxiv.org/abs/2507.00184)
*Jacob Schrum,Olivia Kilday,Emilio Salas,Bess Hagan,Reid Williams*

Main category: cs.LG

TL;DR: The paper explores text-to-level generation in tile-based games using diffusion models, addressing practical challenges like caption/level pairs and playable level generation. It compares results with other models and introduces a GUI for level design.


<details>
  <summary>Details</summary>
Motivation: To advance text-to-level generation in games, addressing gaps in using diffusion models for this purpose and improving usability.

Method: Automatically assigns captions to levels, trains diffusion models with pretrained and simple transformer text encoders, and compares results with other models.

Result: The best diffusion model uses a simple transformer, trains faster, and avoids reliance on large language models. Levels are diverse and playable.

Conclusion: Simple transformers suffice for text embedding in diffusion models, offering efficient training and practical usability, with a GUI for level design.

Abstract: Recent research shows how diffusion models can unconditionally generate
tile-based game levels, but use of diffusion models for text-to-level
generation is underexplored. There are practical considerations for creating a
usable model: caption/level pairs are needed, as is a text embedding model, and
a way of generating entire playable levels, rather than individual scenes. We
present strategies to automatically assign descriptive captions to an existing
level dataset, and train diffusion models using both pretrained text encoders
and simple transformer models trained from scratch. Captions are automatically
assigned to generated levels so that the degree of overlap between input and
output captions can be compared. We also assess the diversity and playability
of the resulting levels. Results are compared with an unconditional diffusion
model and a generative adversarial network, as well as the text-to-level
approaches Five-Dollar Model and MarioGPT. Notably, the best diffusion model
uses a simple transformer model for text embedding, and takes less time to
train than diffusion models employing more complex text encoders, indicating
that reliance on larger language models is not necessary. We also present a GUI
allowing designers to construct long levels from model-generated scenes.

</details>


### [61] [Beyond Sensor Data: Foundation Models of Behavioral Data from Wearables Improve Health Predictions](https://arxiv.org/abs/2507.00191)
*Eray Erturk,Fahad Kamran,Salar Abbaspourazad,Sean Jewell,Harsh Sharma,Yujie Li,Sinead Williamson,Nicholas J Foti,Joseph Futoma*

Main category: cs.LG

TL;DR: A foundation model for behavioral signals from wearable data improves health predictions, outperforming raw sensor data in tasks like sleep prediction.


<details>
  <summary>Details</summary>
Motivation: Behavioral data from wearables is more informative for health predictions than low-level sensor data, but lacks tailored foundation models.

Method: Developed foundation models using 2.5B hours of wearable data from 162K individuals, optimizing architectures and tokenization strategies.

Result: Strong performance on 57 health tasks, especially behavior-driven ones like sleep prediction, with further gains when combined with raw sensor data.

Conclusion: Tailoring foundation models to wearable behavioral data enhances health predictions and enables new applications.

Abstract: Wearable devices record physiological and behavioral signals that can improve
health predictions. While foundation models are increasingly used for such
predictions, they have been primarily applied to low-level sensor data, despite
behavioral data often being more informative due to their alignment with
physiologically relevant timescales and quantities. We develop foundation
models of such behavioral signals using over 2.5B hours of wearable data from
162K individuals, systematically optimizing architectures and tokenization
strategies for this unique dataset. Evaluated on 57 health-related tasks, our
model shows strong performance across diverse real-world applications including
individual-level classification and time-varying health state prediction. The
model excels in behavior-driven tasks like sleep prediction, and improves
further when combined with representations of raw sensor data. These results
underscore the importance of tailoring foundation model design to wearables and
demonstrate the potential to enable new health applications.

</details>


### [62] [What Makes Local Updates Effective: The Role of Data Heterogeneity and Smoothness](https://arxiv.org/abs/2507.00195)
*Kumar Kshitij Patel*

Main category: cs.LG

TL;DR: The thesis explores Local SGD in distributed/federated optimization, proving bounded second-order heterogeneity is key for outperforming centralized methods. It provides tight bounds, a consensus-error framework, and extends to online federated learning.


<details>
  <summary>Details</summary>
Motivation: To understand when and why local update algorithms like Local SGD outperform centralized methods in heterogeneous data settings.

Method: Uses a fine-grained consensus-error analysis framework under third-order smoothness and relaxed heterogeneity assumptions.

Result: Establishes tight bounds for Local SGD, characterizes min-max complexity, and provides regret bounds for online federated learning.

Conclusion: Local updates offer provable advantages under specific heterogeneity conditions, with the thesis serving as a guide for analyzing Local SGD.

Abstract: This thesis contributes to the theoretical understanding of local update
algorithms, especially Local SGD, in distributed and federated optimization
under realistic models of data heterogeneity. A central focus is on the bounded
second-order heterogeneity assumption, which is shown to be both necessary and
sufficient for local updates to outperform centralized or mini-batch methods in
convex and non-convex settings. The thesis establishes tight upper and lower
bounds in several regimes for various local update algorithms and characterizes
the min-max complexity of multiple problem classes. At its core is a
fine-grained consensus-error-based analysis framework that yields sharper
finite-time convergence bounds under third-order smoothness and relaxed
heterogeneity assumptions. The thesis also extends to online federated
learning, providing fundamental regret bounds under both first-order and bandit
feedback. Together, these results clarify when and why local updates offer
provable advantages, and the thesis serves as a self-contained guide for
analyzing Local SGD in heterogeneous environments.

</details>


### [63] [PPFL-RDSN: Privacy-Preserving Federated Learning-based Residual Dense Spatial Networks for Encrypted Lossy Image Reconstruction](https://arxiv.org/abs/2507.00230)
*Peilin He,James Joshi*

Main category: cs.LG

TL;DR: Proposes PPFL-RDSN, a privacy-preserving federated learning framework for high-quality image reconstruction, combining FL, local differential privacy, and model watermarking to ensure security and performance.


<details>
  <summary>Details</summary>
Motivation: Addresses privacy risks (data leakage, inference attacks) and computational costs in centralized training for image reconstruction in collaborative scenarios.

Method: Integrates Federated Learning (FL), local differential privacy, and robust model watermarking to secure data on local devices.

Result: Achieves performance comparable to centralized methods, reduces computational burdens, and mitigates security/privacy vulnerabilities.

Conclusion: PPFL-RDSN is a practical solution for secure, privacy-preserving collaborative computer vision applications.

Abstract: Reconstructing high-quality images from low-resolution inputs using Residual
Dense Spatial Networks (RDSNs) is crucial yet challenging, particularly in
collaborative scenarios where centralized training poses significant privacy
risks, including data leakage and inference attacks, as well as high
computational costs. We propose a novel Privacy-Preserving Federated
Learning-based RDSN (PPFL-RDSN) framework specifically tailored for lossy image
reconstruction. PPFL-RDSN integrates Federated Learning (FL), local
differential privacy, and robust model watermarking techniques, ensuring data
remains secure on local devices, safeguarding sensitive information, and
maintaining model authenticity without revealing underlying data. Empirical
evaluations show that PPFL-RDSN achieves comparable performance to the
state-of-the-art centralized methods while reducing computational burdens, and
effectively mitigates security and privacy vulnerabilities, making it a
practical solution for secure and privacy-preserving collaborative computer
vision applications.

</details>


### [64] [Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations](https://arxiv.org/abs/2507.00234)
*Jiztom Kavalakkatt Francis,Matthew J Darr*

Main category: cs.LG

TL;DR: A hybrid framework combining ResNet and Transformer heatmaps improves interpretability and accuracy in healthcare and industrial monitoring.


<details>
  <summary>Details</summary>
Motivation: Addressing spatial-temporal misalignment in interpretability methods, which limits actionable insights in critical domains.

Method: Integrates gradient-weighted activation maps (ResNet) and Transformer attention rollout for unified visualization.

Result: Achieves 94.1% accuracy on ECG detection and reduces regression error to RMSE = 0.28 kWh, outperforming baselines by 3.8-12.4%.

Conclusion: The framework bridges technical outputs and stakeholder understanding, offering scalable, transparent decision-making.

Abstract: In this paper, we present a novel framework for enhancing model
interpretability by integrating heatmaps produced separately by ResNet and a
restructured 2D Transformer with globally weighted input saliency. We address
the critical problem of spatial-temporal misalignment in existing
interpretability methods, where convolutional networks fail to capture global
context and Transformers lack localized precision - a limitation that impedes
actionable insights in safety-critical domains like healthcare and industrial
monitoring. Our method merges gradient-weighted activation maps (ResNet) and
Transformer attention rollout into a unified visualization, achieving full
spatial-temporal alignment while preserving real-time performance. Empirical
evaluations on clinical (ECG arrhythmia detection) and industrial (energy
consumption prediction) datasets demonstrate significant improvements: the
hybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and
reduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy
Appliance dataset-outperforming standalone ResNet, Transformer, and
InceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps
into domain-specific narratives (e.g., "Elevated ST-segment between 2-4 seconds
suggests myocardial ischemia"), validated via BLEU-4 (0.586) and ROUGE-L
(0.650) scores. By formalizing interpretability as causal fidelity and
spatial-temporal alignment, our approach bridges the gap between technical
outputs and stakeholder understanding, offering a scalable solution for
transparent, time-aware decision-making.

</details>


### [65] [Gym4ReaL: A Suite for Benchmarking Real-World Reinforcement Learning](https://arxiv.org/abs/2507.00257)
*Davide Salaorni,Vincenzo De Paola,Samuele Delpero,Giovanni Dispoto,Paolo Bonetti,Alessio Russo,Giuseppe Calcagno,Francesco Trov,Matteo Papini,Alberto Maria Metelli,Marco Mussi,Marcello Restelli*

Main category: cs.LG

TL;DR: The paper introduces Gym4ReaL, a suite of realistic environments for evaluating RL algorithms in real-world scenarios, highlighting challenges like large state-action spaces and non-stationarity.


<details>
  <summary>Details</summary>
Motivation: Current RL benchmarks focus on idealized environments, neglecting real-world complexities, which limits the applicability of RL in practical settings.

Method: The authors develop Gym4ReaL, a diverse set of tasks that simulate real-world challenges for RL algorithms.

Result: Standard RL algorithms perform competitively against rule-based benchmarks in these realistic settings.

Conclusion: The findings motivate the development of new RL methods to better address real-world complexities.

Abstract: In recent years, \emph{Reinforcement Learning} (RL) has made remarkable
progress, achieving superhuman performance in a wide range of simulated
environments. As research moves toward deploying RL in real-world applications,
the field faces a new set of challenges inherent to real-world settings, such
as large state-action spaces, non-stationarity, and partial observability.
Despite their importance, these challenges are often underexplored in current
benchmarks, which tend to focus on idealized, fully observable, and stationary
environments, often neglecting to incorporate real-world complexities
explicitly. In this paper, we introduce \texttt{Gym4ReaL}, a comprehensive
suite of realistic environments designed to support the development and
evaluation of RL algorithms that can operate in real-world scenarios. The suite
includes a diverse set of tasks that expose algorithms to a variety of
practical challenges. Our experimental results show that, in these settings,
standard RL algorithms confirm their competitiveness against rule-based
benchmarks, motivating the development of new methods to fully exploit the
potential of RL to tackle the complexities of real-world tasks.

</details>


### [66] [Who Should I Listen To? Adaptive Collaboration in Personalized Federated Learning](https://arxiv.org/abs/2507.00259)
*Amr Abourayya,Jens Kleesiek,Bharat Rao,Michael Kamp*

Main category: cs.LG

TL;DR: FEDMOSAIC introduces adaptive collaboration in federated learning, allowing clients to decide whom to trust at the example level, improving performance over state-of-the-art PFL methods.


<details>
  <summary>Details</summary>
Motivation: Addressing data heterogeneity in federated learning, where many PFL methods fail to outperform local or centralized baselines due to mismatched collaboration.

Method: FEDMOSAIC uses federated co-training, where clients exchange predictions on shared unlabeled data, enabling fine-grained trust decisions and adaptive loss weighting.

Result: Empirically outperforms state-of-the-art PFL methods in diverse non-IID settings, with convergence guarantees under standard assumptions.

Conclusion: Data-aware collaboration, as in FEDMOSAIC, enhances robustness and effectiveness in personalized federated learning.

Abstract: Data heterogeneity is a central challenge in federated learning, and
personalized federated learning (PFL) aims to address it by tailoring models to
each client's distribution. Yet many PFL methods fail to outperform local or
centralized baselines, suggesting a mismatch between the collaboration they
enforce and the structure of the data. We propose an approach based on adaptive
collaboration, where clients decide adaptively not only how much to rely on
others, but also whom to trust at the level of individual examples. We
instantiate this principle in FEDMOSAIC, a federated co-training method in
which clients exchange predictions over a shared unlabeled dataset. This
enables fine-grained trust decisions that are difficult to achieve with
parameter sharing alone. Each client adjusts its loss weighting based on the
agreement between private and public data, and contributes to global
pseudo-labels in proportion to its estimated per-example confidence.
Empirically, FEDMOSAIC improves upon state-of-the-art PFL methods across
diverse non-IID settings, and we provide convergence guarantees under standard
assumptions. Our results demonstrate the potential of data-aware collaboration
for robust and effective personalization.

</details>


### [67] [Examining Reject Relations in Stimulus Equivalence Simulations](https://arxiv.org/abs/2507.00265)
*Alexis Carrillo,Asieh Abolpour Mofrad,Anis Yazidi,Moises Betancort*

Main category: cs.LG

TL;DR: The study explores how reject relations affect stimulus equivalence (SE) in computational models, comparing neural networks (FFNs, BERT, GPT) to a probabilistic agent. Results suggest networks may rely on associative learning, not SE.


<details>
  <summary>Details</summary>
Motivation: To investigate the contentious role of reject relations in SE and assess whether neural networks truly form equivalence classes or mimic associative learning.

Method: Used FFNs, BERT, and GPT in 18 MTS simulation conditions, varying training structures, relation types, and negative comparison selections. A probabilistic agent served as a benchmark.

Result: Reject relations influenced performance. High accuracy in some cases mirrored the probabilistic agent, suggesting reliance on associative strategies, not SE.

Conclusion: Neural networks may not demonstrate true SE, highlighting the need for stricter criteria and careful handling of reject relations in computational models.

Abstract: Simulations offer a valuable tool for exploring stimulus equivalence (SE),
yet the potential of reject relations to disrupt the assessment of equivalence
class formation is contentious. This study investigates the role of reject
relations in the acquisition of stimulus equivalence using computational
models. We examined feedforward neural networks (FFNs), bidirectional encoder
representations from transformers (BERT), and generative pre-trained
transformers (GPT) across 18 conditions in matching-to-sample (MTS)
simulations. Conditions varied in training structure (linear series,
one-to-many, and many-to-one), relation type (select-only, reject-only, and
select-reject), and negative comparison selection (standard and biased). A
probabilistic agent served as a benchmark, embodying purely associative
learning. The primary goal was to determine whether artificial neural networks
could demonstrate equivalence class formation or whether their performance
reflected associative learning. Results showed that reject relations influenced
agent performance. While some agents achieved high accuracy on equivalence
tests, particularly with reject relations and biased negative comparisons, this
performance was comparable to the probabilistic agent. These findings suggest
that artificial neural networks, including transformer models, may rely on
associative strategies rather than SE. This underscores the need for careful
consideration of reject relations and more stringent criteria in computational
models of equivalence.

</details>


### [68] [Double Q-learning for Value-based Deep Reinforcement Learning, Revisited](https://arxiv.org/abs/2507.00275)
*Prabhat Nagarajan,Martha White,Marlos C. Machado*

Main category: cs.LG

TL;DR: The paper introduces Deep Double Q-learning (DDQL) to address overestimation in deep RL, showing it outperforms Double DQN without extra hyperparameters.


<details>
  <summary>Details</summary>
Motivation: Overestimation in Q-learning and its deep variants (e.g., Double DQN) motivates the need for better algorithms like DDQL.

Method: DDQL adapts Double Q-learning's core idea of using two Q-functions to de-correlate action-selection and evaluation in deep RL.

Result: DDQL reduces overestimation and outperforms Double DQN across 57 Atari 2600 games.

Conclusion: DDQL is a performant and effective solution for overestimation in deep RL.

Abstract: Overestimation is pervasive in reinforcement learning (RL), including in
Q-learning, which forms the algorithmic basis for many value-based deep RL
algorithms. Double Q-learning is an algorithm introduced to address
Q-learning's overestimation by training two Q-functions and using both to
de-correlate action-selection and action-evaluation in bootstrap targets.
Shortly after Q-learning was adapted to deep RL in the form of deep Q-networks
(DQN), Double Q-learning was adapted to deep RL in the form of Double DQN.
However, Double DQN only loosely adapts Double Q-learning, forgoing the
training of two different Q-functions that bootstrap off one another. In this
paper, we study algorithms that adapt this core idea of Double Q-learning for
value-based deep RL. We term such algorithms Deep Double Q-learning (DDQL). Our
aim is to understand whether DDQL exhibits less overestimation than Double DQN
and whether performant instantiations of DDQL exist. We answer both questions
affirmatively, demonstrating that DDQL reduces overestimation and outperforms
Double DQN in aggregate across 57 Atari 2600 games, without requiring
additional hyperparameters. We also study several aspects of DDQL, including
its network architecture, replay ratio, and minibatch sampling strategy.

</details>


### [69] [Structure-preserving Lift & Learn: Scientific machine learning for nonlinear conservative partial differential equations](https://arxiv.org/abs/2507.00301)
*Harsh Sharma,Juan Diego Draxl Giannoni,Boris Kramer*

Main category: cs.LG

TL;DR: Structure-preserving Lift & Learn method for nonlinear PDEs with conservation laws, using energy-quadratization to derive efficient, physics-respecting reduced-order models.


<details>
  <summary>Details</summary>
Motivation: To develop a method for learning reduced-order models of nonlinear PDEs that preserves underlying physical structures, ensuring accuracy and computational efficiency.

Method: Hybrid learning approach combining energy-quadratization to lift nonlinear PDEs into quadratic forms, then analytically deriving reduced terms and learning linear operators via constrained optimization.

Result: Demonstrated effectiveness through numerical examples (wave, sine-Gordon, Klein-Gordon-Zakharov equations), showing competitive accuracy and efficiency with state-of-the-art methods.

Conclusion: The proposed Lift & Learn method successfully preserves physical structures in reduced-order models, offering a generalizable and efficient solution for nonlinear PDEs.

Abstract: This work presents structure-preserving Lift & Learn, a scientific machine
learning method that employs lifting variable transformations to learn
structure-preserving reduced-order models for nonlinear partial differential
equations (PDEs) with conservation laws. We propose a hybrid learning approach
based on a recently developed energy-quadratization strategy that uses
knowledge of the nonlinearity at the PDE level to derive an equivalent
quadratic lifted system with quadratic system energy. The lifted dynamics
obtained via energy quadratization are linear in the old variables, making
model learning very effective in the lifted setting. Based on the lifted
quadratic PDE model form, the proposed method derives quadratic reduced terms
analytically and then uses those derived terms to formulate a constrained
optimization problem to learn the remaining linear reduced operators in a
structure-preserving way. The proposed hybrid learning approach yields
computationally efficient quadratic reduced-order models that respect the
underlying physics of the high-dimensional problem. We demonstrate the
generalizability of quadratic models learned via the proposed
structure-preserving Lift & Learn method through three numerical examples: the
one-dimensional wave equation with exponential nonlinearity, the
two-dimensional sine-Gordon equation, and the two-dimensional
Klein-Gordon-Zakharov equations. The numerical results show that the proposed
learning approach is competitive with the state-of-the-art structure-preserving
data-driven model reduction method in terms of both accuracy and computational
efficiency.

</details>


### [70] [MamNet: A Novel Hybrid Model for Time-Series Forecasting and Frequency Pattern Analysis in Network Traffic](https://arxiv.org/abs/2507.00304)
*Yujun Zhang,Runlong Li,Xiaoxiang Liang,Xinhao Yang,Tian Su,Bo Liu,Yan Zhou*

Main category: cs.LG

TL;DR: MamNet, a novel model for network traffic prediction and anomaly detection, combines time-domain and frequency-domain analysis, outperforming recent models by 2-4% in accuracy, recall, and F1-Score.


<details>
  <summary>Details</summary>
Motivation: Abnormal network traffic fluctuations can signal security threats or system failures, necessitating efficient prediction and anomaly detection methods.

Method: MamNet integrates Mamba module (time-domain modeling) and Fourier Transform (frequency-domain feature extraction) with multi-scale feature fusion.

Result: Experiments on UNSW-NB15 and CAIDA datasets show MamNet's superior performance in detecting complex traffic patterns and long-term trends.

Conclusion: MamNet effectively detects anomalies across time scales, with potential for further optimization by incorporating external network event information.

Abstract: The abnormal fluctuations in network traffic may indicate potential security
threats or system failures. Therefore, efficient network traffic prediction and
anomaly detection methods are crucial for network security and traffic
management. This paper proposes a novel network traffic prediction and anomaly
detection model, MamNet, which integrates time-domain modeling and
frequency-domain feature extraction. The model first captures the long-term
dependencies of network traffic through the Mamba module (time-domain
modeling), and then identifies periodic fluctuations in the traffic using
Fourier Transform (frequency-domain feature extraction). In the feature fusion
layer, multi-scale information is integrated to enhance the model's ability to
detect network traffic anomalies. Experiments conducted on the UNSW-NB15 and
CAIDA datasets demonstrate that MamNet outperforms several recent mainstream
models in terms of accuracy, recall, and F1-Score. Specifically, it achieves an
improvement of approximately 2% to 4% in detection performance for complex
traffic patterns and long-term trend detection. The results indicate that
MamNet effectively captures anomalies in network traffic across different time
scales and is suitable for anomaly detection tasks in network security and
traffic management. Future work could further optimize the model structure by
incorporating external network event information, thereby improving the model's
adaptability and stability in complex network environments.

</details>


### [71] [Open-ended Scientific Discovery via Bayesian Surprise](https://arxiv.org/abs/2507.00310)
*Dhruv Agarwal,Bodhisattwa Prasad Majumder,Reece Adamson,Megha Chakravorty,Satvika Reddy Gavireddy,Aditya Parashar,Harshit Surana,Bhavana Dalvi Mishra,Andrew McCallum,Ashish Sabharwal,Peter Clark*

Main category: cs.LG

TL;DR: AutoDS introduces a method for open-ended autonomous scientific discovery using Bayesian surprise and Monte Carlo tree search, outperforming competitors by generating more surprising discoveries.


<details>
  <summary>Details</summary>
Motivation: Scientific discovery can be accelerated by allowing AI systems to autonomously explore hypotheses without relying on human-specified questions, addressing limitations of existing methods.

Method: AutoDS uses Bayesian surprise to quantify epistemic shifts and employs Monte Carlo tree search with progressive widening to explore nested hypotheses efficiently.

Result: AutoDS outperforms competitors, producing 5--29% more surprising discoveries under a fixed budget, with two-thirds deemed surprising by domain experts.

Conclusion: AutoDS represents a significant advancement towards open-ended autonomous scientific discovery by effectively navigating hypothesis spaces and generating meaningful results.

Abstract: The promise of autonomous scientific discovery (ASD) hinges not only on
answering questions, but also on knowing which questions to ask. Most recent
works in ASD explore the use of large language models (LLMs) in goal-driven
settings, relying on human-specified research questions to guide hypothesis
generation. However, scientific discovery may be accelerated further by
allowing the AI system to drive exploration by its own criteria. The few
existing approaches in open-ended ASD select hypotheses based on diversity
heuristics or subjective proxies for human interestingness, but the former
struggles to meaningfully navigate the typically vast hypothesis space, and the
latter suffers from imprecise definitions. This paper presents AutoDS -- a
method for open-ended ASD that instead drives scientific exploration using
Bayesian surprise. Here, we quantify the epistemic shift from the LLM's prior
beliefs about a hypothesis to its posterior beliefs after gathering
experimental results. To efficiently explore the space of nested hypotheses,
our method employs a Monte Carlo tree search (MCTS) strategy with progressive
widening using surprisal as the reward function. We evaluate AutoDS in the
setting of data-driven discovery across 21 real-world datasets spanning domains
such as biology, economics, finance, and behavioral science. Our results
demonstrate that under a fixed budget, AutoDS substantially outperforms
competitors by producing 5--29\% more discoveries deemed surprising by the LLM.
Our human evaluation further finds that two-thirds of AutoDS discoveries are
surprising to the domain experts, suggesting this is an important step forward
towards building open-ended ASD systems.

</details>


### [72] [$^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation](https://arxiv.org/abs/2507.00316)
*Siyou Li,Pengyao Qin,Huanan Wu,Dong Nie,Arun J. Thirunavukarasu,Juntao Yu,Le Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Automated radiology report generation (RRG) aims to produce detailed textual
reports from clinical imaging, such as computed tomography (CT) scans, to
improve the accuracy and efficiency of diagnosis and provision of management
advice. RRG is complicated by two key challenges: (1) inherent complexity in
extracting relevant information from imaging data under resource constraints,
and (2) difficulty in objectively evaluating discrepancies between
model-generated and expert-written reports. To address these challenges, we
propose $\mu^2$LLM, a $\underline{\textbf{mu}}$ltiscale
$\underline{\textbf{mu}}$ltimodal large language models for RRG tasks. The
novel ${\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal
features from the multiscale visual tokenizer and the text tokenizer, then
enhances report generation quality through direct preference optimization
(DPO), guided by GREEN-RedLlama. Experimental results on four large CT
image-report medical datasetdemonstrate that our method outperforms existing
approaches, highlighting the potential of our fine-tuned $\mu^2$LLMs on limited
data for RRG tasks.

</details>


### [73] [Exploring Theory-Laden Observations in the Brain Basis of Emotional Experience](https://arxiv.org/abs/2507.00320)
*Christiana Westlin,Ashutosh Singh,Deniz Erdogmus,Georgios Stratis,Lisa Feldman Barrett*

Main category: cs.LG

TL;DR: The paper challenges the assumption that emotion categories are biologically fixed, showing significant variation in brain patterns when analyzed without typological assumptions.


<details>
  <summary>Details</summary>
Motivation: To question the widely held assumption that emotion categories are biologically and psychologically fixed, and to demonstrate how starting assumptions influence scientific conclusions.

Method: Reanalyzed data from a typologically-guided study using an alternative approach that treats emotion categories as variable populations, with minimal assumptions about data structure.

Result: Found significant variation in brain patterns across individuals, contradicting the original study's mappings and supporting the alternative view.

Conclusion: Scientific conclusions are influenced by starting assumptions, and hypotheses should be tested with multiple methods before acceptance.

Abstract: In the science of emotion, it is widely assumed that folk emotion categories
form a biological and psychological typology, and studies are routinely
designed and analyzed to identify emotion-specific patterns. This approach
shapes the observations that studies report, ultimately reinforcing the
assumption that guided the investigation. Here, we reanalyzed data from one
such typologically-guided study that reported mappings between individual brain
patterns and group-averaged ratings of 34 emotion categories. Our reanalysis
was guided by an alternative view of emotion categories as populations of
variable, situated instances, and which predicts a priori that there will be
significant variation in brain patterns within a category across instances.
Correspondingly, our analysis made minimal assumptions about the structure of
the variance present in the data. As predicted, we did not observe the original
mappings and instead observed significant variation across individuals. These
findings demonstrate how starting assumptions can ultimately impact scientific
conclusions and suggest that a hypothesis must be supported using multiple
analytic methods before it is taken seriously.

</details>


### [74] [Data-Driven Exploration for a Class of Continuous-Time Linear--Quadratic Reinforcement Learning Problems](https://arxiv.org/abs/2507.00358)
*Yilie Huang,Xun Yu Zhou*

Main category: cs.LG

TL;DR: The paper introduces an adaptive exploration mechanism for RL in continuous-time stochastic LQ control problems, improving efficiency and matching sublinear regret bounds without extensive tuning.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and tuning challenges of constant or deterministic exploration schedules in RL for LQ control problems.

Method: A model-free, data-driven approach with adaptive entropy regularization (critic) and policy variance (actor) adjustments.

Result: Achieves sublinear regret matching best-known model-free results, with numerical experiments showing faster convergence and better regret performance.

Conclusion: Adaptive exploration enhances learning efficiency and performance in LQ control RL, outperforming non-adaptive and model-based methods.

Abstract: We study reinforcement learning (RL) for the same class of continuous-time
stochastic linear--quadratic (LQ) control problems as in
\cite{huang2024sublinear}, where volatilities depend on both states and
controls while states are scalar-valued and running control rewards are absent.
We propose a model-free, data-driven exploration mechanism that adaptively
adjusts entropy regularization by the critic and policy variance by the actor.
Unlike the constant or deterministic exploration schedules employed in
\cite{huang2024sublinear}, which require extensive tuning for implementations
and ignore learning progresses during iterations, our adaptive exploratory
approach boosts learning efficiency with minimal tuning. Despite its
flexibility, our method achieves a sublinear regret bound that matches the
best-known model-free results for this class of LQ problems, which were
previously derived only with fixed exploration schedules. Numerical experiments
demonstrate that adaptive explorations accelerate convergence and improve
regret performance compared to the non-adaptive model-free and model-based
counterparts.

</details>


### [75] [MoNE: Replacing Redundant Experts with Lightweight Novices for Structured Pruning of MoE](https://arxiv.org/abs/2507.00390)
*Geng Zhang,Yuxuan Han,Yuxuan Lou,Wangbo Zhao,Yiqi Zhang,Yang You*

Main category: cs.LG

TL;DR: MoNE is a novel expert pruning method that replaces redundant experts with lightweight novices, reducing memory overhead while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Deploying MoE-based models incurs high memory costs due to retaining all experts, and existing pruning methods show suboptimal performance.

Method: MoNE evaluates expert redundancy using access frequency and output variance, replacing low-usage experts with lightweight novices.

Result: MoNE outperforms baselines with minimal accuracy loss, improving zero-shot accuracy by up to 3.61 under 50% pruning.

Conclusion: MoNE is effective and robust for compressing MoE models, balancing memory efficiency and performance.

Abstract: Mixture-of-Experts (MoE) enables efficient scaling of large language models
by activating only a subset of experts per input token. However, deploying
MoE-based models incurs significant memory overhead due to the need to retain
all experts in memory. While structured pruning is promising to reduce memory
costs, existing methods often show suboptimal performance and unstable
degradation in three dimensions: model architectures, calibration data sources,
and calibration sample sizes. This paper proposes
Mixture-of-Novices-and-Experts (MoNE), a novel expert pruning method that
replaces redundant experts with lightweight novices to achieve effective and
robust model compression. MoNE evaluates expert redundancy based on two
metrics: access frequency and output variance. Experts exhibiting low usage and
stable outputs are pruned and replaced with lightweight novices-unbiased
estimations of their original outputs-minimizing performance degradation.
Extensive experiments demonstrate that MoNE consistently outperforms baseline
methods with minimal accuracy degradation across the three dimensions,
confirming its effectiveness and robustness. Notably, it improves the average
zero shot accuracy across nine downstream tasks by up to 2.71 under 25\%
pruning ratio and 3.61 under 50\% pruning. The code is available at
https://github.com/zxgx/mode-pd.

</details>


### [76] [HelixPipe: Efficient Distributed Training of Long Sequence Transformers with Attention Parallel Pipeline Parallelism](https://arxiv.org/abs/2507.00394)
*Geng Zhang,Shenggan Cheng,Xuanlei Zhao,Ziming Liu,Yang You*

Main category: cs.LG

TL;DR: HelixPipe is a pipeline parallelism method for long-sequence transformer training, optimizing attention computation and memory usage, outperforming baselines in throughput and scalability.


<details>
  <summary>Details</summary>
Motivation: Existing pipeline parallelisms perform poorly with long transformer sequences due to quadratic attention computation and high memory overhead.

Method: HelixPipe introduces attention parallel partition, a two-fold micro batch schedule, and recomputation without attention and chunked MLP to optimize performance.

Result: HelixPipe achieves a 26% speedup over baselines for a 7B model with 128k sequence length on 64 GPUs, showing better scalability.

Conclusion: HelixPipe effectively addresses long-sequence training challenges, offering superior performance and scalability.

Abstract: As transformer sequence lengths grow, existing pipeline parallelisms incur
suboptimal performance due to the quadratic attention computation and the
substantial memory overhead. To relieve these challenges, we propose HelixPipe,
a novel pipeline parallelism for long sequence transformer training. First,
HelixPipe introduces attention parallel partition, which schedules attention
computations of different micro batches across different pipeline stages in
parallel, reducing pipeline bubbles. Second, it employs a two-fold
first-in-last-out micro batch schedule to balance memory usage and overlap
communication with computation. Additionally, HelixPipe utilizes recomputation
without attention and chunked MLP to mitigate fragmentation and enable longer
sequences. Experiments demonstrate that HelixPipe gains increasing advantages
with longer sequence lengths, and outperforms existing methods in throughput
and scalability across varying pipeline sizes, model sizes, and cluster
configurations. Notably, it achieves a 26\% speedup over baseline methods when
training a 7B model with 128k sequence length on 64 H20 GPUs. Code is available
at https://github.com/code-tunnel/Megatron-LM/tree/dev.

</details>


### [77] [Diffusion Disambiguation Models for Partial Label Learning](https://arxiv.org/abs/2507.00411)
*Jinfu Fan,Xiaohui Zhong,Kangrui Ren,Jiangnan Li,Linqing Huang*

Main category: cs.LG

TL;DR: The paper proposes DDMP, a diffusion disambiguation model for partial label learning (PLL), leveraging diffusion models to denoise ambiguous labels and improve classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Ambiguous labels in PLL hinder accurate classification. The paper aims to address this by using diffusion models to refine labels iteratively.

Method: DDMP constructs pseudo-clean labels for initial diffusion training and uses a transition-aware matrix to estimate ground-truth labels, dynamically updating them during diffusion.

Result: Experiments demonstrate DDMP's effectiveness in improving label disambiguation and classification performance in PLL.

Conclusion: DDMP successfully leverages diffusion models for PLL, enhancing label disambiguation and classifier accuracy.

Abstract: Learning from ambiguous labels is a long-standing problem in practical
machine learning applications. The purpose of \emph{partial label learning}
(PLL) is to identify the ground-truth label from a set of candidate labels
associated with a given instance. Inspired by the remarkable performance of
diffusion models in various generation tasks, this paper explores their
potential to denoise ambiguous labels through the reverse denoising process.
Therefore, this paper reformulates the label disambiguation problem from the
perspective of generative models, where labels are generated by iteratively
refining initial random guesses. This perspective enables the diffusion model
to learn how label information is generated stochastically. By modeling the
generation uncertainty, we can use the maximum likelihood estimate of the label
for classification inference. However, such ambiguous labels lead to a mismatch
between instance and label, which reduces the quality of generated data. To
address this issue, this paper proposes a \emph{diffusion disambiguation model
for PLL} (DDMP), which first uses the potential complementary information
between instances and labels to construct pseudo-clean labels for initial
diffusion training. Furthermore, a transition-aware matrix is introduced to
estimate the potential ground-truth labels, which are dynamically updated
during the diffusion generation. During training, the ground-truth label is
progressively refined, improving the classifier. Experiments show the advantage
of the DDMP and its suitability for PLL.

</details>


### [78] [Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows](https://arxiv.org/abs/2507.00425)
*Ruixiang Zhang,Shuangfei Zhai,Jiatao Gu,Yizhe Zhang,Huangjie Zheng,Tianrong Chen,Miguel Angel Bautista,Josh Susskind,Navdeep Jaitly*

Main category: cs.LG

TL;DR: TarFlowLM introduces a continuous latent space for language modeling using autoregressive normalizing flows, offering flexibility in context, generation, and dependencies.


<details>
  <summary>Details</summary>
Motivation: To explore new modeling flexibility beyond discrete tokens and unidirectional context in autoregressive models.

Method: Proposes TarFlowLM, a transformer-based autoregressive normalizing flow framework for continuous latent space modeling, with novel mixture-based coupling transformations.

Result: Strong likelihood performance on benchmarks and flexible modeling capabilities demonstrated.

Conclusion: TarFlowLM provides a promising alternative to discrete autoregressive models with enhanced flexibility and performance.

Abstract: Autoregressive models have driven remarkable progress in language modeling.
Their foundational reliance on discrete tokens, unidirectional context, and
single-pass decoding, while central to their success, also inspires the
exploration of a design space that could offer new axes of modeling
flexibility. In this work, we explore an alternative paradigm, shifting
language modeling from a discrete token space to a continuous latent space. We
propose a novel framework TarFlowLM, that employs transformer-based
autoregressive normalizing flows to model these continuous representations.
This approach unlocks substantial flexibility, enabling the construction of
models that can capture global bi-directional context through stacked,
alternating-direction autoregressive transformations, support block-wise
generation with flexible token patch sizes, and facilitate a hierarchical
multi-pass generation process. We further propose new mixture-based coupling
transformations designed to capture complex dependencies within the latent
space shaped by discrete data, and demonstrate theoretical connections to
conventional discrete autoregressive models. Extensive experiments on language
modeling benchmarks demonstrate strong likelihood performance and highlight the
flexible modeling capabilities inherent in our framework.

</details>


### [79] [A Recipe for Causal Graph Regression: Confounding Effects Revisited](https://arxiv.org/abs/2507.00440)
*Yujia Yin,Tianyi Qu,Zihao Wang,Yifan Chen*

Main category: cs.LG

TL;DR: The paper introduces causal graph regression (CGR) to address the overlooked challenge of regression tasks in causal graph learning, adapting causal intervention techniques for regression using contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Regression tasks in graph learning are more challenging and underexplored compared to classification, despite the success of causal graph learning (CGL) in improving generalizability under OOD scenarios.

Method: The authors reshape the handling of confounding effects in CGL for classification, generalize causal intervention techniques to regression via contrastive learning, and validate their approach on graph OOD benchmarks.

Result: Extensive experiments demonstrate the efficacy of the proposed CGR method.

Conclusion: The work successfully extends causal graph learning to regression tasks, providing a validated approach and open-source implementation.

Abstract: Through recognizing causal subgraphs, causal graph learning (CGL) has risen
to be a promising approach for improving the generalizability of graph neural
networks under out-of-distribution (OOD) scenarios. However, the empirical
successes of CGL techniques are mostly exemplified in classification settings,
while regression tasks, a more challenging setting in graph learning, are
overlooked. We thus devote this work to tackling causal graph regression (CGR);
to this end we reshape the processing of confounding effects in existing CGL
studies, which mainly deal with classification. Specifically, we reflect on the
predictive power of confounders in graph-level regression, and generalize
classification-specific causal intervention techniques to regression through a
lens of contrastive learning. Extensive experiments on graph OOD benchmarks
validate the efficacy of our proposals for CGR. The model implementation and
the code are provided on https://github.com/causal-graph/CGR.

</details>


### [80] [Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention](https://arxiv.org/abs/2507.00449)
*Zhihao Zhan,Jianan Zhao,Zhaocheng Zhu,Jian Tang*

Main category: cs.LG

TL;DR: The paper addresses the challenge of long-context modeling in NLP by improving state-space models (SSMs) with a novel synthetic task (joint recall) and a hybrid solution (HAX) combining SSMs with sparse attention.


<details>
  <summary>Details</summary>
Motivation: Efficient long-context modeling is critical but challenging due to the quadratic complexity of Transformers and SSMs' limitations in capturing long-range dependencies.

Method: Introduces joint recall as a more complex synthetic task, proves SSMs' limitations, and proposes HAX, integrating SSMs with context-dependent sparse attention.

Result: HAX outperforms SSM baselines and SSMs with context-independent sparse attention on synthetic and real-world benchmarks.

Conclusion: The hybrid approach (HAX) effectively bridges theoretical and practical gaps, enhancing long-context modeling capabilities.

Abstract: Efficient long-context modeling remains a critical challenge for natural
language processing (NLP), as the time complexity of the predominant
Transformer architecture scales quadratically with the sequence length. While
state-space models (SSMs) offer alternative sub-quadratic solutions, they
struggle to capture long-range dependencies effectively. In this work, we focus
on analyzing and improving the long-context modeling capabilities of SSMs. We
show that the widely used synthetic task, associative recall, which requires a
model to recall a value associated with a single key without context,
insufficiently represents the complexities of real-world long-context modeling.
To address this limitation, we extend the associative recall to a novel
synthetic task, \emph{joint recall}, which requires a model to recall the value
associated with a key given in a specified context. Theoretically, we prove
that SSMs do not have the expressiveness to solve multi-query joint recall in
sub-quadratic time complexity. To resolve this issue, we propose a solution
based on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which
has the expressiveness to solve multi-query joint recall with sub-quadratic
computation. To bridge the gap between theoretical analysis and real-world
applications, we propose locality-sensitive Hashing Attention with sparse Key
Selection (HAX), which instantiates the theoretical solution and is further
tailored to natural language domains. Extensive experiments on both synthetic
and real-world long-context benchmarks show that HAX consistently outperforms
SSM baselines and SSMs integrated with context-independent sparse attention
(CISA).

</details>


### [81] [Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design](https://arxiv.org/abs/2507.00445)
*Xingyu Su,Xiner Li,Masatoshi Uehara,Sunwoo Kim,Yulai Zhao,Gabriele Scalia,Ehsan Hajiramezanali,Tommaso Biancalani,Degui Zhi,Shuiwang Ji*

Main category: cs.LG

TL;DR: A framework for fine-tuning diffusion models in biomolecular design using iterative distillation to optimize for non-differentiable rewards, improving stability and efficiency over RL methods.


<details>
  <summary>Details</summary>
Motivation: Real-world applications require diffusion models to optimize for non-differentiable rewards (e.g., physics-based simulations), but RL methods face instability and inefficiency.

Method: Proposes an iterative distillation-based fine-tuning framework: collects off-policy data, simulates reward-based soft-optimal policies, and updates the model via KL divergence minimization.

Result: Empirical results show superior reward optimization in protein, small molecule, and regulatory DNA design tasks.

Conclusion: The method enhances training stability and sample efficiency, outperforming existing RL-based approaches.

Abstract: We address the problem of fine-tuning diffusion models for reward-guided
generation in biomolecular design. While diffusion models have proven highly
effective in modeling complex, high-dimensional data distributions, real-world
applications often demand more than high-fidelity generation, requiring
optimization with respect to potentially non-differentiable reward functions
such as physics-based simulation or rewards based on scientific knowledge.
Although RL methods have been explored to fine-tune diffusion models for such
objectives, they often suffer from instability, low sample efficiency, and mode
collapse due to their on-policy nature. In this work, we propose an iterative
distillation-based fine-tuning framework that enables diffusion models to
optimize for arbitrary reward functions. Our method casts the problem as policy
distillation: it collects off-policy data during the roll-in phase, simulates
reward-based soft-optimal policies during roll-out, and updates the model by
minimizing the KL divergence between the simulated soft-optimal policy and the
current model policy. Our off-policy formulation, combined with KL divergence
minimization, enhances training stability and sample efficiency compared to
existing RL-based methods. Empirical results demonstrate the effectiveness and
superior reward optimization of our approach across diverse tasks in protein,
small molecule, and regulatory DNA design.

</details>


### [82] [Best Agent Identification for General Game Playing](https://arxiv.org/abs/2507.00451)
*Matthew Stephenson,Alex Newcombe,Eric Piette,Dennis Soemers*

Main category: cs.LG

TL;DR: The paper introduces Optimistic-WS, a method for identifying the best-performing algorithm in multi-task domains using multi-armed bandits, showing improved performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: To efficiently and accurately identify the best algorithm for each sub-task in multi-problem domains, particularly in general game frameworks like GVGAI and Ludii.

Method: Treats the problem as best arm identification in multi-armed bandits, using an optimistic selection process based on the Wilson score interval (Optimistic-WS) to rank arms by potential regret reduction.

Result: Demonstrates substantial improvement in average simple regret compared to previous best arm identification algorithms.

Conclusion: Optimistic-WS enhances agent evaluation quality in general game frameworks and other high-runtime multi-task domains.

Abstract: We present an efficient and generalised procedure to accurately identify the
best performing algorithm for each sub-task in a multi-problem domain. Our
approach treats this as a set of best arm identification problems for
multi-armed bandits, where each bandit corresponds to a specific task and each
arm corresponds to a specific algorithm or agent. We propose an optimistic
selection process based on the Wilson score interval (Optimistic-WS) that ranks
each arm across all bandits in terms of their potential regret reduction. We
evaluate the performance of Optimistic-WS on two of the most popular general
game domains, the General Video Game AI (GVGAI) framework and the Ludii general
game playing system, with the goal of identifying the highest performing agent
for each game within a limited number of trials. Compared to previous best arm
identification algorithms for multi-armed bandits, our results demonstrate a
substantial performance improvement in terms of average simple regret. This
novel approach can be used to significantly improve the quality and accuracy of
agent evaluation procedures for general game frameworks, as well as other
multi-task domains with high algorithm runtimes.

</details>


### [83] [Recurrent Memory-Augmented Transformers with Chunked Attention for Long-Context Language Modeling](https://arxiv.org/abs/2507.00453)
*Ankit Kashyap*

Main category: cs.LG

TL;DR: A Transformer architecture with global attention, chunked local attention, and a gated FIFO memory mechanism for efficient long-context language modeling.


<details>
  <summary>Details</summary>
Motivation: To handle both short-range and long-range dependencies in language modeling without quadratic attention cost.

Method: Combines global attention, chunked local attention, and a gated FIFO memory mechanism with rotary positional encoding.

Result: Efficiently manages dependencies and offers a lightweight, extensible design for tasks like dialogue modeling and code completion.

Conclusion: The architecture provides a scalable and modular solution for long-context language tasks.

Abstract: We present a Transformer architecture for long-context language modeling that
combines global attention with two biologically inspired components: chunked
local attention and a gated FIFO memory mechanism. This unified attention block
allows the model to efficiently handle both short-range and long-range
dependencies without increasing attention cost quadratically. The memory module
persistently stores past token representations using a gated update mechanism
inspired by recurrent networks. Rotary positional encoding is applied per
attention head to enable directionally disentangled, scale-invariant positional
signals. The architecture is implemented entirely from scratch in PyTorch, with
no reliance on high-level libraries, enabling transparent and modular
experimentation. Our model offers a lightweight and extensible design for tasks
such as dialogue modeling, code completion, and document understanding.

</details>


### [84] [Diversity Conscious Refined Random Forest](https://arxiv.org/abs/2507.00467)
*Sijan Bhattarai,Saurav Bhandari,Girija Bhusal,Saroj Shakya,Tapendra Pandey*

Main category: cs.LG

TL;DR: A refined Random Forest (RF) classifier is proposed to reduce inference cost and redundancy by dynamically growing trees on informative features and enforcing diversity through clustering.


<details>
  <summary>Details</summary>
Motivation: Standard RF relies on many trees and all features, leading to high costs and redundancy. The goal is to improve efficiency and accuracy.

Method: The model iteratively removes uninformative features, grows new trees as needed, and uses correlation-based clustering to eliminate redundant trees.

Result: Experiments on 8 benchmark datasets show improved accuracy over standard RF with the same number of trees.

Conclusion: The refined RF classifier achieves better accuracy while addressing redundancy and cost issues.

Abstract: Random Forest (RF) is a widely used ensemble learning technique known for its
robust classification performance across diverse domains. However, it often
relies on hundreds of trees and all input features, leading to high inference
cost and model redundancy. In this work, our goal is to grow trees dynamically
only on informative features and then enforce maximal diversity by clustering
and retaining uncorrelated trees. Therefore, we propose a Refined Random Forest
Classifier that iteratively refines itself by first removing the least
informative features and then analytically determines how many new trees should
be grown, followed by correlation-based clustering to remove redundant trees.
The classification accuracy of our model was compared against the standard RF
on the same number of trees. Experiments on 8 multiple benchmark datasets,
including binary and multiclass datasets, demonstrate that the proposed model
achieves improved accuracy compared to standard RF.

</details>


### [85] [Posterior Inference in Latent Space for Scalable Constrained Black-box Optimization](https://arxiv.org/abs/2507.00480)
*Kiyoung Om,Kyuil Sim,Taeyoung Yun,Hyeongyu Kang,Jinkyoo Park*

Main category: cs.LG

TL;DR: A new framework for high-dimensional constrained black-box optimization using flow-based models and posterior inference to address scalability and mode collapse issues.


<details>
  <summary>Details</summary>
Motivation: High-dimensional constrained optimization is challenging due to hard-to-find feasible regions and limitations of existing methods like Bayesian optimization and generative models.

Method: Proposes a two-stage approach: training flow-based models for data distribution and surrogate models, then using posterior inference for candidate selection.

Result: Empirically shows superior performance on synthetic and real-world tasks.

Conclusion: The method effectively addresses scalability and mode collapse, offering a robust solution for constrained optimization.

Abstract: Optimizing high-dimensional black-box functions under black-box constraints
is a pervasive task in a wide range of scientific and engineering problems.
These problems are typically harder than unconstrained problems due to
hard-to-find feasible regions. While Bayesian optimization (BO) methods have
been developed to solve such problems, they often struggle with the curse of
dimensionality. Recently, generative model-based approaches have emerged as a
promising alternative for constrained optimization. However, they suffer from
poor scalability and are vulnerable to mode collapse, particularly when the
target distribution is highly multi-modal. In this paper, we propose a new
framework to overcome these challenges. Our method iterates through two stages.
First, we train flow-based models to capture the data distribution and
surrogate models that predict both function values and constraint violations
with uncertainty quantification. Second, we cast the candidate selection
problem as a posterior inference problem to effectively search for promising
candidates that have high objective values while not violating the constraints.
During posterior inference, we find that the posterior distribution is highly
multi-modal and has a large plateau due to constraints, especially when
constraint feedback is given as binary indicators of feasibility. To mitigate
this issue, we amortize the sampling from the posterior distribution in the
latent space of flow-based models, which is much smoother than that in the data
space. We empirically demonstrate that our method achieves superior performance
on various synthetic and real-world constrained black-box optimization tasks.
Our code is publicly available \href{https://github.com/umkiyoung/CiBO}{here}.

</details>


### [86] [PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning](https://arxiv.org/abs/2507.00485)
*Weiran Guo,Guanjun Liu,Ziyuan Zhou,Ling Wang*

Main category: cs.LG

TL;DR: The paper introduces a backdoor attack framework (PNAct) for Safe Reinforcement Learning, demonstrating its feasibility and risks.


<details>
  <summary>Details</summary>
Motivation: To expose vulnerabilities in Safe RL by showing how backdoor attacks can manipulate agents into unsafe actions.

Method: Proposes PNAct, a framework using Positive and Negative Action samples to implant backdoors, with a designed attack algorithm.

Result: Experiments confirm the effectiveness of PNAct in compromising Safe RL agents.

Conclusion: Highlights the risks of backdoor attacks in Safe RL and their feasibility, urging further scrutiny.

Abstract: Reinforcement Learning (RL) is widely used in tasks where agents interact
with an environment to maximize rewards. Building on this foundation, Safe
Reinforcement Learning (Safe RL) incorporates a cost metric alongside the
reward metric, ensuring that agents adhere to safety constraints during
decision-making. In this paper, we identify that Safe RL is vulnerable to
backdoor attacks, which can manipulate agents into performing unsafe actions.
First, we introduce the relevant concepts and evaluation metrics for backdoor
attacks in Safe RL. It is the first attack framework in the Safe RL field that
involves both Positive and Negative Action sample (PNAct) is to implant
backdoors, where positive action samples provide reference actions and negative
action samples indicate actions to be avoided. We theoretically point out the
properties of PNAct and design an attack algorithm. Finally, we conduct
experiments to evaluate the effectiveness of our proposed backdoor attack
framework, evaluating it with the established metrics. This paper highlights
the potential risks associated with Safe RL and underscores the feasibility of
such attacks. Our code and supplementary material are available at
https://github.com/azure-123/PNAct.

</details>


### [87] [Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher Sampling](https://arxiv.org/abs/2507.00518)
*Walid Bendada,Guillaume Salha-Galvan,Romain Hennequin,Tho Bontempelli,Thomas Bouaba,Tristan Cazenave*

Main category: cs.LG

TL;DR: vMF-exp is a scalable exploration method for large action sets in RL using hyperspherical embeddings, outperforming Boltzmann Exploration in scalability while maintaining similar exploration probabilities.


<details>
  <summary>Details</summary>
Motivation: To address the scalability issues of Boltzmann Exploration (B-exp) in large action sets, especially when actions are represented as hyperspherical embeddings.

Method: vMF-exp samples state embeddings using a von Mises-Fisher distribution and explores their nearest neighbors, enabling scalability to unlimited actions.

Result: Theoretically, vMF-exp matches B-exp's exploration probabilities asymptotically and scales better. Empirical validation includes simulations, real-world data, and deployment in a music streaming recommender system.

Conclusion: vMF-exp is a scalable and effective alternative to B-exp for large action sets with hyperspherical embeddings, validated by theory and real-world applications.

Abstract: This paper introduces von Mises-Fisher exploration (vMF-exp), a scalable
method for exploring large action sets in reinforcement learning problems where
hyperspherical embedding vectors represent these actions. vMF-exp involves
initially sampling a state embedding representation using a von Mises-Fisher
distribution, then exploring this representation's nearest neighbors, which
scales to virtually unlimited numbers of candidate actions. We show that, under
theoretical assumptions, vMF-exp asymptotically maintains the same probability
of exploring each action as Boltzmann Exploration (B-exp), a popular
alternative that, nonetheless, suffers from scalability issues as it requires
computing softmax values for each action. Consequently, vMF-exp serves as a
scalable alternative to B-exp for exploring large action sets with
hyperspherical embeddings. Experiments on simulated data, real-world public
data, and the successful large-scale deployment of vMF-exp on the recommender
system of a global music streaming service empirically validate the key
properties of the proposed method.

</details>


### [88] [Foundation Models for Clinical Records at Health System Scale](https://arxiv.org/abs/2507.00574)
*Haresh Rengaraj Rajamohan,Xiang Gao,Weicheng Zhu,Shih-Lun Huang,Long Chen,Kyunghyun Cho,Cem M. Deniz,Narges Razavian*

Main category: cs.LG

TL;DR: A novel generative pretraining strategy for sequential EHR data is introduced, focusing on next-visit event prediction and handling heterogeneous data types. The model addresses pitfalls in EHR evaluations and rivals fine-tuned baselines in zero-shot tasks.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored potential of large-scale pretraining in healthcare, specifically for structured EHR data, by developing a generative model that captures clinical dependencies without costly fine-tuning.

Method: The model uses autoregressive generation of tokenized clinical events for next-visit prediction, with regularization for repeated events. It inherently handles heterogeneous data types and addresses evaluation pitfalls.

Result: The model performs competitively in zero-shot prediction tasks (dementia and knee osteoarthritis incidence) compared to a fine-tuned baseline, demonstrating its ability to capture clinical dependencies.

Conclusion: The proposed generative pretraining strategy effectively leverages EHR data for clinical predictions without task-specific fine-tuning, highlighting its potential for healthcare applications.

Abstract: Large-scale pretraining has transformed modeling of language and other data
types, but its potential remains underexplored in healthcare with structured
electronic health records (EHRs). We present a novel generative pretraining
strategy for sequential EHR data using next-visit event prediction. Our model
learns to autoregressively generate various tokenized clinical events for the
next visit based on patient history and inherently handles the joint prediction
of heterogeneous data types. Additionally, we introduce regularization on
predicting repeated events and highlight a key pitfall in EHR-based foundation
model evaluations: repeated event tokens can inflate performance metrics when
new onsets are not distinguished from subsequent occurrences. Our model is
evaluated via zero-shot prediction for forecasting dementia and knee
osteoarthritis incidence within 2 and 5 years, and the model performance rivals
a fully fine-tuned masked pretrained Transformer baseline, demonstrating that
our approach captures complex clinical dependencies without requiring costly
task-specific fine-tuning.

</details>


### [89] [Quantum Circuit Structure Optimization for Quantum Reinforcement Learning](https://arxiv.org/abs/2507.00589)
*Seok Bin Son,Joongheon Kim*

Main category: cs.LG

TL;DR: QRL-NAS optimizes PQC structures in quantum reinforcement learning, outperforming fixed-circuit QRL.


<details>
  <summary>Details</summary>
Motivation: RL struggles with high-dimensional spaces; QRL leverages quantum computing but lacks optimized PQC structures.

Method: Proposes QRL-NAS, integrating quantum neural architecture search to optimize PQC structures.

Result: QRL-NAS achieves higher rewards than fixed-circuit QRL.

Conclusion: QRL-NAS is effective and practical for optimizing quantum reinforcement learning.

Abstract: Reinforcement learning (RL) enables agents to learn optimal policies through
environmental interaction. However, RL suffers from reduced learning efficiency
due to the curse of dimensionality in high-dimensional spaces. Quantum
reinforcement learning (QRL) addresses this issue by leveraging superposition
and entanglement in quantum computing, allowing efficient handling of
high-dimensional problems with fewer resources. QRL combines quantum neural
networks (QNNs) with RL, where the parameterized quantum circuit (PQC) acts as
the core computational module. The PQC performs linear and nonlinear
transformations through gate operations, similar to hidden layers in classical
neural networks. Previous QRL studies, however, have used fixed PQC structures
based on empirical intuition without verifying their optimality. This paper
proposes a QRL-NAS algorithm that integrates quantum neural architecture search
(QNAS) to optimize PQC structures within QRL. Experiments demonstrate that
QRL-NAS achieves higher rewards than QRL with fixed circuits, validating its
effectiveness and practical utility.

</details>


### [90] [Residual Reward Models for Preference-based Reinforcement Learning](https://arxiv.org/abs/2507.00611)
*Chenyang Cao,Miguel Rogel-Garca,Mohamed Nabail,Xueqian Wang,Nicholas Rhinehart*

Main category: cs.LG

TL;DR: The paper proposes a Residual Reward Model (RRM) to improve Preference-based Reinforcement Learning (PbRL) by combining prior knowledge with learned rewards, enhancing performance and convergence speed.


<details>
  <summary>Details</summary>
Motivation: PbRL avoids heuristic reward design but suffers from slow convergence due to reward model training. Existing methods face optimization challenges when using different loss functions for pre-training and fine-tuning.

Method: RRM splits the true reward into a prior reward (pre-defined or from IRL) and a learned reward (trained with preferences). State-based and image-based RRM versions are introduced and tested on Meta-World tasks and a real robot.

Result: RRM substantially improves PbRL performance, works with various prior rewards, and accelerates policy learning, achieving success in fewer steps than baselines.

Conclusion: RRM effectively leverages prior knowledge to enhance PbRL, demonstrating superior performance in simulations and real-world robotic tasks.

Abstract: Preference-based Reinforcement Learning (PbRL) provides a way to learn
high-performance policies in environments where the reward signal is hard to
specify, avoiding heuristic and time-consuming reward design. However, PbRL can
suffer from slow convergence speed since it requires training in a reward
model. Prior work has proposed learning a reward model from demonstrations and
fine-tuning it using preferences. However, when the model is a neural network,
using different loss functions for pre-training and fine-tuning can pose
challenges to reliable optimization. In this paper, we propose a method to
effectively leverage prior knowledge with a Residual Reward Model (RRM). An RRM
assumes that the true reward of the environment can be split into a sum of two
parts: a prior reward and a learned reward. The prior reward is a term
available before training, for example, a user's ``best guess'' reward
function, or a reward function learned from inverse reinforcement learning
(IRL), and the learned reward is trained with preferences. We introduce
state-based and image-based versions of RRM and evaluate them on several tasks
in the Meta-World environment suite. Experimental results show that our method
substantially improves the performance of a common PbRL method. Our method
achieves performance improvements for a variety of different types of prior
rewards, including proxy rewards, a reward obtained from IRL, and even a
negated version of the proxy reward. We also conduct experiments with a Franka
Panda to show that our method leads to superior performance on a real robot. It
significantly accelerates policy learning for different tasks, achieving
success in fewer steps than the baseline. The videos are presented at
https://sunlighted.github.io/RRM-web/.

</details>


### [91] [Cooperative Sheaf Neural Networks](https://arxiv.org/abs/2507.00647)
*Andr Ribeiro,Ana Luiza Tenrio,Juan Belieni,Amauri H. Souza,Diego Mesquita*

Main category: cs.LG

TL;DR: Sheaf diffusion lacks cooperative behavior due to missing message directionality. The paper introduces directed cellular sheaves and proposes Cooperative Sheaf Neural Networks (CSNNs) to address this, showing improved performance.


<details>
  <summary>Details</summary>
Motivation: To explore if sheaf diffusion can exhibit cooperative behavior, addressing its limitations in handling heterophilic data and oversmoothing.

Method: Introduces cellular sheaves over directed graphs, characterizes their Laplacians, and proposes CSNNs.

Result: CSNNs outperform existing sheaf diffusion and cooperative GNN methods, with nodes selectively attending to distant nodes.

Conclusion: Directed cellular sheaves and CSNNs enhance cooperative behavior in sheaf diffusion, improving performance and mitigating oversquashing.

Abstract: Sheaf diffusion has recently emerged as a promising design pattern for graph
representation learning due to its inherent ability to handle heterophilic data
and avoid oversmoothing. Meanwhile, cooperative message passing has also been
proposed as a way to enhance the flexibility of information diffusion by
allowing nodes to independently choose whether to propagate/gather information
from/to neighbors. A natural question ensues: is sheaf diffusion capable of
exhibiting this cooperative behavior? Here, we provide a negative answer to
this question. In particular, we show that existing sheaf diffusion methods
fail to achieve cooperative behavior due to the lack of message directionality.
To circumvent this limitation, we introduce the notion of cellular sheaves over
directed graphs and characterize their in- and out-degree Laplacians. We
leverage our construction to propose Cooperative Sheaf Neural Networks (CSNNs).
Theoretically, we characterize the receptive field of CSNN and show it allows
nodes to selectively attend (listen) to arbitrarily far nodes while ignoring
all others in their path, potentially mitigating oversquashing. Our experiments
show that CSNN presents overall better performance compared to prior art on
sheaf diffusion as well as cooperative graph neural networks.

</details>


### [92] [GANs Secretly Perform Approximate Bayesian Model Selection](https://arxiv.org/abs/2507.00651)
*Maurizio Filippone,Marius P. Linhard*

Main category: cs.LG

TL;DR: The paper interprets GANs as probabilistic generative models, linking them to Bayesian neural networks, and proposes regularization strategies based on marginal likelihood optimization for improved performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of GAN optimization and overfitting by reinterpreting GANs as probabilistic models and leveraging Bayesian principles.

Method: The study interprets GANs as Bayesian neural networks with partial stochasticity, establishes universal approximation conditions, and connects adversarial optimization to marginal likelihood.

Result: Proposed strategies improve GAN performance by smoothing the loss landscape and finding solutions with minimum description length, associated with better generalization.

Conclusion: The work provides a deeper understanding of GAN regularization and optimization, leading to performance enhancements and insights into flat minima and generalization.

Abstract: Generative Adversarial Networks (GANs) are popular and successful generative
models. Despite their success, optimization is notoriously challenging and they
require regularization against overfitting. In this work, we explain the
success and limitations of GANs by interpreting them as probabilistic
generative models. This interpretation enables us to view GANs as Bayesian
neural networks with partial stochasticity, allowing us to establish conditions
of universal approximation. We can then cast the adversarial-style optimization
of several variants of GANs as the optimization of a proxy for the marginal
likelihood. Taking advantage of the connection between marginal likelihood
optimization and Occam's razor, we can define regularization and optimization
strategies to smooth the loss landscape and search for solutions with minimum
description length, which are associated with flat minima and good
generalization. The results on a wide range of experiments indicate that these
strategies lead to performance improvements and pave the way to a deeper
understanding of regularization strategies for GANs.

</details>


### [93] [Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for Optimizing the Token Economy of Large Language Models](https://arxiv.org/abs/2507.00653)
*Yilun Zhang*

Main category: cs.LG

TL;DR: The paper introduces the Cognitive Load-Aware Inference (CLAI) framework, applying Cognitive Load Theory to optimize LLM inference, reducing token use by up to 45% without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Address the high computational costs of LLM inference by leveraging cognitive theory for optimization, moving beyond heuristic-based methods.

Method: Proposes CLAI framework with two approaches: CLAI-Prompt (zero-shot) and CLAI-Tune (fine-tuned), formalizing cognitive load metrics ($ICL_{LLM}$, $ECL_{LLM}$, $GCL_{LLM}$) to optimize token allocation.

Result: Achieves up to 45% reduction in token consumption while maintaining accuracy, with CLAI-Tune showing emergent problem decomposition skills.

Conclusion: Emulating human cognitive strategies enhances LLM efficiency, robustness, and capability, offering a sustainable path for AI deployment.

Abstract: The escalating computational costs of Large Language Model (LLM) inference
have become a critical barrier to their widespread and sustainable deployment.
While existing optimization strategies are effective, they are predominantly
based on statistical heuristics or architectural modifications, lacking a
guiding cognitive theory to manage the inference process itself. This paper
aims to bridge this gap by introducing a novel paradigm: the Cognitive
Load-Aware Inference (CLAI) framework, which operationalizes principles from
Cognitive Load Theory (CLT) and neuroscience for LLM inference. We formalize
the concepts of Intrinsic Cognitive Load, Extraneous Cognitive Load, and
Germane Cognitive Load into quantifiable LLM metrics ($ICL_{LLM}$, $ECL_{LLM}$,
and $GCL_{LLM}$), thereby reframing the inference process as a cognitive
economics optimization problem: based on the intrinsic complexity of a problem
($ICL_{LLM}$), minimize wasteful computation ($ECL_{LLM}$), and strategically
allocate the token budget to productive reasoning ($GCL_{LLM}$). We propose two
implementation paths: CLAI-Prompt, a zero-shot method that guides a base LLM
through cognitive control steps via a structured meta-prompt, and CLAI-Tune, a
fine-tuned model that internalizes these principles for spontaneous cognitive
economy. Across a range of benchmarks in complex reasoning, long-context
question answering, and code generation, our methods achieve significant
reductions in token consumption (up to 45\%) without sacrificing accuracy.
Furthermore, CLAI-Tune exhibits an emergent ability to autonomously decompose
difficult problems, a key characteristic of human expert cognition. This work
demonstrates that by emulating the brain's resource management strategies, we
can build more efficient, robust, and capable artificial intelligence systems.

</details>


### [94] [Neural Augmented Kalman Filters for Road Network assisted GNSS positioning](https://arxiv.org/abs/2507.00654)
*Hans van Gorp,Davide Belli,Amir Jalalirad,Bence Major*

Main category: cs.LG

TL;DR: A Temporal Graph Neural Network (TGNN) is proposed to enhance GNSS positioning accuracy in urban areas by integrating road network data into a Kalman Filter, reducing errors by 29%.


<details>
  <summary>Details</summary>
Motivation: GNSS accuracy in dense urban areas is hindered by multipath and non-line-of-sight errors. Road network data can mitigate these issues, but existing methods lack flexibility and robustness.

Method: A TGNN is trained to predict the correct road segment and its uncertainty, which is integrated into the Kalman Filter's measurement update step.

Result: Real-world testing shows a 29% reduction in positioning error compared to GNSS-only Kalman Filter in challenging scenarios.

Conclusion: This is the first deep learning-based approach combining road network data and GNSS measurements for improved urban positioning accuracy.

Abstract: The Global Navigation Satellite System (GNSS) provides critical positioning
information globally, but its accuracy in dense urban environments is often
compromised by multipath and non-line-of-sight errors. Road network data can be
used to reduce the impact of these errors and enhance the accuracy of a
positioning system. Previous works employing road network data are either
limited to offline applications, or rely on Kalman Filter (KF) heuristics with
little flexibility and robustness. We instead propose training a Temporal Graph
Neural Network (TGNN) to integrate road network information into a KF. The TGNN
is designed to predict the correct road segment and its associated uncertainty
to be used in the measurement update step of the KF. We validate our approach
with real-world GNSS data and open-source road networks, observing a 29%
decrease in positioning error for challenging scenarios compared to a GNSS-only
KF. To the best of our knowledge, ours is the first deep learning-based
approach jointly employing road network data and GNSS measurements to determine
the user position on Earth.

</details>


### [95] [Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding](https://arxiv.org/abs/2507.00669)
*Duc Cao-Dinh,Khai Le-Duc,Anh Dao,Bach Phan Tat,Chris Ngo,Duy M. H. Nguyen,Nguyen X. Khanh,Thanh Nguyen-Tang*

Main category: cs.LG

TL;DR: Audio-3DVG integrates audio and spatial info for 3D object localization, outperforming prior methods by decomposing speech into object mention detection and audio-guided attention.


<details>
  <summary>Details</summary>
Motivation: Prior work lacks exploration of spoken language for 3DVG, despite advances in ASR and speech representation.

Method: Decomposes task into Object Mention Detection (multi-label classification) and Audio-Guided Attention (object-speech interaction).

Result: Achieves state-of-the-art in audio-based grounding and competes with text-based methods.

Conclusion: Audio-3DVG shows promise for integrating spoken language into 3D vision tasks.

Abstract: 3D Visual Grounding (3DVG) involves localizing target objects in 3D point
clouds based on natural language. While prior work has made strides using
textual descriptions, leveraging spoken language-known as Audio-based 3D Visual
Grounding-remains underexplored and challenging. Motivated by advances in
automatic speech recognition (ASR) and speech representation learning, we
propose Audio-3DVG, a simple yet effective framework that integrates audio and
spatial information for enhanced grounding. Rather than treating speech as a
monolithic input, we decompose the task into two complementary components.
First, we introduce Object Mention Detection, a multi-label classification task
that explicitly identifies which objects are referred to in the audio, enabling
more structured audio-scene reasoning. Second, we propose an Audio-Guided
Attention module that captures interactions between candidate objects and
relational speech cues, improving target discrimination in cluttered scenes. To
support benchmarking, we synthesize audio descriptions for standard 3DVG
datasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate
that Audio-3DVG not only achieves new state-of-the-art performance in
audio-based grounding, but also competes with text-based methods-highlighting
the promise of integrating spoken language into 3D vision tasks.

</details>


### [96] [Diffusion Classifier Guidance for Non-robust Classifiers](https://arxiv.org/abs/2507.00687)
*Philipp Vaeth,Dibyanshu Kumar,Benjamin Paassen,Magda Gregorov*

Main category: cs.LG

TL;DR: The paper extends classifier guidance to non-robust classifiers, analyzes their noise sensitivity, and proposes a stabilization method using denoised predictions and stochastic optimization techniques.


<details>
  <summary>Details</summary>
Motivation: Classifier guidance typically requires robust classifiers trained on noisy data, limiting its applicability. This work aims to enable general classifiers for guidance.

Method: Analyzes noise sensitivity of classifiers, proposes using one-step denoised predictions and stabilization techniques like exponential moving averages.

Result: The method improves guidance stability while preserving sample diversity and visual quality.

Conclusion: This advances conditional sampling in generative models by broadening the range of usable classifiers.

Abstract: Classifier guidance is intended to steer a diffusion process such that a
given classifier reliably recognizes the generated data point as a certain
class. However, most classifier guidance approaches are restricted to robust
classifiers, which were specifically trained on the noise of the diffusion
forward process. We extend classifier guidance to work with general,
non-robust, classifiers that were trained without noise. We analyze the
sensitivity of both non-robust and robust classifiers to noise of the diffusion
process on the standard CelebA data set, the specialized SportBalls data set
and the high-dimensional real-world CelebA-HQ data set. Our findings reveal
that non-robust classifiers exhibit significant accuracy degradation under
noisy conditions, leading to unstable guidance gradients. To mitigate these
issues, we propose a method that utilizes one-step denoised image predictions
and implements stabilization techniques inspired by stochastic optimization
methods, such as exponential moving averages. Experimental results demonstrate
that our approach improves the stability of classifier guidance while
maintaining sample diversity and visual quality. This work contributes to
advancing conditional sampling techniques in generative models, enabling a
broader range of classifiers to be used as guidance classifiers.

</details>


### [97] [A Test-Function Approach to Incremental Stability](https://arxiv.org/abs/2507.00695)
*Daniel Pfrommer,Max Simchowitz,Ali Jadbabaie*

Main category: cs.LG

TL;DR: The paper introduces a framework linking RL-style value functions to incremental stability, distinct from traditional Lyapunov methods.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between RL value functions and control theory stability analysis, which traditionally relies on Lyapunov functions.

Method: Develops an equivalence between incremental input-to-state stability (ISS) and the regularity of RL-style value functions using Hlder-continuous rewards.

Result: Shows that RL-style value functions can certify stability without relying on Lyapunov functions.

Conclusion: The regularity of value functions offers a new perspective on stability, diverging from classical Lyapunov-based approaches.

Abstract: This paper presents a novel framework for analyzing
Incremental-Input-to-State Stability ($\delta$ISS) based on the idea of using
rewards as "test functions." Whereas control theory traditionally deals with
Lyapunov functions that satisfy a time-decrease condition, reinforcement
learning (RL) value functions are constructed by exponentially decaying a
Lipschitz reward function that may be non-smooth and unbounded on both sides.
Thus, these RL-style value functions cannot be directly understood as Lyapunov
certificates. We develop a new equivalence between a variant of incremental
input-to-state stability of a closed-loop system under given a policy, and the
regularity of RL-style value functions under adversarial selection of a
H\"older-continuous reward function. This result highlights that the regularity
of value functions, and their connection to incremental stability, can be
understood in a way that is distinct from the traditional Lyapunov-based
approach to certifying stability in control theory.

</details>


### [98] [SCAWaveNet: A Spatial-Channel Attention-based Network for Global Significant Wave Height Retrieval](https://arxiv.org/abs/2507.00701)
*Chong Zhang,Xichao Liu,Yibing Zhan,Dapeng Tao,Jun Ni*

Main category: cs.LG

TL;DR: SCAWaveNet, a spatial-channel attention-based network, improves significant wave height (SWH) retrieval by leveraging cross-channel information, outperforming existing models with lower RMSE.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for SWH retrieval fail to fully utilize cross-channel information from CYGNSS data, limiting performance.

Method: SCAWaveNet models each channel as independent attention heads, fusing spatial and channel-wise information, and uses a lightweight attention mechanism for auxiliary parameters.

Result: SCAWaveNet achieves RMSEs of 0.438 m (ERA5) and 0.432 m (NDBC), reducing errors by 3.52% and 5.47% respectively compared to state-of-the-art models.

Conclusion: SCAWaveNet effectively improves SWH retrieval accuracy by integrating spatial and channel-level features, with code publicly available.

Abstract: Recent advancements in spaceborne GNSS missions have produced extensive
global datasets, providing a robust basis for deep learning-based significant
wave height (SWH) retrieval. While existing deep learning models predominantly
utilize CYGNSS data with four-channel information, they often adopt
single-channel inputs or simple channel concatenation without leveraging the
benefits of cross-channel information interaction during training. To address
this limitation, a novel spatial-channel attention-based network, namely
SCAWaveNet, is proposed for SWH retrieval. Specifically, features from each
channel of the DDMs are modeled as independent attention heads, enabling the
fusion of spatial and channel-wise information. For auxiliary parameters, a
lightweight attention mechanism is designed to assign weights along the spatial
and channel dimensions. The final feature integrates both spatial and
channel-level characteristics. Model performance is evaluated using
four-channel CYGNSS data. When ERA5 is used as a reference, SCAWaveNet achieves
an average RMSE of 0.438 m. When using buoy data from NDBC, the average RMSE
reaches 0.432 m. Compared to state-of-the-art models, SCAWaveNet reduces the
average RMSE by at least 3.52% on the ERA5 dataset and by 5.47% on the NDBC
buoy observations. The code is available at
https://github.com/Clifx9908/SCAWaveNet.

</details>


### [99] [Large Reasoning Models are not thinking straight: on the unreliability of thinking trajectories](https://arxiv.org/abs/2507.00711)
*Jhouben Cuesta-Ramirez,Samuel Beaussant,Mehdi Mounsif*

Main category: cs.LG

TL;DR: LLMs trained with RL show overthinking, ignoring correct solutions and generating unnecessary reasoning steps, leading to errors.


<details>
  <summary>Details</summary>
Motivation: To investigate whether benchmark improvements in LLMs reflect genuine reasoning or flawed overthinking.

Method: Experiments on three state-of-the-art models using the AIME2024 math benchmark.

Result: Models often disregard correct solutions and generate ineffective reasoning steps, leading to incorrect conclusions.

Conclusion: Highlights limitations in LLMs' ability to integrate corrective information, challenging robust and interpretable reasoning.

Abstract: Large Language Models (LLMs) trained via Reinforcement Learning (RL) have
recently achieved impressive results on reasoning benchmarks. Yet, growing
evidence shows that these models often generate longer but ineffective chains
of thought (CoTs), calling into question whether benchmark gains reflect real
reasoning improvements. We present new evidence of overthinking, where models
disregard correct solutions even when explicitly provided, instead continuing
to generate unnecessary reasoning steps that often lead to incorrect
conclusions. Experiments on three state-of-the-art models using the AIME2024
math benchmark reveal critical limitations in these models ability to integrate
corrective information, posing new challenges for achieving robust and
interpretable reasoning.

</details>


### [100] [Aleatoric and Epistemic Uncertainty Measures for Ordinal Classification through Binary Reduction](https://arxiv.org/abs/2507.00733)
*Stefan Haas,Eyke Hllermeier*

Main category: cs.LG

TL;DR: The paper introduces novel measures for aleatoric and epistemic uncertainty in ordinal classification, outperforming existing methods in error and OOD detection.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of uncertainty quantification methods for ordinal classification, crucial in fields like medicine and finance.

Method: Proposes entropy- and variance-based measures for ordinal classification by reducing to binary case, tested with gradient-boosted trees and MLPs.

Result: Outperforms standard measures in error detection and shows competitive OOD detection performance.

Conclusion: Highlights the importance of ordinal-aware uncertainty measures for reliable decision-making.

Abstract: Ordinal classification problems, where labels exhibit a natural order, are
prevalent in high-stakes fields such as medicine and finance. Accurate
uncertainty quantification, including the decomposition into aleatoric
(inherent variability) and epistemic (lack of knowledge) components, is crucial
for reliable decision-making. However, existing research has primarily focused
on nominal classification and regression. In this paper, we introduce a novel
class of measures of aleatoric and epistemic uncertainty in ordinal
classification, which is based on a suitable reduction to (entropy- and
variance-based) measures for the binary case. These measures effectively
capture the trade-off in ordinal classification between exact hit-rate and
minimial error distances. We demonstrate the effectiveness of our approach on
various tabular ordinal benchmark datasets using ensembles of gradient-boosted
trees and multi-layer perceptrons for approximate Bayesian inference. Our
method significantly outperforms standard and label-wise entropy and
variance-based measures in error detection, as indicated by misclassification
rates and mean absolute error. Additionally, the ordinal measures show
competitive performance in out-of-distribution (OOD) detection. Our findings
highlight the importance of considering the ordinal nature of classification
problems when assessing uncertainty.

</details>


### [101] [Ordinality in Discrete-level Question Difficulty Estimation: Introducing Balanced DRPS and OrderedLogitNN](https://arxiv.org/abs/2507.00736)
*Arthur Thuy,Ekaterina Loginova,Dries F. Benoit*

Main category: cs.LG

TL;DR: The paper benchmarks QDE methods, introduces OrderedLogitNN, and proposes balanced DRPS for fair evaluation.


<details>
  <summary>Details</summary>
Motivation: Address neglect of ordinal nature in QDE and biased evaluation metrics.

Method: Benchmarks discretized regression, classification, and ordinal regression; introduces OrderedLogitNN and balanced DRPS.

Result: OrderedLogitNN outperforms on complex tasks; balanced DRPS proves robust.

Conclusion: Balanced DRPS and OrderedLogitNN provide a principled foundation for QDE research.

Abstract: Recent years have seen growing interest in Question Difficulty Estimation
(QDE) using natural language processing techniques. Question difficulty is
often represented using discrete levels, framing the task as ordinal regression
due to the inherent ordering from easiest to hardest. However, the literature
has neglected the ordinal nature of the task, relying on classification or
discretized regression models, with specialized ordinal regression methods
remaining unexplored. Furthermore, evaluation metrics are tightly coupled to
the modeling paradigm, hindering cross-study comparability. While some metrics
fail to account for the ordinal structure of difficulty levels, none adequately
address class imbalance, resulting in biased performance assessments. This
study addresses these limitations by benchmarking three types of model outputs
-- discretized regression, classification, and ordinal regression -- using the
balanced Discrete Ranked Probability Score (DRPS), a novel metric that jointly
captures ordinality and class imbalance. In addition to using popular ordinal
regression methods, we propose OrderedLogitNN, extending the ordered logit
model from econometrics to neural networks. We fine-tune BERT on the RACE++ and
ARC datasets and find that OrderedLogitNN performs considerably better on
complex tasks. The balanced DRPS offers a robust and fair evaluation metric for
discrete-level QDE, providing a principled foundation for future research.

</details>


### [102] [Evaluating LLMs and Prompting Strategies for Automated Hardware Diagnosis from Textual User-Reports](https://arxiv.org/abs/2507.00742)
*Carlos Caminha,Maria de Lourdes M. Silva,Iago C. Chaves,Felipe T. Brito,Victor A. E. Farias,Javam C. Machado*

Main category: cs.LG

TL;DR: The study evaluates 27 open-source and 2 proprietary LLMs for identifying faulty components from ambiguous user reports, achieving an F1-score up to 0.76. Three models (mistral-small-24b-instruct, llama-3.2-1b-instruct, gemma-2-2b-it) balance performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Automating fault identification from ambiguous user reports is challenging but crucial for improving user experience and testing efficiency.

Method: Evaluated 27 open-source and 2 proprietary LLMs using four prompting strategies (Zero-Shot, Few-Shot, Chain-of-Thought, CoT+Few-Shot) across 98,948 inferences.

Result: Achieved an F1-score up to 0.76, with three models (mistral-small-24b-instruct, llama-3.2-1b-instruct, gemma-2-2b-it) offering the best balance of performance and efficiency.

Conclusion: Smaller models like llama-3.2-1b-instruct and gemma-2-2b-it provide competitive performance with lower resource usage, making them suitable for end-user devices.

Abstract: Computer manufacturers offer platforms for users to describe device faults
using textual reports such as "My screen is flickering". Identifying the faulty
component from the report is essential for automating tests and improving user
experience. However, such reports are often ambiguous and lack detail, making
this task challenging. Large Language Models (LLMs) have shown promise in
addressing such issues. This study evaluates 27 open-source models (1B-72B
parameters) and 2 proprietary LLMs using four prompting strategies: Zero-Shot,
Few-Shot, Chain-of-Thought (CoT), and CoT+Few-Shot (CoT+FS). We conducted
98,948 inferences, processing over 51 million input tokens and generating 13
million output tokens. We achieve f1-score up to 0.76. Results show that three
models offer the best balance between size and performance:
mistral-small-24b-instruct and two smaller models, llama-3.2-1b-instruct and
gemma-2-2b-it, that offer competitive performance with lower VRAM usage,
enabling efficient inference on end-user devices as modern laptops or
smartphones with NPUs.

</details>


### [103] [A Probabilistic Approach to Wildfire Spread Prediction Using a Denoising Diffusion Surrogate Model](https://arxiv.org/abs/2507.00761)
*Wenbo Yu,Anirbit Ghosh,Tobias Sebastian Finn,Rossella Arcucci,Marc Bocquet,Sibo Cheng*

Main category: cs.LG

TL;DR: A denoising diffusion model is introduced to predict wildfire spread, simulating multiple scenarios to account for uncertainty, unlike traditional deterministic models.


<details>
  <summary>Details</summary>
Motivation: Wildfire spread prediction is challenging due to unpredictability and environmental variability. Traditional models lack the ability to represent uncertainty.

Method: A denoising diffusion model is used to simulate wildfire spread as a range of possible scenarios, generating ensembles of forecasts.

Result: The model produces physically meaningful distributions of fire spread, improving reliability and aiding decision-making.

Conclusion: This AI framework offers smarter, faster, and more reliable wildfire prediction tools for risk assessment and response planning.

Abstract: Thanks to recent advances in generative AI, computers can now simulate
realistic and complex natural processes. We apply this capability to predict
how wildfires spread, a task made difficult by the unpredictable nature of fire
and the variety of environmental conditions it depends on. In this study, We
present the first denoising diffusion model for predicting wildfire spread, a
new kind of AI framework that learns to simulate fires not just as one fixed
outcome, but as a range of possible scenarios. By doing so, it accounts for the
inherent uncertainty of wildfire dynamics, a feature that traditional models
typically fail to represent. Unlike deterministic approaches that generate a
single prediction, our model produces ensembles of forecasts that reflect
physically meaningful distributions of where fire might go next. This
technology could help us develop smarter, faster, and more reliable tools for
anticipating wildfire behavior, aiding decision-makers in fire risk assessment
and response planning.

</details>


### [104] [Leveraging Genetic Algorithms for Efficient Demonstration Generation in Real-World Reinforcement Learning Environments](https://arxiv.org/abs/2507.00762)
*Tom Maus,Asma Atamna,Tobias Glasmachers*

Main category: cs.LG

TL;DR: The paper explores using Genetic Algorithms (GAs) to enhance Reinforcement Learning (RL) performance in industrial sorting tasks by generating expert demonstrations for RL agents.


<details>
  <summary>Details</summary>
Motivation: RL's broader deployment is limited by challenges like sample inefficiency and unstable learning. This study aims to improve RL performance using GA-generated demonstrations.

Method: GA-generated expert demonstrations are integrated into DQN replay buffers and used as warm-start trajectories for PPO agents. Comparisons are made with rule-based heuristics and brute-force optimization.

Result: GA-derived demonstrations significantly improve RL performance, with PPO agents achieving superior cumulative rewards when initialized with GA-generated data.

Conclusion: Hybrid learning paradigms combining heuristic search (GA) and data-driven RL show promise for real-world applications, with the framework made publicly available for further research.

Abstract: Reinforcement Learning (RL) has demonstrated significant potential in certain
real-world industrial applications, yet its broader deployment remains limited
by inherent challenges such as sample inefficiency and unstable learning
dynamics. This study investigates the utilization of Genetic Algorithms (GAs)
as a mechanism for improving RL performance in an industrially inspired sorting
environment. We propose a novel approach in which GA-generated expert
demonstrations are used to enhance policy learning. These demonstrations are
incorporated into a Deep Q-Network (DQN) replay buffer for experience-based
learning and utilized as warm-start trajectories for Proximal Policy
Optimization (PPO) agents to accelerate training convergence. Our experiments
compare standard RL training with rule-based heuristics, brute-force
optimization, and demonstration data, revealing that GA-derived demonstrations
significantly improve RL performance. Notably, PPO agents initialized with
GA-generated data achieved superior cumulative rewards, highlighting the
potential of hybrid learning paradigms, where heuristic search methods
complement data-driven RL. The utilized framework is publicly available and
enables further research into adaptive RL strategies for real-world
applications.

</details>


### [105] [BoltzNCE: Learning Likelihoods for Boltzmann Generation with Stochastic Interpolants and Noise Contrastive Estimation](https://arxiv.org/abs/2507.00846)
*Rishal Aggrwal,Jacky Chen,Nicholas M. Boffi,David Ryan Koes*

Main category: cs.LG

TL;DR: Proposes a method to efficiently sample from Boltzmann distributions using energy-based models and stochastic interpolants, avoiding costly Jacobian computations.


<details>
  <summary>Details</summary>
Motivation: Sampling from Boltzmann distributions is computationally expensive for large systems due to Jacobian calculations in traditional methods.

Method: Uses energy-based models trained with noise contrastive estimation and score matching, combined with stochastic interpolants for annealing.

Result: Achieves comparable free energy profiles and distributions to exact methods, with significant speedup in estimating free energy differences.

Conclusion: The method offers a practical and efficient alternative for sampling Boltzmann distributions in large molecular systems.

Abstract: Efficient sampling from the Boltzmann distribution defined by an energy
function is a key challenge in modeling physical systems such as molecules.
Boltzmann Generators tackle this by leveraging Continuous Normalizing Flows
that transform a simple prior into a distribution that can be reweighted to
match the Boltzmann distribution using sample likelihoods. However, obtaining
likelihoods requires computing costly Jacobians during integration, making it
impractical for large molecular systems. To overcome this, we propose learning
the likelihood of the generated distribution via an energy-based model trained
with noise contrastive estimation and score matching. By using stochastic
interpolants to anneal between the prior and generated distributions, we
combine both the objective functions to efficiently learn the density function.
On the alanine dipeptide system, we demonstrate that our method yields free
energy profiles and energy distributions comparable to those obtained with
exact likelihoods. Additionally, we show that free energy differences between
metastable states can be estimated accurately with orders-of-magnitude speedup.

</details>


### [106] [Quantum Approximate Optimization Algorithm for Spatiotemporal Forecasting of HIV Clusters](https://arxiv.org/abs/2507.00848)
*Don Roosan,Saif Nirzhor,Rubayat Khan,Fahmida Hai,Mohammad Rifat Haidar*

Main category: cs.LG

TL;DR: The paper introduces quantum-accelerated machine learning methods for HIV cluster detection and forecasting, outperforming classical techniques in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: The complexity of HIV epidemiological data necessitates advanced computational methods for precise analysis and intervention planning.

Method: The study uses quantum-accelerated techniques, including QAOA for clustering, a hybrid quantum-classical neural network for forecasting, and quantum Bayesian networks for causal analysis.

Result: QAOA achieved 92% accuracy in cluster detection in 1.6 seconds, the hybrid neural network predicted HIV prevalence with 94% accuracy, and quantum Bayesian analysis identified housing instability as a key factor.

Conclusion: Quantum-enhanced methods improve HIV surveillance precision and efficiency, offering insights for targeted interventions and resource allocation.

Abstract: HIV epidemiological data is increasingly complex, requiring advanced
computation for accurate cluster detection and forecasting. We employed
quantum-accelerated machine learning to analyze HIV prevalence at the ZIP-code
level using AIDSVu and synthetic SDoH data for 2022. Our approach compared
classical clustering (DBSCAN, HDBSCAN) with a quantum approximate optimization
algorithm (QAOA), developed a hybrid quantum-classical neural network for HIV
prevalence forecasting, and used quantum Bayesian networks to explore causal
links between SDoH factors and HIV incidence. The QAOA-based method achieved
92% accuracy in cluster detection within 1.6 seconds, outperforming classical
algorithms. Meanwhile, the hybrid quantum-classical neural network predicted
HIV prevalence with 94% accuracy, surpassing a purely classical counterpart.
Quantum Bayesian analysis identified housing instability as a key driver of HIV
cluster emergence and expansion, with stigma exerting a geographically variable
influence. These quantum-enhanced methods deliver greater precision and
efficiency in HIV surveillance while illuminating critical causal pathways.
This work can guide targeted interventions, optimize resource allocation for
PrEP, and address structural inequities fueling HIV transmission.

</details>


### [107] [Aligning Learning and Endogenous Decision-Making](https://arxiv.org/abs/2507.00851)
*Rares Cristian,Pavithra Harsha,Georgia Perakis,Brian Quanz*

Main category: cs.LG

TL;DR: The paper introduces an end-to-end method for training ML models to account for endogenous uncertainty and downstream effects in decision-making, including a robust optimization variant and a new class of two-stage stochastic problems.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of biased observations and lack of counterfactual information in decision-making, particularly in settings like pricing and assortment recommendations.

Method: Proposes an end-to-end ML training method with robust optimization to handle uncertainty, and introduces a two-stage stochastic optimization framework for information gathering and decision-making.

Result: Demonstrates improved performance over existing methods in computational experiments for pricing and inventory problems.

Conclusion: The framework effectively integrates ML predictions with decision-making under uncertainty, offering robust and near-optimal solutions.

Abstract: Many of the observations we make are biased by our decisions. For instance,
the demand of items is impacted by the prices set, and online checkout choices
are influenced by the assortments presented. The challenge in decision-making
under this setting is the lack of counterfactual information, and the need to
learn it instead. We introduce an end-to-end method under endogenous
uncertainty to train ML models to be aware of their downstream, enabling their
effective use in the decision-making stage. We further introduce a robust
optimization variant that accounts for uncertainty in ML models -- specifically
by constructing uncertainty sets over the space of ML models and optimizing
actions to protect against worst-case predictions. We prove guarantees that
this robust approach can capture near-optimal decisions with high probability
as a function of data. Besides this, we also introduce a new class of two-stage
stochastic optimization problems to the end-to-end learning framework that can
now be addressed through our framework. Here, the first stage is an
information-gathering problem to decide which random variable to poll and gain
information about before making a second-stage decision based off of it. We
present several computational experiments for pricing and inventory
assortment/recommendation problems. We compare against existing methods in
online learning/bandits/offline reinforcement learning and show our approach
has consistent improved performance over these. Just as in the endogenous
setting, the model's prediction also depends on the first-stage decision made.
While this decision does not affect the random variable in this setting, it
does affect the correct point forecast that should be made.

</details>


### [108] [Machine Learning-based Early Detection of Potato Sprouting Using Electrophysiological Signals](https://arxiv.org/abs/2507.00862)
*Davide Andreoletti,Aris Marcolongo,Natasa Sarafijanovic Djukic,Julien Roulet,Stefano Billeter,Andrzej Kurenda,Margot Visse-Mansiaux,Brice Dupuis,Carrol Annette Plummer,Beatrice Paoli,Omran Ayoub*

Main category: cs.LG

TL;DR: The paper proposes a machine learning-based method using electrophysiological signals to predict potato sprouting before visible signs appear, addressing the limitations of current visual identification methods.


<details>
  <summary>Details</summary>
Motivation: The ban on CIPC and the need for efficient storage management due to sprouting's impact on potato quality drive the development of an early prediction method.

Method: The approach involves preprocessing electrophysiological signals, extracting wavelet-domain features, and training supervised ML models with uncertainty quantification.

Result: The method shows promising performance in early sprouting detection, accurately predicting sprouting days for some potatoes with acceptable average error.

Conclusion: While results are promising, further refinements are needed to reduce prediction errors, especially maximum deviations.

Abstract: Accurately predicting potato sprouting before the emergence of any visual
signs is critical for effective storage management, as sprouting degrades both
the commercial and nutritional value of tubers. Effective forecasting allows
for the precise application of anti-sprouting chemicals (ASCs), minimizing
waste and reducing costs. This need has become even more pressing following the
ban on Isopropyl N-(3-chlorophenyl) carbamate (CIPC) or Chlorpropham due to
health and environmental concerns, which has led to the adoption of
significantly more expensive alternative ASCs. Existing approaches primarily
rely on visual identification, which only detects sprouting after morphological
changes have occurred, limiting their effectiveness for proactive management. A
reliable early prediction method is therefore essential to enable timely
intervention and improve the efficiency of post-harvest storage strategies,
where early refers to detecting sprouting before any visible signs appear. In
this work, we address the problem of early prediction of potato sprouting. To
this end, we propose a novel machine learning (ML)-based approach that enables
early prediction of potato sprouting using electrophysiological signals
recorded from tubers using proprietary sensors. Our approach preprocesses the
recorded signals, extracts relevant features from the wavelet domain, and
trains supervised ML models for early sprouting detection. Additionally, we
incorporate uncertainty quantification techniques to enhance predictions.
Experimental results demonstrate promising performance in the early detection
of potato sprouting by accurately predicting the exact day of sprouting for a
subset of potatoes and while showing acceptable average error across all
potatoes. Despite promising results, further refinements are necessary to
minimize prediction errors, particularly in reducing the maximum observed
deviations.

</details>


### [109] [NN-Former: Rethinking Graph Structure in Neural Architecture Representation](https://arxiv.org/abs/2507.00880)
*Ruihan Xu,Haokui Zhang,Yaowei Wang,Wei Zeng,Shiliang Zhang*

Main category: cs.LG

TL;DR: A novel neural predictor combines GNNs and transformers to address their individual shortcomings, leveraging sibling nodes and introducing new token and channel mixers for improved accuracy and latency prediction.


<details>
  <summary>Details</summary>
Motivation: The need for efficient neural network design and deployment drives the development of better predictors for attributes like accuracy and latency. Existing methods (GNNs and transformers) have limitations in feature representation and generalization.

Method: The proposed predictor integrates GNNs and transformers, focusing on sibling nodes in neural architecture topology. It introduces a new token mixer for siblings and a bidirectional graph isomorphism feed-forward network as a channel mixer.

Result: The approach consistently delivers strong performance in accuracy and latency prediction, offering insights for learning DAG topology.

Conclusion: The novel predictor effectively combines GNNs and transformers, addressing their weaknesses and improving performance, with code publicly available.

Abstract: The growing use of deep learning necessitates efficient network design and
deployment, making neural predictors vital for estimating attributes such as
accuracy and latency. Recently, Graph Neural Networks (GNNs) and transformers
have shown promising performance in representing neural architectures. However,
each of both methods has its disadvantages. GNNs lack the capabilities to
represent complicated features, while transformers face poor generalization
when the depth of architecture grows. To mitigate the above issues, we rethink
neural architecture topology and show that sibling nodes are pivotal while
overlooked in previous research. We thus propose a novel predictor leveraging
the strengths of GNNs and transformers to learn the enhanced topology. We
introduce a novel token mixer that considers siblings, and a new channel mixer
named bidirectional graph isomorphism feed-forward network. Our approach
consistently achieves promising performance in both accuracy and latency
prediction, providing valuable insights for learning Directed Acyclic Graph
(DAG) topology. The code is available at https://github.com/XuRuihan/NNFormer.

</details>


### [110] [TABASCO: A Fast, Simplified Model for Molecular Generation with Improved Physical Quality](https://arxiv.org/abs/2507.00899)
*Carlos Vonessen,Charles Harris,Miruna Cretu,Pietro Li*

Main category: cs.LG

TL;DR: TABASCO introduces a non-equivariant transformer for 3D molecular generation, simplifying architecture and improving speed and validity.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with physical plausibility despite strong inductive biases. TABASCO aims to relax these assumptions for better performance.

Method: Uses a standard non-equivariant transformer, treats atoms as sequences, and reconstructs bonds deterministically.

Result: Achieves state-of-the-art validity on GEOM-Drugs, 10x faster inference, and emergent rotational equivariance.

Conclusion: TABASCO offers a minimalist, high-throughput approach for specialized tasks like drug design.

Abstract: State-of-the-art models for 3D molecular generation are based on significant
inductive biases, SE(3), permutation equivariance to respect symmetry and graph
message-passing networks to capture local chemistry, yet the generated
molecules still struggle with physical plausibility. We introduce TABASCO which
relaxes these assumptions: The model has a standard non-equivariant transformer
architecture, treats atoms in a molecule as sequences and reconstructs bonds
deterministically after generation. The absence of equivariant layers and
message passing allows us to significantly simplify the model architecture and
scale data throughput. On the GEOM-Drugs benchmark TABASCO achieves
state-of-the-art PoseBusters validity and delivers inference roughly 10x faster
than the strongest baseline, while exhibiting emergent rotational equivariance
despite symmetry not being hard-coded. Our work offers a blueprint for training
minimalist, high-throughput generative models suited to specialised tasks such
as structure- and pharmacophore-based drug design. We provide a link to our
implementation at github.com/carlosinator/tabasco.

</details>


### [111] [Privacy-Preserving Quantized Federated Learning with Diverse Precision](https://arxiv.org/abs/2507.00920)
*Dang Qua Nguyen,Morteza Hashemi,Erik Perrins,Sergiy A. Vorobyov,David J. Love,Taejoon Kim*

Main category: cs.LG

TL;DR: The paper proposes a novel stochastic quantizer (SQ) for federated learning (FL) to address privacy risks and quantization heterogeneity, improving learning utility while ensuring differential privacy.


<details>
  <summary>Details</summary>
Motivation: FL faces challenges like privacy risks from unprotected model updates and reduced utility due to quantization heterogeneity. Existing solutions often tackle only one issue.

Method: Introduces a stochastic quantizer (SQ) for differential privacy and minimal quantization error, along with cluster size optimization and linear fusion for accurate model aggregation.

Result: Numerical simulations show improved privacy protection and learning utility over the conventional LaplaceSQ-FL algorithm.

Conclusion: The proposed SQ effectively balances privacy and utility in FL, addressing both privacy risks and quantization heterogeneity.

Abstract: Federated learning (FL) has emerged as a promising paradigm for distributed
machine learning, enabling collaborative training of a global model across
multiple local devices without requiring them to share raw data. Despite its
advancements, FL is limited by factors such as: (i) privacy risks arising from
the unprotected transmission of local model updates to the fusion center (FC)
and (ii) decreased learning utility caused by heterogeneity in model
quantization resolution across participating devices. Prior work typically
addresses only one of these challenges because maintaining learning utility
under both privacy risks and quantization heterogeneity is a non-trivial task.
In this paper, our aim is therefore to improve the learning utility of a
privacy-preserving FL that allows clusters of devices with different
quantization resolutions to participate in each FL round. Specifically, we
introduce a novel stochastic quantizer (SQ) that is designed to simultaneously
achieve differential privacy (DP) and minimum quantization error. Notably, the
proposed SQ guarantees bounded distortion, unlike other DP approaches. To
address quantization heterogeneity, we introduce a cluster size optimization
technique combined with a linear fusion approach to enhance model aggregation
accuracy. Numerical simulations validate the benefits of our approach in terms
of privacy protection and learning utility compared to the conventional
LaplaceSQ-FL algorithm.

</details>


### [112] [Understanding Generalization in Node and Link Prediction](https://arxiv.org/abs/2507.00927)
*Antonis Vasileiou,Timo Stoll,Christopher Morris*

Main category: cs.LG

TL;DR: The paper introduces a unified framework to analyze the generalization of MPNNs in node and link prediction, addressing gaps in understanding and unrealistic assumptions in prior work.


<details>
  <summary>Details</summary>
Motivation: Current understanding of MPNNs' generalization in node- and link-level predictions is limited, with existing studies often relying on unrealistic assumptions and neglecting graph structure.

Method: The authors propose a unified framework to analyze MPNNs' generalization, incorporating diverse architectural parameters, loss functions, and graph structure influence.

Result: Empirical results support theoretical insights, enhancing understanding of MPNNs' generalization in inductive and transductive settings.

Conclusion: The framework advances the comprehension of MPNNs' generalization and is applicable beyond graphs to other classification tasks.

Abstract: Using message-passing graph neural networks (MPNNs) for node and link
prediction is crucial in various scientific and industrial domains, which has
led to the development of diverse MPNN architectures. Besides working well in
practical settings, their ability to generalize beyond the training set remains
poorly understood. While some studies have explored MPNNs' generalization in
graph-level prediction tasks, much less attention has been given to node- and
link-level predictions. Existing works often rely on unrealistic i.i.d.\@
assumptions, overlooking possible correlations between nodes or links, and
assuming fixed aggregation and impractical loss functions while neglecting the
influence of graph structure. In this work, we introduce a unified framework to
analyze the generalization properties of MPNNs in inductive and transductive
node and link prediction settings, incorporating diverse architectural
parameters and loss functions and quantifying the influence of graph structure.
Additionally, our proposed generalization framework can be applied beyond
graphs to any classification task under the inductive or transductive setting.
Our empirical study supports our theoretical insights, deepening our
understanding of MPNNs' generalization capabilities in these tasks.

</details>


### [113] [Time Series Foundation Models are Flow Predictors](https://arxiv.org/abs/2507.00945)
*Massimiliano Luca,Ciro Beneduce,Bruno Lepri*

Main category: cs.LG

TL;DR: Time series foundation models (TSFMs) like Moirai and TimesFM outperform baselines in crowd flow prediction, achieving significant improvements in accuracy without spatial data.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of TSFMs for crowd flow prediction, especially in zero-shot settings with limited spatial context.

Method: Evaluated Moirai and TimesFM on three mobility datasets (Bike NYC, Taxi Beijing, Spanish OD flows) in a zero-shot setting, using only temporal data.

Result: TSFMs achieved up to 33% lower RMSE, 39% lower MAE, and 49% higher CPC compared to state-of-the-art methods.

Conclusion: TSFMs offer practical, scalable solutions for accurate flow prediction, even with limited data or missing spatial information.

Abstract: We investigate the effectiveness of time series foundation models (TSFMs) for
crowd flow prediction, focusing on Moirai and TimesFM. Evaluated on three
real-world mobility datasets-Bike NYC, Taxi Beijing, and Spanish national OD
flows-these models are deployed in a strict zero-shot setting, using only the
temporal evolution of each OD flow and no explicit spatial information. Moirai
and TimesFM outperform both statistical and deep learning baselines, achieving
up to 33% lower RMSE, 39% lower MAE and up to 49% higher CPC compared to
state-of-the-art competitors. Our results highlight the practical value of
TSFMs for accurate, scalable flow prediction, even in scenarios with limited
annotated data or missing spatial context.

</details>


### [114] [Benchmarking the Discovery Engine](https://arxiv.org/abs/2507.00964)
*Jack Foxabbott,Arush Tagade,Andrew Cusick,Robbie McCorkell,Leo McKee-Reid,Jugal Patel,Jamie Rumbelow,Jessica Rumbelow,Zohreh Shams*

Main category: cs.LG

TL;DR: The Discovery Engine is an automated system for scientific discovery, outperforming or matching peer-reviewed ML applications in predictive performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the Discovery Engine's capability as a general-purpose tool for automated, interpretable scientific modeling across diverse fields.

Method: Benchmarking the Discovery Engine against five peer-reviewed ML applications in medicine, materials science, social science, and environmental science.

Result: The Discovery Engine matches or exceeds prior predictive performance and provides deeper, actionable insights through interpretability artefacts.

Conclusion: The Discovery Engine shows potential as a new standard for automated, interpretable scientific modeling, enabling complex knowledge discovery.

Abstract: The Discovery Engine is a general purpose automated system for scientific
discovery, which combines machine learning with state-of-the-art ML
interpretability to enable rapid and robust scientific insight across diverse
datasets. In this paper, we benchmark the Discovery Engine against five recent
peer-reviewed scientific publications applying machine learning across
medicine, materials science, social science, and environmental science. In each
case, the Discovery Engine matches or exceeds prior predictive performance
while also generating deeper, more actionable insights through rich
interpretability artefacts. These results demonstrate its potential as a new
standard for automated, interpretable scientific modelling that enables complex
knowledge discovery from data.

</details>


### [115] [Scalable Feature Learning on Huge Knowledge Graphs for Downstream Machine Learning](https://arxiv.org/abs/2507.00965)
*Flix Lefebvre,Gal Varoquaux*

Main category: cs.LG

TL;DR: SEPAL is a scalable embedding algorithm for large knowledge graphs, optimizing embeddings on a core set of entities and propagating them globally, outperforming existing methods on downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Current embedding methods are limited by their focus on link prediction and scalability issues with large graphs.

Method: SEPAL optimizes embeddings on a small core of entities and propagates them globally via message passing.

Result: SEPAL outperforms previous methods on 46 downstream tasks and scales efficiently on commodity hardware.

Conclusion: SEPAL addresses scalability and downstream task performance, making it effective for large knowledge graphs.

Abstract: Many machine learning tasks can benefit from external knowledge. Large
knowledge graphs store such knowledge, and embedding methods can be used to
distill it into ready-to-use vector representations for downstream
applications. For this purpose, current models have however two limitations:
they are primarily optimized for link prediction, via local contrastive
learning, and they struggle to scale to the largest graphs due to GPU memory
limits. To address these, we introduce SEPAL: a Scalable Embedding Propagation
ALgorithm for large knowledge graphs designed to produce high-quality
embeddings for downstream tasks at scale. The key idea of SEPAL is to enforce
global embedding alignment by optimizing embeddings only on a small core of
entities, and then propagating them to the rest of the graph via message
passing. We evaluate SEPAL on 7 large-scale knowledge graphs and 46 downstream
machine learning tasks. Our results show that SEPAL significantly outperforms
previous methods on downstream tasks. In addition, SEPAL scales up its base
embedding model, enabling fitting huge knowledge graphs on commodity hardware.

</details>


### [116] [Reasoning as an Adaptive Defense for Safety](https://arxiv.org/abs/2507.00971)
*Taeyoun Kim,Fahim Tajwar,Aditi Raghunathan,Aviral Kumar*

Main category: cs.LG

TL;DR: TARS trains LLMs to adaptively reason about safety using RL, balancing safety and task completion, and improves robustness to attacks.


<details>
  <summary>Details</summary>
Motivation: To enhance LLM robustness against safety vulnerabilities by adaptive reasoning.

Method: Reinforcement learning with chain-of-thought traces, a balanced reward function, and strategic prompt mixing.

Result: Models show adaptive compute use, better safety-refusal trade-offs, and improved attack robustness.

Conclusion: TARS effectively trains LLMs to handle harmful requests through adaptive reasoning.

Abstract: Reasoning methods that adaptively allocate test-time compute have advanced
LLM performance on easy to verify domains such as math and code. In this work,
we study how to utilize this approach to train models that exhibit a degree of
robustness to safety vulnerabilities, and show that doing so can provide
benefits. We build a recipe called $\textit{TARS}$ (Training Adaptive Reasoners
for Safety), a reinforcement learning (RL) approach that trains models to
reason about safety using chain-of-thought traces and a reward signal that
balances safety with task completion. To build TARS, we identify three critical
design choices: (1) a "lightweight" warmstart SFT stage, (2) a mix of harmful,
harmless, and ambiguous prompts to prevent shortcut behaviors such as too many
refusals, and (3) a reward function to prevent degeneration of reasoning
capabilities during training. Models trained with TARS exhibit adaptive
behaviors by spending more compute on ambiguous queries, leading to better
safety-refusal trade-offs. They also internally learn to better distinguish
between safe and unsafe prompts and attain greater robustness to both white-box
(e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an
effective, open recipe for training LLMs against jailbreaks and harmful
requests by reasoning per prompt.

</details>


### [117] [Description of the Training Process of Neural Networks via Ergodic Theorem : Ghost nodes](https://arxiv.org/abs/2507.01003)
*Eun-Ji Park,Sangwon Yun*

Main category: cs.LG

TL;DR: A unified framework for accelerating deep neural network training via stochastic gradient descent, using Lyapunov exponents to distinguish convergence from stabilization, and introducing ghost nodes to bypass poor basins.


<details>
  <summary>Details</summary>
Motivation: To improve the training efficiency of deep neural networks by distinguishing genuine convergence from stabilization near saddle points and introducing auxiliary ghost nodes for better optimization paths.

Method: Analyzes the geometric landscape of the objective function, uses Lyapunov exponents for diagnostics, and extends classifiers with ghost nodes to provide extra descent directions.

Result: Ghost nodes reduce approximation error, enable bypassing poor basins, and collapse after convergence, preserving the original model's behavior.

Conclusion: The framework accelerates early-stage training while maintaining asymptotic performance, offering a principled architectural intervention.

Abstract: Recent studies have proposed interpreting the training process from an
ergodic perspective. Building on this foundation we present a unified framework
for understanding and accelerating the training of deep neural networks via
stochastic gradient descent. By analyzing the geometric landscape of the
objective function we introduce a practical diagnostic, the running estimate of
the largest Lyapunov exponent, which provably distinguishes genuine convergence
toward stable minimizers from mere statistical stabilization near saddle
points. We then propose a ghost category extension for standard classifiers
that adds auxiliary ghost output nodes so the model gains extra descent
directions that open a lateral corridor around narrow loss barriers and enable
the optimizer to bypass poor basins during the early training phase. We show
that this extension strictly reduces approximation error and that after
sufficient convergence the ghost dimensions collapse and the extended model's
invariant law coincides with that of the original and there exists a path in
the enlarged parameter space along which the total loss does not increase while
the original loss decreases by an arbitrary margin. Taken together these
results provide a principled architecture level intervention that accelerates
early stage trainability while preserving asymptotic behavior.

</details>


### [118] [ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention](https://arxiv.org/abs/2507.01004)
*Yuhong Chou,Zehao Liu,Ruijie Zhu,Xinyi Wan,Tianjian Li,Congying Chu,Qian Liu,Jibin Wu,Zejun Ma*

Main category: cs.LG

TL;DR: ZeCO introduces a Zero Communication Overhead sequence parallelism method for linear attention models, achieving near-linear scalability for long sequences.


<details>
  <summary>Details</summary>
Motivation: Existing Sequence Parallelism (SP) methods for linear attention models suffer from high communication overhead, limiting efficiency.

Method: ZeCO uses All-Scan, a new collective communication primitive, to eliminate communication overhead and distribute workloads efficiently.

Result: ZeCO achieves a 60% speedup on 256 GPUs with an 8M sequence length compared to SOTA SP methods.

Conclusion: ZeCO enables efficient training of next-generation LLMs on ultra-long sequences, overcoming previous bottlenecks.

Abstract: Linear attention mechanisms deliver significant advantages for Large Language
Models (LLMs) by providing linear computational complexity, enabling efficient
processing of ultra-long sequences (e.g., 1M context). However, existing
Sequence Parallelism (SP) methods, essential for distributing these workloads
across devices, become the primary bottleneck due to substantial communication
overhead. In this paper, we introduce ZeCO (Zero Communication Overhead)
sequence parallelism for linear attention models, a new SP method designed to
overcome these limitations and achieve end-to-end near-linear scalability for
long sequence training. For example, training a model with a 1M sequence length
across 64 devices using ZeCO takes roughly the same time as training with an
16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new
collective communication primitive. All-Scan provides each SP rank with
precisely the initial operator state it requires while maintaining a minimal
communication footprint, effectively eliminating communication overhead.
Theoretically, we prove the optimaity of ZeCO, showing that it introduces only
negligible time and space overhead. Empirically, we compare the communication
costs of different sequence parallelism strategies and demonstrate that
All-Scan achieves the fastest communication in SP scenarios. Specifically, on
256 GPUs with an 8M sequence length, ZeCO achieves a 60\% speedup compared to
the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a
clear path toward efficiently training next-generation LLMs on previously
intractable sequence lengths.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [119] [Moment Sampling in Video LLMs for Long-Form Video QA](https://arxiv.org/abs/2507.00033)
*Mustafa Chasmai,Gauri Jagatap,Gouthaman KV,Grant Van Horn,Subhransu Maji,Andrea Fanelli*

Main category: cs.CV

TL;DR: The paper introduces 'moment sampling,' a model-agnostic method to improve frame selection in Video LLMs for long-form VideoQA by using a text-to-video moment retrieval model.


<details>
  <summary>Details</summary>
Motivation: Existing Video LLMs struggle with long-range reasoning in longer videos due to suboptimal frame sub-sampling, which loses key frames or includes redundant ones.

Method: Proposes 'moment sampling,' leveraging a lightweight moment retrieval model to select the most relevant frames based on the question context.

Result: Demonstrates effectiveness through experiments on four long-form VideoQA datasets using four state-of-the-art Video LLMs.

Conclusion: The approach enhances long-form VideoQA performance by focusing on pertinent frames, improving accuracy and efficiency.

Abstract: Recent advancements in video large language models (Video LLMs) have
significantly advanced the field of video question answering (VideoQA). While
existing methods perform well on short videos, they often struggle with
long-range reasoning in longer videos. To scale Video LLMs for longer video
content, frame sub-sampling (selecting frames at regular intervals) is commonly
used. However, this approach is suboptimal, often leading to the loss of
crucial frames or the inclusion of redundant information from multiple similar
frames. Missing key frames impairs the model's ability to answer questions
accurately, while redundant frames lead the model to focus on irrelevant video
segments and increase computational resource consumption. In this paper, we
investigate the use of a general-purpose text-to-video moment retrieval model
to guide the frame sampling process. We propose "moment sampling", a novel,
model-agnostic approach that enables the model to select the most relevant
frames according to the context of the question. Specifically, we employ a
lightweight moment retrieval model to prioritize frame selection. By focusing
on the frames most pertinent to the given question, our method enhances
long-form VideoQA performance in Video LLMs. Through extensive experiments on
four long-form VideoQA datasets, using four state-of-the-art Video LLMs, we
demonstrate the effectiveness of the proposed approach.

</details>


### [120] [Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay](https://arxiv.org/abs/2507.00042)
*Xinrun Xu,Jianwen Yang,Qiuhong Zhang,Zhanbiao Lian,Zhiming Ding,Shan Jiang*

Main category: cs.CV

TL;DR: ER-EMU, an edge model update algorithm, addresses catastrophic forgetting in cloud-edge collaborative object detection by using adaptive experience replay and a novel Domain Distance Metric-based Experience Selection (DDM-ES) algorithm.


<details>
  <summary>Details</summary>
Motivation: Dynamic traffic environments with periodic variations (e.g., day/night) cause models to lose past knowledge when adapting to new data distributions. Existing methods fail to prioritize historical data effectively.

Method: ER-EMU uses a FIFO-managed experience buffer and DDM-ES, which employs MK-MMD to select dissimilar historical data, ensuring training diversity and knowledge retention.

Result: Experiments on the Bellevue traffic video dataset show ER-EMU improves performance in cloud-edge collaborative object detection frameworks.

Conclusion: ER-EMU effectively mitigates catastrophic forgetting by leveraging adaptive experience replay and domain-aware data selection.

Abstract: Continually adapting edge models in cloud-edge collaborative object detection
for traffic monitoring suffers from catastrophic forgetting, where models lose
previously learned knowledge when adapting to new data distributions. This is
especially problematic in dynamic traffic environments characterised by
periodic variations (e.g., day/night, peak hours), where past knowledge remains
valuable. Existing approaches like experience replay and visual prompts offer
some mitigation, but struggle to effectively prioritize and leverage historical
data for optimal knowledge retention and adaptation. Specifically, simply
storing and replaying all historical data can be inefficient, while treating
all historical experiences as equally important overlooks their varying
relevance to the current domain. This paper proposes ER-EMU, an edge model
update algorithm based on adaptive experience replay, to address these
limitations. ER-EMU utilizes a limited-size experience buffer managed using a
First-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based
Experience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel
maximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target
domains, prioritizing the selection of historical data that is most dissimilar
to the current target domain. This ensures training diversity and facilitates
the retention of knowledge from a wider range of past experiences, while also
preventing overfitting to the new domain. The experience buffer is also updated
using a simple random sampling strategy to maintain a balanced representation
of previous domains. Experiments on the Bellevue traffic video dataset,
involving repeated day/night cycles, demonstrate that ER-EMU consistently
improves the performance of several state-of-the-art cloud-edge collaborative
object detection frameworks.

</details>


### [121] [MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations](https://arxiv.org/abs/2507.00043)
*Mehmet Yigit Avci,Pedro Borges,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: MR-CLIP is a contrastive learning framework that aligns MRI images with DICOM metadata to learn contrast-aware representations, eliminating the need for manual labels.


<details>
  <summary>Details</summary>
Motivation: The lack of reliable and standardized metadata in MRI scans complicates tasks like image interpretation and retrieval. Robust contrast-aware representations are needed for advanced clinical applications.

Method: Proposes MR-CLIP, a multimodal contrastive learning framework trained on diverse clinical datasets to align images with metadata.

Result: MR-CLIP captures contrast variations effectively, enabling anatomy-invariant representations and excelling in cross-modal retrieval and contrast classification.

Conclusion: MR-CLIP offers a scalable solution for contrast-aware representations, with potential for broader clinical applications.

Abstract: Accurate interpretation of Magnetic Resonance Imaging scans in clinical
systems is based on a precise understanding of image contrast. This contrast is
primarily governed by acquisition parameters, such as echo time and repetition
time, which are stored in the DICOM metadata. To simplify contrast
identification, broad labels such as T1-weighted or T2-weighted are commonly
used, but these offer only a coarse approximation of the underlying acquisition
settings. In many real-world datasets, such labels are entirely missing,
leaving raw acquisition parameters as the only indicators of contrast. Adding
to this challenge, the available metadata is often incomplete, noisy, or
inconsistent. The lack of reliable and standardized metadata complicates tasks
such as image interpretation, retrieval, and integration into clinical
workflows. Furthermore, robust contrast-aware representations are essential to
enable more advanced clinical applications, such as achieving
modality-invariant representations and data harmonization. To address these
challenges, we propose MR-CLIP, a multimodal contrastive learning framework
that aligns MR images with their DICOM metadata to learn contrast-aware
representations, without relying on manual labels. Trained on a diverse
clinical dataset that spans various scanners and protocols, MR-CLIP captures
contrast variations across acquisitions and within scans, enabling
anatomy-invariant representations. We demonstrate its effectiveness in
cross-modal retrieval and contrast classification, highlighting its scalability
and potential for further clinical applications. The code and weights are
publicly available at https://github.com/myigitavci/MR-CLIP.

</details>


### [122] [HistoART: Histopathology Artifact Detection and Reporting Tool](https://arxiv.org/abs/2507.00044)
*Seyed Kahaki,Alexander R. Webber,Ghada Zamzmi,Adarsh Subbaswamy,Rucha Deshpande,Aldo Badano*

Main category: cs.CV

TL;DR: The paper proposes and compares three methods for detecting artifacts in Whole Slide Imaging (WSI) to improve cancer diagnostics, with the foundation model-based approach (FMA) outperforming others.


<details>
  <summary>Details</summary>
Motivation: Artifacts in WSI compromise image analysis, necessitating robust detection methods to enhance diagnostic accuracy.

Method: Three approaches are compared: (1) FMA with a fine-tuned UNI architecture, (2) DLA using ResNet50, and (3) KBA with handcrafted features.

Result: FMA achieved the highest AUROC (0.995), outperforming DLA (0.977) and KBA (0.940). A quality report scorecard was also developed.

Conclusion: FMA is the most effective for artifact detection in WSI, with potential to improve diagnostic reliability through actionable insights.

Abstract: In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to
digitize tissue specimens for detailed, high-resolution examination; however,
other diagnostic approaches, such as liquid biopsy and molecular testing, are
also utilized based on the cancer type and clinical context. While WSI has
revolutionized digital histopathology by enabling automated, precise analysis,
it remains vulnerable to artifacts introduced during slide preparation and
scanning. These artifacts can compromise downstream image analysis. To address
this challenge, we propose and compare three robust artifact detection
approaches for WSIs: (1) a foundation model-based approach (FMA) using a
fine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning
approach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach
(KBA) leveraging handcrafted features from texture, color, and frequency-based
metrics. The methods target six common artifact types: tissue folds,
out-of-focus regions, air bubbles, tissue damage, marker traces, and blood
contamination. Evaluations were conducted on 50,000+ image patches from diverse
scanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA
achieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]),
outperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978])
and the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into
actionable insights, we developed a quality report scorecard that quantifies
high-quality patches and visualizes artifact distributions.

</details>


### [123] [CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning](https://arxiv.org/abs/2507.00045)
*Ming Li,Chenguang Wang,Yijun Liang,Xiyao Wang,Yuhang Zhou,Xiyang Wu,Yuqing Zhang,Ruiyi Zhang,Tianyi Zhou*

Main category: cs.CV

TL;DR: The paper introduces 'CaughtCheating,' a challenging scenario where MLLMs like GPT-o3 fail, highlighting limitations in visual perception and reasoning compared to human detectives.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for MLLMs are no longer challenging due to their near-ceiling performance, prompting the need for harder tasks to evaluate their true capabilities.

Method: The study investigates hard scenarios, identifying 'CaughtCheating' as a failure case for GPT-o3, inspired by social media requests to detect suspicious clues in images.

Result: GPT-o3's performance drops to nearly zero in 'CaughtCheating,' revealing gaps in visual perception and reasoning.

Conclusion: 'CaughtCheating' serves as a valuable benchmark for advancing MLLMs toward human-level detective abilities, with practical applications.

Abstract: Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have
achieved near-ceiling scores on various existing benchmarks, motivating a
demand for more challenging test tasks. These MLLMs have been reported to excel
in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their
potential as a detective who can notice minuscule cues in an image and weave
them into coherent, situational explanations, leading to a reliable answer. But
can they match the performance of excellent human detectives? To answer this
question, we investigate some hard scenarios where GPT-o3 can still handle, and
find a common scenario where o3's performance drops to nearly zero, which we
name CaughtCheating. It is inspired by the social media requests that ask
others to detect suspicious clues from photos shared by the poster's partner.
We conduct extensive experiments and analysis to understand why existing MLLMs
lack sufficient capability to solve this kind of task. CaughtCheating provides
a class of challenging visual perception and reasoning tasks with great value
and practical usage. Success in these tasks paves the way for MLLMs to acquire
human-level detective perception and reasoning capabilities.

</details>


### [124] [Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process](https://arxiv.org/abs/2507.00046)
*Akshansh Mishra,Eyob Mesele Sefene,Shivraman Thapliyal*

Main category: cs.CV

TL;DR: An evolutionary computing-based image segmentation method using PSO is proposed for defect detection in AFSD processes, integrating gradient and distance transforms for novel visualizations.


<details>
  <summary>Details</summary>
Motivation: To improve defect detection and quality assessment in AFSD processes by leveraging advanced image segmentation and visualization techniques.

Method: Uses PSO for optimal thresholding, integrates gradient magnitude and distance transforms, and employs attention-weighted visualizations like self-attention maps and multi-channel visualization.

Result: PSO identified optimal thresholds (156-173), revealing subtle defects. Multi-channel visualizations provided cohesive interface quality metrics.

Conclusion: Attention-based analysis effectively identifies bonding issues in AFSD, offering quantitative tools for process optimization.

Abstract: This work proposes an evolutionary computing-based image segmentation
approach for analyzing soundness in Additive Friction Stir Deposition (AFSD)
processes. Particle Swarm Optimization (PSO) was employed to determine optimal
segmentation thresholds for detecting defects and features in multilayer AFSD
builds. The methodology integrates gradient magnitude analysis with distance
transforms to create novel attention-weighted visualizations that highlight
critical interface regions. Five AFSD samples processed under different
conditions were analyzed using multiple visualization techniques i.e.
self-attention maps, and multi-channel visualization. These complementary
approaches reveal subtle material transition zones and potential defect regions
which were not readily observable through conventional imaging. The PSO
algorithm automatically identified optimal threshold values (ranging from
156-173) for each sample, enabling precise segmentation of material interfaces.
The multi-channel visualization technique effectively combines boundary
information (red channel), spatial relationships (green channel), and material
density data (blue channel) into cohesive representations that quantify
interface quality. The results demonstrate that attention-based analysis
successfully identifies regions of incomplete bonding and inhomogeneities in
AFSD joints, providing quantitative metrics for process optimization and
quality assessment of additively manufactured components.

</details>


### [125] [AdaDeDup: Adaptive Hybrid Data Pruning for Efficient Large-Scale Object Detection Training](https://arxiv.org/abs/2507.00049)
*Feiyang Kang,Nadine Chang,Maying Shen,Marc T. Law,Rafid Mahmood,Ruoxi Jia,Jose M. Alvarez*

Main category: cs.CV

TL;DR: AdaDeDup is a hybrid data pruning method combining density-based and model-informed feedback to efficiently select informative subsets from large datasets, outperforming baselines with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Large datasets are computationally expensive and redundant, but existing pruning methods are either task-agnostic or inefficient.

Method: AdaDeDup integrates density-based pruning with model feedback, adjusting thresholds per cluster to balance redundancy and informativeness.

Result: AdaDeDup reduces performance degradation (e.g., 54% better than random sampling) and achieves near-original performance while pruning 20% of data.

Conclusion: AdaDeDup enhances data efficiency for large-scale training, offering a practical solution for model optimization.

Abstract: The computational burden and inherent redundancy of large-scale datasets
challenge the training of contemporary machine learning models. Data pruning
offers a solution by selecting smaller, informative subsets, yet existing
methods struggle: density-based approaches can be task-agnostic, while
model-based techniques may introduce redundancy or prove computationally
prohibitive. We introduce Adaptive De-Duplication (AdaDeDup), a novel hybrid
framework that synergistically integrates density-based pruning with
model-informed feedback in a cluster-adaptive manner. AdaDeDup first partitions
data and applies an initial density-based pruning. It then employs a proxy
model to evaluate the impact of this initial pruning within each cluster by
comparing losses on kept versus pruned samples. This task-aware signal
adaptively adjusts cluster-specific pruning thresholds, enabling more
aggressive pruning in redundant clusters while preserving critical data in
informative ones. Extensive experiments on large-scale object detection
benchmarks (Waymo, COCO, nuScenes) using standard models (BEVFormer, Faster
R-CNN) demonstrate AdaDeDup's advantages. It significantly outperforms
prominent baselines, substantially reduces performance degradation (e.g., over
54% versus random sampling on Waymo), and achieves near-original model
performance while pruning 20% of data, highlighting its efficacy in enhancing
data efficiency for large-scale model training. Code is open-sourced.

</details>


### [126] [VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models](https://arxiv.org/abs/2507.00052)
*Binesh Sadanandan,Vahid Behzadan*

Main category: cs.CV

TL;DR: VSF--Med is a framework for evaluating vulnerabilities in medical Vision Language Models (VLMs) using adversarial attacks, visual perturbations, and a scoring rubric.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic security evaluations for VLMs in clinical settings, ensuring their robustness against adversarial threats.

Method: VSF--Med combines text-prompt attack templates, imperceptible visual perturbations, and an eight-dimensional rubric evaluated by LLMs to score vulnerabilities.

Result: The framework tested 30,000 adversarial variants, revealing significant vulnerabilities in state-of-the-art VLMs, with Llama-3.2-11B-Vision-Instruct showing the highest risk.

Conclusion: VSF--Med provides a reproducible and comprehensive tool for assessing and improving the security of medical VLMs.

Abstract: Vision Language Models (VLMs) hold great promise for streamlining
labour-intensive medical imaging workflows, yet systematic security evaluations
in clinical settings remain scarce. We introduce VSF--Med, an end-to-end
vulnerability-scoring framework for medical VLMs that unites three novel
components: (i) a rich library of sophisticated text-prompt attack templates
targeting emerging threat vectors; (ii) imperceptible visual perturbations
calibrated by structural similarity (SSIM) thresholds to preserve clinical
realism; and (iii) an eight-dimensional rubric evaluated by two independent
judge LLMs, whose raw scores are consolidated via z-score normalization to
yield a 0--32 composite risk metric. Built entirely on publicly available
datasets and accompanied by open-source code, VSF--Med synthesizes over 30,000
adversarial variants from 5,000 radiology images and enables reproducible
benchmarking of any medical VLM with a single command. Our consolidated
analysis reports mean z-score shifts of $0.90\sigma$ for
persistence-of-attack-effects, $0.74\sigma$ for prompt-injection effectiveness,
and $0.63\sigma$ for safety-bypass success across state-of-the-art VLMs.
Notably, Llama-3.2-11B-Vision-Instruct exhibits a peak vulnerability increase
of $1.29\sigma$ for persistence-of-attack-effects, while GPT-4o shows increases
of $0.69\sigma$ for that same vector and $0.28\sigma$ for prompt-injection
attacks.

</details>


### [127] [MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding](https://arxiv.org/abs/2507.00068)
*Ziqi Zhong,Daniel Tang*

Main category: cs.CV

TL;DR: MANTA is a framework unifying visual and auditory inputs into a structured textual space for seamless processing with large language models, improving multi-modal learning by addressing alignment, synchronization, representation, and retrieval challenges.


<details>
  <summary>Details</summary>
Motivation: Current multi-modal learning approaches treat modalities separately, leading to inconsistencies in representation and reasoning. MANTA aims to unify and optimize multi-modal processing.

Method: MANTA uses information-theoretic optimization for semantic alignment, adaptive temporal synchronization, hierarchical content representation, and context-aware retrieval. It formalizes these in a mathematical framework.

Result: MANTA improves state-of-the-art models by up to 22.6% in accuracy for Long Video Question Answering, with notable gains in temporal reasoning (23.8%) and cross-modal understanding (25.1%).

Conclusion: MANTA introduces novel techniques for unifying multi-modal representations through structured text, setting new foundations for seamless multi-modal processing.

Abstract: While multi-modal learning has advanced significantly, current approaches
often treat modalities separately, creating inconsistencies in representation
and reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization
via Textual Alignment), a theoretically-grounded framework that unifies visual
and auditory inputs into a structured textual space for seamless processing
with large language models. MANTA addresses four key challenges: (1) semantic
alignment across modalities with information-theoretic optimization, (2)
adaptive temporal synchronization for varying information densities, (3)
hierarchical content representation for multi-scale understanding, and (4)
context-aware retrieval of sparse information from long sequences. We formalize
our approach within a rigorous mathematical framework, proving its optimality
for context selection under token constraints. Extensive experiments on the
challenging task of Long Video Question Answering show that MANTA improves
state-of-the-art models by up to 22.6% in overall accuracy, with particularly
significant gains (27.3%) on videos exceeding 30 minutes. Additionally, we
demonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)
and cross-modal understanding (25.1% improvement). Our framework introduces
novel density estimation techniques for redundancy minimization while
preserving rare signals, establishing new foundations for unifying multimodal
representations through structured text.

</details>


### [128] [An efficient plant disease detection using transfer learning approach](https://arxiv.org/abs/2507.00070)
*Bosubabu Sambana,Hillary Sunday Nnadi,Mohd Anas Wajid,Nwosu Ogochukwu Fidelia,Claudia Camacho-Zuiga,Henry Dozie Ajuzie,Edeh Michael Onyema*

Main category: cs.CV

TL;DR: The study proposes a system using YOLOv7 and YOLOv8 for automated plant disease detection, with YOLOv8 outperforming other methods.


<details>
  <summary>Details</summary>
Motivation: Early detection of plant diseases is critical to prevent crop damage and improve agricultural productivity.

Method: Utilizes transfer learning with YOLOv7 and YOLOv8, fine-tuned on plant leaf images to detect diseases like Powdery Mildew and Tomato mosaic virus.

Result: Achieved high performance metrics (mAP: 91.05, F1-score: 89.40, Precision: 91.22, Recall: 87.66), with YOLOv8 being superior.

Conclusion: The system offers a scalable, automated solution for early disease detection, enhancing crop yield and supporting sustainable agriculture.

Abstract: Plant diseases pose significant challenges to farmers and the agricultural
sector at large. However, early detection of plant diseases is crucial to
mitigating their effects and preventing widespread damage, as outbreaks can
severely impact the productivity and quality of crops. With advancements in
technology, there are increasing opportunities for automating the monitoring
and detection of disease outbreaks in plants. This study proposed a system
designed to identify and monitor plant diseases using a transfer learning
approach. Specifically, the study utilizes YOLOv7 and YOLOv8, two
state-ofthe-art models in the field of object detection. By fine-tuning these
models on a dataset of plant leaf images, the system is able to accurately
detect the presence of Bacteria, Fungi and Viral diseases such as Powdery
Mildew, Angular Leaf Spot, Early blight and Tomato mosaic virus. The model's
performance was evaluated using several metrics, including mean Average
Precision (mAP), F1-score, Precision, and Recall, yielding values of 91.05,
89.40, 91.22, and 87.66, respectively. The result demonstrates the superior
effectiveness and efficiency of YOLOv8 compared to other object detection
methods, highlighting its potential for use in modern agricultural practices.
The approach provides a scalable, automated solution for early any plant
disease detection, contributing to enhanced crop yield, reduced reliance on
manual monitoring, and supporting sustainable agricultural practices.

</details>


### [129] [Diffusion-Based Image Augmentation for Semantic Segmentation in Outdoor Robotics](https://arxiv.org/abs/2507.00153)
*Peter Mortimer,Mirko Maehlisch*

Main category: cs.CV

TL;DR: The paper proposes diffusion-based image augmentation to improve learning-based perception for autonomous vehicles in underrepresented environments like snow, using foundation models and semantic filtering.


<details>
  <summary>Details</summary>
Motivation: Outdoor robots face performance issues in dynamic environments (e.g., snow) due to underrepresented training data.

Method: Uses diffusion-based image augmentation with vision foundation models and open-vocabulary semantic segmentation to filter hallucinations.

Result: Enables fine-tuning models for specific environments (e.g., snow) by controlling semantic distribution in training data.

Conclusion: Diffusion-based augmentation can generalize to other underrepresented environments like sand or volcanic terrains.

Abstract: The performance of leaning-based perception algorithms suffer when deployed
in out-of-distribution and underrepresented environments. Outdoor robots are
particularly susceptible to rapid changes in visual scene appearance due to
dynamic lighting, seasonality and weather effects that lead to scenes
underrepresented in the training data of the learning-based perception system.
In this conceptual paper, we focus on preparing our autonomous vehicle for
deployment in snow-filled environments. We propose a novel method for
diffusion-based image augmentation to more closely represent the deployment
environment in our training data. Diffusion-based image augmentations rely on
the public availability of vision foundation models learned on internet-scale
datasets. The diffusion-based image augmentations allow us to take control over
the semantic distribution of the ground surfaces in the training data and to
fine-tune our model for its deployment environment. We employ open vocabulary
semantic segmentation models to filter out augmentation candidates that contain
hallucinations. We believe that diffusion-based image augmentations can be
extended to many other environments apart from snow surfaces, like sandy
environments and volcanic terrains.

</details>


### [130] [FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion](https://arxiv.org/abs/2507.00162)
*Yu Lu,Yi Yang*

Main category: cs.CV

TL;DR: FreeLong and FreeLong++ are training-free frameworks to improve long video generation by balancing frequency distribution, outperforming existing methods in temporal consistency and visual fidelity.


<details>
  <summary>Details</summary>
Motivation: Extending short-video generation models to longer videos causes degraded temporal consistency and visual fidelity, termed high-frequency distortion.

Method: Proposes FreeLong (dual-branch) and FreeLong++ (multi-branch) to blend global low-frequency and local high-frequency features for multi-band frequency fusion.

Result: Outperforms previous methods in longer video generation (4x, 8x native length) and supports coherent multi-prompt and controllable generation.

Conclusion: FreeLong++ enhances existing models without training, improving long video quality and enabling advanced generation tasks.

Abstract: Recent advances in video generation models have enabled high-quality short
video generation from text prompts. However, extending these models to longer
videos remains a significant challenge, primarily due to degraded temporal
consistency and visual fidelity. Our preliminary observations show that naively
applying short-video generation models to longer sequences leads to noticeable
quality degradation. Further analysis identifies a systematic trend where
high-frequency components become increasingly distorted as video length grows,
an issue we term high-frequency distortion. To address this, we propose
FreeLong, a training-free framework designed to balance the frequency
distribution of long video features during the denoising process. FreeLong
achieves this by blending global low-frequency features, which capture holistic
semantics across the full video, with local high-frequency features extracted
from short temporal windows to preserve fine details. Building on this,
FreeLong++ extends FreeLong dual-branch design into a multi-branch architecture
with multiple attention branches, each operating at a distinct temporal scale.
By arranging multiple window sizes from global to local, FreeLong++ enables
multi-band frequency fusion from low to high frequencies, ensuring both
semantic continuity and fine-grained motion dynamics across longer video
sequences. Without any additional training, FreeLong++ can be plugged into
existing video generation models (e.g. Wan2.1 and LTX-Video) to produce longer
videos with substantially improved temporal consistency and visual fidelity. We
demonstrate that our approach outperforms previous methods on longer video
generation tasks (e.g. 4x and 8x of native length). It also supports coherent
multi-prompt video generation with smooth scene transitions and enables
controllable video generation using long depth or pose sequences.

</details>


### [131] [SelvaBox: A high-resolution dataset for tropical tree crown detection](https://arxiv.org/abs/2507.00170)
*Hugo Baudchon,Arthur Ouaknine,Martin Weiss,Mlisande Teng,Thomas R. Walla,Antoine Caron-Guay,Christopher Pal,Etienne Lalibert*

Main category: cs.CV

TL;DR: SelvaBox is the largest open-access dataset for tropical tree crown detection, improving accuracy with high-resolution imagery and enabling competitive zero-shot performance.


<details>
  <summary>Details</summary>
Motivation: Tropical tree crown detection is crucial for ecosystem studies but hindered by scarce annotated datasets and complex crown structures.

Method: Introduces SelvaBox, a dataset with 83,000 labeled crowns, and benchmarks models using high-resolution inputs and multi-resolution training.

Result: Higher-resolution inputs boost accuracy, and SelvaBox-trained models achieve competitive zero-shot performance. Joint training ranks top across datasets.

Conclusion: SelvaBox advances tropical tree crown detection, with public release of dataset, code, and pre-trained weights.

Abstract: Detecting individual tree crowns in tropical forests is essential to study
these complex and crucial ecosystems impacted by human interventions and
climate change. However, tropical crowns vary widely in size, structure, and
pattern and are largely overlapping and intertwined, requiring advanced remote
sensing methods applied to high-resolution imagery. Despite growing interest in
tropical tree crown detection, annotated datasets remain scarce, hindering
robust model development. We introduce SelvaBox, the largest open-access
dataset for tropical tree crown detection in high-resolution drone imagery. It
spans three countries and contains more than 83,000 manually labeled crowns -
an order of magnitude larger than all previous tropical forest datasets
combined. Extensive benchmarks on SelvaBox reveal two key findings: (1)
higher-resolution inputs consistently boost detection accuracy; and (2) models
trained exclusively on SelvaBox achieve competitive zero-shot detection
performance on unseen tropical tree crown datasets, matching or exceeding
competing methods. Furthermore, jointly training on SelvaBox and three other
datasets at resolutions from 3 to 10 cm per pixel within a unified
multi-resolution pipeline yields a detector ranking first or second across all
evaluated datasets. Our dataset, code, and pre-trained weights are made public.

</details>


### [132] [Graph-Based Deep Learning for Component Segmentation of Maize Plants](https://arxiv.org/abs/2507.00182)
*J. I. Ruz,A. Mndez,E. Rodrguez*

Main category: cs.CV

TL;DR: A novel GNN-based deep learning architecture is proposed for segmenting plant components in LiDAR 3D point clouds, outperforming existing methods with over 80% IoU accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like 2D imaging and CNNs struggle with 3D data and plant component identification, prompting the need for a more effective solution.

Method: Uses GNNs with PCA-enhanced features, KNN for edge establishment, Edge-Conv layers for feature enhancement, and GAT for classifying plant components.

Result: Achieves over 80% IoU average, outperforming other point cloud-based models.

Conclusion: The graph-based deep learning approach significantly improves segmentation accuracy for plant component identification in 3D point clouds.

Abstract: In precision agriculture, one of the most important tasks when exploring crop
production is identifying individual plant components. There are several
attempts to accomplish this task by the use of traditional 2D imaging, 3D
reconstructions, and Convolutional Neural Networks (CNN). However, they have
several drawbacks when processing 3D data and identifying individual plant
components. Therefore, in this work, we propose a novel Deep Learning
architecture to detect components of individual plants on Light Detection and
Ranging (LiDAR) 3D Point Cloud (PC) data sets. This architecture is based on
the concept of Graph Neural Networks (GNN), and feature enhancing with
Principal Component Analysis (PCA). For this, each point is taken as a vertex
and by the use of a K-Nearest Neighbors (KNN) layer, the edges are established,
thus representing the 3D PC data set. Subsequently, Edge-Conv layers are used
to further increase the features of each point. Finally, Graph Attention
Networks (GAT) are applied to classify visible phenotypic components of the
plant, such as the leaf, stem, and soil. This study demonstrates that our
graph-based deep learning approach enhances segmentation accuracy for
identifying individual plant components, achieving percentages above 80% in the
IoU average, thus outperforming other existing models based on point clouds.

</details>


### [133] [Computer Vision for Objects used in Group Work: Challenges and Opportunities](https://arxiv.org/abs/2507.00224)
*Changsoo Jung,Sheikh Mannan,Jack Fitzgerald,Nathaniel Blanchard*

Main category: cs.CV

TL;DR: The paper introduces FiboSB, a 6D pose video dataset for collaborative K-12 education, evaluates existing methods, and improves object detection with YOLO11-x.


<details>
  <summary>Details</summary>
Motivation: Existing systems struggle to capture real-world interactions in collaborative educational settings, which 6D pose estimation could address.

Method: Introduces FiboSB dataset, tests four 6D pose methods, and fine-tunes YOLO11-x for better object detection.

Result: YOLO11-x achieves mAP_50 of 0.898, outperforming other methods on the challenging FiboSB dataset.

Conclusion: FiboSB and improved 6D pose estimation methods pave the way for better AI support in collaborative learning.

Abstract: Interactive and spatially aware technologies are transforming educational
frameworks, particularly in K-12 settings where hands-on exploration fosters
deeper conceptual understanding. However, during collaborative tasks, existing
systems often lack the ability to accurately capture real-world interactions
between students and physical objects. This issue could be addressed with
automatic 6D pose estimation, i.e., estimation of an object's position and
orientation in 3D space from RGB images or videos. For collaborative groups
that interact with physical objects, 6D pose estimates allow AI systems to
relate objects and entities. As part of this work, we introduce FiboSB, a novel
and challenging 6D pose video dataset featuring groups of three participants
solving an interactive task featuring small hand-held cubes and a weight scale.
This setup poses unique challenges for 6D pose because groups are holistically
recorded from a distance in order to capture all participants -- this, coupled
with the small size of the cubes, makes 6D pose estimation inherently
non-trivial. We evaluated four state-of-the-art 6D pose estimation methods on
FiboSB, exposing the limitations of current algorithms on collaborative group
work. An error analysis of these methods reveals that the 6D pose methods'
object detection modules fail. We address this by fine-tuning YOLO11-x for
FiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results,
and analysis of YOLO11-x errors presented here lay the groundwork for
leveraging the estimation of 6D poses in difficult collaborative contexts.

</details>


### [134] [VOCAL: Visual Odometry via ContrAstive Learning](https://arxiv.org/abs/2507.00243)
*Chi-Yao Huang,Zeel Bhatt,Yezhou Yang*

Main category: cs.CV

TL;DR: VOCAL is a novel visual odometry framework using contrastive learning and Bayesian inference to improve interpretability and flexibility in camera state estimation.


<details>
  <summary>Details</summary>
Motivation: Existing learning-based VO methods rely on rigid geometric assumptions, lacking interpretability and theoretical grounding in data-driven frameworks.

Method: VOCAL redefines VO as a label ranking problem, integrating Bayesian inference with representation learning to align visual features with camera states.

Result: VOCAL demonstrates improved interpretability and flexibility on the KITTI dataset, advancing spatial intelligence.

Conclusion: VOCAL offers a more general and explainable approach to visual odometry, addressing limitations of current methods.

Abstract: Breakthroughs in visual odometry (VO) have fundamentally reshaped the
landscape of robotics, enabling ultra-precise camera state estimation that is
crucial for modern autonomous systems. Despite these advances, many
learning-based VO techniques rely on rigid geometric assumptions, which often
fall short in interpretability and lack a solid theoretical basis within fully
data-driven frameworks. To overcome these limitations, we introduce VOCAL
(Visual Odometry via ContrAstive Learning), a novel framework that reimagines
VO as a label ranking challenge. By integrating Bayesian inference with a
representation learning framework, VOCAL organizes visual features to mirror
camera states. The ranking mechanism compels similar camera states to converge
into consistent and spatially coherent representations within the latent space.
This strategic alignment not only bolsters the interpretability of the learned
features but also ensures compatibility with multimodal data sources. Extensive
evaluations on the KITTI dataset highlight VOCAL's enhanced interpretability
and flexibility, pushing VO toward more general and explainable spatial
intelligence.

</details>


### [135] [Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition](https://arxiv.org/abs/2507.00248)
*Nikita Nikitin,Eugene Fomin*

Main category: cs.CV

TL;DR: A lightweight DNN framework for real-time sign language recognition, addressing data scarcity and computational costs, achieves 92% accuracy with low latency.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in sign language recognition like data scarcity, high computational costs, and frame rate discrepancies.

Method: Uses vectorized inputs encoding sign-specific parameters (handshape, palm orientation, etc.) and MediaPipe for landmark extraction. Optimized DNN architecture for edge deployment.

Result: Achieves 92% accuracy in isolated sign recognition with <10ms latency on edge devices. Integrated into 'slait ai' web app.

Conclusion: The framework successfully enables real-time, accurate sign language recognition with minimal resource usage.

Abstract: We present a novel framework for real-time sign language recognition using
lightweight DNNs trained on limited data. Our system addresses key challenges
in sign language recognition, including data scarcity, high computational
costs, and discrepancies in frame rates between training and inference
environments. By encoding sign language specific parameters, such as handshape,
palm orientation, movement, and location into vectorized inputs, and leveraging
MediaPipe for landmark extraction, we achieve highly separable input data
representations. Our DNN architecture, optimized for sub 10MB deployment,
enables accurate classification of 343 signs with less than 10ms latency on
edge devices. The data annotation platform 'slait data' facilitates structured
labeling and vector extraction. Our model achieved 92% accuracy in isolated
sign recognition and has been integrated into the 'slait ai' web application,
where it demonstrates stable inference.

</details>


### [136] [GazeTarget360: Towards Gaze Target Estimation in 360-Degree for Robot Perception](https://arxiv.org/abs/2507.00253)
*Zhuangzhuang Dai,Vincent Gbouna Zakka,Luis J. Manso,Chen Li*

Main category: cs.CV

TL;DR: A system called GazeTarget360 is proposed for 360-degree gaze target estimation in images, integrating eye-contact detection, vision encoding, and multi-scale fusion. It outperforms prior methods in accuracy and generalizability.


<details>
  <summary>Details</summary>
Motivation: Understanding human gaze targets is essential for human-robot interaction tasks like attention estimation and movement anticipation. Existing methods fail in out-of-frame or non-camera-facing scenarios.

Method: GazeTarget360 combines an eye-contact detector, pre-trained vision encoder, and multi-scale-fusion decoder for conditional inference.

Result: Cross-validation shows accurate and reliable gaze target predictions in unseen scenarios, making it highly efficient and deployable.

Conclusion: GazeTarget360 is a pioneering system for realistic gaze target prediction, with publicly available source code.

Abstract: Enabling robots to understand human gaze target is a crucial step to allow
capabilities in downstream tasks, for example, attention estimation and
movement anticipation in real-world human-robot interactions. Prior works have
addressed the in-frame target localization problem with data-driven approaches
by carefully removing out-of-frame samples. Vision-based gaze estimation
methods, such as OpenFace, do not effectively absorb background information in
images and cannot predict gaze target in situations where subjects look away
from the camera. In this work, we propose a system to address the problem of
360-degree gaze target estimation from an image in generalized visual scenes.
The system, named GazeTarget360, integrates conditional inference engines of an
eye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion
decoder. Cross validation results show that GazeTarget360 can produce accurate
and reliable gaze target predictions in unseen scenarios. This makes a
first-of-its-kind system to predict gaze targets from realistic camera footage
which is highly efficient and deployable. Our source code is made publicly
available at: https://github.com/zdai257/DisengageNet.

</details>


### [137] [VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos](https://arxiv.org/abs/2507.00261)
*Zhiyin Lin,Purvi Goel,Joy Yun,C. Karen Liu,Joao Pedro Araujo*

Main category: cs.CV

TL;DR: VirtualFencer extracts 3D fencing motion and strategy from video unsupervised, generating realistic fencing behavior for self-play, real fencer matching, and interactive duels.


<details>
  <summary>Details</summary>
Motivation: Fencing involves diverse, strategic motions influenced by opponent behavior, prompting data-driven modeling.

Method: VirtualFencer extracts 3D motion and strategy from video unsupervised, then generates fencing behavior.

Result: The system successfully fences against itself, real fencers from video, and interactively with professionals.

Conclusion: VirtualFencer demonstrates versatile, realistic fencing behavior generation from unsupervised video analysis.

Abstract: Fencing is a sport where athletes engage in diverse yet strategically logical
motions. While most motions fall into a few high-level actions (e.g. step,
lunge, parry), the execution can vary widely-fast vs. slow, large vs. small,
offensive vs. defensive. Moreover, a fencer's actions are informed by a
strategy that often comes in response to the opponent's behavior. This
combination of motion diversity with underlying two-player strategy motivates
the application of data-driven modeling to fencing. We present VirtualFencer, a
system capable of extracting 3D fencing motion and strategy from in-the-wild
video without supervision, and then using that extracted knowledge to generate
realistic fencing behavior. We demonstrate the versatile capabilities of our
system by having it (i) fence against itself (self-play), (ii) fence against a
real fencer's motion from online video, and (iii) fence interactively against a
professional fencer.

</details>


### [138] [Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections](https://arxiv.org/abs/2507.00263)
*Vignesh Ram Nithin Kappagantula,Shayan Hassantabar*

Main category: cs.CV

TL;DR: An efficient ML pipeline for organizing vacation rental images by room type and bed configuration, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The unorganized nature of VR property images makes it hard for travelers to understand spatial layouts and sleeping arrangements.

Method: A pipeline combining supervised room-type detection, overlap detection, clustering, and MLLM for bed-type mapping.

Result: Strong performance, surpassing contrastive learning and pretrained embedding methods.

Conclusion: The proposed pipeline effectively organizes VR images, aiding travelers in understanding property layouts.

Abstract: The rapid growth of vacation rental (VR) platforms has led to an increasing
volume of property images, often uploaded without structured categorization.
This lack of organization poses significant challenges for travelers attempting
to understand the spatial layout of a property, particularly when multiple
rooms of the same type are present. To address this issue, we introduce an
effective approach for solving the room scene discovery and grouping problem,
as well as identifying bed types within each bedroom group. This grouping is
valuable for travelers to comprehend the spatial organization, layout, and the
sleeping configuration of the property. We propose a computationally efficient
machine learning pipeline characterized by low latency and the ability to
perform effectively with sample-efficient learning, making it well-suited for
real-time and data-scarce environments. The pipeline integrates a supervised
room-type detection model, a supervised overlap detection model to identify the
overlap similarity between two images, and a clustering algorithm to group the
images of the same space together using the similarity scores. Additionally,
the pipeline maps each bedroom group to the corresponding bed types specified
in the property's metadata, based on the visual content present in the group's
images using a Multi-modal Large Language Model (MLLM) model. We evaluate the
aforementioned models individually and also assess the pipeline in its
entirety, observing strong performance that significantly outperforms
established approaches such as contrastive learning and clustering with
pretrained embeddings.

</details>


### [139] [Self-Supervised Multiview Xray Matching](https://arxiv.org/abs/2507.00287)
*Mohamad Dabboussi,Malo Huard,Yann Gousseau,Pietro Gori*

Main category: cs.CV

TL;DR: A self-supervised pipeline for multi-view X-ray correspondence, using synthetic data (DRRs) and transformers, improves fracture detection without manual annotations.


<details>
  <summary>Details</summary>
Motivation: Current AI methods lack robust multi-view X-ray correspondence, which is critical for clinical diagnosis.

Method: Uses synthetic X-ray views (DRRs) from CT volumes, trains a transformer model for many-to-many correspondence, and applies it to real data for fracture detection.

Result: Improved performance in multi-view fracture classification on both synthetic and real datasets.

Conclusion: Learning correspondences from synthetic data enhances real-world multi-view fracture detection, reducing reliance on manual annotations.

Abstract: Accurate interpretation of multi-view radiographs is crucial for diagnosing
fractures, muscular injuries, and other anomalies. While significant advances
have been made in AI-based analysis of single images, current methods often
struggle to establish robust correspondences between different X-ray views, an
essential capability for precise clinical evaluations. In this work, we present
a novel self-supervised pipeline that eliminates the need for manual annotation
by automatically generating a many-to-many correspondence matrix between
synthetic X-ray views. This is achieved using digitally reconstructed
radiographs (DRR), which are automatically derived from unannotated CT volumes.
Our approach incorporates a transformer-based training phase to accurately
predict correspondences across two or more X-ray views. Furthermore, we
demonstrate that learning correspondences among synthetic X-ray views can be
leveraged as a pretraining strategy to enhance automatic multi-view fracture
detection on real data. Extensive evaluations on both synthetic and real X-ray
datasets show that incorporating correspondences improves performance in
multi-view fracture classification.

</details>


### [140] [Reducing Variability of Multiple Instance Learning Methods for Digital Pathology](https://arxiv.org/abs/2507.00292)
*Ali Mammadov,Loc Le Folgoc,Guillaume Hocquet,Pietro Gori*

Main category: cs.CV

TL;DR: The paper introduces a Multi-Fidelity, Model Fusion strategy to reduce performance variability in Multiple Instance Learning (MIL) methods for WSI classification.


<details>
  <summary>Details</summary>
Motivation: High variability in MIL methods' performance (up to 10-15 AUC points) due to factors like weight initialization, batch ordering, and learning rate makes reliable comparisons difficult.

Method: Train multiple models for a few epochs, average the most stable ones based on validation scores, and apply this fusion strategy to existing MIL models.

Result: Validated on 2 datasets, 3 initialization strategies, and 5 MIL methods (2000+ experiments), the approach reduces variability, simplifies hyperparameter tuning, and improves reproducibility.

Conclusion: The proposed strategy effectively mitigates performance variability in MIL methods while maintaining computational efficiency.

Abstract: Digital pathology has revolutionized the field by enabling the digitization
of tissue samples into whole slide images (WSIs). However, the high resolution
and large size of WSIs present significant challenges when it comes to applying
Deep Learning models. As a solution, WSIs are often divided into smaller
patches with a global label (\textit{i.e., diagnostic}) per slide, instead of a
(too) costly pixel-wise annotation. By treating each slide as a bag of patches,
Multiple Instance Learning (MIL) methods have emerged as a suitable solution
for WSI classification. A major drawback of MIL methods is their high
variability in performance across different runs, which can reach up to 10-15
AUC points on the test set, making it difficult to compare different MIL
methods reliably. This variability mainly comes from three factors: i) weight
initialization, ii) batch (shuffling) ordering, iii) and learning rate. To
address that, we introduce a Multi-Fidelity, Model Fusion strategy for MIL
methods. We first train multiple models for a few epochs and average the most
stable and promising ones based on validation scores. This approach can be
applied to any existing MIL model to reduce performance variability. It also
simplifies hyperparameter tuning and improves reproducibility while maintaining
computational efficiency. We extensively validate our approach on WSI
classification tasks using 2 different datasets, 3 initialization strategies
and 5 MIL methods, for a total of more than 2000 experiments.

</details>


### [141] [Beyond Low-Rank Tuning: Model Prior-Guided Rank Allocation for Effective Transfer in Low-Data and Large-Gap Regimes](https://arxiv.org/abs/2507.00327)
*Chuyan Zhang,Kefan Wang,Yun Gu*

Main category: cs.CV

TL;DR: SR-LoRA improves LoRA by using stable rank for efficient rank allocation, enhancing adaptability without extra computational costs.


<details>
  <summary>Details</summary>
Motivation: Fixed low-rank structure in LoRA limits adaptability in domain gaps; current adaptive methods are computationally intensive.

Method: SR-LoRA leverages stable rank of pre-trained weights for layer-wise rank allocation.

Result: Outperforms adaptive LoRA variants in few-shot tasks with domain gaps, balancing performance and efficiency.

Conclusion: SR-LoRA provides a principled, efficient solution for rank allocation in LoRA, improving adaptability.

Abstract: Low-Rank Adaptation (LoRA) has proven effective in reducing computational
costs while maintaining performance comparable to fully fine-tuned foundation
models across various tasks. However, its fixed low-rank structure restricts
its adaptability in scenarios with substantial domain gaps, where higher ranks
are often required to capture domain-specific complexities. Current adaptive
LoRA methods attempt to overcome this limitation by dynamically expanding or
selectively allocating ranks, but these approaches frequently depend on
computationally intensive techniques such as iterative pruning, rank searches,
or additional regularization. To address these challenges, we introduce Stable
Rank-Guided Low-Rank Adaptation (SR-LoRA), a novel framework that utilizes the
stable rank of pre-trained weight matrices as a natural prior for layer-wise
rank allocation. By leveraging the stable rank, which reflects the intrinsic
dimensionality of the weights, SR-LoRA enables a principled and efficient
redistribution of ranks across layers, enhancing adaptability without incurring
additional search costs. Empirical evaluations on few-shot tasks with
significant domain gaps show that SR-LoRA consistently outperforms recent
adaptive LoRA variants, achieving a superior trade-off between performance and
efficiency. Our code is available at
https://github.com/EndoluminalSurgicalVision-IMR/SR-LoRA.

</details>


### [142] [MammoTracker: Mask-Guided Lesion Tracking in Temporal Mammograms](https://arxiv.org/abs/2507.00328)
*Xuan Liu,Yinhao Ren,Marc D. Ryser,Lars J. Grimm,Joseph Y. Lo*

Main category: cs.CV

TL;DR: MammoTracker, a mask-guided lesion tracking framework, improves automated lesion correspondence in mammograms using a coarse-to-fine strategy, outperforming baselines by 8%.


<details>
  <summary>Details</summary>
Motivation: Automated lesion tracking in temporal mammograms is challenging but crucial for breast cancer monitoring and early diagnosis.

Method: MammoTracker uses a coarse-to-fine strategy with global search, local search, and score refinement modules. A new dataset with 730 cases and 20000 lesion pairs supports training and evaluation.

Result: MammoTracker achieves 0.455 average overlap and 0.509 accuracy, outperforming baselines by 8%.

Conclusion: MammoTracker enhances CAD-based lesion progression analysis, with its dataset publicly available for further research.

Abstract: Accurate lesion tracking in temporal mammograms is essential for monitoring
breast cancer progression and facilitating early diagnosis. However, automated
lesion correspondence across exams remains a challenges in computer-aided
diagnosis (CAD) systems, limiting their effectiveness. We propose MammoTracker,
a mask-guided lesion tracking framework that automates lesion localization
across consecutively exams. Our approach follows a coarse-to-fine strategy
incorporating three key modules: global search, local search, and score
refinement. To support large-scale training and evaluation, we introduce a new
dataset with curated prior-exam annotations for 730 mass and calcification
cases from the public EMBED mammogram dataset, yielding over 20000 lesion
pairs, making it the largest known resource for temporal lesion tracking in
mammograms. Experimental results demonstrate that MammoTracker achieves 0.455
average overlap and 0.509 accuracy, surpassing baseline models by 8%,
highlighting its potential to enhance CAD-based lesion progression analysis.
Our dataset will be available at
https://gitlab.oit.duke.edu/railabs/LoGroup/mammotracker.

</details>


### [143] [Populate-A-Scene: Affordance-Aware Human Video Generation](https://arxiv.org/abs/2507.00334)
*Mengyi Shan,Zecheng He,Haoyu Ma,Felix Juefei-Xu,Peizhao Zhang,Tingbo Hou,Ching-Yao Chuang*

Main category: cs.CV

TL;DR: A study repurposes a video generation model to simulate interactive worlds by predicting human-environment interactions from a single scene image and text prompt, without explicit conditions.


<details>
  <summary>Details</summary>
Motivation: To explore the affordance perception potential of text-to-video models and their ability to simulate human-environment interactions without labeled datasets.

Method: Fine-tune a text-to-video model to insert a person into a scene based on a prompt, ensuring coherent behavior and scene affordance, using cross-attention heatmaps for analysis.

Result: The model successfully infers human affordance for video generation from a single image, demonstrating inherent affordance perception without labeled data.

Conclusion: Text-to-video models can be repurposed as interactive world simulators, uncovering affordance perception without explicit conditions or labeled datasets.

Abstract: Can a video generation model be repurposed as an interactive world simulator?
We explore the affordance perception potential of text-to-video models by
teaching them to predict human-environment interaction. Given a scene image and
a prompt describing human actions, we fine-tune the model to insert a person
into the scene, while ensuring coherent behavior, appearance, harmonization,
and scene affordance. Unlike prior work, we infer human affordance for video
generation (i.e., where to insert a person and how they should behave) from a
single scene image, without explicit conditions like bounding boxes or body
poses. An in-depth study of cross-attention heatmaps demonstrates that we can
uncover the inherent affordance perception of a pre-trained video model without
labeled affordance datasets.

</details>


### [144] [Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video](https://arxiv.org/abs/2507.00339)
*Alexander Moore,Amar Saini,Kylie Cancilla,Doug Poland,Carmen Carrano*

Main category: cs.CV

TL;DR: MOVi-MC-AC is a new dataset for amodal segmentation and content completion, featuring multi-camera views and ground-truth amodal content, with 5.8M object instances.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack multi-camera views and ground-truth amodal content, limiting research in object detection, tracking, and segmentation.

Method: Simulates cluttered scenes with household objects in multi-camera video, providing consistent object IDs and amodal labels.

Result: Introduces the largest amodal dataset with 5.8M object instances and first ground-truth amodal content.

Conclusion: MOVi-MC-AC advances computer vision research by addressing gaps in multi-camera settings and amodal content completion.

Abstract: Amodal segmentation and amodal content completion require using object priors
to estimate occluded masks and features of objects in complex scenes. Until
now, no data has provided an additional dimension for object context: the
possibility of multiple cameras sharing a view of a scene. We introduce
MOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the
largest amodal segmentation and first amodal content dataset to date. Cluttered
scenes of generic household objects are simulated in multi-camera video.
MOVi-MC-AC contributes to the growing literature of object detection, tracking,
and segmentation by including two new contributions to the deep learning for
computer vision world. Multiple Camera (MC) settings where objects can be
identified and tracked between various unique camera perspectives are rare in
both synthetic and real-world video. We introduce a new complexity to synthetic
video by providing consistent object ids for detections and segmentations
between both frames and multiple cameras each with unique features and motion
patterns on a single scene. Amodal Content (AC) is a reconstructive task in
which models predict the appearance of target objects through occlusions. In
the amodal segmentation literature, some datasets have been released with
amodal detection, tracking, and segmentation labels. While other methods rely
on slow cut-and-paste schemes to generate amodal content pseudo-labels, they do
not account for natural occlusions present in the modal masks. MOVi-MC-AC
provides labels for ~5.8 million object instances, setting a new maximum in the
amodal dataset literature, along with being the first to provide ground-truth
amodal content. The full dataset is available at
https://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,

</details>


### [145] [CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation](https://arxiv.org/abs/2507.00356)
*Zhiwei Yi,Xin Cheng,Jingyu Ma,Ruifei Zhu,Junwei Tian,Yuanxiu Zhou,Xinge Zhao,Hongzhe Li*

Main category: cs.CV

TL;DR: The paper introduces CGEarthEye, a high-resolution remote sensing vision foundation model (RSVFM) for Jilin-1 satellite data, leveraging a 2.1B-parameter framework and a novel 15M-scale SSL dataset (JLSSD) for superior performance in EO tasks.


<details>
  <summary>Details</summary>
Motivation: Limited access to ultra-high-resolution RS imagery hinders RSVFM progress. Jilin-1's abundant sub-meter data offers an opportunity to bridge this gap.

Method: Proposes CGEarthEye with five backbones (2.1B parameters) and JLSSD dataset (15M samples). Uses seasonal, augmentation, and masked patch token contrastive strategies for pre-training.

Result: Achieves SOTA performance on 10 benchmark datasets across four RS tasks, excelling in feature visualization, convergence, efficiency, and practical applications.

Conclusion: CGEarthEye's capabilities promise broader, efficient use of Jilin-1 data in EO applications.

Abstract: Deep learning methods have significantly advanced the development of
intelligent rinterpretation in remote sensing (RS), with foundational model
research based on large-scale pre-training paradigms rapidly reshaping various
domains of Earth Observation (EO). However, compared to the open accessibility
and high spatiotemporal coverage of medium-resolution data, the limited
acquisition channels for ultra-high-resolution optical RS imagery have
constrained the progress of high-resolution remote sensing vision foundation
models (RSVFM). As the world's largest sub-meter-level commercial RS satellite
constellation, the Jilin-1 constellation possesses abundant sub-meter-level
image resources. This study proposes CGEarthEye, a RSVFM framework specifically
designed for Jilin-1 satellite characteristics, comprising five backbones with
different parameter scales with totaling 2.1 billion parameters. To enhance the
representational capacity of the foundation model, we developed JLSSD, the
first 15-million-scale multi-temporal self-supervised learning (SSL) dataset
featuring global coverage with quarterly temporal sampling within a single
year, constructed through multi-level representation clustering and sampling
strategies. The framework integrates seasonal contrast, augmentation-based
contrast, and masked patch token contrastive strategies for pre-training.
Comprehensive evaluations across 10 benchmark datasets covering four typical RS
tasks demonstrate that the CGEarthEye consistently achieves state-of-the-art
(SOTA) performance. Further analysis reveals CGEarthEye's superior
characteristics in feature visualization, model convergence, parameter
efficiency, and practical mapping applications. This study anticipates that the
exceptional representation capabilities of CGEarthEye will facilitate broader
and more efficient applications of Jilin-1 data in traditional EO application.

</details>


### [146] [GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And Dynamic Density Control](https://arxiv.org/abs/2507.00363)
*Xingjun Wang,Lianlei Shan*

Main category: cs.CV

TL;DR: A method to improve 3D Gaussian Splatting (3DGS) by addressing initialization, optimization, and density control challenges, achieving high-fidelity real-time rendering.


<details>
  <summary>Details</summary>
Motivation: 3DGS relies on accurate initialization and struggles with optimizing unstructured Gaussian distributions and adaptive density control.

Method: Geometry-guided initialization, surface-aligned optimization, and dynamic adaptive density control.

Result: High-fidelity real-time rendering with improved visual quality, outperforming or matching state-of-the-art methods.

Conclusion: The proposed method enhances 3DGS, offering better geometric accuracy and visual fidelity in complex scenes.

Abstract: We propose a method to enhance 3D Gaussian Splatting (3DGS)~\cite{Kerbl2023},
addressing challenges in initialization, optimization, and density control.
Gaussian Splatting is an alternative for rendering realistic images while
supporting real-time performance, and it has gained popularity due to its
explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate
initialization and faces difficulties in optimizing unstructured Gaussian
distributions into ordered surfaces, with limited adaptive density control
mechanism proposed so far. Our first key contribution is a geometry-guided
initialization to predict Gaussian parameters, ensuring precise placement and
faster convergence. We then introduce a surface-aligned optimization strategy
to refine Gaussian placement, improving geometric accuracy and aligning with
the surface normals of the scene. Finally, we present a dynamic adaptive
density control mechanism that adjusts Gaussian density based on regional
complexity, for visual fidelity. These innovations enable our method to achieve
high-fidelity real-time rendering and significant improvements in visual
quality, even in complex scenes. Our method demonstrates comparable or superior
results to state-of-the-art methods, rendering high-fidelity images in real
time.

</details>


### [147] [An Improved U-Net Model for Offline handwriting signature denoising](https://arxiv.org/abs/2507.00365)
*Wanghui Xiao*

Main category: cs.CV

TL;DR: A study proposes an improved U-net-based model for denoising offline handwriting signatures, enhancing clarity and recognition robustness by incorporating discrete wavelet and PCA transforms.


<details>
  <summary>Details</summary>
Motivation: Handwriting signatures are crucial for identity recognition but often contain noise from historical documents, complicating forensic analysis.

Method: The study uses an improved U-net structure with discrete wavelet transform and PCA transform to suppress noise in signature samples.

Result: The model outperforms traditional methods, improving image clarity and supporting more reliable signature analysis.

Conclusion: The proposed model effectively enhances signature denoising, aiding forensic and recognition tasks.

Abstract: Handwriting signatures, as an important means of identity recognition, are
widely used in multiple fields such as financial transactions, commercial
contracts and personal affairs due to their legal effect and uniqueness. In
forensic science appraisals, the analysis of offline handwriting signatures
requires the appraiser to provide a certain number of signature samples, which
are usually derived from various historical contracts or archival materials.
However, the provided handwriting samples are often mixed with a large amount
of interfering information, which brings severe challenges to handwriting
identification work. This study proposes a signature handwriting denoising
model based on the improved U-net structure, aiming to enhance the robustness
of the signature recognition system. By introducing discrete wavelet transform
and PCA transform, the model's ability to suppress noise has been enhanced. The
experimental results show that this modelis significantly superior to the
traditional methods in denoising effect, can effectively improve the clarity
and readability of the signed images, and provide more reliable technical
support for signature analysis and recognition.

</details>


### [148] [Out-of-Distribution Detection with Adaptive Top-K Logits Integration](https://arxiv.org/abs/2507.00368)
*Hikaru Shijo,Yutaka Yoshihama,Kenichi Yadani,Norifumi Murata*

Main category: cs.CV

TL;DR: The paper introduces ATLI, a method for OOD detection by adaptively combining top-k logits, improving over MaxLogit and other methods.


<details>
  <summary>Details</summary>
Motivation: Neural networks often overconfidently predict OOD samples, risking safety. Detecting OOD data is essential.

Method: Proposes ATLI, which adaptively selects and combines top-k logits for OOD detection.

Result: ATLI reduces FPR95 by 6.73% vs. MaxLogit and 2.67% vs. other methods on ImageNet-1K.

Conclusion: ATLI effectively improves OOD detection by leveraging adaptive logit integration.

Abstract: Neural networks often make overconfident predictions from out-of-distribution
(OOD) samples. Detection of OOD data is therefore crucial to improve the safety
of machine learning. The simplest and most powerful method for OOD detection is
MaxLogit, which uses the model's maximum logit to provide an OOD score. We have
discovered that, in addition to the maximum logit, some other logits are also
useful for OOD detection. Based on this finding, we propose a new method called
ATLI (Adaptive Top-k Logits Integration), which adaptively determines effective
top-k logits that are specific to each model and combines the maximum logit
with the other top-k logits. In this study we evaluate our proposed method
using ImageNet-1K benchmark. Extensive experiments showed our proposed method
to reduce the false positive rate (FPR95) by 6.73% compared to the MaxLogit
approach, and decreased FPR95 by an additional 2.67% compared to other
state-of-the-art methods.

</details>


### [149] [PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance point cloud reconstruction via joint-channel NeRF with multi-view image instance matching](https://arxiv.org/abs/2507.00371)
*Xin Yang,Ruiming Du,Hanyang Huang,Jiayang Xie,Pengyao Xie,Leisen Fang,Ziyue Guo,Nanjun Jiang,Yu Jiang,Haiyan Cen*

Main category: cs.CV

TL;DR: PlantSegNeRF, a novel method for high-precision plant organ segmentation from multi-view RGB images, outperforms existing techniques in accuracy and generalizability.


<details>
  <summary>Details</summary>
Motivation: Existing plant point cloud segmentation methods lack resolution, accuracy, and generalizability across species.

Method: PlantSegNeRF uses 2D instance segmentation, instance matching, and implicit scene rendering (NeRF) to generate high-precision instance point clouds.

Result: Achieved significant improvements in precision, recall, F1-score, and IoU, with notable gains in instance segmentation metrics.

Conclusion: PlantSegNeRF advances organ-level phenotyping and provides high-quality 3D data for large-scale plant models.

Abstract: Organ segmentation of plant point clouds is a prerequisite for the
high-resolution and accurate extraction of organ-level phenotypic traits.
Although the fast development of deep learning has boosted much research on
segmentation of plant point clouds, the existing techniques for organ
segmentation still face limitations in resolution, segmentation accuracy, and
generalizability across various plant species. In this study, we proposed a
novel approach called plant segmentation neural radiance fields (PlantSegNeRF),
aiming to directly generate high-precision instance point clouds from
multi-view RGB image sequences for a wide range of plant species. PlantSegNeRF
performed 2D instance segmentation on the multi-view images to generate
instance masks for each organ with a corresponding ID. The multi-view instance
IDs corresponding to the same plant organ were then matched and refined using a
specially designed instance matching module. The instance NeRF was developed to
render an implicit scene, containing color, density, semantic and instance
information. The implicit scene was ultimately converted into high-precision
plant instance point clouds based on the volume density. The results proved
that in semantic segmentation of point clouds, PlantSegNeRF outperformed the
commonly used methods, demonstrating an average improvement of 16.1%, 18.3%,
17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the
second-best results on structurally complex datasets. More importantly,
PlantSegNeRF exhibited significant advantages in plant point cloud instance
segmentation tasks. Across all plant datasets, it achieved average improvements
of 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively.
This study extends the organ-level plant phenotyping and provides a
high-throughput way to supply high-quality 3D data for the development of
large-scale models in plant science.

</details>


### [150] [Efficient Depth- and Spatially-Varying Image Simulation for Defocus Deblur](https://arxiv.org/abs/2507.00372)
*Xinge Yang,Chuong Nguyen,Wenbin Wang,Kaizhang Kang,Wolfgang Heidrich,Xiaoxing Li*

Main category: cs.CV

TL;DR: Proposes a scalable dataset synthesis method for defocus and aberrations in fixed-focus cameras, improving generalization to real-world images.


<details>
  <summary>Details</summary>
Motivation: Addresses shallow depth of field and domain gaps in deep learning models for fixed-focus cameras.

Method: Efficient dataset synthesis modeling defocus and aberrations without real-world fine-tuning.

Result: Network trained on synthetic images generalizes well to high-resolution real-world images.

Conclusion: The method effectively bridges the domain gap for fixed-focus camera applications.

Abstract: Modern cameras with large apertures often suffer from a shallow depth of
field, resulting in blurry images of objects outside the focal plane. This
limitation is particularly problematic for fixed-focus cameras, such as those
used in smart glasses, where adding autofocus mechanisms is challenging due to
form factor and power constraints. Due to unmatched optical aberrations and
defocus properties unique to each camera system, deep learning models trained
on existing open-source datasets often face domain gaps and do not perform well
in real-world settings. In this paper, we propose an efficient and scalable
dataset synthesis approach that does not rely on fine-tuning with real-world
data. Our method simultaneously models depth-dependent defocus and spatially
varying optical aberrations, addressing both computational complexity and the
scarcity of high-quality RGB-D datasets. Experimental results demonstrate that
a network trained on our low resolution synthetic images generalizes
effectively to high resolution (12MP) real-world images across diverse scenes.

</details>


### [151] [Customizable ROI-Based Deep Image Compression](https://arxiv.org/abs/2507.00373)
*Ian Jin,Fanxin Xia,Feng Ding,Xinfeng Zhang,Meiqin Liu,Yao Zhao,Weisi Lin,Lili Meng*

Main category: cs.CV

TL;DR: A customizable ROI-based deep image compression method is proposed, allowing users to define ROI via text and adjust quality trade-offs between ROI and non-ROI.


<details>
  <summary>Details</summary>
Motivation: Existing ROI-based compression lacks flexibility for diverse user preferences in defining ROI and balancing quality between ROI and non-ROI.

Method: Introduces Text-controlled Mask Acquisition (TMA) for text-based ROI customization, Customizable Value Assign (CVA) for adjustable quality trade-offs, and Latent Mask Attention (LMA) for optimizing latent representations.

Result: The method effectively supports customizable ROI definition and quality trade-off management.

Conclusion: The proposed paradigm successfully addresses customization needs in ROI-based image compression.

Abstract: Region of Interest (ROI)-based image compression optimizes bit allocation by
prioritizing ROI for higher-quality reconstruction. However, as the users
(including human clients and downstream machine tasks) become more diverse,
ROI-based image compression needs to be customizable to support various
preferences. For example, different users may define distinct ROI or require
different quality trade-offs between ROI and non-ROI. Existing ROI-based image
compression schemes predefine the ROI, making it unchangeable, and lack
effective mechanisms to balance reconstruction quality between ROI and non-ROI.
This work proposes a paradigm for customizable ROI-based deep image
compression. First, we develop a Text-controlled Mask Acquisition (TMA) module,
which allows users to easily customize their ROI for compression by just
inputting the corresponding semantic \emph{text}. It makes the encoder
controlled by text. Second, we design a Customizable Value Assign (CVA)
mechanism, which masks the non-ROI with a changeable extent decided by users
instead of a constant one to manage the reconstruction quality trade-off
between ROI and non-ROI. Finally, we present a Latent Mask Attention (LMA)
module, where the latent spatial prior of the mask and the latent
Rate-Distortion Optimization (RDO) prior of the image are extracted and fused
in the latent space, and further used to optimize the latent representation of
the source image. Experimental results demonstrate that our proposed
customizable ROI-based deep image compression paradigm effectively addresses
the needs of customization for ROI definition and mask acquisition as well as
the reconstruction quality trade-off management between the ROI and non-ROI.

</details>


### [152] [MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis](https://arxiv.org/abs/2507.00377)
*Jianhao Xie,Ziang Zhang,Zhenyu Weng,Yuesheng Zhu,Guibo Luo*

Main category: cs.CV

TL;DR: MedDiff-FT fine-tunes a diffusion model for medical image generation, ensuring anatomical coherence and diversity, improving segmentation performance by 1% Dice score.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations in deep learning for medical image segmentation due to scarce high-quality training data and diffusion models' reliance on large datasets.

Method: Fine-tunes a diffusion foundation model, uses dynamic adaptive guiding masks, stochastic mask generators, and automated quality assessment.

Result: Improves segmentation performance by 1% Dice score on five medical datasets.

Conclusion: MedDiff-FT balances quality, diversity, and efficiency, offering a practical solution for medical data augmentation.

Abstract: Recent advancements in deep learning for medical image segmentation are often
limited by the scarcity of high-quality training data.While diffusion models
provide a potential solution by generating synthetic images, their
effectiveness in medical imaging remains constrained due to their reliance on
large-scale medical datasets and the need for higher image quality. To address
these challenges, we present MedDiff-FT, a controllable medical image
generation method that fine-tunes a diffusion foundation model to produce
medical images with structural dependency and domain specificity in a
data-efficient manner. During inference, a dynamic adaptive guiding mask
enforces spatial constraints to ensure anatomically coherent synthesis, while a
lightweight stochastic mask generator enhances diversity through hierarchical
randomness injection. Additionally, an automated quality assessment protocol
filters suboptimal outputs using feature-space metrics, followed by mask
corrosion to refine fidelity. Evaluated on five medical segmentation
datasets,MedDiff-FT's synthetic image-mask pairs improve SOTA method's
segmentation performance by an average of 1% in Dice score. The framework
effectively balances generation quality, diversity, and computational
efficiency, offering a practical solution for medical data augmentation. The
code is available at https://github.com/JianhaoXie1/MedDiff-FT.

</details>


### [153] [Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space](https://arxiv.org/abs/2507.00392)
*Yingping Liang,Yutao Hu,Wenqi Shao,Ying Fu*

Main category: cs.CV

TL;DR: The paper introduces Lift to Match (L2M), a two-stage framework for robust feature matching by leveraging 3D-aware features from single-view images.


<details>
  <summary>Details</summary>
Motivation: Existing feature matching methods rely on scarce multi-view datasets and lack 3D awareness, limiting generalization.

Method: L2M uses a 3D-aware feature encoder (stage 1) and a feature decoder with novel-view rendering (stage 2) for robust matching.

Result: The method achieves superior generalization in zero-shot benchmarks.

Conclusion: L2M effectively enhances feature matching robustness by incorporating 3D geometry from single-view images.

Abstract: Feature matching plays a fundamental role in many computer vision tasks, yet
existing methods heavily rely on scarce and clean multi-view image collections,
which constrains their generalization to diverse and challenging scenarios.
Moreover, conventional feature encoders are typically trained on single-view 2D
images, limiting their capacity to capture 3D-aware correspondences. In this
paper, we propose a novel two-stage framework that lifts 2D images to 3D space,
named as \textbf{Lift to Match (L2M)}, taking full advantage of large-scale and
diverse single-view images. To be specific, in the first stage, we learn a
3D-aware feature encoder using a combination of multi-view image synthesis and
3D feature Gaussian representation, which injects 3D geometry knowledge into
the encoder. In the second stage, a novel-view rendering strategy, combined
with large-scale synthetic data generation from single-view images, is employed
to learn a feature decoder for robust feature matching, thus achieving
generalization across diverse domains. Extensive experiments demonstrate that
our method achieves superior generalization across zero-shot evaluation
benchmarks, highlighting the effectiveness of the proposed framework for robust
feature matching.

</details>


### [154] [Few-shot Classification as Multi-instance Verification: Effective Backbone-agnostic Transfer across Domains](https://arxiv.org/abs/2507.00401)
*Xin Xu,Eibe Frank,Geoffrey Holmes*

Main category: cs.CV

TL;DR: The paper introduces the 'MIV-head', a novel approach for cross-domain few-shot learning without fine-tuning backbones, achieving competitive accuracy with lower adaptation costs.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of few-shot learning with frozen, 'black-box' backbones, where fine-tuning is impractical, by redefining the problem as multiple instance verification tasks.

Method: Proposes the MIV-head, a backbone-agnostic and efficient classification head trained on few-shot target domain data, tested on Meta-dataset benchmark with CNN and ViT backbones.

Result: MIV-head outperforms traditional classification heads and matches state-of-the-art adapter methods in accuracy while being more cost-effective.

Conclusion: The MIV-head is a viable solution for cross-domain few-shot learning without backbone fine-tuning, validated by empirical results and ablation studies.

Abstract: We investigate cross-domain few-shot learning under the constraint that
fine-tuning of backbones (i.e., feature extractors) is impossible or infeasible
-- a scenario that is increasingly common in practical use cases. Handling the
low-quality and static embeddings produced by frozen, "black-box" backbones
leads to a problem representation of few-shot classification as a series of
multiple instance verification (MIV) tasks. Inspired by this representation, we
introduce a novel approach to few-shot domain adaptation, named the "MIV-head",
akin to a classification head that is agnostic to any pretrained backbone and
computationally efficient. The core components designed for the MIV-head, when
trained on few-shot data from a target domain, collectively yield strong
performance on test data from that domain. Importantly, it does so without
fine-tuning the backbone, and within the "meta-testing" phase. Experimenting
under various settings and on an extension of the Meta-dataset benchmark for
cross-domain few-shot image classification, using representative off-the-shelf
convolutional neural network and vision transformer backbones pretrained on
ImageNet1K, we show that the MIV-head achieves highly competitive accuracy when
compared to state-of-the-art "adapter" (or partially fine-tuning) methods
applied to the same backbones, while incurring substantially lower adaptation
cost. We also find well-known "classification head" approaches lag far behind
in terms of accuracy. Ablation study empirically justifies the core components
of our approach. We share our code at https://github.com/xxweka/MIV-head.

</details>


### [155] [DiGA3D: Coarse-to-Fine Diffusional Propagation of Geometry and Appearance for Versatile 3D Inpainting](https://arxiv.org/abs/2507.00429)
*Jingyi Pan,Dan Xu,Qiong Luo*

Main category: cs.CV

TL;DR: DiGA3D is a unified 3D inpainting pipeline using diffusion models to address appearance and geometry inconsistencies, improving robustness and consistency across tasks.


<details>
  <summary>Details</summary>
Motivation: Challenges in unified 3D inpainting include lack of robustness in single-reference methods, appearance inconsistency in multi-view inpainting, and geometry inconsistency with significant changes.

Method: DiGA3D uses multiple reference views, an Attention Feature Propagation (AFP) mechanism, and a Texture-Geometry Score Distillation Sampling (TG-SDS) loss for consistency.

Result: Extensive experiments show DiGA3D's effectiveness in handling multiple 3D inpainting tasks.

Conclusion: DiGA3D provides a versatile and robust solution for 3D inpainting, addressing key challenges with diffusion models.

Abstract: Developing a unified pipeline that enables users to remove, re-texture, or
replace objects in a versatile manner is crucial for text-guided 3D inpainting.
However, there are still challenges in performing multiple 3D inpainting tasks
within a unified framework: 1) Single reference inpainting methods lack
robustness when dealing with views that are far from the reference view. 2)
Appearance inconsistency arises when independently inpainting multi-view images
with 2D diffusion priors; 3) Geometry inconsistency limits performance when
there are significant geometric changes in the inpainting regions. To tackle
these challenges, we introduce DiGA3D, a novel and versatile 3D inpainting
pipeline that leverages diffusion models to propagate consistent appearance and
geometry in a coarse-to-fine manner. First, DiGA3D develops a robust strategy
for selecting multiple reference views to reduce errors during propagation.
Next, DiGA3D designs an Attention Feature Propagation (AFP) mechanism that
propagates attention features from the selected reference views to other views
via diffusion models to maintain appearance consistency. Furthermore, DiGA3D
introduces a Texture-Geometry Score Distillation Sampling (TG-SDS) loss to
further improve the geometric consistency of inpainted 3D scenes. Extensive
experiments on multiple 3D inpainting tasks demonstrate the effectiveness of
our method. The project page is available at https://rorisis.github.io/DiGA3D/.

</details>


### [156] [MFH: Marrying Frequency Domain with Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2507.00430)
*Huanxin Yang,Qiwen Wang*

Main category: cs.CV

TL;DR: The paper introduces MFH, a method combining frequency domain analysis with HMER using DCT, improving recognition accuracy on complex mathematical expressions.


<details>
  <summary>Details</summary>
Motivation: HMER struggles with complex formula structures and layouts; frequency domain analysis is proposed to enhance recognition.

Method: Incorporates discrete cosine transform (DCT) into HMER, leveraging frequency domain information for structural analysis.

Result: MFH-CoMER achieves 61.66%/62.07%/63.72% accuracy on CROHME 2014/2016/2019 test sets.

Conclusion: Frequency domain analysis effectively enhances HMER performance, as demonstrated by consistent improvements across datasets.

Abstract: Handwritten mathematical expression recognition (HMER) suffers from complex
formula structures and character layouts in sequence prediction. In this paper,
we incorporate frequency domain analysis into HMER and propose a method that
marries frequency domain with HMER (MFH), leveraging the discrete cosine
transform (DCT). We emphasize the structural analysis assistance of frequency
information for recognizing mathematical formulas. When implemented on various
baseline models, our network exhibits a consistent performance enhancement,
demonstrating the efficacy of frequency domain information. Experiments show
that our MFH-CoMER achieves noteworthy accuracyrates of 61.66%/62.07%/63.72% on
the CROHME 2014/2016/2019 test sets. The source code is available at
https://github.com/Hryxyhe/MFH.

</details>


### [157] [Latent Posterior-Mean Rectified Flow for Higher-Fidelity Perceptual Face Restoration](https://arxiv.org/abs/2507.00447)
*Xin Luo,Menglin Zhang,Yunwei Lan,Tianyu Zhang,Rui Li,Chang Liu,Dong Liu*

Main category: cs.CV

TL;DR: Latent-PMRF improves face restoration by reformulating PMRF in VAE latent space, aligning better with human perception and achieving faster convergence.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of PMRF's pixel-space modeling in aligning with human perception for face restoration.

Method: Reformulate PMRF in VAE latent space, using latent representations for minimum distortion estimation and bounding distortion by VAE reconstruction error.

Result: Latent-PMRF outperforms existing methods in PD-tradeoff, achieves 5.79X speedup in FID, and shows superior reconstruction and restoration.

Conclusion: Latent-PMRF offers a more efficient and effective solution for face restoration, aligning better with human perception.

Abstract: The Perception-Distortion tradeoff (PD-tradeoff) theory suggests that face
restoration algorithms must balance perceptual quality and fidelity. To achieve
minimal distortion while maintaining perfect perceptual quality, Posterior-Mean
Rectified Flow (PMRF) proposes a flow based approach where source distribution
is minimum distortion estimations. Although PMRF is shown to be effective, its
pixel-space modeling approach limits its ability to align with human
perception, where human perception is defined as how humans distinguish between
two image distributions. In this work, we propose Latent-PMRF, which
reformulates PMRF in the latent space of a variational autoencoder (VAE),
facilitating better alignment with human perception during optimization. By
defining the source distribution on latent representations of minimum
distortion estimation, we bound the minimum distortion by the VAE's
reconstruction error. Moreover, we reveal the design of VAE is crucial, and our
proposed VAE significantly outperforms existing VAEs in both reconstruction and
restoration. Extensive experiments on blind face restoration demonstrate the
superiority of Latent-PMRF, offering an improved PD-tradeoff compared to
existing methods, along with remarkable convergence efficiency, achieving a
5.79X speedup over PMRF in terms of FID. Our code will be available as
open-source.

</details>


### [158] [ATSTrack: Enhancing Visual-Language Tracking by Aligning Temporal and Spatial Scales](https://arxiv.org/abs/2507.00454)
*Yihao Zhen,Qiang Wang,Yu Qiao,Liangqiong Qu,Huijie Fan*

Main category: cs.CV

TL;DR: ATSTrack addresses misalignment in Visual-Language Tracking by aligning temporal and spatial scales of visual and language inputs, improving feature modification.


<details>
  <summary>Details</summary>
Motivation: The misalignment between visual inputs and language descriptions due to target movement and inherent scale differences hinders tracking performance.

Method: ATSTrack decomposes language descriptions into phrases aligned with visual inputs and uses a Visual-Language token for fine-grained feature modification.

Result: ATSTrack achieves performance comparable to existing methods.

Conclusion: The proposed method effectively reduces misalignment and enhances tracking accuracy by addressing scale differences.

Abstract: A main challenge of Visual-Language Tracking (VLT) is the misalignment
between visual inputs and language descriptions caused by target movement.
Previous trackers have explored many effective feature modification methods to
preserve more aligned features. However, an important yet unexplored factor
ultimately hinders their capability, which is the inherent differences in the
temporal and spatial scale of information between visual and language inputs.
To address this issue, we propose a novel visual-language tracker that enhances
the effect of feature modification by \textbf{A}ligning \textbf{T}emporal and
\textbf{S}patial scale of different input components, named as
\textbf{ATSTrack}. Specifically, we decompose each language description into
phrases with different attributes based on their temporal and spatial
correspondence with visual inputs, and modify their features in a fine-grained
manner. Moreover, we introduce a Visual-Language token that comprises modified
linguistic information from the previous frame to guide the model to extract
visual features that are more relevant to language description, thereby
reducing the impact caused by the differences in spatial scale. Experimental
results show that our proposed ATSTrack achieves performance comparable to
existing methods. Our code will be released.

</details>


### [159] [Unleashing the Potential of All Test Samples: Mean-Shift Guided Test-Time Adaptation](https://arxiv.org/abs/2507.00462)
*Jizhou Han,Chenhao Ding,SongLin Dong,Yuhang He,Xinyuan Gao,Yihong Gong*

Main category: cs.CV

TL;DR: MS-TTA improves CLIP's feature representation using kNN Mean-Shift for better test-time adaptation without training.


<details>
  <summary>Details</summary>
Motivation: VLMs like CLIP struggle with distribution shifts at test time, and existing methods ignore low-confidence samples.

Method: Proposes MS-TTA, a training-free approach using kNN Mean-Shift to refine all test samples for better feature compactness and class separability.

Result: Outperforms state-of-the-art training-free TTA methods on OOD and cross-dataset benchmarks.

Conclusion: MS-TTA achieves robust adaptation without additional training, enhancing CLIP's performance.

Abstract: Visual-language models (VLMs) like CLIP exhibit strong generalization but
struggle with distribution shifts at test time. Existing training-free
test-time adaptation (TTA) methods operate strictly within CLIP's original
feature space, relying on high-confidence samples while overlooking the
potential of low-confidence ones. We propose MS-TTA, a training-free approach
that enhances feature representations beyond CLIP's space using a single-step
k-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA
improves feature compactness and class separability, leading to more stable
adaptation. Additionally, a cache of refined embeddings further enhances
inference by providing Mean Shift enhanced logits. Extensive evaluations on OOD
and cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms
state-of-the-art training-free TTA methods, achieving robust adaptation without
requiring additional training.

</details>


### [160] [Bisecle: Binding and Separation in Continual Learning for Video Language Understanding](https://arxiv.org/abs/2507.00469)
*Yue Tan,Xiaoqian Hu,Hao Xue,Celso De Melo,Flora D. Salim*

Main category: cs.CV

TL;DR: The paper introduces Bisecle, a method for video-language continual learning inspired by the hippocampus, addressing catastrophic forgetting and update conflict in VLMs.


<details>
  <summary>Details</summary>
Motivation: Real-world videos evolve continuously, requiring models to adapt without prohibitive computational costs. Existing methods struggle with forgetting and conflict in large multimodal models.

Method: Bisecle uses multi-directional supervision for cross-modal relationships and contrastive prompt learning to isolate task-specific knowledge, mimicking hippocampal memory mechanisms.

Result: Bisecle mitigates forgetting and improves cross-task generalization on VideoQA benchmarks.

Conclusion: The hippocampus-inspired approach enables robust and efficient continual learning for video understanding.

Abstract: Frontier vision-language models (VLMs) have made remarkable improvements in
video understanding tasks. However, real-world videos typically exist as
continuously evolving data streams (e.g., dynamic scenes captured by wearable
glasses), necessitating models to continually adapt to shifting data
distributions and novel scenarios. Considering the prohibitive computational
costs of fine-tuning models on new tasks, usually, a small subset of parameters
is updated while the bulk of the model remains frozen. This poses new
challenges to existing continual learning frameworks in the context of large
multimodal foundation models, i.e., catastrophic forgetting and update
conflict. While the foundation models struggle with parameter-efficient
continual learning, the hippocampus in the human brain has evolved highly
efficient mechanisms for memory formation and consolidation. Inspired by the
rapid Binding and pattern separation mechanisms in the hippocampus, in this
work, we propose Bisecle for video-language continual learning, where a
multi-directional supervision module is used to capture more cross-modal
relationships and a contrastive prompt learning scheme is designed to isolate
task-specific knowledge to facilitate efficient memory storage. Binding and
separation processes further strengthen the ability of VLMs to retain complex
experiences, enabling robust and efficient continual learning in video
understanding tasks. We perform a thorough evaluation of the proposed Bisecle,
demonstrating its ability to mitigate forgetting and enhance cross-task
generalization on several VideoQA benchmarks.

</details>


### [161] [ARIG: Autoregressive Interactive Head Generation for Real-time Conversations](https://arxiv.org/abs/2507.00472)
*Ying Guo,Xi Liu,Cheng Zhen,Pengfei Yan,Xiaoming Wei*

Main category: cs.CV

TL;DR: The paper proposes ARIG, an autoregressive frame-wise framework for real-time interactive head generation, improving realism by addressing limitations of clip-wise methods and explicit switching.


<details>
  <summary>Details</summary>
Motivation: Face-to-face communication drives the need for realistic virtual agents with seamless listening and speaking capabilities, overcoming issues like signal acquisition and smooth switching.

Method: ARIG uses a non-vector-quantized autoregressive process and diffusion for motion prediction, with dual-track dual-modal signals for interactive behavior understanding (IBU) and conversational state understanding (CSU).

Result: Extensive experiments confirm ARIG's effectiveness in real-time generation and interaction realism.

Conclusion: ARIG advances interactive head generation by enabling smoother, more realistic real-time responses through improved motion prediction and contextual understanding.

Abstract: Face-to-face communication, as a common human activity, motivates the
research on interactive head generation. A virtual agent can generate motion
responses with both listening and speaking capabilities based on the audio or
motion signals of the other user and itself. However, previous clip-wise
generation paradigm or explicit listener/speaker generator-switching methods
have limitations in future signal acquisition, contextual behavioral
understanding, and switching smoothness, making it challenging to be real-time
and realistic. In this paper, we propose an autoregressive (AR) based
frame-wise framework called ARIG to realize the real-time generation with
better interaction realism. To achieve real-time generation, we model motion
prediction as a non-vector-quantized AR process. Unlike discrete codebook-index
prediction, we represent motion distribution using diffusion procedure,
achieving more accurate predictions in continuous space. To improve interaction
realism, we emphasize interactive behavior understanding (IBU) and detailed
conversational state understanding (CSU). In IBU, based on dual-track
dual-modal signals, we summarize short-range behaviors through
bidirectional-integrated learning and perform contextual understanding over
long ranges. In CSU, we use voice activity signals and context features of IBU
to understand the various states (interruption, feedback, pause, etc.) that
exist in actual conversations. These serve as conditions for the final
progressive motion prediction. Extensive experiments have verified the
effectiveness of our model.

</details>


### [162] [ADAptation: Reconstruction-based Unsupervised Active Learning for Breast Ultrasound Diagnosis](https://arxiv.org/abs/2507.00474)
*Yaofei Duan,Yuhao Huang,Xin Yang,Luyi Han,Xinyu Xie,Zhiyuan Zhu,Ping He,Ka-Hou Chan,Ligang Cui,Sio-Kei Im,Dong Ni,Tao Tan*

Main category: cs.CV

TL;DR: Proposes ADAptation, an unsupervised Active Learning framework for Domain Adaptation, using diffusion models and dual-scoring to handle distribution shifts in medical imaging.


<details>
  <summary>Details</summary>
Motivation: Address performance drops in deep learning models due to distribution shifts between training and test domains, reducing annotation costs.

Method: Uses diffusion models for domain homogenization, hypersphere-constrained contrastive learning, and a dual-scoring mechanism for sample selection.

Result: Outperforms existing AL methods on breast ultrasound datasets, demonstrating effectiveness for clinical domain adaptation.

Conclusion: ADAptation is a robust solution for domain adaptation in medical imaging, balancing cost and performance.

Abstract: Deep learning-based diagnostic models often suffer performance drops due to
distribution shifts between training (source) and test (target) domains.
Collecting and labeling sufficient target domain data for model retraining
represents an optimal solution, yet is limited by time and scarce resources.
Active learning (AL) offers an efficient approach to reduce annotation costs
while maintaining performance, but struggles to handle the challenge posed by
distribution variations across different datasets. In this study, we propose a
novel unsupervised Active learning framework for Domain Adaptation, named
ADAptation, which efficiently selects informative samples from multi-domain
data pools under limited annotation budget. As a fundamental step, our method
first utilizes the distribution homogenization capabilities of diffusion models
to bridge cross-dataset gaps by translating target images into source-domain
style. We then introduce two key innovations: (a) a hypersphere-constrained
contrastive learning network for compact feature clustering, and (b) a
dual-scoring mechanism that quantifies and balances sample uncertainty and
representativeness. Extensive experiments on four breast ultrasound datasets
(three public and one in-house/multi-center) across five common deep
classifiers demonstrate that our method surpasses existing strong AL-based
competitors, validating its effectiveness and generalization for clinical
domain adaptation. The code is available at the anonymized link:
https://github.com/miccai25-966/ADAptation.

</details>


### [163] [Just Noticeable Difference for Large Multimodal Models](https://arxiv.org/abs/2507.00490)
*Zijian Chen,Yuan Tian,Yuze Sun,Wei Sun,Zicheng Zhang,Weisi Lin,Guangtao Zhai,Wenjun Zhang*

Main category: cs.CV

TL;DR: The paper introduces LMM-JND, a concept to quantify visual blind spots in large multimodal models (LMMs), and presents VPA-JND, a dataset to study these issues. It reveals shortcomings in LMMs like GPT-4o and InternVL2.5 compared to human vision.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic study on perceptual boundaries in LMMs and their potential security risks due to visual blind spots.

Method: Proposes LMM-JND and its pipeline, constructs the VPA-JND dataset (21.5k images, 489k stimuli, 12 distortion types), and evaluates LMMs on visual perception tasks.

Result: State-of-the-art LMMs exhibit significant visual blind spots, underperforming in basic comparison tasks compared to humans. Vision and language backbone designs correlate with performance.

Conclusion: LMM-JND is a valuable metric for studying LMMs, and addressing these blind spots is crucial for security and model refinement.

Abstract: Just noticeable difference (JND), the minimum change that the human visual
system (HVS) can perceive, has been studied for decades. Although recent work
has extended this line of research into machine vision, there has been a
scarcity of studies systematically exploring its perceptual boundaries across
multiple tasks and stimulus types, particularly in the current era of rapidly
advancing large multimodal models (LMMs), where studying the multifaceted
capabilities of models has become a mainstream focus. Moreover, the perceptual
defects of LMMs are not investigated thoroughly, resulting in potential
security issues and suboptimal response efficiency. In this paper, we take an
initial attempt and demonstrate that there exist significant visual blind spots
in current LMMs. To systemically quantify this characteristic, we propose a new
concept, {\bf LMM-JND}, together with its determination pipeline. Targeting
uncovering the behavior commonalities in HVS-aligned visual perception tasks,
we delve into several LMM families and construct a large-scale dataset, named
VPA-JND, which contains 21.5k reference images with over 489k stimuli across 12
distortion types, to facilitate LMM-JND studies. VPA-JND exposes areas where
state-of-the-art LMMs, including GPT-4o and the InternVL2.5 series, struggle
with basic comparison queries and fall significantly short of human-level
visual performance. We further explore the effects of vision and language
backbones and find a notable correlation between their design philosophy that
may instruct the future refinement of LMMs for their visual acuity. Together,
our research underscores the significance of LMM-JND as a unique perspective
for studying LMMs, and predictable LMM-JND is crucial for security concerns.
This work will be available at https://github.com/zijianchen98/LMM-JND.

</details>


### [164] [Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models](https://arxiv.org/abs/2507.00493)
*Fenil R. Doshi,Thomas Fel,Talia Konkle,George Alvarez*

Main category: cs.CV

TL;DR: The paper introduces the Configural Shape Score (CSS) to evaluate models' ability to recognize objects based on global part arrangement, beyond just local texture. It finds that high-CSS models, like DINOv2 and SigLIP2, rely on long-range interactions and outperform others. The study advocates for integrating both texture and shape cues in vision systems.


<details>
  <summary>Details</summary>
Motivation: Current vision models overly rely on local texture cues, neglecting global shape configurations, which limits their robustness and human-like performance. The paper aims to objectively measure configural competence in models.

Method: The CSS metric evaluates models using Object-Anagram pairs that preserve texture but permute part arrangement. The study tests 86 models, including convolutional, transformer, and hybrid architectures, and uses mechanistic probes like attention masks and representational-similarity analyses.

Result: High-CSS models (e.g., DINOv2, SigLIP2) excel in configural sensitivity, relying on long-range interactions. Mechanistic probes confirm a transition from local to global coding. The BagNet control fails, ruling out shortcuts. CSS also predicts other shape-dependent evaluations.

Conclusion: Robust vision systems should integrate both texture and shape cues, avoiding artificial trade-offs. Architectural and learning frameworks must support this dual reliance for human-like performance.

Abstract: Humans are able to recognize objects based on both local texture cues and the
configuration of object parts, yet contemporary vision models primarily harvest
local texture cues, yielding brittle, non-compositional features. Work on
shape-vs-texture bias has pitted shape and texture representations in
opposition, measuring shape relative to texture, ignoring the possibility that
models (and humans) can simultaneously rely on both types of cues, and
obscuring the absolute quality of both types of representation. We therefore
recast shape evaluation as a matter of absolute configural competence,
operationalized by the Configural Shape Score (CSS), which (i) measures the
ability to recognize both images in Object-Anagram pairs that preserve local
texture while permuting global part arrangement to depict different object
categories. Across 86 convolutional, transformer, and hybrid models, CSS (ii)
uncovers a broad spectrum of configural sensitivity with fully self-supervised
and language-aligned transformers -- exemplified by DINOv2, SigLIP2 and
EVA-CLIP -- occupying the top end of the CSS spectrum. Mechanistic probes
reveal that (iii) high-CSS networks depend on long-range interactions:
radius-controlled attention masks abolish performance showing a distinctive
U-shaped integration profile, and representational-similarity analyses expose a
mid-depth transition from local to global coding. A BagNet control remains at
chance (iv), ruling out "border-hacking" strategies. Finally, (v) we show that
configural shape score also predicts other shape-dependent evals. Overall, we
propose that the path toward truly robust, generalizable, and human-like vision
systems may not lie in forcing an artificial choice between shape and texture,
but rather in architectural and learning frameworks that seamlessly integrate
both local-texture and global configural shape.

</details>


### [165] [Laplace-Mamba: Laplace Frequency Prior-Guided Mamba-CNN Fusion Network for Image Dehazing](https://arxiv.org/abs/2507.00501)
*Yongzhen Wang,Liangliang Chen,Bingwen Hu,Heng Liu,Xiao-Ping Zhang,Mingqiang Wei*

Main category: cs.CV

TL;DR: Laplace-Mamba integrates Laplace frequency prior with a hybrid Mamba-CNN architecture for efficient image dehazing, outperforming state-of-the-art methods in quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: SSM-based approaches struggle with reconstructing localized structures and handling high-dimensional data, leading to poor recovery of fine image features.

Method: Uses Laplace decomposition to split the image into low-frequency (global texture) and high-frequency (edges/details) components, processed via dual pathways: SSMs for low-frequency and CNNs for high-frequency.

Result: Outperforms state-of-the-art methods in restoration quality and efficiency across multiple benchmarks.

Conclusion: Laplace-Mamba effectively addresses SSM limitations by combining Laplace prior and hybrid architecture, achieving superior performance.

Abstract: Recent progress in image restoration has underscored Spatial State Models
(SSMs) as powerful tools for modeling long-range dependencies, owing to their
appealing linear complexity and computational efficiency. However, SSM-based
approaches exhibit limitations in reconstructing localized structures and tend
to be less effective when handling high-dimensional data, frequently resulting
in suboptimal recovery of fine image features. To tackle these challenges, we
introduce Laplace-Mamba, a novel framework that integrates Laplace frequency
prior with a hybrid Mamba-CNN architecture for efficient image dehazing.
Leveraging the Laplace decomposition, the image is disentangled into
low-frequency components capturing global texture and high-frequency components
representing edges and fine details. This decomposition enables specialized
processing via dual parallel pathways: the low-frequency branch employs SSMs
for global context modeling, while the high-frequency branch utilizes CNNs to
refine local structural details, effectively addressing diverse haze scenarios.
Notably, the Laplace transformation facilitates information-preserving
downsampling of low-frequency components in accordance with the Nyquist theory,
thereby significantly improving computational efficiency. Extensive evaluations
across multiple benchmarks demonstrate that our method outperforms
state-of-the-art approaches in both restoration quality and efficiency. The
source code and pretrained models are available at
https://github.com/yz-wang/Laplace-Mamba.

</details>


### [166] [ExPaMoE: An Expandable Parallel Mixture of Experts for Continual Test-Time Adaptation](https://arxiv.org/abs/2507.00502)
*JianChao Zhao,Songlin Dong*

Main category: cs.CV

TL;DR: ExPaMoE is a novel framework for Continual Test-Time Adaptation (CTTA) that decouples domain-general and domain-specific knowledge using a dual-branch expert design and dynamically expands its expert pool to handle evolving distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Existing CTTA methods struggle with feature entanglement and catastrophic forgetting due to shared model parameters across domains.

Method: ExPaMoE uses an Expandable Parallel Mixture-of-Experts architecture with token-guided feature separation and a Spectral-Aware Online Domain Discriminator (SODD) for real-time distribution change detection.

Result: ExPaMoE outperforms prior methods on benchmarks like CIFAR-10C, CIFAR-100C, ImageNet-C, and Cityscapes-to-ACDC, and introduces ImageNet++ for long-term adaptation evaluation.

Conclusion: ExPaMoE demonstrates robustness, scalability, and resistance to forgetting, making it superior for CTTA scenarios.

Abstract: Continual Test-Time Adaptation (CTTA) aims to enable models to adapt
on-the-fly to a stream of unlabeled data under evolving distribution shifts.
However, existing CTTA methods typically rely on shared model parameters across
all domains, making them vulnerable to feature entanglement and catastrophic
forgetting in the presence of large or non-stationary domain shifts. To address
this limitation, we propose \textbf{ExPaMoE}, a novel framework based on an
\emph{Expandable Parallel Mixture-of-Experts} architecture. ExPaMoE decouples
domain-general and domain-specific knowledge via a dual-branch expert design
with token-guided feature separation, and dynamically expands its expert pool
based on a \emph{Spectral-Aware Online Domain Discriminator} (SODD) that
detects distribution changes in real-time using frequency-domain cues.
Extensive experiments demonstrate the superiority of ExPaMoE across diverse
CTTA scenarios. We evaluate our method on standard benchmarks including
CIFAR-10C, CIFAR-100C, ImageNet-C, and Cityscapes-to-ACDC for semantic
segmentation. Additionally, we introduce \textbf{ImageNet++}, a large-scale and
realistic CTTA benchmark built from multiple ImageNet-derived datasets, to
better reflect long-term adaptation under complex domain evolution. ExPaMoE
consistently outperforms prior arts, showing strong robustness, scalability,
and resistance to forgetting.

</details>


### [167] [LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for MLLMs](https://arxiv.org/abs/2507.00505)
*Haoran Lou,Chunxiao Fan,Ziyan Liu,Yuexin Wu,Xinxiang Wang*

Main category: cs.CV

TL;DR: LLaVA-SP enhances multimodal large language models by adding six spatial visual tokens to improve local feature representation, outperforming LLaVA-1.5 with minimal latency increase.


<details>
  <summary>Details</summary>
Motivation: CLIP-ViT struggles with local relationships between image patches, limiting detailed visual understanding in MLLMs.

Method: Introduces a novel Projector with convolutional kernels for spatial tokens, two model variants (Cropping and Pooling), and fine-tuning with LoRA.

Result: Achieves significant performance gains in multimodal benchmarks, surpassing LLaVA-1.5 with similar inference speed.

Conclusion: LLaVA-SP effectively addresses local feature representation in MLLMs, offering improved performance without compromising efficiency.

Abstract: The architecture of multimodal large language models (MLLMs) commonly
connects a vision encoder, often based on CLIP-ViT, to a large language model.
While CLIP-ViT works well for capturing global image features, it struggles to
model local relationships between adjacent patches, leading to weaker visual
representation, which in turn affects the detailed understanding ability of
MLLMs. To solve this, we propose LLaVA-SP, which \textbf{ only adds six spatial
visual tokens} to the original visual tokens to enhance the visual
representation. Our approach offers three key advantages: 1)We propose a novel
Projector, which uses convolutional kernels to derive visual spatial tokens
from ViT patch features, simulating two visual spatial ordering approaches:
``from central region to global" and ``from abstract to specific". Then, a
cross-attention mechanism is applied to fuse fine-grained visual information,
enriching the overall visual representation. 2) We present two model variants:
LLaVA-SP-Cropping, which focuses on detail features through progressive
cropping, and LLaVA-SP-Pooling, which captures global semantics through
adaptive pooling, enabling the model to handle diverse visual understanding
tasks. 3) Extensive experiments show that LLaVA-SP, fine-tuned with LoRA,
achieves significant performance improvements across various multimodal
benchmarks, outperforming the state-of-the-art LLaVA-1.5 model in multiple
tasks with nearly identical inference latency. The code and models are
available at
\href{https://github.com/CnFaker/LLaVA-SP}{\texttt{https://github.com/CnFaker/LLaVA-SP}}.

</details>


### [168] [SCING:Towards More Efficient and Robust Person Re-Identification through Selective Cross-modal Prompt Tuning](https://arxiv.org/abs/2507.00506)
*Yunfei Xie,Yuxuan Cheng,Juncheng Wu,Haoyu Zhang,Yuyin Zhou,Shoudong Han*

Main category: cs.CV

TL;DR: A framework called SCING improves cross-modal alignment in person re-identification by using selective visual prompts and perturbation-driven consistency, achieving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for adapting vision-language models in ReID tasks are complex or ignore cross-modal interaction, leading to inefficiency or poor alignment.

Method: SCING introduces Selective Visual Prompt Fusion (SVIP) for dynamic feature injection and Perturbation-Driven Consistency Alignment (PDCA) for robust feature alignment under perturbations.

Result: The method outperforms on benchmarks like Market1501 and DukeMTMC-ReID, balancing performance and computational cost.

Conclusion: SCING offers a simple, effective solution for cross-modal alignment in ReID, eliminating heavy adapters while maintaining efficiency.

Abstract: Recent advancements in adapting vision-language pre-training models like CLIP
for person re-identification (ReID) tasks often rely on complex adapter design
or modality-specific tuning while neglecting cross-modal interaction, leading
to high computational costs or suboptimal alignment. To address these
limitations, we propose a simple yet effective framework named Selective
Cross-modal Prompt Tuning (SCING) that enhances cross-modal alignment and
robustness against real-world perturbations. Our method introduces two key
innovations: Firstly, we proposed Selective Visual Prompt Fusion (SVIP), a
lightweight module that dynamically injects discriminative visual features into
text prompts via a cross-modal gating mechanism. Moreover, the proposed
Perturbation-Driven Consistency Alignment (PDCA) is a dual-path training
strategy that enforces invariant feature alignment under random image
perturbations by regularizing consistency between original and augmented
cross-modal embeddings. Extensive experiments are conducted on several popular
benchmarks covering Market1501, DukeMTMC-ReID, Occluded-Duke, Occluded-REID,
and P-DukeMTMC, which demonstrate the impressive performance of the proposed
method. Notably, our framework eliminates heavy adapters while maintaining
efficient inference, achieving an optimal trade-off between performance and
computational overhead. The code will be released upon acceptance.

</details>


### [169] [Topology-Constrained Learning for Efficient Laparoscopic Liver Landmark Detection](https://arxiv.org/abs/2507.00519)
*Ruize Cui,Jiaan Zhang,Jialun Pei,Kai Wang,Pheng-Ann Heng,Jing Qin*

Main category: cs.CV

TL;DR: TopoNet is a topology-constrained learning framework for laparoscopic liver landmark detection, combining RGB texture and depth-informed topological structures with a boundary-aware fusion module and topological constraints for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Liver landmarks are critical for minimizing surgical risk, but their tubular structures and dynamic deformations make automatic detection challenging.

Method: TopoNet uses a snake-CNN dual-path encoder for RGB and depth data, a boundary-aware topology fusion module, and topological constraint losses (center-line and persistence).

Result: TopoNet achieves high accuracy and computational efficiency on L3D and P2ILF datasets, showing clinical potential.

Conclusion: TopoNet's innovative approach effectively addresses the challenges of liver landmark detection, offering promising clinical applications.

Abstract: Liver landmarks provide crucial anatomical guidance to the surgeon during
laparoscopic liver surgery to minimize surgical risk. However, the tubular
structural properties of landmarks and dynamic intraoperative deformations pose
significant challenges for automatic landmark detection. In this study, we
introduce TopoNet, a novel topology-constrained learning framework for
laparoscopic liver landmark detection. Our framework adopts a snake-CNN
dual-path encoder to simultaneously capture detailed RGB texture information
and depth-informed topological structures. Meanwhile, we propose a
boundary-aware topology fusion (BTF) module, which adaptively merges RGB-D
features to enhance edge perception while preserving global topology.
Additionally, a topological constraint loss function is embedded, which
contains a center-line constraint loss and a topological persistence loss to
ensure homotopy equivalence between predictions and labels. Extensive
experiments on L3D and P2ILF datasets demonstrate that TopoNet achieves
outstanding accuracy and computational complexity, highlighting the potential
for clinical applications in laparoscopic liver surgery. Our code will be
available at https://github.com/cuiruize/TopoNet.

</details>


### [170] [Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving](https://arxiv.org/abs/2507.00525)
*Djamahl Etchegaray,Yuxia Fu,Zi Huang,Yadan Luo*

Main category: cs.CV

TL;DR: Box-QAymo is a dataset and benchmark for evaluating VLMs on spatial and temporal reasoning via user-specified bounding boxes, addressing gaps in real-world interpretability for autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Current VLMs lack the ability to handle localized, user-driven queries in real-world driving scenarios, limiting their interpretability and safety.

Method: Introduces Box-QAymo, a dataset with hierarchical evaluation (binary checks, attribute prediction, motion understanding, spatiotemporal reasoning) and rigorous quality control.

Result: Reveals significant limitations in VLMs for perception-based queries, highlighting the gap in real-world performance.

Conclusion: Box-QAymo lays the groundwork for more robust and interpretable autonomous driving systems by improving VLM communication with users.

Abstract: Interpretable communication is essential for safe and trustworthy autonomous
driving, yet current vision-language models (VLMs) often operate under
idealized assumptions and struggle to capture user intent in real-world
scenarios. Existing driving-oriented VQA datasets are limited to full-scene
descriptions or waypoint prediction, preventing the assessment of whether VLMs
can respond to localized user-driven queries. We introduce Box-QAymo, a
box-referring dataset and benchmark designed to both evaluate and finetune VLMs
on spatial and temporal reasoning over user-specified objects. Users express
intent by drawing bounding boxes, offering a fast and intuitive interface for
focused queries in complex scenes. Specifically, we propose a hierarchical
evaluation protocol that begins with binary sanity-check questions to assess
basic model capacities, and progresses to (1) attribute prediction for
box-referred objects, (2) motion understanding of target instances, and (3)
spatiotemporal motion reasoning over inter-object dynamics across frames. To
support this, we crowd-sourced fine-grained object classes and visual
attributes that reflect the complexity drivers encounter, and extract object
trajectories to construct temporally grounded QA pairs. Rigorous quality
control through negative sampling, temporal consistency checks, and
difficulty-aware balancing guarantee dataset robustness and diversity. Our
comprehensive evaluation reveals significant limitations in current VLMs when
queried about perception questions, highlighting the gap in achieving
real-world performance. This work provides a foundation for developing more
robust and interpretable autonomous driving systems that can communicate
effectively with users under real-world conditions. Project page and dataset
are available at https://djamahl99.github.io/qaymo-pages/.

</details>


### [171] [Not All Attention Heads Are What You Need: Refining CLIP's Image Representation with Attention Ablation](https://arxiv.org/abs/2507.00537)
*Feng Lin,Marco Chen,Haokui Zhang,Xiaotian Yu,Guangming Lu,Rong Xiao*

Main category: cs.CV

TL;DR: The paper introduces Attention Ablation Technique (AAT) to improve CLIP's performance by ablating detrimental attention heads, achieving up to 11.1% higher recall in cross-modal retrieval.


<details>
  <summary>Details</summary>
Motivation: Certain attention heads in CLIP's image encoder may harm performance; removing them could enhance downstream task results.

Method: Proposes AAT, which manipulates attention weights to suppress harmful heads, using two strategies for different scenarios.

Result: AAT consistently improves performance across domains, notably boosting recall by 11.1% in cross-modal retrieval.

Conclusion: AAT effectively refines vision-language models without added inference cost, demonstrating its potential for model optimization.

Abstract: This paper studies the role of attention heads in CLIP's image encoder. While
CLIP has exhibited robust performance across diverse applications, we
hypothesize that certain attention heads negatively affect final
representations and that ablating them can improve performance in downstream
tasks. To capitalize on this insight, we propose a simple yet effective method,
called Attention Ablation Technique (AAT), to suppress the contribution of
specific heads by manipulating attention weights. By integrating two
alternative strategies tailored for different application scenarios, AAT
systematically identifies and ablates detrimental attention heads to enhance
representation quality. Experiments demonstrate that AAT consistently improves
downstream task performance across various domains, boosting recall rate by up
to 11.1% on CLIP-family models for cross-modal retrieval. The results highlight
the potential of AAT to effectively refine large-scale vision-language models
with virtually no increase in inference cost.

</details>


### [172] [LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail Conserved Anti-Aliasing](https://arxiv.org/abs/2507.00554)
*Zhenya Yang,Bingchen Gong,Kai Chen,Qi Dou*

Main category: cs.CV

TL;DR: LOD-GS introduces a Level-of-Detail-sensitive filtering framework for 3D Gaussian Splatting to dynamically adjust filtering strength, addressing aliasing artifacts more effectively than existing methods.


<details>
  <summary>Details</summary>
Motivation: Aliasing artifacts in 3D Gaussian Splatting (3DGS) persist due to insensitive filtering methods, leading to under-filtering or over-smoothing.

Method: LOD-GS uses basis functions to model appearance variations based on sampling rate, jointly optimized with 3D Gaussians. It considers both focal length and camera distance for filtering.

Result: The method achieves state-of-the-art rendering quality and effectively eliminates aliasing, validated on public and a new synthetic dataset.

Conclusion: LOD-GS provides a superior solution for aliasing in 3DGS, with open-sourced code and dataset for broader use.

Abstract: Despite the advancements in quality and efficiency achieved by 3D Gaussian
Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent
challenge. Existing approaches primarily rely on low-pass filtering to mitigate
aliasing. However, these methods are not sensitive to the sampling rate, often
resulting in under-filtering and over-smoothing renderings. To address this
limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework
for Gaussian Splatting, which dynamically predicts the optimal filtering
strength for each 3D Gaussian primitive. Specifically, we introduce a set of
basis functions to each Gaussian, which take the sampling rate as input to
model appearance variations, enabling sampling-rate-sensitive filtering. These
basis function parameters are jointly optimized with the 3D Gaussian in an
end-to-end manner. The sampling rate is influenced by both focal length and
camera distance. However, existing methods and datasets rely solely on
down-sampling to simulate focal length changes for anti-aliasing evaluation,
overlooking the impact of camera distance. To enable a more comprehensive
assessment, we introduce a new synthetic dataset featuring objects rendered at
varying camera distances. Extensive experiments on both public datasets and our
newly collected dataset demonstrate that our method achieves SOTA rendering
quality while effectively eliminating aliasing. The code and dataset have been
open-sourced.

</details>


### [173] [Zero-shot Skeleton-based Action Recognition with Prototype-guided Feature Alignment](https://arxiv.org/abs/2507.00566)
*Kai Zhou,Shuhai Zhang,Zeng You,Jinwu Hu,Mingkui Tan,Fei Liu*

Main category: cs.CV

TL;DR: The paper proposes PGFA, a prototype-guided feature alignment paradigm for zero-shot skeleton-based action recognition, addressing limitations in existing methods like insufficient skeleton feature discrimination and alignment bias.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with generalizing to unseen actions due to poor skeleton-text alignment and neglect of alignment bias.

Method: PGFA introduces an end-to-end cross-modal contrastive training framework and a prototype-guided text feature alignment strategy.

Result: PGFA outperforms the top competitor SMIE, achieving significant accuracy improvements on NTU-60, NTU-120, and PKU-MMD datasets.

Conclusion: PGFA effectively addresses alignment issues and improves zero-shot skeleton-based action recognition performance.

Abstract: Zero-shot skeleton-based action recognition aims to classify unseen
skeleton-based human actions without prior exposure to such categories during
training. This task is extremely challenging due to the difficulty in
generalizing from known to unknown actions. Previous studies typically use
two-stage training: pre-training skeleton encoders on seen action categories
using cross-entropy loss and then aligning pre-extracted skeleton and text
features, enabling knowledge transfer to unseen classes through skeleton-text
alignment and language models' generalization. However, their efficacy is
hindered by 1) insufficient discrimination for skeleton features, as the fixed
skeleton encoder fails to capture necessary alignment information for effective
skeleton-text alignment; 2) the neglect of alignment bias between skeleton and
unseen text features during testing. To this end, we propose a prototype-guided
feature alignment paradigm for zero-shot skeleton-based action recognition,
termed PGFA. Specifically, we develop an end-to-end cross-modal contrastive
training framework to improve skeleton-text alignment, ensuring sufficient
discrimination for skeleton features. Additionally, we introduce a
prototype-guided text feature alignment strategy to mitigate the adverse impact
of the distribution discrepancy during testing. We provide a theoretical
analysis to support our prototype-guided text feature alignment strategy and
empirically evaluate our overall PGFA on three well-known datasets. Compared
with the top competitor SMIE method, our PGFA achieves absolute accuracy
improvements of 22.96%, 12.53%, and 18.54% on the NTU-60, NTU-120, and PKU-MMD
datasets, respectively.

</details>


### [174] [Out-of-distribution detection in 3D applications: a review](https://arxiv.org/abs/2507.00570)
*Zizhao Li,Xueyang Kang,Joseph West,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: The paper provides a comprehensive overview of Out-of-Distribution (OOD) detection in AI, focusing on its importance for reliable systems, especially in 3D applications like autonomous driving. It covers use cases, datasets, methodologies, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of traditional object recognition methods that assume a closed set of classes, which hinders real-world generalization. OOD detection is crucial for reliable AI in dynamic environments.

Method: Comparative analysis of OOD detection methodologies, including model structures, uncertainty indicators, distributional distance taxonomies, and uncertainty calibration techniques.

Result: A detailed framework for OOD detection, highlighting benchmarks, evaluation metrics, and emerging techniques like adversarially robust detection and failure identification.

Conclusion: The paper serves as a guide for researchers, offering theoretical and practical insights into OOD detection, with a focus on advancing reliable and robust AI systems, particularly in 3D vision.

Abstract: The ability to detect objects that are not prevalent in the training set is a
critical capability in many 3D applications, including autonomous driving.
Machine learning methods for object recognition often assume that all object
categories encountered during inference belong to a closed set of classes
present in the training data. This assumption limits generalization to the real
world, as objects not seen during training may be misclassified or entirely
ignored. As part of reliable AI, OOD detection identifies inputs that deviate
significantly from the training distribution. This paper provides a
comprehensive overview of OOD detection within the broader scope of trustworthy
and uncertain AI. We begin with key use cases across diverse domains, introduce
benchmark datasets spanning multiple modalities, and discuss evaluation
metrics. Next, we present a comparative analysis of OOD detection
methodologies, exploring model structures, uncertainty indicators, and
distributional distance taxonomies, alongside uncertainty calibration
techniques. Finally, we highlight promising research directions, including
adversarially robust OOD detection and failure identification, particularly
relevant to 3D applications. The paper offers both theoretical and practical
insights into OOD detection, showcasing emerging research opportunities such as
3D vision integration. These insights help new researchers navigate the field
more effectively, contributing to the development of reliable, safe, and robust
AI systems.

</details>


### [175] [AI-Generated Video Detection via Perceptual Straightening](https://arxiv.org/abs/2507.00583)
*Christian Intern,Robert Geirhos,Markus Olhofer,Sunny Liu,Barbara Hammer,David Klindt*

Main category: cs.CV

TL;DR: ReStraV detects AI-generated videos by analyzing temporal inconsistencies in neural representations, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The rise of realistic synthetic videos challenges content authentication, requiring better detection methods.

Method: ReStraV uses a pre-trained vision transformer to quantify temporal curvature and stepwise distance in videos, training a classifier on these metrics.

Result: Achieves 97.17% accuracy and 98.63% AUROC, outperforming existing methods.

Conclusion: ReStraV offers an efficient, low-cost solution and advances the use of neural representation geometry for detection.

Abstract: The rapid advancement of generative AI enables highly realistic synthetic
videos, posing significant challenges for content authentication and raising
urgent concerns about misuse. Existing detection methods often struggle with
generalization and capturing subtle temporal inconsistencies. We propose
ReStraV(Representation Straightening Video), a novel approach to distinguish
natural from AI-generated videos. Inspired by the "perceptual straightening"
hypothesis -- which suggests real-world video trajectories become more straight
in neural representation domain -- we analyze deviations from this expected
geometric property. Using a pre-trained self-supervised vision transformer
(DINOv2), we quantify the temporal curvature and stepwise distance in the
model's representation domain. We aggregate statistics of these measures for
each video and train a classifier. Our analysis shows that AI-generated videos
exhibit significantly different curvature and distance patterns compared to
real videos. A lightweight classifier achieves state-of-the-art detection
performance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark),
substantially outperforming existing image- and video-based methods. ReStraV is
computationally efficient, it is offering a low-cost and effective detection
solution. This work provides new insights into using neural representation
geometry for AI-generated video detection.

</details>


### [176] [Similarity Memory Prior is All You Need for Medical Image Segmentation](https://arxiv.org/abs/2507.00585)
*Tang Hao,Guo ZhiQing,Wang LieJun,Liu Chao*

Main category: cs.CV

TL;DR: Sim-MPNet, a medical image segmentation model, uses Dynamic Memory Weights-Loss Attention (DMW-LA) and Double-Similarity Global Internal Enhancement Module (DS-GIM) to improve segmentation by leveraging similarity memory priors and feature distribution analysis.


<details>
  <summary>Details</summary>
Motivation: Inspired by 'grandmother cells' in macaque V1, the study aims to enhance medical image segmentation by mimicking biological recognition mechanisms.

Method: Sim-MPNet employs DMW-LA for dynamic memory updates and DS-GIM for feature distribution analysis using cosine similarity and Euclidean distance.

Result: Experiments on four datasets show Sim-MPNet outperforms state-of-the-art methods in segmentation accuracy.

Conclusion: Sim-MPNet effectively leverages biological insights for advanced medical image segmentation, with code publicly available.

Abstract: In recent years, it has been found that "grandmother cells" in the primary
visual cortex (V1) of macaques can directly recognize visual input with complex
shapes. This inspires us to examine the value of these cells in promoting the
research of medical image segmentation. In this paper, we design a Similarity
Memory Prior Network (Sim-MPNet) for medical image segmentation. Specifically,
we propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and
remembers the category features of specific lesions or organs in medical images
through the similarity memory prior in the prototype memory bank, thus helping
the network to learn subtle texture changes between categories. DMW-LA also
dynamically updates the similarity memory prior in reverse through Weight-Loss
Dynamic (W-LD) update strategy, effectively assisting the network directly
extract category features. In addition, we propose the Double-Similarity Global
Internal Enhancement Module (DS-GIM) to deeply explore the internal differences
in the feature distribution of input data through cosine similarity and
euclidean distance. Extensive experiments on four public datasets show that
Sim-MPNet has better segmentation performance than other state-of-the-art
methods. Our code is available on https://github.com/vpsg-research/Sim-MPNet.

</details>


### [177] [Context-Aware Academic Emotion Dataset and Benchmark](https://arxiv.org/abs/2507.00586)
*Luming Zhao,Jingwen Xuan,Jiamin Lou,Yonghui Yu,Wenwu Yang*

Main category: cs.CV

TL;DR: The paper introduces RAER, a novel dataset for academic emotion recognition, and CLIP-CAER, a context-aware method leveraging CLIP, outperforming existing facial expression recognition techniques.


<details>
  <summary>Details</summary>
Motivation: Academic emotion recognition is underexplored due to lack of datasets. The paper aims to address this gap by introducing a diverse dataset and a context-aware method.

Method: The authors propose CLIP-CAER, using learnable text prompts in CLIP to integrate facial and context cues from videos.

Result: CLIP-CAER outperforms state-of-the-art methods, highlighting context's importance in academic emotion recognition.

Conclusion: The study advances academic emotion recognition with a novel dataset and context-aware approach, demonstrating superior performance.

Abstract: Academic emotion analysis plays a crucial role in evaluating students'
engagement and cognitive states during the learning process. This paper
addresses the challenge of automatically recognizing academic emotions through
facial expressions in real-world learning environments. While significant
progress has been made in facial expression recognition for basic emotions,
academic emotion recognition remains underexplored, largely due to the scarcity
of publicly available datasets. To bridge this gap, we introduce RAER, a novel
dataset comprising approximately 2,700 video clips collected from around 140
students in diverse, natural learning contexts such as classrooms, libraries,
laboratories, and dormitories, covering both classroom sessions and individual
study. Each clip was annotated independently by approximately ten annotators
using two distinct sets of academic emotion labels with varying granularity,
enhancing annotation consistency and reliability. To our knowledge, RAER is the
first dataset capturing diverse natural learning scenarios. Observing that
annotators naturally consider context cues-such as whether a student is looking
at a phone or reading a book-alongside facial expressions, we propose CLIP-CAER
(CLIP-based Context-aware Academic Emotion Recognition). Our method utilizes
learnable text prompts within the vision-language model CLIP to effectively
integrate facial expression and context cues from videos. Experimental results
demonstrate that CLIP-CAER substantially outperforms state-of-the-art
video-based facial expression recognition methods, which are primarily designed
for basic emotions, emphasizing the crucial role of context in accurately
recognizing academic emotions. Project page: https://zgsfer.github.io/CAER

</details>


### [178] [Overtake Detection in Trucks Using CAN Bus Signals: A Comparative Study of Machine Learning Methods](https://arxiv.org/abs/2507.00593)
*Fernando Alonso-Fernandez,Talha Hanif Butt,Prayag Tiwari*

Main category: cs.CV

TL;DR: The study evaluates overtake detection in trucks using CAN bus data, comparing ANN, RF, and SVM classifiers. Training with diverse data from multiple vehicles improves performance, and a score-level fusion strategy achieves high accuracy.


<details>
  <summary>Details</summary>
Motivation: Safe overtaking in trucks is crucial for accident prevention and traffic efficiency, requiring accurate prediction for ADAS.

Method: Used CAN bus data from five Volvo trucks, tested ANN, RF, and SVM classifiers, and applied score-level fusion for improved performance.

Result: Achieved TNR=93% and TPR=86.5% with fusion, showing that diverse training data and per-vehicle analysis enhance accuracy.

Conclusion: Diverse training data and fusion strategies improve overtake detection, supporting ADAS applications in real-world conditions.

Abstract: Safe overtaking manoeuvres in trucks are vital for preventing accidents and
ensuring efficient traffic flow. Accurate prediction of such manoeuvres is
essential for Advanced Driver Assistance Systems (ADAS) to make timely and
informed decisions. In this study, we focus on overtake detection using
Controller Area Network (CAN) bus data collected from five in-service trucks
provided by the Volvo Group. We evaluate three common classifiers for vehicle
manoeuvre detection, Artificial Neural Networks (ANN), Random Forest (RF), and
Support Vector Machines (SVM), and analyse how different preprocessing
configurations affect performance. We find that variability in traffic
conditions strongly influences the signal patterns, particularly in the
no-overtake class, affecting classification performance if training data lacks
adequate diversity. Since the data were collected under unconstrained,
real-world conditions, class diversity cannot be guaranteed a priori. However,
training with data from multiple vehicles improves generalisation and reduces
condition-specific bias. Our pertruck analysis also reveals that classification
accuracy, especially for overtakes, depends on the amount of training data per
vehicle. To address this, we apply a score-level fusion strategy, which yields
the best per-truck performance across most cases. Overall, we achieve an
accuracy via fusion of TNR=93% (True Negative Rate) and TPR=86.5% (True
Positive Rate). This research has been part of the BIG FUN project, which
explores how Artificial Intelligence can be applied to logged vehicle data to
understand and predict driver behaviour, particularly in relation to Camera
Monitor Systems (CMS), being introduced as digital replacements for traditional
exterior mirrors.

</details>


### [179] [World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model](https://arxiv.org/abs/2507.00603)
*Yupeng Zheng,Pengxuan Yang,Zebin Xing,Qichao Zhang,Yuhang Zheng,Yinfeng Gao,Pengfei Li,Teng Zhang,Zhongpu Xia,Peng Jia,Dongbin Zhao*

Main category: cs.CV

TL;DR: World4Drive is an end-to-end autonomous driving framework using vision foundation models for self-supervised trajectory planning without costly perception annotations.


<details>
  <summary>Details</summary>
Motivation: To eliminate reliance on expensive perception supervision and enable annotation-free, end-to-end planning in autonomous driving.

Method: Extracts scene features and intentions via vision foundation models, generates multi-modal trajectories, predicts future states, and selects the best trajectory using a world model selector.

Result: Achieves state-of-the-art performance with 18.1% lower L2 error, 46.7% reduced collision rate, and 3.75x faster training convergence.

Conclusion: World4Drive demonstrates effective self-supervised planning, outperforming benchmarks without manual perception annotations.

Abstract: End-to-end autonomous driving directly generates planning trajectories from
raw sensor data, yet it typically relies on costly perception supervision to
extract scene information. A critical research challenge arises: constructing
an informative driving world model to enable perception annotation-free,
end-to-end planning via self-supervised learning. In this paper, we present
World4Drive, an end-to-end autonomous driving framework that employs vision
foundation models to build latent world models for generating and evaluating
multi-modal planning trajectories. Specifically, World4Drive first extracts
scene features, including driving intention and world latent representations
enriched with spatial-semantic priors provided by vision foundation models. It
then generates multi-modal planning trajectories based on current scene
features and driving intentions and predicts multiple intention-driven future
states within the latent space. Finally, it introduces a world model selector
module to evaluate and select the best trajectory. We achieve perception
annotation-free, end-to-end planning through self-supervised alignment between
actual future observations and predicted observations reconstructed from the
latent space. World4Drive achieves state-of-the-art performance without manual
perception annotations on both the open-loop nuScenes and closed-loop NavSim
benchmarks, demonstrating an 18.1\% relative reduction in L2 error, 46.7% lower
collision rate, and 3.75 faster training convergence. Codes will be accessed at
https://github.com/ucaszyp/World4Drive.

</details>


### [180] [ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2507.00898)
*Zifu Wan,Ce Zhang,Silong Yong,Martin Q. Ma,Simon Stepputtis,Louis-Philippe Morency,Deva Ramanan,Katia Sycara,Yaqi Xie*

Main category: cs.CV

TL;DR: ONLY is a training-free decoding method for LVLMs that reduces hallucination by amplifying key textual information using a text-to-visual entropy ratio, enabling efficient real-time deployment with minimal computational cost.


<details>
  <summary>Details</summary>
Motivation: LVLMs suffer from hallucination, limiting their reliability in real-world applications. Existing contrastive decoding methods are slow due to multiple queries.

Method: ONLY uses a single query and one-layer intervention during decoding, selectively amplifying crucial textual information via a text-to-visual entropy ratio.

Result: ONLY outperforms state-of-the-art methods across benchmarks with minimal computational cost.

Conclusion: ONLY offers an efficient, real-time solution to mitigate hallucination in LVLMs, enhancing reliability for practical use.

Abstract: Recent Large Vision-Language Models (LVLMs) have introduced a new paradigm
for understanding and reasoning about image input through textual responses.
Although they have achieved remarkable performance across a range of
multi-modal tasks, they face the persistent challenge of hallucination, which
introduces practical weaknesses and raises concerns about their reliable
deployment in real-world applications. Existing work has explored contrastive
decoding approaches to mitigate this issue, where the output of the original
LVLM is compared and contrasted with that of a perturbed version. However,
these methods require two or more queries that slow down LVLM response
generation, making them less suitable for real-time applications. To overcome
this limitation, we propose ONLY, a training-free decoding approach that
requires only a single query and a one-layer intervention during decoding,
enabling efficient real-time deployment. Specifically, we enhance textual
outputs by selectively amplifying crucial textual information using a
text-to-visual entropy ratio for each token. Extensive experimental results
demonstrate that our proposed ONLY consistently outperforms state-of-the-art
methods across various benchmarks while requiring minimal implementation effort
and computational cost. Code is available at https://github.com/zifuwan/ONLY.

</details>


### [181] [De-Simplifying Pseudo Labels to Enhancing Domain Adaptive Object Detection](https://arxiv.org/abs/2507.00608)
*Zehua Fu,Chenguang Liu,Yuyu Chen,Jiaqi Zhou,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: The paper addresses the performance gap between self-labeling and domain alignment methods in Unsupervised Domain Adaptation (UDA) for object detection, attributing it to simple-label bias. It proposes DeSimPL, a method to mitigate this bias using an instance-level memory bank and adversarial samples, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: The high cost of labeled data in traffic and transportation scenarios motivates UDA for object detection. While domain alignment methods lead, self-labeling methods lag due to simple-label bias.

Method: DeSimPL introduces an instance-level memory bank for pseudo label updating, adversarial samples to balance training, and an adaptive weighted loss to handle false positives.

Result: DeSimPL reduces simple-label bias, significantly improving self-labeling detector performance, validated on four benchmarks.

Conclusion: The proposed DeSimPL effectively addresses simple-label bias, bridging the performance gap between self-labeling and domain alignment methods in UDA for object detection.

Abstract: Despite its significant success, object detection in traffic and
transportation scenarios requires time-consuming and laborious efforts in
acquiring high-quality labeled data. Therefore, Unsupervised Domain Adaptation
(UDA) for object detection has recently gained increasing research attention.
UDA for object detection has been dominated by domain alignment methods, which
achieve top performance. Recently, self-labeling methods have gained popularity
due to their simplicity and efficiency. In this paper, we investigate the
limitations that prevent self-labeling detectors from achieving commensurate
performance with domain alignment methods. Specifically, we identify the high
proportion of simple samples during training, i.e., the simple-label bias, as
the central cause. We propose a novel approach called De-Simplifying Pseudo
Labels (DeSimPL) to mitigate the issue. DeSimPL utilizes an instance-level
memory bank to implement an innovative pseudo label updating strategy. Then,
adversarial samples are introduced during training to enhance the proportion.
Furthermore, we propose an adaptive weighted loss to avoid the model suffering
from an abundance of false positive pseudo labels in the late training period.
Experimental results demonstrate that DeSimPL effectively reduces the
proportion of simple samples during training, leading to a significant
performance improvement for self-labeling detectors. Extensive experiments
conducted on four benchmarks validate our analysis and conclusions.

</details>


### [182] [UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions](https://arxiv.org/abs/2507.00648)
*Siyuan Yao,Rui Zhu,Ziqi Wang,Wenqi Ren,Yanyang Yan,Xiaochun Cao*

Main category: cs.CV

TL;DR: UMDATrack improves visual object tracking in adverse weather by using a domain adaptation framework, a scenario generator, and custom adapters.


<details>
  <summary>Details</summary>
Motivation: Existing tracking methods struggle with adverse weather conditions due to domain shifts, leading to performance drops.

Method: Proposes UMDATrack with a controllable scenario generator, domain-customized adapter (DCA), and target-aware confidence alignment (TCA).

Result: Outperforms existing trackers significantly in adverse weather conditions.

Conclusion: UMDATrack sets a new state-of-the-art for robust visual tracking in diverse weather scenarios.

Abstract: Visual object tracking has gained promising progress in past decades. Most of
the existing approaches focus on learning target representation in
well-conditioned daytime data, while for the unconstrained real-world scenarios
with adverse weather conditions, e.g. nighttime or foggy environment, the
tremendous domain shift leads to significant performance degradation. In this
paper, we propose UMDATrack, which is capable of maintaining high-quality
target state prediction under various adverse weather conditions within a
unified domain adaptation framework. Specifically, we first use a controllable
scenario generator to synthesize a small amount of unlabeled videos (less than
2% frames in source daytime datasets) in multiple weather conditions under the
guidance of different text prompts. Afterwards, we design a simple yet
effective domain-customized adapter (DCA), allowing the target objects'
representation to rapidly adapt to various weather conditions without redundant
model updating. Furthermore, to enhance the localization consistency between
source and target domains, we propose a target-aware confidence alignment
module (TCA) following optimal transport theorem. Extensive experiments
demonstrate that UMDATrack can surpass existing advanced visual trackers and
lead new state-of-the-art performance by a significant margin. Our code is
available at https://github.com/Z-Z188/UMDATrack.

</details>


### [183] [LoD-Loc v2: Aerial Visual Localization over Low Level-of-Detail City Models using Explicit Silhouette Alignment](https://arxiv.org/abs/2507.00659)
*Juelin Zhu,Shuaibang Peng,Long Wang,Hanlin Tan,Yu Liu,Maojun Zhang,Shen Yan*

Main category: cs.CV

TL;DR: LoD-Loc v2 introduces a coarse-to-fine strategy for aerial visual localization over low-LoD city models, improving accuracy and enabling localization where previous methods failed.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on high-LoD city models, but low-LoD models are more common and planned for nationwide use. Enabling localization on low-LoD models can expand drone applications globally.

Method: LoD-Loc v2 uses building segmentation, pose cost volume for coarse pose selection, and particle filtering for fine pose estimation.

Result: The method improves accuracy with high-LoD models and enables localization with low-LoD models, outperforming baselines and broadening convergence.

Conclusion: LoD-Loc v2 successfully addresses the limitations of previous methods, unlocking potential for global urban drone localization.

Abstract: We propose a novel method for aerial visual localization over low
Level-of-Detail (LoD) city models. Previous wireframe-alignment-based method
LoD-Loc has shown promising localization results leveraging LoD models.
However, LoD-Loc mainly relies on high-LoD (LoD3 or LoD2) city models, but the
majority of available models and those many countries plan to construct
nationwide are low-LoD (LoD1). Consequently, enabling localization on low-LoD
city models could unlock drones' potential for global urban localization. To
address these issues, we introduce LoD-Loc v2, which employs a coarse-to-fine
strategy using explicit silhouette alignment to achieve accurate localization
over low-LoD city models in the air. Specifically, given a query image, LoD-Loc
v2 first applies a building segmentation network to shape building silhouettes.
Then, in the coarse pose selection stage, we construct a pose cost volume by
uniformly sampling pose hypotheses around a prior pose to represent the pose
probability distribution. Each cost of the volume measures the degree of
alignment between the projected and predicted silhouettes. We select the pose
with maximum value as the coarse pose. In the fine pose estimation stage, a
particle filtering method incorporating a multi-beam tracking approach is used
to efficiently explore the hypothesis space and obtain the final pose
estimation. To further facilitate research in this field, we release two
datasets with LoD1 city models covering 10.7 km , along with real RGB queries
and ground-truth pose annotations. Experimental results show that LoD-Loc v2
improves estimation accuracy with high-LoD models and enables localization with
low-LoD models for the first time. Moreover, it outperforms state-of-the-art
baselines by large margins, even surpassing texture-model-based methods, and
broadens the convergence basin to accommodate larger prior errors.

</details>


### [184] [A Unified Transformer-Based Framework with Pretraining For Whole Body Grasping Motion Generation](https://arxiv.org/abs/2507.00676)
*Edward Effendy,Kuan-Wei Tseng,Rei Kawakami*

Main category: cs.CV

TL;DR: A transformer-based framework for whole-body grasping, combining pose generation and motion infilling, with a modular design for adaptability.


<details>
  <summary>Details</summary>
Motivation: To enable realistic and stable object interactions by addressing pose generation and motion continuity in whole-body grasping.

Method: Three-stage pipeline: Grasp Pose Generation, Temporal Infilling, and LiftUp Transformer. Includes Generalized Pretraining for data efficiency.

Result: Outperforms state-of-the-art baselines on the GRAB dataset in coherence, stability, and visual realism.

Conclusion: The framework is effective for whole-body grasping and adaptable to other human-motion applications.

Abstract: Accepted in the ICIP 2025
  We present a novel transformer-based framework for whole-body grasping that
addresses both pose generation and motion infilling, enabling realistic and
stable object interactions. Our pipeline comprises three stages: Grasp Pose
Generation for full-body grasp generation, Temporal Infilling for smooth motion
continuity, and a LiftUp Transformer that refines downsampled joints back to
high-resolution markers. To overcome the scarcity of hand-object interaction
data, we introduce a data-efficient Generalized Pretraining stage on large,
diverse motion datasets, yielding robust spatio-temporal representations
transferable to grasping tasks. Experiments on the GRAB dataset show that our
method outperforms state-of-the-art baselines in terms of coherence, stability,
and visual realism. The modular design also supports easy adaptation to other
human-motion applications.

</details>


### [185] [Cage-Based Deformation for Transferable and Undefendable Point Cloud Attack](https://arxiv.org/abs/2507.00690)
*Keke Tang,Ziyong Du,Weilong Peng,Xiaofei Wang,Peican Zhu,Ligang Liu,Zhihong Tian*

Main category: cs.CV

TL;DR: CageAttack introduces a cage-based deformation framework for generating natural adversarial point clouds, balancing transferability, undefendability, and plausibility.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attacks on point clouds either limit transferability or introduce unnatural distortions, undermining plausibility.

Method: CageAttack constructs a cage around the target object, applies perturbations to cage vertices, and propagates these smoothly to the point cloud.

Result: Outperforms state-of-the-art methods on seven 3D classifiers across three datasets.

Conclusion: CageAttack effectively balances natural deformation with adversarial effectiveness, offering a superior approach for point cloud attacks.

Abstract: Adversarial attacks on point clouds often impose strict geometric constraints
to preserve plausibility; however, such constraints inherently limit
transferability and undefendability. While deformation offers an alternative,
existing unstructured approaches may introduce unnatural distortions, making
adversarial point clouds conspicuous and undermining their plausibility. In
this paper, we propose CageAttack, a cage-based deformation framework that
produces natural adversarial point clouds. It first constructs a cage around
the target object, providing a structured basis for smooth, natural-looking
deformation. Perturbations are then applied to the cage vertices, which
seamlessly propagate to the point cloud, ensuring that the resulting
deformations remain intrinsic to the object and preserve plausibility.
Extensive experiments on seven 3D deep neural network classifiers across three
datasets show that CageAttack achieves a superior balance among
transferability, undefendability, and plausibility, outperforming
state-of-the-art methods. Codes will be made public upon acceptance.

</details>


### [186] [Rectifying Magnitude Neglect in Linear Attention](https://arxiv.org/abs/2507.00698)
*Qihang Fan,Huaibo Huang,Yuang Ai,ran He*

Main category: cs.CV

TL;DR: The paper introduces Magnitude-Aware Linear Attention (MALA) to address the performance gap between Linear Attention and Softmax Attention by incorporating Query magnitude, achieving strong results across various tasks.


<details>
  <summary>Details</summary>
Motivation: Linear Attention's linear complexity is advantageous for vision tasks but suffers performance degradation due to ignoring Query magnitude, unlike Softmax Attention.

Method: Proposes MALA, which modifies Linear Attention to include Query magnitude, aligning its attention score distribution closer to Softmax Attention.

Result: MALA achieves strong performance on tasks like image classification, object detection, NLP, and more, outperforming standard Linear Attention.

Conclusion: MALA effectively bridges the performance gap between Linear and Softmax Attention while maintaining linear complexity, making it a versatile solution for diverse tasks.

Abstract: As the core operator of Transformers, Softmax Attention exhibits excellent
global modeling capabilities. However, its quadratic complexity limits its
applicability to vision tasks. In contrast, Linear Attention shares a similar
formulation with Softmax Attention while achieving linear complexity, enabling
efficient global information modeling. Nevertheless, Linear Attention suffers
from a significant performance degradation compared to standard Softmax
Attention. In this paper, we analyze the underlying causes of this issue based
on the formulation of Linear Attention. We find that, unlike Softmax Attention,
Linear Attention entirely disregards the magnitude information of the Query.
This prevents the attention score distribution from dynamically adapting as the
Query scales. As a result, despite its structural similarity to Softmax
Attention, Linear Attention exhibits a significantly different attention score
distribution. Based on this observation, we propose Magnitude-Aware Linear
Attention (MALA), which modifies the computation of Linear Attention to fully
incorporate the Query's magnitude. This adjustment allows MALA to generate an
attention score distribution that closely resembles Softmax Attention while
exhibiting a more well-balanced structure. We evaluate the effectiveness of
MALA on multiple tasks, including image classification, object detection,
instance segmentation, semantic segmentation, natural language processing,
speech recognition, and image generation. Our MALA achieves strong results on
all of these tasks. Code will be available at https://github.com/qhfan/MALA

</details>


### [187] [BEV-VAE: Multi-view Image Generation with Spatial Consistency for Autonomous Driving](https://arxiv.org/abs/2507.00707)
*Zeming Chen,Hang Zhao*

Main category: cs.CV

TL;DR: BEV-VAE proposes a method for multi-view image generation in autonomous driving using a structured 3D representation, achieving consistent and controllable synthesis.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack explicit 3D modeling for multi-view image generation, which is crucial for autonomous driving.

Method: BEV-VAE trains a multi-view image variational autoencoder for a unified BEV latent space and uses a latent diffusion transformer for scene generation.

Result: Experiments on nuScenes and Argoverse 2 show strong performance in 3D consistent reconstruction and generation.

Conclusion: BEV-VAE effectively addresses the need for structured 3D representation in multi-view image generation for autonomous driving.

Abstract: Multi-view image generation in autonomous driving demands consistent 3D scene
understanding across camera views. Most existing methods treat this problem as
a 2D image set generation task, lacking explicit 3D modeling. However, we argue
that a structured representation is crucial for scene generation, especially
for autonomous driving applications. This paper proposes BEV-VAE for consistent
and controllable view synthesis. BEV-VAE first trains a multi-view image
variational autoencoder for a compact and unified BEV latent space and then
generates the scene with a latent diffusion transformer. BEV-VAE supports
arbitrary view generation given camera configurations, and optionally 3D
layouts. Experiments on nuScenes and Argoverse 2 (AV2) show strong performance
in both 3D consistent reconstruction and generation. The code is available at:
https://github.com/Czm369/bev-vae.

</details>


### [188] [TopoStreamer: Temporal Lane Segment Topology Reasoning in Autonomous Driving](https://arxiv.org/abs/2507.00709)
*Yiming Yang,Yueru Luo,Bingkun He,Hongbin Lin,Suzhong Fu,Chao Yan,Kun Tang,Xinrui Yan,Chao Zheng,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: TopoStreamer improves lane segment topology reasoning with streaming attribute constraints, dynamic positional encoding, and denoising, outperforming existing methods by +3.4% mAP and +2.1% OLS.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack consistent positional embedding and temporal attribute learning, hindering accurate road network reconstruction for autonomous driving.

Method: TopoStreamer introduces streaming attribute constraints, dynamic lane boundary positional encoding, and lane segment denoising to enhance temporal consistency and positional learning.

Result: TopoStreamer achieves +3.4% mAP in lane segment perception and +2.1% OLS in centerline perception on OpenLane-V2.

Conclusion: TopoStreamer effectively addresses limitations in lane segment topology reasoning, improving performance for autonomous driving systems.

Abstract: Lane segment topology reasoning constructs a comprehensive road network by
capturing the topological relationships between lane segments and their
semantic types. This enables end-to-end autonomous driving systems to perform
road-dependent maneuvers such as turning and lane changing. However, the
limitations in consistent positional embedding and temporal multiple attribute
learning in existing methods hinder accurate roadnet reconstruction. To address
these issues, we propose TopoStreamer, an end-to-end temporal perception model
for lane segment topology reasoning. Specifically, TopoStreamer introduces
three key improvements: streaming attribute constraints, dynamic lane boundary
positional encoding, and lane segment denoising. The streaming attribute
constraints enforce temporal consistency in both centerline and boundary
coordinates, along with their classifications. Meanwhile, dynamic lane boundary
positional encoding enhances the learning of up-to-date positional information
within queries, while lane segment denoising helps capture diverse lane segment
patterns, ultimately improving model performance. Additionally, we assess the
accuracy of existing models using a lane boundary classification metric, which
serves as a crucial measure for lane-changing scenarios in autonomous driving.
On the OpenLane-V2 dataset, TopoStreamer demonstrates significant improvements
over state-of-the-art methods, achieving substantial performance gains of +3.4%
mAP in lane segment perception and +2.1% OLS in centerline perception tasks.

</details>


### [189] [UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement](https://arxiv.org/abs/2507.00721)
*Xiao Zhang,Fei Wei,Yong Wang,Wenda Zhao,Feiyi Li,Xiangxiang Chu*

Main category: cs.CV

TL;DR: The paper proposes UPRE, a framework for zero-shot domain adaptation (ZSDA) that jointly optimizes textual prompts and visual representations to address misalignment issues in Vision-Language Models (VLMs).


<details>
  <summary>Details</summary>
Motivation: Existing ZSDA methods using VLMs overlook misalignment between detection tasks and VLMs, relying on manually crafted prompts.

Method: UPRE introduces multi-view domain prompts combining linguistic priors with detection knowledge, visual representation enhancement for domain style variations, and multi-level enhancement strategies.

Result: Experiments on nine benchmark datasets show UPRE's superior performance in ZSDA detection.

Conclusion: UPRE effectively addresses ZSDA challenges by aligning multi-modal representations and enhancing visual diversity.

Abstract: Zero-shot domain adaptation (ZSDA) presents substantial challenges due to the
lack of images in the target domain. Previous approaches leverage
Vision-Language Models (VLMs) to tackle this challenge, exploiting their
zero-shot learning capabilities. However, these methods primarily address
domain distribution shifts and overlook the misalignment between the detection
task and VLMs, which rely on manually crafted prompts. To overcome these
limitations, we propose the unified prompt and representation enhancement
(UPRE) framework, which jointly optimizes both textual prompts and visual
representations. Specifically, our approach introduces a multi-view domain
prompt that combines linguistic domain priors with detection-specific
knowledge, and a visual representation enhancement module that produces domain
style variations. Furthermore, we introduce multi-level enhancement strategies,
including relative domain distance and positive-negative separation, which
align multi-modal representations at the image level and capture diverse visual
representations at the instance level, respectively. Extensive experiments
conducted on nine benchmark datasets demonstrate the superior performance of
our framework in ZSDA detection scenarios. Code is available at
https://github.com/AMAP-ML/UPRE.

</details>


### [190] [Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features](https://arxiv.org/abs/2507.00724)
*Linghui Zhu,Yiming Li,Haiqin Weng,Yan Liu,Tianwei Zhang,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: The paper proposes a harmless model ownership verification method for personalized large vision models by decoupling common and dataset-specific features, addressing the ineffectiveness of existing defenses for fine-tuned models.


<details>
  <summary>Details</summary>
Motivation: Existing defense methods for traditional DNNs are ineffective or risky for fine-tuned models, necessitating a new approach to protect intellectual property.

Method: The method involves creating shadow models to isolate dataset-specific features, training a meta-classifier to identify stolen models, and using hypothesis testing for robust verification.

Result: Extensive experiments confirm the method's effectiveness in detecting various model stealing attacks.

Conclusion: The proposed method provides a robust and secure solution for verifying ownership of personalized models.

Abstract: Large vision models achieve remarkable performance in various downstream
tasks, primarily by personalizing pre-trained models through fine-tuning with
private and valuable local data, which makes the personalized model a valuable
intellectual property for its owner. Similar to the era of traditional DNNs,
model stealing attacks also pose significant risks to these personalized
models. However, in this paper, we reveal that most existing defense methods
(developed for traditional DNNs), typically designed for models trained from
scratch, either introduce additional security risks, are prone to misjudgment,
or are even ineffective for fine-tuned models. To alleviate these problems,
this paper proposes a harmless model ownership verification method for
personalized models by decoupling similar common features. In general, our
method consists of three main stages. In the first stage, we create shadow
models that retain common features of the victim model while disrupting
dataset-specific features. We represent the dataset-specific features of the
victim model by the output differences between the shadow and victim models.
After that, a meta-classifier is trained to identify stolen models by
determining whether suspicious models contain the dataset-specific features of
the victim. In the third stage, we conduct model ownership verification by
hypothesis test to mitigate randomness and enhance robustness. Extensive
experiments on benchmark datasets verify the effectiveness of the proposed
method in detecting different types of model stealing simultaneously.

</details>


### [191] [Biorthogonal Tunable Wavelet Unit with Lifting Scheme in Convolutional Neural Network](https://arxiv.org/abs/2507.00739)
*An Le,Hung Nguyen,Sungbal Seo,You-Suk Bae,Truong Nguyen*

Main category: cs.CV

TL;DR: A novel biorthogonal tunable wavelet unit using a lifting scheme improves CNN performance in image classification and anomaly detection.


<details>
  <summary>Details</summary>
Motivation: To enhance flexibility in filter design by relaxing orthogonality and equal filter length constraints, improving CNN operations like convolution and pooling.

Method: Constructs a biorthogonal tunable wavelet unit via a lifting scheme and integrates it into ResNet architectures.

Result: Improved classification accuracy on CIFAR-10 (2.12%) and DTD (9.73%), and competitive anomaly detection performance on MVTec dataset.

Conclusion: The proposed wavelet unit effectively enhances CNN performance in classification and anomaly detection tasks.

Abstract: This work introduces a novel biorthogonal tunable wavelet unit constructed
using a lifting scheme that relaxes both the orthogonality and equal filter
length constraints, providing greater flexibility in filter design. The
proposed unit enhances convolution, pooling, and downsampling operations,
leading to improved image classification and anomaly detection in convolutional
neural networks (CNN). When integrated into an 18-layer residual neural network
(ResNet-18), the approach improved classification accuracy on CIFAR-10 by 2.12%
and on the Describable Textures Dataset (DTD) by 9.73%, demonstrating its
effectiveness in capturing fine-grained details. Similar improvements were
observed in ResNet-34. For anomaly detection in the hazelnut category of the
MVTec Anomaly Detection dataset, the proposed method achieved competitive and
wellbalanced performance in both segmentation and detection tasks,
outperforming existing approaches in terms of accuracy and robustness.

</details>


### [192] [Improving the Reasoning of Multi-Image Grounding in MLLMs via Reinforcement Learning](https://arxiv.org/abs/2507.00748)
*Bob Zhang,Haoran Li,Tao Zhang,Cilin Yan,Jiayin Cai,Xiaolong Jiang,Yanbin Hao*

Main category: cs.CV

TL;DR: The paper introduces a Reinforcement Learning (RL) post-training strategy to enhance Multimodal Large Language Models (MLLMs) for multi-image grounding tasks, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with complex multi-image compositions and multimodal instructions, highlighting limitations in cross-image reasoning and generalization.

Method: The approach involves synthesizing chain-of-thought (CoT) data for cold-start initialization, supervised fine-tuning (SFT) with LoRA, and using rule-based RL to refine reasoning paths.

Result: The method improves performance by +9.04% on MIG-Bench and +4.98% on out-of-domain benchmarks, with additional gains in multi-image perception.

Conclusion: The RL-based post-training strategy effectively addresses MLLMs' limitations in multi-image grounding, demonstrating strong generalization and performance gains.

Abstract: Recently, Multimodal Large Language Models (MLLMs) excel at visual grounding
in single-image scenarios with textual references. However, their performance
degrades when handling real-world applications involving complex multi-image
compositions and multimodal instructions, which reveals limitations in
cross-image reasoning and generalization. To address these challenges, we adopt
a Reinforcement Learning (RL) based post-training strategy to improve the
reasoning performance of MLLMs in multi-image grounding tasks. Our approach
begins with synthesizing high-quality chain-of-thought (CoT) data for
cold-start initialization, followed by supervised fine-tuning (SFT) using
low-rank adaptation (LoRA). The cold-start training stage enables the model to
identify correct solutions. Subsequently, we perform rejection sampling using
the merged SFT model to curate high-quality RL data and leverage rule-based RL
to guide the model toward optimal reasoning paths. Extensive experimental
results demonstrate the effectiveness of our approach, achieving +9.04\%
improvements on MIG-Bench and +4.98\% improvements on several out-of-domain
reasoning grounding benchmarks over the SFT baseline. Furthermore, our approach
exhibits strong generalization in multi-image perception, with gains of +3.1\%
and +2.4\% over the base model on subsets of the BLINK and MMIU benchmarks,
respectively.

</details>


### [193] [Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation](https://arxiv.org/abs/2507.00752)
*Hao Xing,Kai Zhe Boey,Yuankai Wu,Darius Burschka,Gordon Cheng*

Main category: cs.CV

TL;DR: A Multi-Modal Graph Convolutional Network (MMGCN) is proposed to improve temporal segmentation of human actions by integrating low-frame-rate visual data with high-frame-rate motion data, reducing over-segmentation errors.


<details>
  <summary>Details</summary>
Motivation: Accurate temporal segmentation is crucial for robots in collaborative settings, but noise in pose and object detection leads to over-segmentation errors.

Method: MMGCN uses sinusoidal encoding for 3D skeletons, temporal graph fusion for multi-modal alignment, and SmoothLabelMix for data augmentation to enhance temporal consistency.

Result: Outperforms state-of-the-art methods on the Bimanual Actions Dataset, achieving F1@10: 94.5% and F1@25: 92.8%.

Conclusion: The proposed MMGCN effectively mitigates fragmentation and improves action segmentation accuracy.

Abstract: Accurate temporal segmentation of human actions is critical for intelligent
robots in collaborative settings, where a precise understanding of sub-activity
labels and their temporal structure is essential. However, the inherent noise
in both human pose estimation and object detection often leads to
over-segmentation errors, disrupting the coherence of action sequences. To
address this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that
integrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g.,
30 fps) motion data (skeleton and object detections) to mitigate fragmentation.
Our framework introduces three key contributions. First, a sinusoidal encoding
strategy that maps 3D skeleton coordinates into a continuous sin-cos space to
enhance spatial representation robustness. Second, a temporal graph fusion
module that aligns multi-modal inputs with differing resolutions via
hierarchical feature aggregation, Third, inspired by the smooth transitions
inherent to human actions, we design SmoothLabelMix, a data augmentation
technique that mixes input sequences and labels to generate synthetic training
examples with gradual action transitions, enhancing temporal consistency in
predictions and reducing over-segmentation artifacts.
  Extensive experiments on the Bimanual Actions Dataset, a public benchmark for
human-object interaction understanding, demonstrate that our approach
outperforms state-of-the-art methods, especially in action segmentation
accuracy, achieving F1@10: 94.5% and F1@25: 92.8%.

</details>


### [194] [Language-Unlocked ViT (LUViT): Empowering Self-Supervised Vision Transformers with LLMs](https://arxiv.org/abs/2507.00754)
*Selim Kuzucu,Muhammad Ferjad Naeem,Anna Kukleva,Federico Tombari,Bernt Schiele*

Main category: cs.CV

TL;DR: LUViT bridges the modality gap between LLMs and ViTs via joint pre-training, enhancing vision tasks by aligning LLM and ViT features.


<details>
  <summary>Details</summary>
Motivation: Address the modality mismatch between text-centric LLMs and vision-centric ViTs to better leverage LLM knowledge for vision tasks.

Method: Uses Masked Auto-Encoding (MAE) for ViT pre-training and Low-Rank Adaptation (LoRA) layers in the LLM block, trained jointly with MAE.

Result: LUViT improves performance on downstream vision tasks by aligning LLM and ViT features effectively.

Conclusion: LUViT provides a more efficient and effective way to integrate LLM knowledge into vision tasks.

Abstract: The integration of Large Language Model (LLMs) blocks with Vision
Transformers (ViTs) holds immense promise for vision-only tasks by leveraging
the rich semantic knowledge and reasoning capabilities of LLMs. However, a
fundamental challenge lies in the inherent modality mismatch between
text-centric pretraining of LLMs and vision-centric training of ViTs. Direct
fusion often fails to fully exploit the LLM's potential and suffers from
unstable finetuning. As a result, LLM blocks are kept frozen while only the
vision components are learned. As a remedy to these challenges, we introduce
Language-Unlocked Vision Transformers (LUViT), a novel approach that bridges
this modality mismatch through a synergistic pre-training strategy. LUViT
co-adapts a ViT backbone and an LLM fusion block by (1) employing Masked
Auto-Encoding (MAE) to pre-train the ViT for richer visual representations, and
(2) concurrently training Low-Rank Adaptation (LoRA) layers within the LLM
block using the MAE objective. This joint optimization guides the ViT to
produce LLM-aligned features and the LLM to effectively interpret visual
information. We demonstrate through extensive experiments that LUViT
significantly improves performance on various downstream vision tasks,
showcasing a more effective and efficient pathway to harness LLM knowledge for
visual understanding.

</details>


### [195] [Towards Open-World Human Action Segmentation Using Graph Convolutional Networks](https://arxiv.org/abs/2507.00756)
*Hao Xing,Kai Zhe Boey,Gordon Cheng*

Main category: cs.CV

TL;DR: The paper addresses open-world human-object interaction segmentation by proposing a structured framework with innovations like EPGCN, Mixup-based training, and Temporal Clustering loss, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with open-world scenarios where novel actions emerge, making exhaustive training impractical. The paper aims to detect and segment unseen actions without manual annotation.

Method: Proposes a framework with: 1) EPGCN for spatiotemporal feature upsampling, 2) Mixup-based training for synthetic out-of-distribution data, and 3) Temporal Clustering loss to group in-distribution actions and distance out-of-distribution samples.

Result: Achieves 16.9% and 34.6% relative gains in open-set segmentation (F1@50) and out-of-distribution detection (AUROC) on Bimanual Actions and H2O datasets.

Conclusion: The framework effectively addresses open-world action segmentation, with ablation studies confirming the impact of each component.

Abstract: Human-object interaction segmentation is a fundamental task of daily activity
understanding, which plays a crucial role in applications such as assistive
robotics, healthcare, and autonomous systems. Most existing learning-based
methods excel in closed-world action segmentation, they struggle to generalize
to open-world scenarios where novel actions emerge. Collecting exhaustive
action categories for training is impractical due to the dynamic diversity of
human activities, necessitating models that detect and segment
out-of-distribution actions without manual annotation. To address this issue,
we formally define the open-world action segmentation problem and propose a
structured framework for detecting and segmenting unseen actions. Our framework
introduces three key innovations: 1) an Enhanced Pyramid Graph Convolutional
Network (EPGCN) with a novel decoder module for robust spatiotemporal feature
upsampling. 2) Mixup-based training to synthesize out-of-distribution data,
eliminating reliance on manual annotations. 3) A novel Temporal Clustering loss
that groups in-distribution actions while distancing out-of-distribution
samples.
  We evaluate our framework on two challenging human-object interaction
recognition datasets: Bimanual Actions and 2 Hands and Object (H2O) datasets.
Experimental results demonstrate significant improvements over state-of-the-art
action segmentation models across multiple open-set evaluation metrics,
achieving 16.9% and 34.6% relative gains in open-set segmentation (F1@50) and
out-of-distribution detection performances (AUROC), respectively. Additionally,
we conduct an in-depth ablation study to assess the impact of each proposed
component, identifying the optimal framework configuration for open-world
action segmentation.

</details>


### [196] [OptiPrune: Boosting Prompt-Image Consistency with Attention-Guided Noise and Dynamic Token Selection](https://arxiv.org/abs/2507.00789)
*Ziji Lu*

Main category: cs.CV

TL;DR: OptiPrune combines noise optimization and token pruning to improve semantic alignment in text-to-image diffusion models while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing methods either increase computational overhead or reduce semantic fidelity. OptiPrune aims to balance both.

Method: Uses distribution-aware noise optimization and similarity-based token pruning to enhance alignment and efficiency.

Result: Achieves state-of-the-art prompt-image consistency with lower computational cost.

Conclusion: OptiPrune effectively addresses semantic alignment and efficiency in text-to-image diffusion models.

Abstract: Text-to-image diffusion models often struggle to achieve accurate semantic
alignment between generated images and text prompts while maintaining
efficiency for deployment on resource-constrained hardware. Existing approaches
either incur substantial computational overhead through noise optimization or
compromise semantic fidelity by aggressively pruning tokens. In this work, we
propose OptiPrune, a unified framework that combines distribution-aware initial
noise optimization with similarity-based token pruning to address both
challenges simultaneously. Specifically, (1) we introduce a distribution-aware
noise optimization module guided by attention scores to steer the initial
latent noise toward semantically meaningful regions, mitigating issues such as
subject neglect and feature entanglement; (2) we design a hardware-efficient
token pruning strategy that selects representative base tokens via patch-wise
similarity, injects randomness to enhance generalization, and recovers pruned
tokens using maximum similarity copying before attention operations. Our method
preserves the Gaussian prior during noise optimization and enables efficient
inference without sacrificing alignment quality. Experiments on benchmark
datasets, including Animal-Animal, demonstrate that OptiPrune achieves
state-of-the-art prompt-image consistency with significantly reduced
computational cost.

</details>


### [197] [LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling](https://arxiv.org/abs/2507.00790)
*Huaqiu Li,Yong Wang,Tongwen Huang,Hailang Huang,Haoqian Wang,Xiangxiang Chu*

Main category: cs.CV

TL;DR: A novel, dataset-free unified image restoration method using a pretrained latent diffusion model with multimodal understanding and recurrent refinement.


<details>
  <summary>Details</summary>
Motivation: Existing methods are either task-specific or rely on paired datasets, limiting generalizability and suffering from closed-set constraints.

Method: Uses a pretrained latent diffusion model with multimodal understanding for semantic priors, aligns degraded input with diffusion model preferences via a lightweight module, and employs recurrent refinement for posterior sampling.

Result: Outperforms state-of-the-art methods in experiments, demonstrating effectiveness and robustness.

Conclusion: The proposed approach is effective, robust, and generalizable for unified image restoration without needing paired datasets.

Abstract: Unified image restoration is a significantly challenging task in low-level
vision. Existing methods either make tailored designs for specific tasks,
limiting their generalizability across various types of degradation, or rely on
training with paired datasets, thereby suffering from closed-set constraints.
To address these issues, we propose a novel, dataset-free, and unified approach
through recurrent posterior sampling utilizing a pretrained latent diffusion
model. Our method incorporates the multimodal understanding model to provide
sematic priors for the generative model under a task-blind condition.
Furthermore, it utilizes a lightweight module to align the degraded input with
the generated preference of the diffusion model, and employs recurrent
refinement for posterior sampling. Extensive experiments demonstrate that our
method outperforms state-of-the-art methods, validating its effectiveness and
robustness. Our code and data will be available at
https://github.com/AMAP-ML/LD-RPS.

</details>


### [198] [Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters](https://arxiv.org/abs/2507.00792)
*Hendric Voss,Stefan Kopp*

Main category: cs.CV

TL;DR: A novel real-time inverse kinematics (IK) solver for realistic human-like movement generation, leveraging TensorFlow for efficiency and addressing common challenges like error accumulation and joint limits.


<details>
  <summary>Details</summary>
Motivation: Generating accurate and realistic virtual human movements is crucial for applications in computer graphics, virtual environments, robotics, and biomechanics.

Method: Uses automatic differentiation and just-in-time compilation of TensorFlow to handle complex human skeletons. Treats forward and inverse kinematics as differentiable operations.

Result: Achieves real-time performance with rapid convergence, minimal computational overhead, and improved success rates compared to existing methods like CCD, FABRIK, and IPOPT.

Conclusion: The proposed IK solver is effective for realistic human motion modeling, outperforming traditional iterative-based algorithms.

Abstract: Generating accurate and realistic virtual human movements in real-time is of
high importance for a variety of applications in computer graphics, interactive
virtual environments, robotics, and biomechanics. This paper introduces a novel
real-time inverse kinematics (IK) solver specifically designed for realistic
human-like movement generation. Leveraging the automatic differentiation and
just-in-time compilation of TensorFlow, the proposed solver efficiently handles
complex articulated human skeletons with high degrees of freedom. By treating
forward and inverse kinematics as differentiable operations, our method
effectively addresses common challenges such as error accumulation and
complicated joint limits in multi-constrained problems, which are critical for
realistic human motion modeling. We demonstrate the solver's effectiveness on
the SMPLX human skeleton model, evaluating its performance against widely used
iterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK,
and the nonlinear optimization algorithm IPOPT. Our experiments cover both
simple end-effector tasks and sophisticated, multi-constrained problems with
realistic joint limits. Results indicate that our IK solver achieves real-time
performance, exhibiting rapid convergence, minimal computational overhead per
iteration, and improved success rates compared to existing methods. The project
code is available at https://github.com/hvoss-techfak/TF-JAX-IK

</details>


### [199] [TRACE: Temporally Reliable Anatomically-Conditioned 3D CT Generation with Enhanced Efficiency](https://arxiv.org/abs/2507.00802)
*Minye Shao,Xingyu Miao,Haoran Duan,Zeyu Wang,Jingkun Chen,Yawen Huang,Xian Wu,Jingjing Deng,Yang Long,Yefeng Zheng*

Main category: cs.CV

TL;DR: TRACE is a framework for generating 3D medical images using a 2D multimodal-conditioned diffusion approach, addressing limitations like anatomical fidelity and computational cost.


<details>
  <summary>Details</summary>
Motivation: Current 3D medical image generation methods lack anatomical fidelity, have restricted axial length, and are computationally expensive, limiting their use in resource-constrained regions.

Method: TRACE models 2D slices as video frame pairs, using segmentation priors, radiology reports, and optical flow for alignment and coherence. An overlapping-frame strategy links frames into sequences for 3D reconstruction.

Result: TRACE balances computational efficiency with anatomical fidelity and spatiotemporal consistency, demonstrated in experiments.

Conclusion: TRACE offers a reliable and efficient solution for 3D medical image generation, suitable for clinical practice and resource-limited settings.

Abstract: 3D medical image generation is essential for data augmentation and patient
privacy, calling for reliable and efficient models suited for clinical
practice. However, current methods suffer from limited anatomical fidelity,
restricted axial length, and substantial computational cost, placing them
beyond reach for regions with limited resources and infrastructure. We
introduce TRACE, a framework that generates 3D medical images with
spatiotemporal alignment using a 2D multimodal-conditioned diffusion approach.
TRACE models sequential 2D slices as video frame pairs, combining segmentation
priors and radiology reports for anatomical alignment, incorporating optical
flow to sustain temporal coherence. During inference, an overlapping-frame
strategy links frame pairs into a flexible length sequence, reconstructed into
a spatiotemporally and anatomically aligned 3D volume. Experimental results
demonstrate that TRACE effectively balances computational efficiency with
preserving anatomical fidelity and spatiotemporal consistency. Code is
available at: https://github.com/VinyehShaw/TRACE.

</details>


### [200] [CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs](https://arxiv.org/abs/2507.00817)
*Jiaming Zhang,Rui Hu,Qing Guo,Wei Yang Bryan Lim*

Main category: cs.CV

TL;DR: CAVALRY-V is a novel adversarial attack framework targeting V-MLLMs, outperforming existing methods by disrupting cross-modal integration and leveraging efficient two-stage generation.


<details>
  <summary>Details</summary>
Motivation: The vulnerability of V-MLLMs to adversarial attacks is underexplored due to challenges like cross-modal reasoning and temporal dependencies.

Method: Introduces a dual-objective loss function and a two-stage generator framework for efficient adversarial attack generation.

Result: Achieves 22.8% average improvement over baselines on video understanding benchmarks and 34.4% gain on image tasks.

Conclusion: CAVALRY-V is a foundational approach for adversarial research in multimodal systems, demonstrating flexibility and effectiveness.

Abstract: Video Multimodal Large Language Models (V-MLLMs) have shown impressive
capabilities in temporal reasoning and cross-modal understanding, yet their
vulnerability to adversarial attacks remains underexplored due to unique
challenges: complex cross-modal reasoning mechanisms, temporal dependencies,
and computational constraints. We present CAVALRY-V (Cross-modal
Language-Vision Adversarial Yielding for Videos), a novel framework that
directly targets the critical interface between visual perception and language
generation in V-MLLMs. Our approach introduces two key innovations: (1) a
dual-objective semantic-visual loss function that simultaneously disrupts the
model's text generation logits and visual representations to undermine
cross-modal integration, and (2) a computationally efficient two-stage
generator framework that combines large-scale pre-training for cross-model
transferability with specialized fine-tuning for spatiotemporal coherence.
Empirical evaluation on comprehensive video understanding benchmarks
demonstrates that CAVALRY-V significantly outperforms existing attack methods,
achieving 22.8% average improvement over the best baseline attacks on both
commercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5,
InternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves
flexibility through implicit temporal coherence modeling rather than explicit
regularization, enabling significant performance improvements even on image
understanding (34.4% average gain). This capability demonstrates CAVALRY-V's
potential as a foundational approach for adversarial research across multimodal
systems.

</details>


### [201] [Instant Particle Size Distribution Measurement Using CNNs Trained on Synthetic Data](https://arxiv.org/abs/2507.00822)
*Yasser El Jarida,Youssef Iraqi,Loubna Mekouar*

Main category: cs.CV

TL;DR: A CNN-based method using synthetic particle images for real-time PSD measurement, outperforming traditional methods in efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate PSD measurement is crucial in industries like mining and pharmaceuticals, but traditional methods are manual and slow.

Method: Uses synthetic particle images generated with Blender to train CNNs (ResNet-50, InceptionV3, EfficientNet-B0) for PSD estimation.

Result: EfficientNet-B0 showed the best computational efficiency, with all models achieving comparable accuracy.

Conclusion: Synthetic data enables robust CNN training, offering potential for automated industrial PSD monitoring.

Abstract: Accurate particle size distribution (PSD) measurement is important in
industries such as mining, pharmaceuticals, and fertilizer manufacturing,
significantly influencing product quality and operational efficiency.
Traditional PSD methods like sieve analysis and laser diffraction are manual,
time-consuming, and limited by particle overlap. Recent developments in
convolutional neural networks (CNNs) enable automated, real-time PSD estimation
directly from particle images. In this work, we present a CNN-based methodology
trained on realistic synthetic particle imagery generated using Blender's
advanced rendering capabilities. Synthetic data sets using this method can
replicate various industrial scenarios by systematically varying particle
shapes, textures, lighting, and spatial arrangements that closely resemble the
actual configurations. We evaluated three CNN-based architectures, ResNet-50,
InceptionV3, and EfficientNet-B0, for predicting critical PSD parameters (d10,
d50, d90). Results demonstrated comparable accuracy across models, with
EfficientNet-B0 achieving the best computational efficiency suitable for
real-time industrial deployment. This approach shows the effectiveness of
realistic synthetic data for robust CNN training, which offers significant
potential for automated industrial PSD monitoring. The code is released at :
https://github.com/YasserElj/Synthetic-Granular-Gen

</details>


### [202] [High-Frequency Semantics and Geometric Priors for End-to-End Detection Transformers in Challenging UAV Imagery](https://arxiv.org/abs/2507.00825)
*Hongxing Peng,Lide Chen,Hui Zhu,Yan Chen*

Main category: cs.CV

TL;DR: HEGS-DETR is a real-time Detection Transformer for UAV-based object detection, addressing challenges like small targets and cluttered backgrounds with novel modules like HFESNet, ESOP, SQR, and GAPE, achieving improved performance on the VisDrone dataset.


<details>
  <summary>Details</summary>
Motivation: Current UAV-OD algorithms struggle with small targets, high-density distributions, and cluttered backgrounds due to reliance on hand-crafted components and generic architectures, limiting performance and generalization.

Method: Proposes HEGS-DETR with HFESNet for high-frequency detail preservation, ESOP for small object detection, and SQR and GAPE for decoder stability and localization accuracy.

Result: Achieves 5.1% AP$_{50}$ and 3.8% AP increase over baseline on VisDrone, with real-time speed and reduced parameters.

Conclusion: HEGS-DETR effectively addresses UAV-OD challenges, offering improved accuracy and efficiency.

Abstract: Unmanned Aerial Vehicle-based Object Detection (UAV-OD) faces substantial
challenges, including small target sizes, high-density distributions, and
cluttered backgrounds in UAV imagery. Current algorithms often depend on
hand-crafted components like anchor boxes, which demand fine-tuning and exhibit
limited generalization, and Non-Maximum Suppression (NMS), which is
threshold-sensitive and prone to misclassifying dense objects. These generic
architectures thus struggle to adapt to aerial imaging characteristics,
resulting in performance limitations. Moreover, emerging end-to-end frameworks
have yet to effectively mitigate these aerial-specific challenges.To address
these issues, we propose HEGS-DETR, a comprehensively enhanced, real-time
Detection Transformer framework tailored for UAVs. First, we introduce the
High-Frequency Enhanced Semantics Network (HFESNet) as a novel backbone.
HFESNet preserves critical high-frequency spatial details to extract robust
semantic features, thereby improving discriminative capability for small and
occluded targets in complex backgrounds. Second, our Efficient Small Object
Pyramid (ESOP) strategy strategically fuses high-resolution feature maps with
minimal computational overhead, significantly boosting small object detection.
Finally, the proposed Selective Query Recollection (SQR) and Geometry-Aware
Positional Encoding (GAPE) modules enhance the detector's decoder stability and
localization accuracy, effectively optimizing bounding boxes and providing
explicit spatial priors for dense scenes. Experiments on the VisDrone dataset
demonstrate that HEGS-DETR achieves a 5.1\% AP$_{50}$ and 3.8\% AP increase
over the baseline, while maintaining real-time speed and reducing parameter
count by 4M.

</details>


### [203] [Do Echo Top Heights Improve Deep Learning Nowcasts?](https://arxiv.org/abs/2507.00845)
*Peter Pavlk,Marc Schleiss,Anna Bou Ezzeddine,Viera Rozinajov*

Main category: cs.CV

TL;DR: The paper explores using Echo Top Height (ETH) as an auxiliary input for deep learning-based precipitation nowcasting, finding mixed results but highlighting its potential for improving low rain-rate predictions.


<details>
  <summary>Details</summary>
Motivation: Improve short-term rainfall prediction by incorporating vertical information (ETH) alongside 2D radar reflectivity, addressing limitations of current deep learning models.

Method: Implemented a 3D U-Net to process radar reflectivity and ETH as separate inputs, analyzing their relationship and impact on nowcasting.

Result: ETH improved predictions at low rain rates but led to underestimation and inconsistency at higher intensities, with case studies showing varied effects.

Conclusion: ETH shows promise for nowcasting but requires further refinement; the study provides a foundation for evaluating additional variables in future work.

Abstract: Precipitation nowcasting -- the short-term prediction of rainfall using
recent radar observations -- is critical for weather-sensitive sectors such as
transportation, agriculture, and disaster mitigation. While recent deep
learning models have shown promise in improving nowcasting skill, most
approaches rely solely on 2D radar reflectivity fields, discarding valuable
vertical information available in the full 3D radar volume. In this work, we
explore the use of Echo Top Height (ETH), a 2D projection indicating the
maximum altitude of radar reflectivity above a given threshold, as an auxiliary
input variable for deep learning-based nowcasting. We examine the relationship
between ETH and radar reflectivity, confirming its relevance for predicting
rainfall intensity. We implement a single-pass 3D U-Net that processes both the
radar reflectivity and ETH as separate input channels. While our models are
able to leverage ETH to improve skill at low rain-rate thresholds, results are
inconsistent at higher intensities and the models with ETH systematically
underestimate precipitation intensity. Three case studies are used to
illustrate how ETH can help in some cases, but also confuse the models and
increase the error variance. Nonetheless, the study serves as a foundation for
critically assessing the potential contribution of additional variables to
nowcasting performance.

</details>


### [204] [UAVD-Mamba: Deformable Token Fusion Vision Mamba for Multimodal UAV Detection](https://arxiv.org/abs/2507.00849)
*Wei Li,Jiaman Tang,Yang Li,Beihao Xia,Ligang Tan,Hongmao Qin*

Main category: cs.CV

TL;DR: UAVD-Mamba, a multimodal UAV object detection framework, improves detection by combining deformable tokens, multimodal feature fusion, and multiscale processing, outperforming baselines by 3.6% mAP.


<details>
  <summary>Details</summary>
Motivation: Challenges like occlusions, small objects, and irregular shapes in UAV object detection necessitate robust multimodal methods.

Method: Proposes UAVD-Mamba with Deformable Token Mamba Blocks (DTMB) for geometric adaptability, separate DTMBs for RGB/IR fusion, and multiscale stacking. Includes cross-enhanced attention and modified YOLOv11 components.

Result: Outperforms OAFA by 3.6% mAP on the DroneVehicle dataset.

Conclusion: UAVD-Mamba effectively addresses UAV detection challenges through multimodal fusion and multiscale processing, demonstrating superior performance.

Abstract: Unmanned Aerial Vehicle (UAV) object detection has been widely used in
traffic management, agriculture, emergency rescue, etc. However, it faces
significant challenges, including occlusions, small object sizes, and irregular
shapes. These challenges highlight the necessity for a robust and efficient
multimodal UAV object detection method. Mamba has demonstrated considerable
potential in multimodal image fusion. Leveraging this, we propose UAVD-Mamba, a
multimodal UAV object detection framework based on Mamba architectures. To
improve geometric adaptability, we propose the Deformable Token Mamba Block
(DTMB) to generate deformable tokens by incorporating adaptive patches from
deformable convolutions alongside normal patches from normal convolutions,
which serve as the inputs to the Mamba Block. To optimize the multimodal
feature complementarity, we design two separate DTMBs for the RGB and infrared
(IR) modalities, with the outputs from both DTMBs integrated into the Mamba
Block for feature extraction and into the Fusion Mamba Block for feature
fusion. Additionally, to improve multiscale object detection, especially for
small objects, we stack four DTMBs at different scales to produce multiscale
feature representations, which are then sent to the Detection Neck for Mamba
(DNM). The DNM module, inspired by the YOLO series, includes modifications to
the SPPF and C3K2 of YOLOv11 to better handle the multiscale features. In
particular, we employ cross-enhanced spatial attention before the DTMB and
cross-channel attention after the Fusion Mamba Block to extract more
discriminative features. Experimental results on the DroneVehicle dataset show
that our method outperforms the baseline OAFA method by 3.6% in the mAP metric.
Codes will be released at https://github.com/GreatPlum-hnu/UAVD-Mamba.git.

</details>


### [205] [Robust Component Detection for Flexible Manufacturing: A Deep Learning Approach to Tray-Free Object Recognition under Variable Lighting](https://arxiv.org/abs/2507.00852)
*Fatemeh Sadat Daneshmand*

Main category: cs.CV

TL;DR: A Mask R-CNN-based vision system enables robots to detect and grasp pen components in any orientation under varying lighting, improving flexibility and reducing setup time.


<details>
  <summary>Details</summary>
Motivation: Industry 4.0 demands robots that can handle objects in unstructured environments without rigid positioning constraints.

Method: Implemented and evaluated a Mask R-CNN-based approach for object detection and grasping in a pen manufacturing line, addressing challenges like arbitrary orientations, lighting variations, and cost-effective cameras.

Result: Achieved 95% detection accuracy across lighting conditions, eliminated structured placement needs, and reduced setup time by 30%.

Conclusion: The system is practical for industrial deployment, validated under diverse lighting scenarios, enhancing manufacturing flexibility.

Abstract: Flexible manufacturing systems in Industry 4.0 require robots capable of
handling objects in unstructured environments without rigid positioning
constraints. This paper presents a computer vision system that enables
industrial robots to detect and grasp pen components in arbitrary orientations
without requiring structured trays, while maintaining robust performance under
varying lighting conditions. We implement and evaluate a Mask R-CNN-based
approach on a complete pen manufacturing line at ZHAW, addressing three
critical challenges: object detection without positional constraints,
robustness to extreme lighting variations, and reliable performance with
cost-effective cameras. Our system achieves 95% detection accuracy across
diverse lighting conditions while eliminating the need for structured component
placement, demonstrating a 30% reduction in setup time and significant
improvement in manufacturing flexibility. The approach is validated through
extensive testing under four distinct lighting scenarios, showing practical
applicability for real-world industrial deployment.

</details>


### [206] [SafeMap: Robust HD Map Construction from Incomplete Observations](https://arxiv.org/abs/2507.00861)
*Xiaoshuai Hao,Lingdong Kong,Rong Yin,Pengwei Wang,Jing Zhang,Yunfeng Diao,Shu Zhao*

Main category: cs.CV

TL;DR: SafeMap is a novel framework for robust HD map construction in autonomous driving, addressing incomplete camera data with Gaussian-based view reconstruction and distillation-based BEV correction.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with incomplete multi-view camera data, necessitating a solution like SafeMap to ensure accuracy even with missing views.

Method: SafeMap integrates G-PVR (Gaussian-based Perspective View Reconstruction) for dynamic prioritization of informative regions and D-BEVC (Distillation-based Bird's-Eye-View Correction) to refine BEV representations from incomplete data.

Result: SafeMap outperforms previous methods in both complete and incomplete scenarios, demonstrating superior performance and reliability.

Conclusion: SafeMap provides a plug-and-play solution for robust HD map generation, enhancing autonomous driving systems.

Abstract: Robust high-definition (HD) map construction is vital for autonomous driving,
yet existing methods often struggle with incomplete multi-view camera data.
This paper presents SafeMap, a novel framework specifically designed to secure
accuracy even when certain camera views are missing. SafeMap integrates two key
components: the Gaussian-based Perspective View Reconstruction (G-PVR) module
and the Distillation-based Bird's-Eye-View (BEV) Correction (D-BEVC) module.
G-PVR leverages prior knowledge of view importance to dynamically prioritize
the most informative regions based on the relationships among available camera
views. Furthermore, D-BEVC utilizes panoramic BEV features to correct the BEV
representations derived from incomplete observations. Together, these
components facilitate the end-to-end map reconstruction and robust HD map
generation. SafeMap is easy to implement and integrates seamlessly into
existing systems, offering a plug-and-play solution for enhanced robustness.
Experimental results demonstrate that SafeMap significantly outperforms
previous methods in both complete and incomplete scenarios, highlighting its
superior performance and reliability.

</details>


### [207] [Is Visual in-Context Learning for Compositional Medical Tasks within Reach?](https://arxiv.org/abs/2507.00868)
*Simon Rei,Zdravko Marinov,Alexander Jaus,Constantin Seibold,M. Saquib Sarfraz,Erik Rodner,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: A novel method for training visual in-context learners to handle multiple tasks adaptively without re-training, focusing on compositional tasks and synthetic task generation.


<details>
  <summary>Details</summary>
Motivation: To enable a single model to adapt to sequences of tasks and solve complex, multi-step vision tasks flexibly at test time.

Method: Examines visual in-context learning architectures, introduces a synthetic compositional task generation engine, and explores masking-based training objectives.

Result: Provides insights for multi-modal medical task sequences and identifies challenges in training models for compositional tasks.

Conclusion: The approach advances adaptive visual in-context learning but highlights unresolved challenges for complex task sequences.

Abstract: In this paper, we explore the potential of visual in-context learning to
enable a single model to handle multiple tasks and adapt to new tasks during
test time without re-training. Unlike previous approaches, our focus is on
training in-context learners to adapt to sequences of tasks, rather than
individual tasks. Our goal is to solve complex tasks that involve multiple
intermediate steps using a single model, allowing users to define entire vision
pipelines flexibly at test time. To achieve this, we first examine the
properties and limitations of visual in-context learning architectures, with a
particular focus on the role of codebooks. We then introduce a novel method for
training in-context learners using a synthetic compositional task generation
engine. This engine bootstraps task sequences from arbitrary segmentation
datasets, enabling the training of visual in-context learners for compositional
tasks. Additionally, we investigate different masking-based training objectives
to gather insights into how to train models better for solving complex,
compositional tasks. Our exploration not only provides important insights
especially for multi-modal medical task sequences but also highlights
challenges that need to be addressed.

</details>


### [208] [GaussianVLM: Scene-centric 3D Vision-Language Models using Language-aligned Gaussian Splats for Embodied Reasoning and Beyond](https://arxiv.org/abs/2507.00886)
*Anna-Maria Halacheva,Jan-Nico Zaech,Xi Wang,Danda Pani Paudel,Luc Van Gool*

Main category: cs.CV

TL;DR: A scene-centric 3D Vision-Language Model (VLM) for 3D Gaussian splat scenes is proposed, embedding linguistic features directly into 3D representations and using a dual sparsifier for efficient processing. It outperforms prior methods by fivefold in generalization.


<details>
  <summary>Details</summary>
Motivation: Current 3D VLMs rely heavily on object detectors, causing bottlenecks and limiting taxonomic flexibility. This work aims to overcome these issues by integrating language directly into 3D scene representations.

Method: The approach embeds language into Gaussian primitives for early modality alignment and uses a dual sparsifier (task-guided and location-guided pathways) to distill dense representations into compact, task-relevant tokens.

Result: The proposed Gaussian splatting-based VLM achieves a fivefold improvement in generalization over prior methods, especially in out-of-domain settings.

Conclusion: The method successfully addresses limitations of current 3D VLMs by leveraging direct language embedding and efficient representation processing, demonstrating significant performance gains.

Abstract: As multimodal language models advance, their application to 3D scene
understanding is a fast-growing frontier, driving the development of 3D
Vision-Language Models (VLMs). Current methods show strong dependence on object
detectors, introducing processing bottlenecks and limitations in taxonomic
flexibility. To address these limitations, we propose a scene-centric 3D VLM
for 3D Gaussian splat scenes that employs language- and task-aware scene
representations. Our approach directly embeds rich linguistic features into the
3D scene representation by associating language with each Gaussian primitive,
achieving early modality alignment. To process the resulting dense
representations, we introduce a dual sparsifier that distills them into
compact, task-relevant tokens via task-guided and location-guided pathways,
producing sparse, task-aware global and local scene tokens. Notably, we present
the first Gaussian splatting-based VLM, leveraging photorealistic 3D
representations derived from standard RGB images, demonstrating strong
generalization: it improves performance of prior 3D VLM five folds, in
out-of-the-domain settings.

</details>


### [209] [Masks make discriminative models great again!](https://arxiv.org/abs/2507.00916)
*Tianshi Cao,Marie-Julie Rakotosaona,Ben Poole,Federico Tombari,Michael Niemeyer*

Main category: cs.CV

TL;DR: Image2GS improves 3D scene reconstruction from a single image by focusing on visible regions using masked training, outperforming baselines in visible areas.


<details>
  <summary>Details</summary>
Motivation: The challenge of reconstructing photorealistic 3D scenes from a single image, particularly the image-to-3D lifting component, is addressed by decoupling it from the completion problem.

Method: Uses visibility masks from optimized 3D Gaussian splats to train discriminatively on visible regions, excluding unseen areas.

Result: Significantly improves reconstruction quality in visible regions and remains competitive with state-of-the-art models on complete scenes.

Conclusion: Specialized techniques for image-to-3D lifting outperform general approaches, highlighting the difficulty of fitting unseen regions.

Abstract: We present Image2GS, a novel approach that addresses the challenging problem
of reconstructing photorealistic 3D scenes from a single image by focusing
specifically on the image-to-3D lifting component of the reconstruction
process. By decoupling the lifting problem (converting an image to a 3D model
representing what is visible) from the completion problem (hallucinating
content not present in the input), we create a more deterministic task suitable
for discriminative models. Our method employs visibility masks derived from
optimized 3D Gaussian splats to exclude areas not visible from the source view
during training. This masked training strategy significantly improves
reconstruction quality in visible regions compared to strong baselines.
Notably, despite being trained only on masked regions, Image2GS remains
competitive with state-of-the-art discriminative models trained on full target
images when evaluated on complete scenes. Our findings highlight the
fundamental struggle discriminative models face when fitting unseen regions and
demonstrate the advantages of addressing image-to-3D lifting as a distinct
problem with specialized techniques.

</details>


### [210] [MVP: Winning Solution to SMP Challenge 2025 Video Track](https://arxiv.org/abs/2507.00950)
*Liliang Ye,Yunyao Zhang,Yafeng Wu,Yi-Ping Phoebe Chen,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.CV

TL;DR: MVP, a multimodal framework, won the SMP Challenge 2025 by integrating video features, user metadata, and contextual data for accurate video popularity prediction.


<details>
  <summary>Details</summary>
Motivation: Predicting video popularity on social media aids in recommendation, trend detection, and engagement, necessitating robust multimodal solutions.

Method: MVP combines deep video features, user metadata, and contextual info, preprocesses data (log-transformations, outlier removal), and uses gradient-boosted regression.

Result: MVP ranked first in the SMP Challenge 2025, proving its effectiveness for multimodal video popularity prediction.

Conclusion: MVP is a reliable solution for predicting video popularity on social media, with potential applications in content recommendation and trend analysis.

Abstract: Social media platforms serve as central hubs for content dissemination,
opinion expression, and public engagement across diverse modalities. Accurately
predicting the popularity of social media videos enables valuable applications
in content recommendation, trend detection, and audience engagement. In this
paper, we present Multimodal Video Predictor (MVP), our winning solution to the
Video Track of the SMP Challenge 2025. MVP constructs expressive post
representations by integrating deep video features extracted from pretrained
models with user metadata and contextual information. The framework applies
systematic preprocessing techniques, including log-transformations and outlier
removal, to improve model robustness. A gradient-boosted regression model is
trained to capture complex patterns across modalities. Our approach ranked
first in the official evaluation of the Video Track, demonstrating its
effectiveness and reliability for multimodal video popularity prediction on
social platforms. The source code is available at
https://anonymous.4open.science/r/SMPDVideo.

</details>


### [211] [Surgical Neural Radiance Fields from One Image](https://arxiv.org/abs/2507.00969)
*Alberto Neri,Maximilan Fehrentz,Veronica Penza,Leonardo S. Mattos,Nazim Haouchine*

Main category: cs.CV

TL;DR: Single-image NeRF training for surgical scenarios using preoperative data and neural style transfer.


<details>
  <summary>Details</summary>
Motivation: Overcome the impracticality of collecting extensive multi-view data intraoperatively for NeRF in surgical settings.

Method: Leverage preoperative MRI data and neural style transfer (WTC2 and STROTSS) to create a training dataset from a single intraoperative image.

Result: High reconstruction fidelity and stylistic alignment, validated with clinical neurosurgical cases.

Conclusion: Feasible single-image NeRF training for surgery, surpassing traditional multi-view limitations.

Abstract: Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D
reconstruction and view synthesis, yet their reliance on extensive multi-view
data limits their application in surgical intraoperative settings where only
limited data is available. In particular, collecting such extensive data
intraoperatively is impractical due to time constraints. This work addresses
this challenge by leveraging a single intraoperative image and preoperative
data to train NeRF efficiently for surgical scenarios.
  Methods: We leverage preoperative MRI data to define the set of camera
viewpoints and images needed for robust and unobstructed training.
Intraoperatively, the appearance of the surgical image is transferred to the
pre-constructed training set through neural style transfer, specifically
combining WTC2 and STROTSS to prevent over-stylization. This process enables
the creation of a dataset for instant and fast single-image NeRF training.
  Results: The method is evaluated with four clinical neurosurgical cases.
Quantitative comparisons to NeRF models trained on real surgical microscope
images demonstrate strong synthesis agreement, with similarity metrics
indicating high reconstruction fidelity and stylistic alignment. When compared
with ground truth, our method demonstrates high structural similarity,
confirming good reconstruction quality and texture preservation.
  Conclusion: Our approach demonstrates the feasibility of single-image NeRF
training in surgical settings, overcoming the limitations of traditional
multi-view methods.

</details>


### [212] [RTMap: Real-Time Recursive Mapping with Change Detection and Localization](https://arxiv.org/abs/2507.00980)
*Yuheng Du,Sheng Yang,Lingxuan Wang,Zhenghua Hou,Chengying Cai,Zhitao Tan,Mingxia Chen,Shi-Sheng Huang,Qiang Li*

Main category: cs.CV

TL;DR: RTMap enhances single-traversal HD mapping by crowdsourcing a multi-traversal HD map, addressing uncertainty, localization, and real-time road changes, improving map quality and localization accuracy.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of online HD mapping methods like perceptual inaccuracies, occlusion, and lack of multi-agent fusion.

Method: Proposes RTMap, an end-to-end solution with uncertainty-aware positional modeling, probabilistic localization, and real-time road change detection.

Result: Demonstrates improved prior-aided map quality and localization accuracy on public datasets.

Conclusion: RTMap effectively supports downstream tasks while asynchronously improving map accuracy and freshness.

Abstract: While recent online HD mapping methods relieve burdened offline pipelines and
solve map freshness, they remain limited by perceptual inaccuracies, occlusion
in dense traffic, and an inability to fuse multi-agent observations. We propose
RTMap to enhance these single-traversal methods by persistently crowdsourcing a
multi-traversal HD map as a self-evolutional memory. On onboard agents, RTMap
simultaneously addresses three core challenges in an end-to-end fashion: (1)
Uncertainty-aware positional modeling for HD map elements, (2)
probabilistic-aware localization w.r.t. the crowdsourced prior-map, and (3)
real-time detection for possible road structural changes. Experiments on
several public autonomous driving datasets demonstrate our solid performance on
both the prior-aided map quality and the localization accuracy, demonstrating
our effectiveness of robustly serving downstream prediction and planning
modules while gradually improving the accuracy and freshness of the
crowdsourced prior-map asynchronously. Our source-code will be made publicly
available at https://github.com/CN-ADLab/RTMap (Camera ready version
incorporating reviewer suggestions will be updated soon).

</details>


### [213] [Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations](https://arxiv.org/abs/2507.00981)
*Jack Nugent,Siyang Wu,Zeyu Ma,Beining Han,Meenal Parakh,Abhishek Joshi,Lingjie Mei,Alexander Raistrick,Xinyuan Li,Jia Deng*

Main category: cs.CV

TL;DR: PDE introduces a procedural benchmark for evaluating robustness in monocular depth estimation, addressing gaps in standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: Standard benchmarks focus on accuracy but lack robustness evaluation, limiting comprehensive assessment of depth models.

Method: PDE uses procedural generation to create 3D scenes with controlled perturbations (object, camera, material, lighting changes).

Result: Analysis reveals challenging perturbations for state-of-the-art depth models, providing insights for future research.

Conclusion: PDE offers a systematic robustness evaluation tool, encouraging further research in depth estimation.

Abstract: Recent years have witnessed substantial progress on monocular depth
estimation, particularly as measured by the success of large models on standard
benchmarks. However, performance on standard benchmarks does not offer a
complete assessment, because most evaluate accuracy but not robustness. In this
work, we introduce PDE (Procedural Depth Evaluation), a new benchmark which
enables systematic robustness evaluation. PDE uses procedural generation to
create 3D scenes that test robustness to various controlled perturbations,
including object, camera, material and lighting changes. Our analysis yields
interesting findings on what perturbations are challenging for state-of-the-art
depth models, which we hope will inform further research. Code and data are
available at https://github.com/princeton-vl/proc-depth-eval.

</details>


### [214] [UniGlyph: Unified Segmentation-Conditioned Diffusion for Precise Visual Text Synthesis](https://arxiv.org/abs/2507.00992)
*Yuanrui Wang,Cong Han,YafeiLi,Zhipeng Jin,Xiawei Li,SiNan Du,Wen Tao,Yi Yang,shuanglong li,Chun Yuan,Liu Lin*

Main category: cs.CV

TL;DR: A segmentation-guided framework for text-to-image generation improves visual text rendering by using pixel-level text masks and achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Accurate visual text rendering in text-to-image generation is hindered by issues like blurred glyphs and semantic drift, with existing methods lacking flexibility and style control.

Method: Proposes a framework with a fine-tuned bilingual segmentation model for text mask extraction and a diffusion model with adaptive glyph conditioning and region-specific loss.

Result: Outperforms prior methods on benchmarks, excelling in small text rendering and complex layouts.

Conclusion: The approach validates strong generalization and readiness for deployment, advancing text-to-image generation.

Abstract: Text-to-image generation has greatly advanced content creation, yet
accurately rendering visual text remains a key challenge due to blurred glyphs,
semantic drift, and limited style control. Existing methods often rely on
pre-rendered glyph images as conditions, but these struggle to retain original
font styles and color cues, necessitating complex multi-branch designs that
increase model overhead and reduce flexibility. To address these issues, we
propose a segmentation-guided framework that uses pixel-level visual text masks
-- rich in glyph shape, color, and spatial detail -- as unified conditional
inputs. Our method introduces two core components: (1) a fine-tuned bilingual
segmentation model for precise text mask extraction, and (2) a streamlined
diffusion model augmented with adaptive glyph conditioning and a
region-specific loss to preserve textual fidelity in both content and style.
Our approach achieves state-of-the-art performance on the AnyText benchmark,
significantly surpassing prior methods in both Chinese and English settings. To
enable more rigorous evaluation, we also introduce two new benchmarks:
GlyphMM-benchmark for testing layout and glyph consistency in complex
typesetting, and MiniText-benchmark for assessing generation quality in
small-scale text regions. Experimental results show that our model outperforms
existing methods by a large margin in both scenarios, particularly excelling at
small text rendering and complex layout preservation, validating its strong
generalization and deployment readiness.

</details>


### [215] [GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning](https://arxiv.org/abs/2507.01006)
*Wenyi Hong,Wenmeng Yu,Xiaotao Gu,Guo Wang,Guobing Gan,Haomiao Tang,Jiale Cheng,Ji Qi,Junhui Ji,Lihang Pan,Shuaiqi Duan,Weihan Wang,Yan Wang,Yean Cheng,Zehai He,Zhe Su,Zhen Yang,Ziyang Pan,Aohan Zeng,Baoxu Wang,Boyan Shi,Changyu Pang,Chenhui Zhang,Da Yin,Fan Yang,Guoqing Chen,Jiazheng Xu,Jiali Chen,Jing Chen,Jinhao Chen,Jinghao Lin,Jinjiang Wang,Junjie Chen,Leqi Lei,Leyi Pan,Mingzhi Zhang,Qinkai Zheng,Sheng Yang,Shi Zhong,Shiyu Huang,Shuyuan Zhao,Siyan Xue,Shangqin Tu,Shengbiao Meng,Tianshu Zhang,Tianwei Luo,Tianxiang Hao,Tianle Gong,Wenkai Li,Wei Jia,Xin Lyu,Xuancheng Huang,Yanling Wang,Yadong Xue,Yanfeng Wang,Yifan An,Yifan Du,Yiming Shi,Yiheng Huang,Yilin Niu,Yuan Wang,Yuanchang Yue,Yuchen Li,Yutao Zhang,Yuxuan Zhang,Zhanxiao Du,Zhenyu Hou,Zhao Xue,Zhengxiao Du,Zihan Wang,Peng Zhang,Debing Liu,Bin Xu,Juanzi Li,Minlie Huang,Yuxiao Dong,Jie Tang*

Main category: cs.CV

TL;DR: GLM-4.1V-Thinking is a vision-language model (VLM) that excels in multimodal reasoning, outperforming comparable models on diverse tasks and even rivaling larger or closed-source models like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: To advance general-purpose multimodal reasoning by developing a capable vision-language model with a reasoning-centric training framework.

Method: Combines large-scale pre-training for vision foundation with Reinforcement Learning with Curriculum Sampling (RLCS) to enhance capabilities across tasks like STEM, video understanding, and coding.

Result: Achieves state-of-the-art performance on 28 benchmarks, outperforming Qwen2.5-VL-7B and matching Qwen2.5-VL-72B and GPT-4o in some tasks.

Conclusion: GLM-4.1V-Thinking demonstrates strong multimodal reasoning capabilities, making it a competitive open-source alternative to larger or proprietary models.

Abstract: We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to
advance general-purpose multimodal reasoning. In this report, we share our key
findings in the development of the reasoning-centric training framework. We
first develop a capable vision foundation model with significant potential
through large-scale pre-training, which arguably sets the upper bound for the
final performance. Reinforcement Learning with Curriculum Sampling (RLCS) then
unlocks the full potential of the model, leading to comprehensive capability
enhancement across a diverse range of tasks, including STEM problem solving,
video understanding, content recognition, coding, grounding, GUI-based agents,
and long document understanding, among others. To facilitate research in this
field, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art
performance among models of comparable size. In a comprehensive evaluation
across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all
tasks and achieves comparable or even superior performance on 18 benchmarks
relative to the significantly larger Qwen2.5-VL-72B. Notably,
GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance
compared to closed-source models such as GPT-4o on challenging tasks including
long document understanding and STEM reasoning, further underscoring its strong
capabilities. Code, models and more information are released at
https://github.com/THUDM/GLM-4.1V-Thinking.

</details>


### [216] [ShapeEmbed: a self-supervised learning framework for 2D contour quantification](https://arxiv.org/abs/2507.01009)
*Anna Foix Romero,Craig Russell,Alexander Krull,Virginie Uhlmann*

Main category: cs.CV

TL;DR: ShapeEmbed is a self-supervised framework for learning invariant shape descriptors from 2D object contours, outperforming traditional and autoencoder-based methods in classification tasks.


<details>
  <summary>Details</summary>
Motivation: Shape quantification must remain invariant to transformations like size, orientation, and position changes, which traditional methods struggle with.

Method: ShapeEmbed encodes object contours (as Euclidean distance matrices) into invariant descriptors using self-supervised learning.

Result: The framework outperforms existing methods in shape classification on natural and biological images.

Conclusion: ShapeEmbed is particularly relevant for biological imaging due to its robust invariant shape descriptors.

Abstract: The shape of objects is an important source of visual information in a wide
range of applications. One of the core challenges of shape quantification is to
ensure that the extracted measurements remain invariant to transformations that
preserve an object's intrinsic geometry, such as changing its size,
orientation, and position in the image. In this work, we introduce ShapeEmbed,
a self-supervised representation learning framework designed to encode the
contour of objects in 2D images, represented as a Euclidean distance matrix,
into a shape descriptor that is invariant to translation, scaling, rotation,
reflection, and point indexing. Our approach overcomes the limitations of
traditional shape descriptors while improving upon existing state-of-the-art
autoencoder-based approaches. We demonstrate that the descriptors learned by
our framework outperform their competitors in shape classification tasks on
natural and biological images. We envision our approach to be of particular
relevance to biological imaging applications.

</details>


### [217] [DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution](https://arxiv.org/abs/2507.01012)
*Zhe Kong,Le Li,Yong Zhang,Feng Gao,Shaoshu Yang,Tao Wang,Kaihao Zhang,Zhuoliang Kang,Xiaoming Wei,Guanying Chen,Wenhan Luo*

Main category: cs.CV

TL;DR: DAM-VSR is a novel framework for video super-resolution (VSR) that disentangles appearance enhancement and motion control, leveraging video diffusion models and image super-resolution for improved detail and temporal consistency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generating temporally consistent and detailed frames in real-world VSR due to complex degradations and limitations of existing methods.

Method: Proposes DAM-VSR, which separates VSR into appearance enhancement (using reference image super-resolution) and motion control (using video ControlNet), with a motion-aligned bidirectional sampling strategy for longer videos.

Result: Achieves state-of-the-art performance on real-world and AIGC data, showcasing strong detail generation capabilities.

Conclusion: DAM-VSR effectively combines generative priors of video diffusion models with image super-resolution, offering a robust solution for VSR challenges.

Abstract: Real-world video super-resolution (VSR) presents significant challenges due
to complex and unpredictable degradations. Although some recent methods utilize
image diffusion models for VSR and have shown improved detail generation
capabilities, they still struggle to produce temporally consistent frames. We
attempt to use Stable Video Diffusion (SVD) combined with ControlNet to address
this issue. However, due to the intrinsic image-animation characteristics of
SVD, it is challenging to generate fine details using only low-quality videos.
To tackle this problem, we propose DAM-VSR, an appearance and motion
disentanglement framework for VSR. This framework disentangles VSR into
appearance enhancement and motion control problems. Specifically, appearance
enhancement is achieved through reference image super-resolution, while motion
control is achieved through video ControlNet. This disentanglement fully
leverages the generative prior of video diffusion models and the detail
generation capabilities of image super-resolution models. Furthermore, equipped
with the proposed motion-aligned bidirectional sampling strategy, DAM-VSR can
conduct VSR on longer input videos. DAM-VSR achieves state-of-the-art
performance on real-world data and AIGC data, demonstrating its powerful detail
generation capabilities.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [218] [Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data](https://arxiv.org/abs/2507.00152)
*Ekaterina Borisova,Fabio Barth,Nils Feldhus,Raia Abu Ahmad,Malte Ostendorff,Pedro Ortiz Suarez,Georg Rehm,Sebastian Mller*

Main category: cs.CL

TL;DR: The paper explores LLMs' efficiency in processing tabular data, comparing text-based and multimodal models across domains and modalities, and introduces the TableEval benchmark.


<details>
  <summary>Details</summary>
Motivation: To investigate the underexplored efficiency of LLMs in handling tabular data, especially in scientific contexts and across different table representations.

Method: Cross-domain and cross-modality evaluation of LLMs on table understanding tasks, including interpretability analysis and the introduction of the TableEval benchmark with 3017 tables in five formats.

Result: LLMs show robustness across table modalities but struggle with scientific tables.

Conclusion: The study highlights LLMs' limitations in processing scientific tabular data and provides a benchmark for future research.

Abstract: Tables are among the most widely used tools for representing structured data
in research, business, medicine, and education. Although LLMs demonstrate
strong performance in downstream tasks, their efficiency in processing tabular
data remains underexplored. In this paper, we investigate the effectiveness of
both text-based and multimodal LLMs on table understanding tasks through a
cross-domain and cross-modality evaluation. Specifically, we compare their
performance on tables from scientific vs. non-scientific contexts and examine
their robustness on tables represented as images vs. text. Additionally, we
conduct an interpretability analysis to measure context usage and input
relevance. We also introduce the TableEval benchmark, comprising 3017 tables
from scholarly publications, Wikipedia, and financial reports, where each table
is provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.
Our findings indicate that while LLMs maintain robustness across table
modalities, they face significant challenges when processing scientific tables.

</details>


### [219] [Prompting as Scientific Inquiry](https://arxiv.org/abs/2507.00163)
*Ari Holtzman,Chenhao Tan*

Main category: cs.CL

TL;DR: Prompting is a critical and scientific method for studying and controlling large language models (LLMs), not just an unscientific workaround.


<details>
  <summary>Details</summary>
Motivation: The paper aims to reframe prompting as a legitimate scientific approach to understanding and interacting with LLMs, countering the perception of it as mere 'alchemy.'

Method: The argument is built by comparing prompting to behavioral science and mechanistic interpretability, emphasizing its role in probing LLMs through language.

Result: The paper concludes that prompting is a fundamental and scientific tool for studying LLMs, not inferior to other methods.

Conclusion: Prompting should be recognized as a key component in the science of LLMs, akin to behavioral science for complex organisms.

Abstract: Prompting is the primary method by which we study and control large language
models. It is also one of the most powerful: nearly every major capability
attributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was
first unlocked through prompting. Yet prompting is rarely treated as science
and is frequently frowned upon as alchemy. We argue that this is a category
error. If we treat LLMs as a new kind of complex and opaque organism that is
trained rather than programmed, then prompting is not a workaround: it is
behavioral science. Mechanistic interpretability peers into the neural
substrate, prompting probes the model in its native interface: language. We
contend that prompting is not inferior, but rather a key component in the
science of LLMs.

</details>


### [220] [LineRetriever: Planning-Aware Observation Reduction for Web Agents](https://arxiv.org/abs/2507.00210)
*Imene Kerboua,Sahar Omidi Shayegan,Megh Thakkar,Xing Han L,Massimo Caccia,Vronique Eglin,Alexandre Aussem,Jrmy Espinas,Alexandre Lacoste*

Main category: cs.CL

TL;DR: The paper introduces LineRetriever, a method to optimize retrieval for adaptive planning in web navigation by prioritizing observation lines relevant to future actions, addressing context limits of large language models.


<details>
  <summary>Details</summary>
Motivation: Current retrieval methods lose critical information for adaptive planning in web navigation due to context limits of models, necessitating a solution that preserves plan-relevant data.

Method: LineRetriever uses a language model to retrieve observation lines most relevant to future navigation steps, focusing on planning horizon rather than just semantic similarity.

Result: Experiments show LineRetriever reduces observation size while maintaining performance within context limits.

Conclusion: LineRetriever effectively addresses the challenge of adaptive planning in web navigation by optimizing retrieval for action prediction.

Abstract: While large language models have demonstrated impressive capabilities in web
navigation tasks, the extensive context of web pages, often represented as DOM
or Accessibility Tree (AxTree) structures, frequently exceeds model context
limits. Current approaches like bottom-up truncation or embedding-based
retrieval lose critical information about page state and action history. This
is particularly problematic for adaptive planning in web agents, where
understanding the current state is essential for determining future actions. We
hypothesize that embedding models lack sufficient capacity to capture
plan-relevant information, especially when retrieving content that supports
future action prediction. This raises a fundamental question: how can retrieval
methods be optimized for adaptive planning in web navigation tasks? In
response, we introduce \textit{LineRetriever}, a novel approach that leverages
a language model to identify and retrieve observation lines most relevant to
future navigation steps. Unlike traditional retrieval methods that focus solely
on semantic similarity, \textit{LineRetriever} explicitly considers the
planning horizon, prioritizing elements that contribute to action prediction.
Our experiments demonstrate that \textit{LineRetriever} can reduce the size of
the observation at each step for the web agent while maintaining consistent
performance within the context limitations.

</details>


### [221] [Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning](https://arxiv.org/abs/2507.00214)
*Mads Henrichsen,Rasmus Krebs*

Main category: cs.CL

TL;DR: A two-stage approach enhances text classification by using LLM-generated reasoning, improving accuracy by 8.7 percentage points.


<details>
  <summary>Details</summary>
Motivation: Standard models lack explicit reasoning, limiting performance and interpretability. This work aims to improve both by leveraging LLM-generated reasoning.

Method: 1. Fine-tune Llama-3.2-1B-Instruct to generate reasoning (R) from Q&A. 2. Use this to augment training data for a downstream model predicting R and emotion (A).

Result: The model (Q->RA) outperforms the baseline (Q->A) by 8.7 points in accuracy on emotion classification.

Conclusion: LLM-generated reasoning enhances model performance and interpretability, benefiting diverse NLP tasks.

Abstract: Standard classification models often map inputs directly to labels without
explicit reasoning, potentially limiting their performance, robustness, and
interpretability. This paper introduces a novel two-stage approach to enhance
text classification by leveraging Large Language Model (LLM)-generated
reasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model
(henceforth Llama-R-Gen) on a general-purpose reasoning dataset
(syvai/reasoning-gen) to generate textual reasoning (R) given a question and
its answer. In the second stage, this generally trained Llama-R-Gen is used
offline to create an augmented training dataset for a downstream generative
model. This downstream model, based on Llama-3.2-1B-Instruct, takes only the
input text (Q) and is trained to output the generated reasoning (R) immediately
followed by the predicted emotion (A). We demonstrate this methodology on the
dair-ai/emotion dataset for emotion classification. Our experiments show that
the generative model trained to output reasoning and the emotion (Classifier
Q->RA) achieves a significant improvement of 8.7 percentage points in accuracy
(for emotion prediction) compared to a baseline generative model trained solely
to output the emotion (Classifier Q->A), highlighting the strong generalization
capabilities of the reasoning generation and the benefit of explicit reasoning
training. This work underscores the potential of LLM-generated reasonings for
creating richer training datasets, thereby improving the performance of diverse
downstream NLP tasks and providing explicit explanations.

</details>


### [222] [Towards Style Alignment in Cross-Cultural Translation](https://arxiv.org/abs/2507.00216)
*Shreya Havaldar,Adam Stein,Eric Wong,Lyle Ungar*

Main category: cs.CL

TL;DR: RASTA improves LLM translation by aligning speaker and listener styles, addressing cultural misalignment and bias toward neutrality.


<details>
  <summary>Details</summary>
Motivation: Cultural differences often cause style misalignment in communication, especially in non-Western languages, leading to lost politeness and other nuances.

Method: RASTA (Retrieval-Augmented STylistic Alignment) uses learned stylistic concepts to align LLM translations with cultural norms.

Result: RASTA mitigates failures in style translation, reducing bias toward neutrality and improving performance in non-Western languages.

Conclusion: RASTA effectively aligns LLM translations with intended cultural styles, enhancing cross-cultural communication.

Abstract: Successful communication depends on the speaker's intended style (i.e., what
the speaker is trying to convey) aligning with the listener's interpreted style
(i.e., what the listener perceives). However, cultural differences often lead
to misalignment between the two; for example, politeness is often lost in
translation. We characterize the ways that LLMs fail to translate style -
biasing translations towards neutrality and performing worse in non-Western
languages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic
Alignment), a method that leverages learned stylistic concepts to encourage LLM
translation to appropriately convey cultural communication norms and align
style.

</details>


### [223] [Linearly Decoding Refused Knowledge in Aligned Language Models](https://arxiv.org/abs/2507.00239)
*Aryan Shrivastava,Ari Holtzman*

Main category: cs.CL

TL;DR: Instruction-tuned LMs retain harmful information internally despite refusal mechanisms, as shown by linear probes decoding such data from hidden states.


<details>
  <summary>Details</summary>
Motivation: To investigate if harmful information refused by LMs is still accessible internally and how it influences model behavior.

Method: Used linear probes on LM hidden states to decode refused information and tested transferability between base and instruction-tuned models.

Result: Linear probes successfully decoded refused information (e.g., country IQ) with high accuracy, and base model probes transferred to instruction-tuned versions.

Conclusion: Instruction-tuning suppresses but does not eliminate harmful information, leaving it linearly accessible and influential in downstream tasks.

Abstract: Most commonly used language models (LMs) are instruction-tuned and aligned
using a combination of fine-tuning and reinforcement learning, causing them to
refuse users requests deemed harmful by the model. However, jailbreak prompts
can often bypass these refusal mechanisms and elicit harmful responses. In this
work, we study the extent to which information accessed via jailbreak prompts
is decodable using linear probes trained on LM hidden states. We show that a
great deal of initially refused information is linearly decodable. For example,
across models, the response of a jailbroken LM for the average IQ of a country
can be predicted by a linear probe with Pearson correlations exceeding $0.8$.
Surprisingly, we find that probes trained on base models (which do not refuse)
sometimes transfer to their instruction-tuned versions and are capable of
revealing information that jailbreaks decode generatively, suggesting that the
internal representations of many refused properties persist from base LMs
through instruction-tuning. Importantly, we show that this information is not
merely "leftover" in instruction-tuned models, but is actively used by them: we
find that probe-predicted values correlate with LM generated pairwise
comparisons, indicating that the information decoded by our probes align with
suppressed generative behavior that may be expressed more subtly in other
downstream tasks. Overall, our results suggest that instruction-tuning does not
wholly eliminate or even relocate harmful information in representation
space-they merely suppress its direct expression, leaving it both linearly
accessible and indirectly influential in downstream behavior.

</details>


### [224] [The Algebraic Structure of Morphosyntax](https://arxiv.org/abs/2507.00244)
*Isabella Senturia,Matilde Marcolli*

Main category: cs.CL

TL;DR: A mathematical model of the morphology-syntax interface is presented, using magma and operad theory to describe word formation and morphosyntactic tree construction.


<details>
  <summary>Details</summary>
Motivation: To formalize the interface between morphology and syntax within the Strong Minimalist Thesis, addressing word formation and structure without morphological movement.

Method: Uses a magma of morphological trees and an operadic correspondence to pair syntactic and morphological data, extending morphological inputs for coproduct decomposition.

Result: The model describes morphosyntactic tree formation via operadic correspondence and reinterpretation of Distributed Morphology operations.

Conclusion: The framework provides a flexible boundary between syntax and morphology, formalizing their interaction mathematically.

Abstract: Within the context of the mathematical formulation of Merge and the Strong
Minimalist Thesis, we present a mathematical model of the morphology-syntax
interface. In this setting, morphology has compositional properties responsible
for word formation, organized into a magma of morphological trees. However,
unlike syntax, we do not have movement within morphology. A coproduct
decomposition exists, but it requires extending the set of morphological trees
beyond those which are generated solely by the magma, to a larger set of
possible morphological inputs to syntactic trees. These participate in the
formation of morphosyntactic trees as an algebra over an operad, and a
correspondence between algebras over an operad. The process of structure
formation for morphosyntactic trees can then be described in terms of this
operadic correspondence that pairs syntactic and morphological data and the
morphology coproduct. We reinterpret in this setting certain operations of
Distributed Morphology as transformation that allow for flexibility in moving
the boundary between syntax and morphology within the morphosyntactic objects.

</details>


### [225] [EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning](https://arxiv.org/abs/2507.00246)
*Sanchit Ahuja,Praneetha Vaddamanu,Barun Patra*

Main category: cs.CL

TL;DR: Non-English languages can be more token-efficient for reasoning in LRMs while maintaining accuracy, with gains persisting even after translation. Multilingual model strength influences improvement.


<details>
  <summary>Details</summary>
Motivation: To explore if English is the most token-efficient language for reasoning in LRMs, given their multilingual pretraining.

Method: Evaluated three open-source RLMs (DeepSeek R1, Qwen 2.5, Qwen 3) on four math datasets across seven diverse languages.

Result: Non-English reasoning reduces token usage and preserves accuracy, with gains persisting post-translation. Improvement varies by model's multilingual strength.

Conclusion: Multilingual reasoning offers efficiency and accuracy benefits, emphasizing the need for strong multilingual foundations in LRMs.

Abstract: Despite recent advances in Language Reasoning Models (LRMs), most research
focuses solely on English, even though many models are pretrained on
multilingual data. In this work, we investigate: Is English the most
token-efficient language for reasoning? We evaluate three open-source RLMs:
DeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven
typologically diverse languages. We find that reasoning in non-English
languages not only reduces token usage, but also preserves accuracy. These
gains persist even after translating the reasoning traces into English,
suggesting genuine shifts in reasoning behavior rather than surface-level
linguistic effects. The extent of improvement, however, depends on the models
multilingual strength. Our findings motivate a broader view of reasoning in
language models, highlighting the potential of multilingual reasoning and the
importance of strong multilingual foundations. The code for our work can be
found: https://github.com/microsoft/EfficientXLang.

</details>


### [226] [Impact of Fine-Tuning Methods on Memorization in Large Language Models](https://arxiv.org/abs/2507.00258)
*Jie Hou,Chuxiong Wu,Lannan Luo,Qiang Zeng*

Main category: cs.CL

TL;DR: Prompt-based fine-tuning is more privacy-preserving than parameter-based fine-tuning, showing lower vulnerability to membership inference attacks (MIAs).


<details>
  <summary>Details</summary>
Motivation: To address the privacy risks from memorization during fine-tuning of large language models (LLMs), which have been overlooked.

Method: Categorize fine-tuning approaches and evaluate their memorization impact using MIAs.

Result: Prompt-based fine-tuning performs competitively and is less vulnerable to MIAs, unlike parameter-based methods.

Conclusion: Prompt-based fine-tuning is a better privacy-preserving option compared to parameter-based fine-tuning.

Abstract: As the capabilities of pre-trained large language models (LLMs) continue to
advance, the "pre-train and fine-tune" paradigm has become increasingly
mainstream, leading to the development of various fine-tuning methods. However,
the privacy risks arising from memorization during fine-tuning have received
relatively little attention. To address this gap, we categorize popular
fine-tuning approaches and assess their impact on memorization through the lens
of membership inference attacks (MIAs). Our results show that, compared to
parameter-based fine-tuning, prompt-based fine-tuning achieves competitive
performance while exhibiting lower vulnerability to MIAs. Furthermore,
prompt-based methods maintain low memorization regardless of model scale. These
findings suggest that parameter-based fine-tuning is more prone to leaking
private information, whereas prompt-based fine-tuning serves as a more
privacy-preserving option.

</details>


### [227] [Natural language processing for African languages](https://arxiv.org/abs/2507.00297)
*David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: The paper addresses challenges in NLP for low-resource African languages, focusing on data quality, multilingual models, and creating labeled datasets for tasks like named entity recognition and machine translation.


<details>
  <summary>Details</summary>
Motivation: To improve NLP performance for underrepresented African languages by addressing data scarcity, noise, and lack of labeled datasets.

Method: Analyzes noise in existing corpora, curates high-quality data, evaluates word embeddings and multilingual PLMs, and adapts models using small monolingual texts. Creates labeled datasets for 21 African languages.

Result: Demonstrates that data quality impacts semantic representations, highlights limitations of word embeddings, and shows the potential of multilingual PLMs for low-resource languages.

Conclusion: The work advances NLP for African languages by improving data quality, adapting models, and providing labeled datasets, enabling better performance in tasks like named entity recognition and machine translation.

Abstract: Recent advances in word embeddings and language models use large-scale,
unlabelled data and self-supervised learning to boost NLP performance.
Multilingual models, often trained on web-sourced data like Wikipedia, face
challenges: few low-resource languages are included, their data is often noisy,
and lack of labeled datasets makes it hard to evaluate performance outside
high-resource languages like English. In this dissertation, we focus on
languages spoken in Sub-Saharan Africa where all the indigenous languages in
this region can be regarded as low-resourced in terms of the availability of
labelled data for NLP tasks and unlabelled data found on the web. We analyse
the noise in the publicly available corpora, and curate a high-quality corpus,
demonstrating that the quality of semantic representations learned in word
embeddings does not only depend on the amount of data but on the quality of
pre-training data. We demonstrate empirically the limitations of word
embeddings, and the opportunities the multilingual pre-trained language model
(PLM) offers especially for languages unseen during pre-training and
low-resource scenarios. We further study how to adapt and specialize
multilingual PLMs to unseen African languages using a small amount of
monolingual texts. To address the under-representation of the African languages
in NLP research, we developed large scale human-annotated labelled datasets for
21 African languages in two impactful NLP tasks: named entity recognition and
machine translation. We conduct an extensive empirical evaluation using
state-of-the-art methods across supervised, weakly-supervised, and transfer
learning settings.

</details>


### [228] [Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones](https://arxiv.org/abs/2507.00322)
*Daking Rai,Samuel Miller,Kevin Moran,Ziyu Yao*

Main category: cs.CL

TL;DR: Language models struggle with syntactic tasks like balanced parentheses due to unreliable components. RASteer improves performance by enhancing reliable components.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate persistent errors in LMs for syntactic tasks.

Method: Analyzed LM components (attention heads, FF neurons) and introduced RASteer to boost reliable components.

Result: RASteer improved balanced parentheses accuracy from 0% to ~100% and arithmetic reasoning by ~20%.

Conclusion: RASteer effectively mitigates LM errors by prioritizing reliable components without harming general coding ability.

Abstract: Despite remarkable advances in coding capabilities, language models (LMs)
still struggle with simple syntactic tasks such as generating balanced
parentheses. In this study, we investigate the underlying mechanisms behind the
persistence of these errors across LMs of varying sizes (124M-7B) to both
understand and mitigate the errors. Our study reveals that LMs rely on a number
of components (attention heads and FF neurons) that independently make their
own predictions. While some components reliably promote correct answers across
a generalized range of inputs (i.e., implementing "sound mechanisms''), others
are less reliable and introduce noise by promoting incorrect tokens (i.e.,
implementing "faulty mechanisms''). Errors occur when the faulty mechanisms
overshadow the sound ones and dominantly affect the predictions. Motivated by
this insight, we introduce RASteer, a steering method to systematically
identify and increase the contribution of reliable components for improving
model performance. RASteer substantially improves performance on balanced
parentheses tasks, boosting accuracy of some models from $0$% to around $100$%
without impairing the models' general coding ability. We further demonstrate
its broader applicability in arithmetic reasoning tasks, achieving performance
gains of up to around $20$%.

</details>


### [229] [Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios](https://arxiv.org/abs/2507.00330)
*Mohna Chakraborty,Adithya Kulkarni,Qi Li*

Main category: cs.CL

TL;DR: COLDSELECT improves prompt-based methods by jointly selecting verbalizers and instances, optimizing for diversity and minimal uncertainty, outperforming baselines in cold-start scenarios.


<details>
  <summary>Details</summary>
Motivation: Prompt-based methods are sensitive to template, verbalizer, and instance selection, especially in cold-start settings. Existing work ignores dependencies between instances and verbalizers.

Method: COLDSELECT maps PLM vocabulary and embeddings into a shared space, applies dimensionality reduction and clustering, and optimizes for diversity and minimal uncertainty.

Result: Experiments on eight benchmarks show COLDSELECT reduces uncertainty and enhances generalization, outperforming baselines.

Conclusion: COLDSELECT effectively addresses cold-start challenges by jointly optimizing verbalizer and instance selection, improving performance and generalization.

Abstract: Prompt-based methods leverage the knowledge of pre-trained language models
(PLMs) trained with a masked language modeling (MLM) objective; however, these
methods are sensitive to template, verbalizer, and few-shot instance selection,
particularly in cold-start settings with no labeled data. Existing studies
overlook the dependency between instances and verbalizers, where instance-label
probabilities depend on verbalizer token proximity in the embedding space. To
address this, we propose COLDSELECT, a joint verbalizer and instance selection
approach that models data diversity. COLDSELECT maps PLM vocabulary and
$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction
and clustering to ensure efficient and diverse selection. By optimizing for
minimal uncertainty and maximal diversity, COLDSELECT captures data
relationships effectively. Experiments on eight benchmarks demonstrate
COLDSELECT's superiority in reducing uncertainty and enhancing generalization,
outperforming baselines in verbalizer and few-shot instance selection for
cold-start scenarios.

</details>


### [230] [Question Decomposition for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.00355)
*Paul J. L. Ammann,Jonas Golde,Alan Akbik*

Main category: cs.CL

TL;DR: The paper proposes enhancing RAG with question decomposition and reranking to improve retrieval and answer accuracy for multi-hop questions.


<details>
  <summary>Details</summary>
Motivation: Standard RAG struggles with multi-hop questions due to distributed facts across documents.

Method: Decompose queries into sub-questions, retrieve passages for each, merge and rerank the candidate pool.

Result: Improved retrieval (MRR@10: +36.7%) and answer accuracy (F1: +11.6%) over standard RAG.

Conclusion: Question decomposition and reranking enhance RAG for multi-hop questions without extra training.

Abstract: Grounding large language models (LLMs) in verifiable external sources is a
well-established strategy for generating reliable answers. Retrieval-augmented
generation (RAG) is one such approach, particularly effective for tasks like
question answering: it retrieves passages that are semantically related to the
question and then conditions the model on this evidence. However, multi-hop
questions, such as "Which company among NVIDIA, Apple, and Google made the
biggest profit in 2023?," challenge RAG because relevant facts are often
distributed across multiple documents rather than co-occurring in one source,
making it difficult for standard RAG to retrieve sufficient information. To
address this, we propose a RAG pipeline that incorporates question
decomposition: (i) an LLM decomposes the original query into sub-questions,
(ii) passages are retrieved for each sub-question, and (iii) the merged
candidate pool is reranked to improve the coverage and precision of the
retrieved evidence. We show that question decomposition effectively assembles
complementary documents, while reranking reduces noise and promotes the most
relevant passages before answer generation. Although reranking itself is
standard, we show that pairing an off-the-shelf cross-encoder reranker with
LLM-driven question decomposition bridges the retrieval gap on multi-hop
questions and provides a practical, drop-in enhancement, without any extra
training or specialized indexing. We evaluate our approach on the MultiHop-RAG
and HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy
(F1: +11.6%) over standard RAG baselines.

</details>


### [231] [Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics](https://arxiv.org/abs/2507.00380)
*Vojtch Lanz,Jan Haji jr*

Main category: cs.CL

TL;DR: The paper explores Gregorian chant melodies using unsupervised segmentation with Pitman-Yor models, achieving top mode classification results but finding no strong evidence for centonisation.


<details>
  <summary>Details</summary>
Motivation: To investigate if an optimal segmentation of Gregorian melodies, inspired by memorization, supports the centonisation theory.

Method: Uses nested hierarchical Pitman-Yor language models for unsupervised segmentation of chant melodies.

Result: Achieves state-of-the-art mode classification; finds memory efficiency linked to mode but no clear centonisation evidence.

Conclusion: Memory-optimal segmentation doesn't align with centonisation, though formulaic patterns exist at melody boundaries.

Abstract: The idea that Gregorian melodies are constructed from some vocabulary of
segments has long been a part of chant scholarship. This so-called
"centonisation" theory has received much musicological criticism, but frequent
re-use of certain melodic segments has been observed in chant melodies, and the
intractable number of possible segmentations allowed the option that some
undiscovered segmentation exists that will yet prove the value of
centonisation, and recent empirical results have shown that segmentations can
outperform music-theoretical features in mode classification. Inspired by the
fact that Gregorian chant was memorised, we search for an optimal unsupervised
segmentation of chant melody using nested hierarchical Pitman-Yor language
models. The segmentation we find achieves state-of-the-art performance in mode
classification. Modeling a monk memorising the melodies from one liturgical
manuscript, we then find empirical evidence for the link between mode
classification and memory efficiency, and observe more formulaic areas at the
beginnings and ends of melodies corresponding to the practical role of modality
in performance. However, the resulting segmentations themselves indicate that
even such a memory-optimal segmentation is not what is understood as
centonisation.

</details>


### [232] [Causal Prompting for Implicit Sentiment Analysis with Large Language Models](https://arxiv.org/abs/2507.00389)
*Jing Ren,Wenhao Zhou,Bowen Li,Mujie Liu,Nguyen Linh Dan Le,Jiade Cen,Liping Chen,Ziqi Xu,Xiwei Xu,Xiaodong Li*

Main category: cs.CL

TL;DR: CAPITAL is a causal prompting framework for Implicit Sentiment Analysis (ISA) that improves accuracy and robustness by integrating front-door adjustment into chain-of-thought reasoning.


<details>
  <summary>Details</summary>
Motivation: Current prompting-based methods for ISA rely on majority voting over reasoning paths without causal validation, leading to biases and spurious correlations.

Method: CAPITAL decomposes causal effects into prompt influence on reasoning chains and their impact on output, using encoder-based clustering, NWGM approximation, and contrastive learning.

Result: CAPITAL outperforms baselines in accuracy and robustness on benchmark ISA datasets, especially under adversarial conditions.

Conclusion: CAPITAL provides a principled way to integrate causal inference into LLM prompting, enhancing bias-aware sentiment reasoning.

Abstract: Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied
rather than explicitly stated, requiring models to perform deeper reasoning
over subtle contextual cues. While recent prompting-based methods using Large
Language Models (LLMs) have shown promise in ISA, they often rely on majority
voting over chain-of-thought (CoT) reasoning paths without evaluating their
causal validity, making them susceptible to internal biases and spurious
correlations. To address this challenge, we propose CAPITAL, a causal prompting
framework that incorporates front-door adjustment into CoT reasoning. CAPITAL
decomposes the overall causal effect into two components: the influence of the
input prompt on the reasoning chains, and the impact of those chains on the
final output. These components are estimated using encoder-based clustering and
the NWGM approximation, with a contrastive learning objective used to better
align the encoder's representation with the LLM's reasoning space. Experiments
on benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently
outperforms strong prompting baselines in both accuracy and robustness,
particularly under adversarial conditions. This work offers a principled
approach to integrating causal inference into LLM prompting and highlights its
benefits for bias-aware sentiment reasoning. The source code and case study are
available at: https://github.com/whZ62/CAPITAL.

</details>


### [233] [Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions](https://arxiv.org/abs/2507.00439)
*Gauri Kambhatla,Sanjana Gautam,Angela Zhang,Alex Liu,Ravi Srinivasan,Junyi Jessy Li,Matthew Lease*

Main category: cs.CL

TL;DR: Simple supervision improves language model alignment with diverse groups, evaluated across datasets and models.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of subjective answers across population groups is valuable.

Method: Use of simple supervision to align language models with diverse groups, evaluated over multiple datasets and models.

Result: Improved alignment with diverse groups, with findings on when to use the approach.

Conclusion: The approach is simple, general, and provides a benchmark for future research.

Abstract: The ability to accurately predict how different population groups would
answer subjective questions would have great value. In this work, we show that
use of relatively simple supervision can greatly improve language model
alignment with diverse population groups, as measured over three datasets
spanning various topics. Beyond evaluating average performance, we also report
how alignment varies across specific groups. The simplicity and generality of
our approach promotes easy adoption, while our broad findings provide useful
guidance for when to use or not use our approach in practice. By conducting
evaluation over many LLMs and prompting strategies, along with open-sourcing
our work, we provide a useful benchmark to stimulate future research.

</details>


### [234] [Pitfalls of Evaluating Language Models with Open Benchmarks](https://arxiv.org/abs/2507.00460)
*Md. Najib Hasan,Mohammad Fakhruddin Babar,Souvika Sarkar,Monowar Hasan,Santu Karmaker*

Main category: cs.CL

TL;DR: Open LLM benchmarks like HELM and BIG-bench enable fair LM comparisons but have pitfalls. This study shows how 'cheating' models can top benchmarks without real-world utility, highlighting the need for private/dynamic benchmarks and reevaluating practices.


<details>
  <summary>Details</summary>
Motivation: To expose weaknesses in open LLM benchmarks by demonstrating how models can exploit public test sets to achieve high rankings without genuine effectiveness.

Method: Constructed 'cheating' models (smaller variants of BART, T5, GPT-2) fine-tuned on public test sets and evaluated on HELM.

Result: These models achieved top rankings on HELM despite poor generalization and limited practical utility.

Conclusion: Open benchmarks alone are insufficient; private/dynamic benchmarks and reevaluation of practices are needed for robust LM assessments.

Abstract: Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer
standardized, transparent protocols that facilitate the fair comparison,
reproducibility, and iterative advancement of Language Models (LMs). However,
their openness also introduces critical and underexplored pitfalls. This study
exposes these weaknesses by systematically constructing ``cheating'' models --
smaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets
-- which achieve top rankings on a prominent open, holistic benchmark (HELM)
despite poor generalization and limited practical utility. Our findings
underscore three key insights: \ca high leaderboard performance on open
benchmarks may not always reflect real-world effectiveness; \cb private or
dynamic benchmarks must complement open evaluations to safeguard integrity; and
\cc a fundamental reevaluation of current benchmarking practices is essential
to ensure robust and trustworthy LM assessments.

</details>


### [235] [TeamCMU at Touch: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search](https://arxiv.org/abs/2507.00509)
*To Eun Kim,Joo Coelho,Gbemileke Onilude,Jai Singh*

Main category: cs.CL

TL;DR: The paper proposes a modular pipeline for managing ads in RAG-based conversational systems, focusing on seamless integration and detection using synthetic data and classifier-guided strategies.


<details>
  <summary>Details</summary>
Motivation: The integration of ads in generative search systems blurs boundaries between content and promotions, raising transparency and trust concerns.

Method: A pipeline with an ad-rewriter and ad-classifier is developed, using synthetic data for training and two strategies: supervised fine-tuning and best-of-N sampling.

Result: The ad-classifier performs robustly, and classifier-guided optimization improves ad stealth for seamless integration.

Conclusion: The work introduces an adversarial co-evolution framework for ad-aware generative systems and robust classifiers.

Abstract: As conversational search engines increasingly adopt generation-based
paradigms powered by Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG), the integration of advertisements into generated responses
presents both commercial opportunities and challenges for user experience.
Unlike traditional search, where advertisements are clearly delineated,
generative systems blur the boundary between informational content and
promotional material, raising concerns around transparency and trust. In this
work, we propose a modular pipeline for advertisement management in RAG-based
conversational systems, consisting of an ad-rewriter for seamless ad
integration and a robust ad-classifier for detection. We leverage synthetic
data to train high-performing classifiers, which are then used to guide two
complementary ad-integration strategies: supervised fine-tuning of the
ad-rewriter and a best-of-N sampling approach that selects the least detectable
ad-integrated response among multiple candidates. Our evaluation focuses on two
core questions: the effectiveness of ad classifiers in detecting diverse ad
integration strategies, and the training methods that best support coherent,
minimally intrusive ad insertion. Experimental results show that our
ad-classifier, trained on synthetic advertisement data inspired by marketing
strategies and enhanced through curriculum learning, achieves robust detection
performance. Additionally, we demonstrate that classifier-guided optimization,
through both fine-tuning and best-of-N sampling, significantly improves ad
stealth, enabling more seamless integration. These findings contribute an
adversarial co-evolution framework for developing more sophisticated ad-aware
generative search systems and robust ad classifiers.

</details>


### [236] [NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data](https://arxiv.org/abs/2507.00534)
*Tahir Javed,Kaushal Bhogale,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: Nirantar is a framework for evaluating continual learning in multilingual and multi-domain ASR, using real-world data from 22 languages and 208 districts in India. It introduces dynamic, non-uniform shifts and benchmarks existing methods, showing no single approach performs consistently well.


<details>
  <summary>Details</summary>
Motivation: To address real-world continual learning challenges in multilingual and multi-domain ASR, as prior work relied on simulated episodes.

Method: Leverages incrementally collected data across languages and domains, introducing Language-Incremental (LIL), Domain-Incremental (DIL), and LIDIL scenarios.

Result: Benchmarked existing CL methods, revealing none perform consistently well across scenarios.

Conclusion: Highlights the need for more robust continual learning strategies in multilingual and multi-domain ASR.

Abstract: We introduce Nirantar, a comprehensive framework for evaluating continual
learning (CL) in multilingual and multi-domain ASR. Designed to reflect
real-world CL challenges, Nirantar leverages data collected incrementally
across 22 languages and 208 districts in India through natural episodes. This
enables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL),
and the novel Language-Incremental Domain-Incremental Learning (LIDIL)
scenarios. Unlike prior work that relies on simulated episodes, Nirantar
presents dynamic, non-uniform language and domain shifts, making it an ideal
testbed for CL research. With 3250 hours of human-transcribed speech, including
1720 hours newly introduced in this work, our framework enables systematic
benchmarking of CL methods. We evaluate existing approaches and demonstrate
that no single method performs consistently well, underscoring the need for
more robust CL strategies.

</details>


### [237] [Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction](https://arxiv.org/abs/2507.00540)
*Shixiao Wang,Yifan Zhuang,Runsheng Zhang,Zhijun Song*

Main category: cs.CL

TL;DR: A Capsule Network-based algorithm improves intent recognition accuracy in human-computer interaction by modeling semantic intent hierarchically and dynamically routing information.


<details>
  <summary>Details</summary>
Motivation: Addressing the insufficient accuracy in intent recognition for human-computer interaction by leveraging structured semantic modeling.

Method: Uses vectorized capsule structures and dynamic routing to capture hierarchical semantic relationships, with a convolutional feature extraction module and margin-based loss for enhanced performance.

Result: Outperforms traditional and deep learning methods in accuracy, F1-score, and intent detection rate, with verified stability and effectiveness.

Conclusion: The proposed structured modeling approach enhances intent recognition under complex semantic conditions.

Abstract: This paper proposes a user semantic intent modeling algorithm based on
Capsule Networks to address the problem of insufficient accuracy in intent
recognition for human-computer interaction. The method represents semantic
features in input text through a vectorized capsule structure. It uses a
dynamic routing mechanism to transfer information across multiple capsule
layers. This helps capture hierarchical relationships and part-whole structures
between semantic entities more effectively. The model uses a convolutional
feature extraction module as the low-level encoder. After generating initial
semantic capsules, it forms high-level abstract intent representations through
an iterative routing process. To further enhance performance, a margin-based
mechanism is introduced into the loss function. This improves the model's
ability to distinguish between intent classes. Experiments are conducted using
a public natural language understanding dataset. Multiple mainstream models are
used for comparison. Results show that the proposed model outperforms
traditional methods and other deep learning structures in terms of accuracy,
F1-score, and intent detection rate. The study also analyzes the effect of the
number of dynamic routing iterations on model performance. A convergence curve
of the loss function during training is provided. These results verify the
stability and effectiveness of the proposed method in semantic modeling.
Overall, this study presents a new structured modeling approach to improve
intent recognition under complex semantic conditions.

</details>


### [238] [Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm](https://arxiv.org/abs/2507.00547)
*Malmi Amadoru*

Main category: cs.CL

TL;DR: The paper addresses methodological challenges in computationally intensive research, focusing on topic modelling, and provides guidelines to ensure rigour.


<details>
  <summary>Details</summary>
Motivation: The opacity and lack of transparency in advanced computational algorithms undermine trust in research, necessitating methodological guidance.

Method: The author illustrates the structural topic modelling algorithm and presents guidelines for ensuring rigour in its application.

Result: The guidelines are applicable to topic modelling and other algorithms with adjustments, aiding novice researchers and reviewers.

Conclusion: The paper contributes to methodological rigour in topic modelling and broader computationally intensive research.

Abstract: The rise of advanced computational algorithms has opened new avenues for
computationally intensive research approaches to theory development. However,
the opacity of these algorithms and lack of transparency and rigour in their
application pose methodological challenges, potentially undermining trust in
research. The discourse on methodological rigour in this new genre of research
is still emerging. Against this backdrop, I attempt to offer guidance on
methodological rigour, particularly in the context of topic modelling
algorithms. By illustrating the application of the structural topic modelling
algorithm and presenting a set of guidelines, I discuss how to ensure rigour in
topic modelling studies. Although the guidelines are for the application of
topic modelling algorithms, they can be applied to other algorithms with
context-specific adjustments. The guidelines are helpful, especially for novice
researchers applying topic modelling, and editors and reviewers handling topic
modelling manuscripts. I contribute to the literature on topic modelling and
join the emerging dialogue on methodological rigour in computationally
intensive theory construction research.

</details>


### [239] [TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification](https://arxiv.org/abs/2507.00579)
*Miriam Anschtz,Ekaterina Gikalo,Niklas Herbster,Georg Groh*

Main category: cs.CL

TL;DR: The paper addresses LLM hallucinations, focusing on multilingual data. It proposes a retrieval-BERT pipeline for hallucination detection, achieving top-10 results in eight languages.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs limit trustworthiness and deployment, with most research neglecting multilingual contexts.

Method: A two-part pipeline combining retrieval-based fact verification (Wikipedia) and BERT fine-tuned for hallucination patterns.

Result: Competitive performance across languages, top-10 in eight, including English, and supports additional languages.

Conclusion: The multilingual hallucination identifier enhances LLM outputs and future usefulness.

Abstract: Hallucinations are one of the major problems of LLMs, hindering their
trustworthiness and deployment to wider use cases. However, most of the
research on hallucinations focuses on English data, neglecting the multilingual
nature of LLMs. This paper describes our submission to the SemEval-2025 Task-3
- Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related
Observable Overgeneration Mistakes. We propose a two-part pipeline that
combines retrieval-based fact verification against Wikipedia with a BERT-based
system fine-tuned to identify common hallucination patterns. Our system
achieves competitive results across all languages, reaching top-10 results in
eight languages, including English. Moreover, it supports multiple languages
beyond the fourteen covered by the shared task. This multilingual hallucination
identifier can help to improve LLM outputs and their usefulness in the future.

</details>


### [240] [Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based](https://arxiv.org/abs/2507.00601)
*Shuangquan Lyu,Yingnan Deng,Guiran Liu,Zhen Qi,Ruotong Wang*

Main category: cs.CL

TL;DR: A unified framework combining knowledge transfer and parameter-efficient fine-tuning for low-resource language adaptation, enhancing performance and stability.


<details>
  <summary>Details</summary>
Motivation: Addressing the limited transfer and adaptation capabilities of large language models in low-resource scenarios.

Method: Uses knowledge alignment loss, soft prompt tuning, lightweight adaptation modules, freezing strategies, and prompt injection.

Result: Outperforms existing models on cross-lingual tasks like MLQA, XQuAD, and PAWS-X, especially in data-scarce conditions.

Conclusion: The method enhances adaptability while preserving general model capabilities, suitable for multilingual and complex semantic tasks.

Abstract: This paper addresses the limited transfer and adaptation capabilities of
large language models in low-resource language scenarios. It proposes a unified
framework that combines a knowledge transfer module with parameter-efficient
fine-tuning strategies. The method introduces knowledge alignment loss and soft
prompt tuning to guide the model in effectively absorbing the structural
features of target languages or tasks under minimal annotation. This enhances
both generalization performance and training stability. The framework includes
lightweight adaptation modules to reduce computational costs. During training,
it integrates freezing strategies and prompt injection to preserve the model's
original knowledge while enabling quick adaptation to new tasks. The study also
conducts stability analysis experiments and synthetic pseudo-data transfer
experiments to systematically evaluate the method's applicability and
robustness across different low-resource tasks. Experimental results show that
compared with existing multilingual pre-trained models and mainstream transfer
methods, the proposed approach achieves higher performance and stability on
cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates
particularly strong advantages under extremely data-scarce conditions. The
proposed method offers strong generality and scalability. It enhances
task-specific adaptability while preserving the general capabilities of large
language models. This makes it well-suited for complex semantic modeling and
multilingual processing tasks.

</details>


### [241] [Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies](https://arxiv.org/abs/2507.00606)
*Tao Xiong,Xavier Hu,Wenyan Fan,Shengyu Zhang*

Main category: cs.CL

TL;DR: Mixture of Reasoning (MoR) is a framework that embeds diverse reasoning strategies into LLMs, eliminating the need for manual prompt engineering and improving task adaptability.


<details>
  <summary>Details</summary>
Motivation: Current LLMs rely on manually crafted, task-specific prompts, limiting adaptability and efficiency. MoR aims to automate reasoning strategies for broader applicability.

Method: MoR involves two phases: Thought Generation (creating reasoning chain templates) and SFT Dataset Construction (pairing templates with datasets for supervised fine-tuning).

Result: MoR improves performance significantly, with MoR150 achieving 2.2% and 13.5% improvements over baselines using CoT prompting.

Conclusion: MoR provides a generalizable solution for robust reasoning across diverse tasks without task-specific prompts.

Abstract: Large language models (LLMs) excel in complex tasks through advanced
prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but
their reliance on manually crafted, task-specific prompts limits adaptability
and efficiency. We introduce Mixture of Reasoning (MoR), a training framework
that embeds diverse reasoning strategies into LLMs for autonomous,
task-adaptive reasoning without external prompt engineering. MoR has two
phases: Thought Generation, creating reasoning chain templates with models like
GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets
for supervised fine-tuning.Our experiments show that MoR significantly enhances
performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting
and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need
for task-specific prompts, offering a generalizable solution for robust
reasoning across diverse tasks.

</details>


### [242] [SAFER: Probing Safety in Reward Models with Sparse Autoencoder](https://arxiv.org/abs/2507.00665)
*Sihang Li,Wei Shi,Ziyuan Xie,Tao Liang,Guojun Ma,Xiang Wang*

Main category: cs.CL

TL;DR: SAFER is a framework using Sparse Autoencoders to interpret and improve reward models in RLHF, enabling human-understandable insights into safety decisions and targeted data manipulation.


<details>
  <summary>Details</summary>
Motivation: Reward models in RLHF are opaque; SAFER aims to make them interpretable and improvable for better alignment with human values.

Method: Uses Sparse Autoencoders to analyze reward model activations, identify safety-relevant features, and apply targeted data poisoning/denoising.

Result: SAFER effectively degrades or enhances safety alignment with minimal data changes, maintaining general performance.

Conclusion: SAFER advances reward model interpretation, auditing, and refinement for safer LLM alignment.

Abstract: Reinforcement learning from human feedback (RLHF) is a key paradigm for
aligning large language models (LLMs) with human values, yet the reward models
at its core remain largely opaque. In this work, we present sparse Autoencoder
For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting
and improving reward models through mechanistic analysis. Leveraging Sparse
Autoencoders (SAEs), we uncover human-interpretable features in reward model
activations, enabling insight into safety-relevant decision-making. We apply
SAFER to safety-oriented preference datasets and quantify the salience of
individual features by activation differences between chosen and rejected
responses. Using these feature-level signals, we design targeted data poisoning
and denoising strategies. Experiments show that SAFER can precisely degrade or
enhance safety alignment with minimal data modification, without sacrificing
general chat performance. Our approach contributes to interpreting, auditing
and refining reward models in high-stakes LLM alignment tasks. Our codes are
available at https://github.com/xzy-101/SAFER-code. \textit{This paper
discusses topics related to large language model safety and may include
discussions or examples that highlight potential risks or unsafe outcomes.}

</details>


### [243] [Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English](https://arxiv.org/abs/2507.00700)
*Ahmed Sabir,Azinovi Gasper,Mengsay Loem,Rajesh Sharma*

Main category: cs.CL

TL;DR: VLMs trained on Japanese and English exhibit culturally grounded attentional patterns, mirroring holistic (East Asian) and analytic (Western) tendencies in their outputs.


<details>
  <summary>Details</summary>
Motivation: To explore whether VLMs internalize cultural cognition from their training data, specifically comparing Japanese (holistic) and English (analytic) language influences.

Method: Comparative analysis of image descriptions generated by VLMs trained on Japanese and English datasets.

Result: VLMs reproduce cultural behaviors, showing holistic tendencies for Japanese and analytic tendencies for English.

Conclusion: Cultural cognition implicitly shapes VLM outputs, reflecting the attentional patterns of their training languages.

Abstract: Cross-cultural research in perception and cognition has shown that
individuals from different cultural backgrounds process visual information in
distinct ways. East Asians, for example, tend to adopt a holistic perspective,
attending to contextual relationships, whereas Westerners often employ an
analytical approach, focusing on individual objects and their attributes. In
this study, we investigate whether Vision-Language Models (VLMs) trained
predominantly on different languages, specifically Japanese and English,
exhibit similar culturally grounded attentional patterns. Using comparative
analysis of image descriptions, we examine whether these models reflect
differences in holistic versus analytic tendencies. Our findings suggest that
VLMs not only internalize the structural properties of language but also
reproduce cultural behaviors embedded in the training data, indicating that
cultural cognition may implicitly shape model outputs.

</details>


### [244] [AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation](https://arxiv.org/abs/2507.00718)
*Elizabeth Fons,Elena Kochkina,Rachneet Kaur,Zhen Zeng,Berowne Hlavaty,Charese Smiley,Svitlana Vyetrenko,Manuela Veloso*

Main category: cs.CL

TL;DR: The paper proposes a framework for using LLMs to generate financial reports from time series data, with automated highlighting to categorize insights.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' potential in financial report generation and evaluate their factual grounding and reasoning capabilities.

Method: A framework involving prompt engineering, model selection, evaluation, and an automated highlighting system to categorize insights.

Result: LLMs can produce coherent and informative financial reports, as shown in experiments with real and synthetic time series data.

Conclusion: The approach effectively evaluates LLMs' capabilities in financial report generation, demonstrating their potential in this domain.

Abstract: This paper explores the potential of large language models (LLMs) to generate
financial reports from time series data. We propose a framework encompassing
prompt engineering, model selection, and evaluation. We introduce an automated
highlighting system to categorize information within the generated reports,
differentiating between insights derived directly from time series data,
stemming from financial reasoning, and those reliant on external knowledge.
This approach aids in evaluating the factual grounding and reasoning
capabilities of the models. Our experiments, utilizing both data from the real
stock market indices and synthetic time series, demonstrate the capability of
LLMs to produce coherent and informative financial reports.

</details>


### [245] [LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing](https://arxiv.org/abs/2507.00769)
*Daniel Fein,Sebastian Russo,Violet Xiang,Kabir Jolly,Rafael Rafailov,Nick Haber*

Main category: cs.CL

TL;DR: LitBench is introduced as a standardized benchmark for evaluating creative writing by LLMs, identifying Claude-3.7-Sonnet as the best OTS judge and trained reward models outperforming OTS judges.


<details>
  <summary>Details</summary>
Motivation: The challenge of evaluating creative writing by LLMs due to lack of ground truths and unreliable OTS judges motivates the creation of LitBench.

Method: LitBench includes a dataset of human-labeled story comparisons, benchmarking zero-shot LLM judges, training reward models (Bradley-Terry and generative), and validating rankings via a human study.

Result: Claude-3.7-Sonnet achieves 73% agreement with humans; trained reward models reach 78% accuracy, outperforming OTS judges. Human study confirms alignment with preferences.

Conclusion: LitBench provides a reliable resource for automated evaluation and optimization of creative writing systems, with trained reward models showing superior performance.

Abstract: Evaluating creative writing generated by large language models (LLMs) remains
challenging because open-ended narratives lack ground truths. Without
performant automated evaluation methods, off-the-shelf (OTS) language models
are employed as zero-shot judges, yet their reliability is unclear in this
context. In pursuit of robust evaluation for creative writing, we introduce
LitBench, the first standardized benchmark and paired dataset for creative
writing verification, comprising a held-out test set of 2,480 debiased,
human-labeled story comparisons drawn from Reddit and a 43,827-pair training
corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot
LLM judges, (ii) train Bradley Terry and generative reward models, and (iii)
conduct an online human study to validate reward model rankings on newly
LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the
strongest off-the-shelf judge, reaching 73% agreement with human preferences;
among trained reward models, Bradley-Terry and Generative reward models both
attain an accuracy of 78%, outperforming all off-the-shelf judges. An online
human study further confirms that our trained reward models consistently align
with human preferences in novel LLM-generated stories. We release LitBench and
reward models at
https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,
providing a vetted resource for reliable, automated evaluation and optimization
of creative writing systems.

</details>


### [246] [A Diagrammatic Calculus for a Functional Model of Natural Language Semantics](https://arxiv.org/abs/2507.00782)
*Matthieu Pierre Boyer*

Main category: cs.CL

TL;DR: A functional programming approach enhances natural language semantics by formalizing a category-based type and effect system and using a diagrammatic calculus for efficient sentence denotation computation.


<details>
  <summary>Details</summary>
Motivation: To increase the expressivity of traditional denotation-style semantics in natural language processing.

Method: Formalize a category-based type and effect system and construct a diagrammatic calculus for modeling parsing and effect handling.

Result: Efficient computation of sentence denotations.

Conclusion: The approach successfully integrates functional programming with natural language semantics, improving expressivity and computational efficiency.

Abstract: In this paper, we study a functional programming approach to natural language
semantics, allowing us to increase the expressivity of a more traditional
denotation style. We will formalize a category based type and effect system,
and construct a diagrammatic calculus to model parsing and handling of effects,
and use it to efficiently compute the denotations for sentences.

</details>


### [247] [Generative AI and the future of scientometrics: current topics and future questions](https://arxiv.org/abs/2507.00783)
*Benedetto Lepori,Jens Peter Andersen,Karsten Donnay*

Main category: cs.CL

TL;DR: The paper reviews GenAI's use in scientometrics, discussing its potential and limitations in mimicking human reasoning and its impact on the field.


<details>
  <summary>Details</summary>
Motivation: To explore GenAI's role in scientometrics and its broader implications, including its ability to mimic human reasoning and affect textual characteristics in science.

Method: The paper introduces GenAI's generative and probabilistic nature, critically reviews its applications in scientometrics, and examines its potential impact on textual characteristics in science.

Result: GenAI excels in language generation tasks like labelling but struggles with stable semantics or structured knowledge. Its rapid evolution necessitates systematic model comparisons.

Conclusion: Empirical work and theoretical reflection are crucial to interpret GenAI's evolving impact on scientometrics and knowledge production.

Abstract: The aim of this paper is to review the use of GenAI in scientometrics, and to
begin a debate on the broader implications for the field. First, we provide an
introduction on GenAI's generative and probabilistic nature as rooted in
distributional linguistics. And we relate this to the debate on the extent to
which GenAI might be able to mimic human 'reasoning'. Second, we leverage this
distinction for a critical engagement with recent experiments using GenAI in
scientometrics, including topic labelling, the analysis of citation contexts,
predictive applications, scholars' profiling, and research assessment. GenAI
shows promise in tasks where language generation dominates, such as labelling,
but faces limitations in tasks that require stable semantics, pragmatic
reasoning, or structured domain knowledge. However, these results might become
quickly outdated. Our recommendation is, therefore, to always strive to
systematically compare the performance of different GenAI models for specific
tasks. Third, we inquire whether, by generating large amounts of scientific
language, GenAI might have a fundamental impact on our field by affecting
textual characteristics used to measure science, such as authors, words, and
references. We argue that careful empirical work and theoretical reflection
will be essential to remain capable of interpreting the evolving patterns of
knowledge production.

</details>


### [248] [Many LLMs Are More Utilitarian Than One](https://arxiv.org/abs/2507.00814)
*Anita Keshmirian,Razan Baltaji,Babak Hemmatian,Hadi Asghari,Lav R. Varshney*

Main category: cs.CL

TL;DR: LLMs in groups show a utilitarian boost in moral dilemmas, similar to humans, but the underlying mechanisms differ.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs function collectively in moral judgment compared to individual agents, inspired by human group dynamics.

Method: Tested six LLM models on moral dilemmas in Solo (independent reasoning) and Group (multi-turn discussions) conditions.

Result: LLMs in groups found moral violations more acceptable, with some prioritizing overall well-being or showing reduced norm sensitivity.

Conclusion: LLM collective behavior mimics human group reasoning superficially, but the drivers differ, impacting AI alignment and multi-agent design.

Abstract: Moral judgment is integral to large language model (LLM) alignment and social
reasoning. As multi-agent systems gain prominence, it becomes crucial to
understand how LLMs function collectively during collaboration, compared to
individual agents. In human moral judgment, group deliberation leads to a
utilitarian boost: a tendency to endorse norm violations that maximize benefits
for the greatest number of people despite harms. We study whether a similar
dynamic emerges in multi-agent LLM systems. We tested six models on
well-established sets of moral dilemmas across two conditions: (1) Solo, where
models reasoned independently, and (2) Group, where they engaged in multi-turn
discussions in pairs or triads. In personal moral dilemmas, where agents must
decide to directly harm one individual to maximize the utility for others, all
models found moral violations to be more acceptable when part of a group than
individually, similar to human experiments. Some models endorsed actions that
maximized overall well-being, even if they benefited strangers over familiar
individuals. Others became more willing to violate moral norms in groups.
However, while human groups show a similar action bias, the mechanism for their
utilitarian boost differs from LLMs. Whereas the human shift comes from
heightened sensitivity to decision outcomes, LLM groups show either reduced
norm sensitivity or enhanced impartiality. This suggests that while the surface
behavior of LLM collectives mimics human group reasoning, the underlying
drivers differ. We discuss the implications for AI alignment, multi-agent
design, and artificial moral reasoning.

</details>


### [249] [ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering](https://arxiv.org/abs/2507.00828)
*Alexander Hoyle,Lorena Calvo-Bartolom,Jordan Boyd-Graber,Philip Resnik*

Main category: cs.CL

TL;DR: A scalable human evaluation protocol and LLM-based proxy for topic model evaluations, validated as statistically indistinguishable from human annotators.


<details>
  <summary>Details</summary>
Motivation: Current evaluations for topic models and document clustering either misalign with human preferences or rely on impractical expert labels.

Method: Design a protocol where annotators (or an LLM proxy) infer categories for topic/cluster groups and apply them to documents, then validate proxies with crowdworker annotations.

Result: LLM proxies perform comparably to human annotators, making them viable for automated evaluations.

Conclusion: The proposed protocol and LLM proxies offer a scalable and practical solution for evaluating topic models and document clustering.

Abstract: Topic model and document-clustering evaluations either use automated metrics
that align poorly with human preferences or require expert labels that are
intractable to scale. We design a scalable human evaluation protocol and a
corresponding automated approximation that reflect practitioners' real-world
usage of models. Annotators -- or an LLM-based proxy -- review text items
assigned to a topic or cluster, infer a category for the group, then apply that
category to other documents. Using this protocol, we collect extensive
crowdworker annotations of outputs from a diverse set of topic models on two
datasets. We then use these annotations to validate automated proxies, finding
that the best LLM proxies are statistically indistinguishable from a human
annotator and can therefore serve as a reasonable substitute in automated
evaluations. Package, web interface, and data are at
https://github.com/ahoho/proxann

</details>


### [250] [Stylometry recognizes human and LLM-generated texts in short samples](https://arxiv.org/abs/2507.00838)
*Karol Przystalski,Jan K. Argasiski,Iwona Grabska-Gradziska,Jeremi K. Ochab*

Main category: cs.CL

TL;DR: The paper uses stylometry to differentiate between texts written by humans and LLMs, achieving high accuracy in classification.


<details>
  <summary>Details</summary>
Motivation: Addressing issues of model attribution, intellectual property, and ethical AI use by identifying emergent writing patterns in LLM-generated texts.

Method: Created a benchmark dataset with human and LLM-generated texts, applied tree-based models using stylometric features for classification.

Result: Achieved up to .87 Matthews correlation coefficient in multiclass and .98 accuracy in binary classification (Wikipedia vs. GPT-4).

Conclusion: Stylometry can effectively distinguish machine-generated from human-written texts for well-defined text types.

Abstract: The paper explores stylometry as a method to distinguish between texts
created by Large Language Models (LLMs) and humans, addressing issues of model
attribution, intellectual property, and ethical AI use. Stylometry has been
used extensively to characterise the style and attribute authorship of texts.
By applying it to LLM-generated texts, we identify their emergent writing
patterns. The paper involves creating a benchmark dataset based on Wikipedia,
with (a) human-written term summaries, (b) texts generated purely by LLMs
(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text
summarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods
(Dipper, T5). The 10-sentence long texts were classified by tree-based models
(decision trees and LightGBM) using human-designed (StyloMetrix) and
n-gram-based (our own pipeline) stylometric features that encode lexical,
grammatical, syntactic, and punctuation patterns. The cross-validated results
reached a performance of up to .87 Matthews correlation coefficient in the
multiclass scenario with 7 classes, and accuracy between .79 and 1. in binary
classification, with the particular example of Wikipedia and GPT-4 reaching up
to .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed
features characteristic of the encyclopaedic text type, individual overused
words, as well as a greater grammatical standardisation of LLMs with respect to
human-written texts. These results show -- crucially, in the context of the
increasingly sophisticated LLMs -- that it is possible to distinguish machine-
from human-generated texts at least for a well-defined text type.

</details>


### [251] [TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation](https://arxiv.org/abs/2507.00875)
*Xi Xuan,King-kui Sin,Yufei Zhou,Chunyu Kit*

Main category: cs.CL

TL;DR: TransLaw, a multi-agent framework using LLMs, excels in translating Hong Kong legal judgments, outperforming GPT-4o in accuracy and coherence but lagging behind human experts in nuanced terminology and style.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenges of translating Hong Kong legal judgments, such as complex terminology and cultural nuances, using LLMs.

Method: TransLaw employs three specialized agents (Translator, Annotator, Proofreader) to collaboratively produce accurate and coherent translations, with customizable LLM configurations.

Result: The framework outperforms GPT-4o in legal semantic accuracy and structural coherence but falls short of human experts in nuanced terminology and stylistic naturalness.

Conclusion: TransLaw demonstrates the potential of LLMs in legal translation, offering cost-effective solutions while highlighting areas for improvement to match human expertise.

Abstract: Multi-agent systems empowered by large language models (LLMs) have
demonstrated remarkable capabilities in a wide range of downstream
applications, including machine translation. However, the potential of LLMs in
translating Hong Kong legal judgments remains uncertain due to challenges such
as intricate legal terminology, culturally embedded nuances, and strict
linguistic structures. In this work, we introduce TransLaw, a novel multi-agent
framework implemented for real-world Hong Kong case law translation. It employs
three specialized agents, namely, Translator, Annotator, and Proofreader, to
collaboratively produce translations for high accuracy in legal meaning,
appropriateness in style, and adequate coherence and cohesion in structure.
This framework supports customizable LLM configurations and achieves tremendous
cost reduction compared to professional human translation services. We
evaluated its performance using 13 open-source and commercial LLMs as agents
and obtained interesting findings, including that it surpasses GPT-4o in legal
semantic accuracy, structural coherence, and stylistic fidelity, yet trails
human experts in contextualizing complex terminology and stylistic naturalness.
Our platform website is available at CityUHK, and our bilingual judgment corpus
used for the evaluation is available at Hugging Face.

</details>


### [252] [Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations](https://arxiv.org/abs/2507.00883)
*Aditya Tomar,Nihar Ranjan Sahoo,Ashish Mittal,Rudra Murthy,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: The paper explores cultural bias in math problem presentation, adapting GSM8K for five regions and testing LLMs, finding performance gaps favoring US-centric data but resilience in reasoning-capable models.


<details>
  <summary>Details</summary>
Motivation: To address cultural neutrality in math problem presentation and evaluate LLMs' robustness to cultural variations.

Method: Created culturally adapted GSM8K variants for five regions, tested six LLMs with five prompting strategies.

Result: Models performed best on US-centric data, worse on adapted versions; reasoning-capable models showed resilience.

Conclusion: Cultural context affects math problem-solving; reasoning helps mitigate performance gaps.

Abstract: Although mathematics is often considered culturally neutral, the way
mathematical problems are presented can carry implicit cultural context.
Existing benchmarks like GSM8K are predominantly rooted in Western norms,
including names, currencies, and everyday scenarios. In this work, we create
culturally adapted variants of the GSM8K test set for five regions Africa,
India, China, Korea, and Japan using prompt-based transformations followed by
manual verification. We evaluate six large language models (LLMs), ranging from
8B to 72B parameters, across five prompting strategies to assess their
robustness to cultural variation in math problem presentation. Our findings
reveal a consistent performance gap: models perform best on the original
US-centric dataset and comparatively worse on culturally adapted versions.
However, models with reasoning capabilities are more resilient to these shifts,
suggesting that deeper reasoning helps bridge cultural presentation gaps in
mathematical tasks

</details>


### [253] [Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check](https://arxiv.org/abs/2507.00885)
*Nicholas Lourie,Michael Y. Hu,Kyunghyun Cho*

Main category: cs.CL

TL;DR: Downstream scaling laws predict task performance from pretraining losses, but linear trends are rare (39% of cases) and sensitive to experimental changes. Understanding their success conditions is crucial.


<details>
  <summary>Details</summary>
Motivation: To clarify whether downstream scaling laws can reliably predict task performance and identify the conditions under which they succeed or fail.

Method: Meta-analysis of existing data on downstream scaling laws, examining the prevalence of linear scaling trends and the impact of experimental changes.

Result: Linear scaling trends occur only 39% of the time, and minor experimental changes can alter scaling behavior significantly.

Conclusion: Scaling laws' success is context-dependent; understanding deviations from linear trends is essential for modeling pretraining loss and task performance.

Abstract: Downstream scaling laws aim to predict task performance at larger scales from
pretraining losses at smaller scales. Whether this prediction should be
possible is unclear: some works demonstrate that task performance follows clear
linear scaling trends under transformation, whereas others point out
fundamental challenges to downstream scaling laws, such as emergence and
inverse scaling. In this work, we conduct a meta-analysis of existing data on
downstream scaling laws, finding that close fit to linear scaling laws only
occurs in a minority of cases: 39% of the time. Furthermore, seemingly benign
changes to the experimental setting can completely change the scaling trend.
Our analysis underscores the need to understand the conditions under which
scaling laws succeed. To fully model the relationship between pretraining loss
and downstream task performance, we must embrace the cases in which scaling
behavior deviates from linear trends.

</details>


### [254] [MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes](https://arxiv.org/abs/2507.00891)
*Yuheng Wang,Xianhe Tang,Pufeng Huang*

Main category: cs.CL

TL;DR: The paper introduces MemeCMD, a Chinese multi-turn dialogue dataset with contextually retrieved memes, addressing the lack of multimodal expressiveness in existing datasets.


<details>
  <summary>Details</summary>
Motivation: Existing dialogue datasets lack multimodal expressiveness and contextual nuance, limiting their usefulness for multimodal conversational AI.

Method: The dataset combines a large-scale, MLLM-annotated meme library with auto-generated dialogues by dual agents, using a retrieval framework and adaptive threshold for contextual relevance.

Result: Experiments show the approach effectively generates contextually appropriate and diverse meme-incorporated dialogues.

Conclusion: MemeCMD offers a scalable, privacy-preserving resource for advancing multimodal conversational AI.

Abstract: Memes are widely used in online social interactions, providing vivid,
intuitive, and often humorous means to express intentions and emotions.
Existing dialogue datasets are predominantly limited to either manually
annotated or pure-text conversations, lacking the expressiveness and contextual
nuance that multimodal interactions provide.To address these challenges, we
introduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue
dataset with contextually retrieved memes. Our dataset combines a large-scale,
MLLM-annotated meme library with dialogues auto-generated by dual agents across
diverse scenarios. We introduce a retrieval framework and adaptive threshold to
ensure contextually relevant, naturally spaced meme usage. Experiments
demonstrate the effectiveness of our approach in generating contextually
appropriate and diverse meme-incorporated dialogues, offering a scalable and
privacy-preserving resource for advancing multimodal conversational AI.

</details>


### [255] [The Cognate Data Bottleneck in Language Phylogenetics](https://arxiv.org/abs/2507.00911)
*Luise Huser,Alexandros Stamatakis*

Main category: cs.CL

TL;DR: The paper highlights the challenge of applying computational phylogenetic methods to cognate data due to insufficient dataset sizes and demonstrates the inadequacy of automatically extracted datasets from resources like BabelNet.


<details>
  <summary>Details</summary>
Motivation: To explore the feasibility of leveraging computational phylogenetic methods for cognate data, given the limitations of current manually collected datasets.

Method: Automatically extracting cognate datasets from BabelNet and evaluating their consistency with established gold standard trees.

Result: Phylogenetic inferences from the extracted datasets yield inconsistent trees, suggesting the datasets are unsuitable.

Conclusion: Current computational approaches requiring large datasets cannot be applied to cognate data, leaving their applicability in historical linguistics uncertain.

Abstract: To fully exploit the potential of computational phylogenetic methods for
cognate data one needs to leverage specific (complex) models an machine
learning-based techniques. However, both approaches require datasets that are
substantially larger than the manually collected cognate data currently
available. To the best of our knowledge, there exists no feasible approach to
automatically generate larger cognate datasets. We substantiate this claim by
automatically extracting datasets from BabelNet, a large multilingual
encyclopedic dictionary. We demonstrate that phylogenetic inferences on the
respective character matrices yield trees that are largely inconsistent with
the established gold standard ground truth trees. We also discuss why we
consider it as being unlikely to be able to extract more suitable character
matrices from other multilingual resources. Phylogenetic data analysis
approaches that require larger datasets can therefore not be applied to cognate
data. Thus, it remains an open question how, and if these computational
approaches can be applied in historical linguistics.

</details>


### [256] [Discourse Heuristics For Paradoxically Moral Self-Correction](https://arxiv.org/abs/2507.00985)
*Guangliang Liu,Zimo Qi,Xitong Zhang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: Moral self-correction in LLMs is promising but flawed, relying on superficial heuristics. The paper analyzes these issues and proposes solutions using curated datasets.


<details>
  <summary>Details</summary>
Motivation: To address the paradoxes in moral self-correction of LLMs, where superficial heuristics limit effectiveness and consistency.

Method: Analyzing discourse constructions in fine-tuning corpora to uncover heuristic shortcuts and their impact.

Result: Moral self-correction relies on heuristic shortcuts, causing inconsistency. Proposed solution leverages curated datasets.

Conclusion: Improving moral self-correction requires addressing heuristic reliance and generalization challenges.

Abstract: Moral self-correction has emerged as a promising approach for aligning the
output of Large Language Models (LLMs) with human moral values. However, moral
self-correction techniques are subject to two primary paradoxes. First, despite
empirical and theoretical evidence to support the effectiveness of
self-correction, this LLM capability only operates at a superficial level.
Second, while LLMs possess the capability of self-diagnosing immoral aspects of
their output, they struggle to identify the cause of this moral inconsistency
during their self-correction process. To better understand and address these
paradoxes, we analyze the discourse constructions in fine-tuning corpora
designed to enhance moral self-correction, uncovering the existence of the
heuristics underlying effective constructions. We demonstrate that moral
self-correction relies on discourse constructions that reflect heuristic
shortcuts, and that the presence of these heuristic shortcuts during
self-correction leads to inconsistency when attempting to enhance both
self-correction and self-diagnosis capabilities jointly. Based on our findings,
we propose a solution to improve moral self-correction by leveraging the
heuristics of curated datasets. We also highlight the generalization challenges
of this capability, particularly in terms of learning from situated context and
model scales.

</details>


### [257] [Should We Still Pretrain Encoders with Masked Language Modeling?](https://arxiv.org/abs/2507.00994)
*Hippolyte Gisserot-Boukhlef,Nicolas Boizard,Manuel Faysse,Duarte M. Alves,Emmanuel Malherbe,Andr F. T. Martins,Cline Hudelot,Pierre Colombo*

Main category: cs.CL

TL;DR: The paper investigates whether CLM or MLM is better for text representation tasks, finding MLM generally superior but CLM more data-efficient. A biphasic CLM-then-MLM strategy is optimal under fixed computational budgets.


<details>
  <summary>Details</summary>
Motivation: To determine if CLM's advantages in text representation are inherent or due to confounding factors like model scale.

Method: Large-scale pretraining ablations with 30 models (210M-1B parameters) and 15,000+ fine-tuning runs, comparing MLM and CLM.

Result: MLM performs better overall, but CLM is more data-efficient and stable. A biphasic CLM-then-MLM approach yields optimal performance.

Conclusion: A biphasic training strategy leveraging CLM and MLM is computationally efficient and effective, especially when initializing from pretrained CLM models.

Abstract: Learning high-quality text representations is fundamental to a wide range of
NLP tasks. While encoder pretraining has traditionally relied on Masked
Language Modeling (MLM), recent evidence suggests that decoder models
pretrained with Causal Language Modeling (CLM) can be effectively repurposed as
encoders, often surpassing traditional encoders on text representation
benchmarks. However, it remains unclear whether these gains reflect an inherent
advantage of the CLM objective or arise from confounding factors such as model
and data scale. In this paper, we address this question through a series of
large-scale, carefully controlled pretraining ablations, training a total of 30
models ranging from 210 million to 1 billion parameters, and conducting over
15,000 fine-tuning and evaluation runs. We find that while training with MLM
generally yields better performance across text representation tasks,
CLM-trained models are more data-efficient and demonstrate improved fine-tuning
stability. Building on these findings, we experimentally show that a biphasic
training strategy that sequentially applies CLM and then MLM, achieves optimal
performance under a fixed computational training budget. Moreover, we
demonstrate that this strategy becomes more appealing when initializing from
readily available pretrained CLM models (from the existing LLM ecosystem),
reducing the computational burden needed to train best-in-class encoder models.
We release all project artifacts at https://hf.co/MLMvsCLM to foster further
research.

</details>


### [258] [La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America](https://arxiv.org/abs/2507.00999)
*Mara Grandury,Javier Aula-Blasco,Jlia Falco,Clmentine Fourrier,Miguel Gonzlez,Gonzalo Martnez,Gonzalo Santamara,Rodrigo Agerri,Nuria Aldama,Luis Chiruzzo,Javier Conde,Helena Gmez,Marta Guerrero,Guido Ivetta,Natalia Lpez,Flor Miriam Plaza-del-Arco,Mara Teresa Martn-Valdivia,Helena Montoro,Carmen Muoz,Pedro Reviriego,Leire Rosado,Alejandro Vaca,Mara Estrella Vallecillo-Rodrguez,Jorge Vallego,Irune Zubiaga*

Main category: cs.CL

TL;DR: La Leaderboard is the first open-source leaderboard for evaluating generative LLMs in Spanish and its varieties, aiming to promote linguistic and cultural diversity.


<details>
  <summary>Details</summary>
Motivation: To encourage LLM development for the Spanish-speaking community by providing a standardized evaluation platform.

Method: Combines 66 datasets in Basque, Catalan, Galician, and Spanish varieties, evaluating 50 models with a focus on fewer few-shot examples to reduce environmental impact.

Result: Establishes a community-driven evaluation standard and showcases model performance across diverse languages and varieties.

Conclusion: La Leaderboard sets a precedent for multilingual LLM evaluation and encourages similar initiatives for other languages.

Abstract: Leaderboards showcase the current capabilities and limitations of Large
Language Models (LLMs). To motivate the development of LLMs that represent the
linguistic and cultural diversity of the Spanish-speaking community, we present
La Leaderboard, the first open-source leaderboard to evaluate generative LLMs
in languages and language varieties of Spain and Latin America. La Leaderboard
is a community-driven project that aims to establish an evaluation standard for
everyone interested in developing LLMs for the Spanish-speaking community. This
initial version combines 66 datasets in Basque, Catalan, Galician, and
different Spanish varieties, showcasing the evaluation results of 50 models. To
encourage community-driven development of leaderboards in other languages, we
explain our methodology, including guidance on selecting the most suitable
evaluation setup for each downstream task. In particular, we provide a
rationale for using fewer few-shot examples than typically found in the
literature, aiming to reduce environmental impact and facilitate access to
reproducible results for a broader research community.

</details>


### [259] [SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks](https://arxiv.org/abs/2507.01001)
*Yilun Zhao,Kaiyan Zhang,Tiansheng Hu,Sihong Wu,Ronan Le Bras,Taira Anderson,Jonathan Bragg,Joseph Chee Chang,Jesse Dodge,Matt Latzke,Yixin Liu,Charles McGrady,Xiangru Tang,Zihang Wang,Chen Zhao,Hannaneh Hajishirzi,Doug Downey,Arman Cohan*

Main category: cs.CL

TL;DR: SciArena is a community-driven platform for evaluating foundation models on scientific literature tasks, using collective voting. It supports 23 models, has 13,000+ votes, and includes a meta-evaluation benchmark, SciArena-Eval.


<details>
  <summary>Details</summary>
Motivation: To create an open, collaborative platform for evaluating foundation models on scientific tasks, leveraging community input for more realistic and diverse assessments.

Method: Uses community voting (like Chatbot Arena) to evaluate models on open-ended, literature-grounded tasks. Analyzes data for diversity, real-world alignment, and annotator consistency.

Result: Collected 13,000+ votes, confirmed diverse and aligned questions, and strong annotator agreement. Introduced SciArena-Eval for automated evaluation benchmarking.

Conclusion: SciArena provides a robust, community-driven evaluation framework, but automated methods need improvement, as shown by SciArena-Eval.

Abstract: We present SciArena, an open and collaborative platform for evaluating
foundation models on scientific literature tasks. Unlike traditional benchmarks
for scientific literature understanding and synthesis, SciArena engages the
research community directly, following the Chatbot Arena evaluation approach of
community voting on model comparisons. By leveraging collective intelligence,
SciArena offers a community-driven evaluation of model performance on
open-ended scientific tasks that demand literature-grounded, long-form
responses. The platform currently supports 23 open-source and proprietary
foundation models and has collected over 13,000 votes from trusted researchers
across diverse scientific domains. We analyze the data collected so far and
confirm that the submitted questions are diverse, aligned with real-world
literature needs, and that participating researchers demonstrate strong
self-consistency and inter-annotator agreement in their evaluations. We discuss
the results and insights based on the model ranking leaderboard. To further
promote research in building model-based automated evaluation systems for
literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based
on our collected preference data. The benchmark measures the accuracy of models
in judging answer quality by comparing their pairwise assessments with human
votes. Our experiments highlight the benchmark's challenges and emphasize the
need for more reliable automated evaluation methods.

</details>

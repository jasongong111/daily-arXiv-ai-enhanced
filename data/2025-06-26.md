<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 23]
- [cs.LG](#cs.LG) [Total: 75]
- [cs.CV](#cs.CV) [Total: 48]
- [cs.CL](#cs.CL) [Total: 33]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Prover Agent: An Agent-based Framework for Formal Mathematical Proofs](https://arxiv.org/abs/2506.19923)
*Kaito Baba,Chaoran Liu,Shuhei Kurita,Akiyoshi Sannai*

Main category: cs.AI

TL;DR: Prover Agent combines LLMs and Lean for automated theorem proving, achieving 86.1% success on MiniF2F with fewer samples than prior methods.


<details>
  <summary>Details</summary>
Motivation: To enhance automated theorem proving by integrating LLMs with formal proof assistants like Lean, improving efficiency and success rates.

Method: Coordinates an informal reasoning LLM, a formal prover model, and Lean feedback, while generating auxiliary lemmas for proof strategies.

Result: Achieves 86.1% success on MiniF2F, setting a new state-of-the-art for SLMs with lower sample budgets.

Conclusion: Prover Agent demonstrates the effectiveness of combining LLMs and formal tools, with generated lemmas aiding in solving complex problems.

Abstract: We present Prover Agent, a novel AI agent for automated theorem proving that
integrates large language models (LLMs) with a formal proof assistant, Lean.
Prover Agent coordinates an informal reasoning LLM, a formal prover model, and
feedback from Lean while also generating auxiliary lemmas to assist in
discovering the overall proof strategy. It achieves an 86.1% success rate on
the MiniF2F benchmark, establishing a new state-of-the-art among methods using
small language models (SLMs) with a much lower sample budget than previous
approaches. We also present case studies illustrating how these generated
lemmas contribute to solving challenging problems.

</details>


### [2] [Context Attribution with Multi-Armed Bandit Optimization](https://arxiv.org/abs/2506.19977)
*Deng Pan,Keerthiram Murugesan,Nuno Moniz,Nitesh Chawla*

Main category: cs.AI

TL;DR: A novel framework for context attribution in generative QA systems using combinatorial multi-armed bandits (CMAB) and Combinatorial Thompson Sampling (CTS) to improve query efficiency and attribution fidelity.


<details>
  <summary>Details</summary>
Motivation: To build interpretable and trustworthy generative QA systems by identifying which parts of retrieved context contribute to the model's answers.

Method: Formulates context attribution as a CMAB problem, treating context segments as bandit arms and using CTS to explore context subsets efficiently. Defines a reward function based on normalized token likelihoods.

Result: Achieves competitive attribution quality with fewer model queries compared to traditional methods like SHAP.

Conclusion: The proposed method efficiently balances exploration and exploitation, improving query efficiency while maintaining high attribution fidelity.

Abstract: Understanding which parts of the retrieved context contribute to a large
language model's generated answer is essential for building interpretable and
trustworthy generative QA systems. We propose a novel framework that formulates
context attribution as a combinatorial multi-armed bandit (CMAB) problem. Each
context segment is treated as a bandit arm, and we employ Combinatorial
Thompson Sampling (CTS) to efficiently explore the exponentially large space of
context subsets under a limited query budget. Our method defines a reward
function based on normalized token likelihoods, capturing how well a subset of
segments supports the original model response. Unlike traditional
perturbation-based attribution methods such as SHAP, which sample subsets
uniformly and incur high computational costs, our approach adaptively balances
exploration and exploitation by leveraging posterior estimates of segment
relevance. This leads to substantially improved query efficiency while
maintaining high attribution fidelity. Extensive experiments on diverse
datasets and LLMs demonstrate that our method achieves competitive attribution
quality with fewer model queries.

</details>


### [3] [QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges](https://arxiv.org/abs/2506.20008)
*Abdul Basit,Minghao Shao,Haider Asif,Nouhaila Innan,Muhammad Kashif,Alberto Marchisio,Muhammad Shafique*

Main category: cs.AI

TL;DR: The paper benchmarks LLMs for quantum code generation using PennyLane and QHack challenges, introducing QHackBench. It evaluates RAG-enhanced models and a multi-agent pipeline, finding RAG comparable to standard prompting, especially in complex tasks.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored potential of LLMs in quantum computing, specifically for PennyLane-based code generation, using real-world challenges from QHack.

Method: Introduces QHackBench, evaluates LLMs under vanilla prompting and RAG, and assesses functional correctness, syntactic validity, and execution success. A multi-agent pipeline refines incorrect solutions.

Result: RAG-enhanced models perform similarly to standard prompting, particularly in complex quantum algorithms. The multi-agent pipeline improves execution success rates.

Conclusion: The study highlights the potential of LLMs in quantum programming and releases QHackBench and tools to advance AI-assisted quantum research.

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated strong
potential in code generation, yet their effectiveness in quantum computing
remains underexplored. This paper benchmarks LLMs for PennyLane-based quantum
code generation using real-world challenges from the Quantum Hackathon (QHack).
We introduce QHackBench, a novel benchmark dataset derived from QHack
competitions, and evaluate model performance under vanilla prompting and
Retrieval-Augmented Generation (RAG). Our structured evaluation framework
assesses functional correctness, syntactic validity, and execution success
across varying challenge difficulties. Results indicate that RAG-enhanced
models, supplemented with an augmented PennyLane dataset, approximately
generate similar results as the standard prompting, particularly in complex
quantum algorithms. Additionally, we introduce a multi-agent evaluation
pipeline that iteratively refines incorrect solutions, further enhancing
execution success rates. To foster further research, we commit to publicly
releasing QHackBench, along with our evaluation framework and experimental
results, enabling continued advancements in AI-assisted quantum programming.

</details>


### [4] [Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks](https://arxiv.org/abs/2506.20009)
*Konstantinos Vrettos,Michail E. Klontzas*

Main category: cs.AI

TL;DR: Custom RAG models outperform commercial LLMs in medical tasks, offering higher accuracy and lower energy/CO2 emissions.


<details>
  <summary>Details</summary>
Motivation: Address environmental and ethical concerns of AI in healthcare, focusing on resource efficiency and patient privacy.

Method: Developed a customizable RAG framework for medical tasks, tested with open-source LLMs, and compared performance/energy metrics to commercial models.

Result: RAG models, especially llama3.1:8B, achieved higher accuracy (58.5%) and lower energy/CO2 emissions than commercial models like o4-mini and DeepSeekV3-R1.

Conclusion: Local LLMs with RAG frameworks can outperform commercial models in medical tasks while being more sustainable, aligning with UN goals.

Abstract: Background The increasing adoption of Artificial Intelligence (AI) in
healthcare has sparked growing concerns about its environmental and ethical
implications. Commercial Large Language Models (LLMs), such as ChatGPT and
DeepSeek, require substantial resources, while the utilization of these systems
for medical purposes raises critical issues regarding patient privacy and
safety. Methods We developed a customizable Retrieval-Augmented Generation
(RAG) framework for medical tasks, which monitors its energy usage and CO2
emissions. This system was then used to create RAGs based on various
open-source LLMs. The tested models included both general purpose models like
llama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs
performance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs
o4-mini model. A dataset of medical questions was used for the evaluation.
Results Custom RAG models outperformed commercial models in accuracy and energy
consumption. The RAG model built on llama3.1:8B achieved the highest accuracy
(58.5%) and was significantly better than other models, including o4-mini and
DeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption
and CO2 footprint among all models, with a Performance per kWh of 0.52 and a
total CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x
times more accuracy points per kWh and 172% less electricity usage while
maintaining higher accuracy. Conclusion Our study demonstrates that local LLMs
can be leveraged to develop RAGs that outperform commercial, online LLMs in
medical tasks, while having a smaller environmental impact. Our modular
framework promotes sustainable AI development, reducing electricity usage and
aligning with the UNs Sustainable Development Goals.

</details>


### [5] [Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models](https://arxiv.org/abs/2506.20018)
*Zechun Deng,Ziwei Liu,Ziqian Bi,Junhao Song,Chia Xin Liang,Joe Yeong,Junfeng Hao*

Main category: cs.AI

TL;DR: The paper explores real-time decision support systems using low-latency AI, Edge-IoT, and human-AI teamwork, focusing on large language models, model compression, and edge device analytics.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient, adaptable AI-driven decision tools in resource-limited scenarios.

Method: Reviews holistic AI-driven tools, Edge-IoT integration, and human-AI collaboration, with a focus on large language models and model compression techniques.

Result: Provides practical insights into development strategies and applications, identifying opportunities for more efficient AI systems.

Conclusion: Sets the stage for future advancements in AI-supported real-time decision support, emphasizing its transformative potential.

Abstract: This paper investigates real-time decision support systems that leverage
low-latency AI models, bringing together recent progress in holistic AI-driven
decision tools, integration with Edge-IoT technologies, and approaches for
effective human-AI teamwork. It looks into how large language models can assist
decision-making, especially when resources are limited. The research also
examines the effects of technical developments such as DeLLMa, methods for
compressing models, and improvements for analytics on edge devices, while also
addressing issues like limited resources and the need for adaptable frameworks.
Through a detailed review, the paper offers practical perspectives on
development strategies and areas of application, adding to the field by
pointing out opportunities for more efficient and flexible AI-supported
systems. The conclusions set the stage for future breakthroughs in this
fast-changing area, highlighting how AI can reshape real-time decision support.

</details>


### [6] [Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning](https://arxiv.org/abs/2506.20020)
*Saloni Dash,Amélie Reymond,Emma S. Spiro,Aylin Caliskan*

Main category: cs.AI

TL;DR: Persona-assigned LLMs exhibit human-like motivated reasoning, reducing veracity discernment and showing identity-congruent biases, which are hard to mitigate with debiasing prompts.


<details>
  <summary>Details</summary>
Motivation: To explore if LLMs exhibit motivated reasoning like humans, especially when assigned personas, and its societal implications.

Method: Tested 8 LLMs with 8 personas across political and socio-demographic attributes on veracity discernment and scientific evidence evaluation tasks.

Result: Persona-assigned LLMs showed reduced veracity discernment (up to 9%) and political personas were 90% more likely to evaluate evidence congruently with their identity. Debiasing methods were ineffective.

Conclusion: LLMs exhibit human-like motivated reasoning, raising concerns about exacerbating biases in AI and human decision-making.

Abstract: Reasoning in humans is prone to biases due to underlying motivations like
identity protection, that undermine rational decision-making and judgment. This
motivated reasoning at a collective level can be detrimental to society when
debating critical issues such as human-driven climate change or vaccine safety,
and can further aggravate political polarization. Prior studies have reported
that large language models (LLMs) are also susceptible to human-like cognitive
biases, however, the extent to which LLMs selectively reason toward
identity-congruent conclusions remains largely unexplored. Here, we investigate
whether assigning 8 personas across 4 political and socio-demographic
attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and
proprietary) across two reasoning tasks from human-subject studies -- veracity
discernment of misinformation headlines and evaluation of numeric scientific
evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity
discernment relative to models without personas. Political personas
specifically, are up to 90% more likely to correctly evaluate scientific
evidence on gun control when the ground truth is congruent with their induced
political identity. Prompt-based debiasing methods are largely ineffective at
mitigating these effects. Taken together, our empirical findings are the first
to suggest that persona-assigned LLMs exhibit human-like motivated reasoning
that is hard to mitigate through conventional debiasing prompts -- raising
concerns of exacerbating identity-congruent reasoning in both LLMs and humans.

</details>


### [7] [DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction](https://arxiv.org/abs/2506.20059)
*Weijieying Ren,Tianxiang Zhao,Lei Wang,Tianchun Wang,Vasant Honavar*

Main category: cs.AI

TL;DR: DiaLLM integrates EHR data into medical dialogues for test recommendation, result interpretation, and diagnosis prediction, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing medical LLMs ignore EHR data and focus narrowly on diagnosis, limiting clinical utility.

Method: Uses Clinical Test Reference (CTR) to map EHR codes, reinforcement learning for evidence acquisition, and reject sampling for efficiency.

Result: Outperforms baselines in test recommendation and diagnosis prediction.

Conclusion: DiaLLM aligns better with real-world medical practice by leveraging EHR data and advanced learning strategies.

Abstract: Recent advances in Large Language Models (LLMs) have led to remarkable
progresses in medical consultation. However, existing medical LLMs overlook the
essential role of Electronic Health Records (EHR) and focus primarily on
diagnosis recommendation, limiting their clinical applicability. We propose
DiaLLM, the first medical LLM that integrates heterogeneous EHR data into
clinically grounded dialogues, enabling clinical test recommendation, result
interpretation, and diagnosis prediction to better align with real-world
medical practice. To construct clinically grounded dialogues from EHR, we
design a Clinical Test Reference (CTR) strategy that maps each clinical code to
its corresponding description and classifies test results as "normal" or
"abnormal". Additionally, DiaLLM employs a reinforcement learning framework for
evidence acquisition and automated diagnosis. To handle the large action space,
we introduce a reject sampling strategy to reduce redundancy and improve
exploration efficiency. Furthermore, a confirmation reward and a
class-sensitive diagnosis reward are designed to guide accurate diagnosis
prediction. Extensive experimental results demonstrate that DiaLLM outperforms
baselines in clinical test recommendation and diagnosis prediction.

</details>


### [8] [AI Copilots for Reproducibility in Science: A Case Study](https://arxiv.org/abs/2506.20130)
*Adrien Bibal,Steven N. Minton,Deborah Khider,Yolanda Gil*

Main category: cs.AI

TL;DR: OpenPub, an AI-powered platform, introduces the Reproducibility Copilot to streamline computational reproducibility in research, reducing reproduction time significantly and identifying common barriers.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring independent reproducibility of research findings in open science initiatives.

Method: The Reproducibility Copilot analyzes manuscripts, code, and supplementary materials to generate structured Jupyter Notebooks and recommendations. Feasibility tests were conducted using papers with known reproducibility benchmarks.

Result: OpenPub reduced reproduction time from over 30 hours to about 1 hour, achieving high coverage of reproducible figures, tables, and results. It systematically identified barriers like missing hyperparameters and incomplete datasets.

Conclusion: AI-driven tools like OpenPub can significantly ease reproducibility efforts, enhancing transparent and verifiable scientific communication, with potential for broader open science applications.

Abstract: Open science initiatives seek to make research outputs more transparent,
accessible, and reusable, but ensuring that published findings can be
independently reproduced remains a persistent challenge. This paper introduces
OpenPub, an AI-powered platform that supports researchers, reviewers, and
readers through a suite of modular copilots focused on key open science tasks.
In this work, we present the Reproducibility Copilot, which analyzes
manuscripts, code, and supplementary materials to generate structured Jupyter
Notebooks and recommendations aimed at facilitating computational, or "rote",
reproducibility. We conducted feasibility tests using previously studied
research papers with known reproducibility benchmarks. Results indicate that
OpenPub can substantially reduce reproduction time - from over 30 hours to
about 1 hour - while achieving high coverage of figures, tables, and results
suitable for computational reproduction. The system systematically detects
barriers to reproducibility, including missing hyperparameters, undocumented
preprocessing steps, and incomplete or inaccessible datasets. These findings
suggest that AI-driven tools can meaningfully reduce the burden of
reproducibility efforts and contribute to more transparent and verifiable
scientific communication. The modular copilot architecture also provides a
foundation for extending AI assistance to additional open science objectives
beyond reproducibility.

</details>


### [9] [Language Modeling by Language Models](https://arxiv.org/abs/2506.20249)
*Junyan Cheng,Peter Clark,Kyle Richardson*

Main category: cs.AI

TL;DR: Genesys, a multi-agent LLM system, discovers novel LM architectures through a genetic programming approach, outperforming existing methods and achieving competitive results with known architectures.


<details>
  <summary>Details</summary>
Motivation: To explore if LLMs can model the discovery of novel LM architectures, inspired by real research processes.

Method: A multi-agent LLM approach simulating research stages (ideation, implementation, evaluation) using a genetic programming backbone and scaling laws.

Result: 1,162 new designs discovered, with 1,062 verified; best designs outperform GPT2 and Mamba2 on 6/9 benchmarks.

Conclusion: Genesys demonstrates efficient and effective autonomous discovery of competitive LM architectures.

Abstract: Can we leverage LLMs to model the process of discovering novel language model
(LM) architectures? Inspired by real research, we propose a multi-agent LLM
approach that simulates the conventional stages of research, from ideation and
literature search (proposal stage) to design implementation (code generation),
generative pre-training, and downstream evaluation (verification). Using ideas
from scaling laws, our system, Genesys, employs a Ladder of Scales approach;
new designs are proposed, adversarially reviewed, implemented, and selectively
verified at increasingly larger model scales (14M$\sim$350M parameters) with a
narrowing budget (the number of models we can train at each scale). To help
make discovery efficient and factorizable, Genesys uses a novel genetic
programming backbone, which we show has empirical advantages over commonly used
direct prompt generation workflows (e.g., $\sim$86\% percentage point
improvement in successful design generation, a key bottleneck). We report
experiments involving 1,162 newly discovered designs (1,062 fully verified
through pre-training) and find the best designs to be highly competitive with
known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common
benchmarks). We couple these results with comprehensive system-level ablations
and formal results, which give broader insights into the design of effective
autonomous discovery systems.

</details>


### [10] [Enterprise Large Language Model Evaluation Benchmark](https://arxiv.org/abs/2506.20274)
*Liya Wang,David Yi,Damien Jose,John Passarelli,James Gao,Jordan Leventis,Kang Li*

Main category: cs.AI

TL;DR: A 14-task framework based on Bloom's Taxonomy is proposed to evaluate LLMs for enterprise tasks, addressing data noise and annotation costs with a scalable pipeline. Open-source models like DeepSeek R1 perform well in reasoning but lag in judgment tasks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like MMLU fail to assess enterprise-specific task complexities, necessitating a tailored evaluation framework.

Method: Developed a scalable pipeline combining LLM-as-a-Labeler, LLM-as-a-Judge, and CRAG to create a 9,700-sample benchmark. Evaluated six leading models.

Result: Open-source models rival proprietary ones in reasoning but lag in judgment tasks due to overthinking. The benchmark highlights enterprise performance gaps.

Conclusion: The framework offers enterprises a blueprint for tailored LLM evaluations and insights for model optimization, advancing practical deployment.

Abstract: Large Language Models (LLMs) ) have demonstrated promise in boosting
productivity across AI-powered tools, yet existing benchmarks like Massive
Multitask Language Understanding (MMLU) inadequately assess enterprise-specific
task complexities. We propose a 14-task framework grounded in Bloom's Taxonomy
to holistically evaluate LLM capabilities in enterprise contexts. To address
challenges of noisy data and costly annotation, we develop a scalable pipeline
combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented
generation (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six
leading models shows open-source contenders like DeepSeek R1 rival proprietary
models in reasoning tasks but lag in judgment-based scenarios, likely due to
overthinking. Our benchmark reveals critical enterprise performance gaps and
offers actionable insights for model optimization. This work provides
enterprises a blueprint for tailored evaluations and advances practical LLM
deployment.

</details>


### [11] [Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards](https://arxiv.org/abs/2506.20332)
*Jihao Gu,Qihang Ai,Yingyao Wang,Pi Bu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Ziming Wang,Yingxiu Zhao,Ming-Liang Zhang,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: Mobile-R1 enhances mobile agents' exploration and error correction via multi-turn reinforcement learning with task-level rewards, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods limit agents' dynamic interaction with environments, causing local optima and reduced exploration.

Method: Three-stage training: initial finetuning, single-step action-level reward training, and multi-turn task-level reward training.

Result: Improved performance; dataset with 24,521 annotations and 500-trajectory benchmark established.

Conclusion: Mobile-R1 advances mobile agent capabilities and resources are open-sourced for further research.

Abstract: Vision-language model-based mobile agents have gained the ability to not only
understand complex instructions and mobile screenshots, but also optimize their
action outputs via thinking and reasoning, benefiting from reinforcement
learning, such as Group Relative Policy Optimization (GRPO). However, existing
research centers on offline reinforcement learning training or online
optimization using action-level rewards, which limits the agent's dynamic
interaction with the environment. This often results in agents settling into
local optima, thereby weakening their ability for exploration and error action
correction. To address these challenges, we introduce an approach called
Mobile-R1, which employs interactive multi-turn reinforcement learning with
task-level rewards for mobile agents. Our training framework consists of three
stages: initial format finetuning, single-step online training via action-level
reward, followed by online training via task-level reward based on multi-turn
trajectories. This strategy is designed to enhance the exploration and error
correction capabilities of Mobile-R1, leading to significant performance
improvements. Moreover, we have collected a dataset covering 28 Chinese
applications with 24,521 high-quality manual annotations and established a new
benchmark with 500 trajectories. We will open source all resources, including
the dataset, benchmark, model weight, and codes:
https://mobile-r1.github.io/Mobile-R1/.

</details>


### [12] [Tabular Feature Discovery With Reasoning Type Exploration](https://arxiv.org/abs/2506.20357)
*Sungwon Han,Sungkyu Park,Seungeon Lee*

Main category: cs.AI

TL;DR: REFeat is a novel method using LLMs with structured reasoning to generate diverse and informative features for tabular data, outperforming existing approaches in accuracy and feature quality.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based feature generation methods often produce simple or repetitive features due to biases and lack of reasoning guidance.

Method: REFeat leverages multiple reasoning types to guide LLMs in generating diverse and meaningful features.

Result: Experiments on 59 datasets show REFeat achieves higher predictive accuracy and discovers more diverse features.

Conclusion: Incorporating rich reasoning and adaptive strategies into LLM-driven feature discovery holds promise for improving tabular data feature engineering.

Abstract: Feature engineering for tabular data remains a critical yet challenging step
in machine learning. Recently, large language models (LLMs) have been used to
automatically generate new features by leveraging their vast knowledge.
However, existing LLM-based approaches often produce overly simple or
repetitive features, partly due to inherent biases in the transformations the
LLM chooses and the lack of structured reasoning guidance during generation. In
this paper, we propose a novel method REFeat, which guides an LLM to discover
diverse and informative features by leveraging multiple types of reasoning to
steer the feature generation process. Experiments on 59 benchmark datasets
demonstrate that our approach not only achieves higher predictive accuracy on
average, but also discovers more diverse and meaningful features. These results
highlight the promise of incorporating rich reasoning paradigms and adaptive
strategy selection into LLM-driven feature discovery for tabular data.

</details>


### [13] [Paladin-mini: A Compact and Efficient Grounding Model Excelling in Real-World Scenarios](https://arxiv.org/abs/2506.20384)
*Dror Ivry,Oran Nahum*

Main category: cs.AI

TL;DR: The paper introduces Paladin-mini, a compact classifier for grounding claims, and a new benchmark dataset, demonstrating its performance against state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of grounding claims in documents by providing supportive evidence.

Method: Developed Paladin-mini, a 3.8B parameter classifier, and grounding-benchmark dataset for evaluation.

Result: Paladin-mini shows robust performance in real-world scenarios, benchmarked against state-of-the-art.

Conclusion: The contributions improve claim grounding with a compact model and a dedicated evaluation benchmark.

Abstract: This paper introduces two significant contributions to address the issue of
grounding claims in a given context. Grounding means that given a context
(document) and a claim, there's at least one supportive evidence for the claim
in the document. We will introduce Paladin-mini, a compact (3.8B parameters)
open-source classifier model (used for labeling data as grounded or ungrounded)
engineered for robust performance in real-world scenarios, and the
grounding-benchmark, a new evaluation dataset designed to assess performance on
critical reasoning tasks. We'll also demonstrate the results of Paladin-mini
with benchmarks against the current State-of-the-art and share clear and
reproducible results.

</details>


### [14] [Smart Ride and Delivery Services with Electric Vehicles: Leveraging Bidirectional Charging for Profit Optimisation](https://arxiv.org/abs/2506.20401)
*Jinchun Du,Bojie Shen,Muhammad Aamir Cheema,Adel N. Toosi*

Main category: cs.AI

TL;DR: The paper introduces the Electric Vehicle Orienteering Problem with V2G (EVOP-V2G), a profit-maximization problem for EV drivers managing charging/discharging and customer requests. It proposes two metaheuristic algorithms (EA and LNS) that outperform baselines, doubling profits and scaling well.


<details>
  <summary>Details</summary>
Motivation: The integration of EVs into ride-hailing and delivery services, coupled with V2G technology, creates new challenges and opportunities for optimizing profits while managing charging and discharging.

Method: The problem is formulated as a Mixed Integer Programming (MIP) model, with two metaheuristic algorithms: an evolutionary algorithm (EA) and large neighborhood search (LNS).

Result: Experiments on real-world data show the methods double driver profits compared to baselines, with near-optimal performance on small instances and scalability on larger ones.

Conclusion: The work demonstrates a promising approach for smarter, more profitable EV-based mobility systems that support the energy grid.

Abstract: With the rising popularity of electric vehicles (EVs), modern service
systems, such as ride-hailing delivery services, are increasingly integrating
EVs into their operations. Unlike conventional vehicles, EVs often have a
shorter driving range, necessitating careful consideration of charging when
fulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology -
allowing EVs to also discharge energy back to the grid - new opportunities and
complexities emerge. We introduce the Electric Vehicle Orienteering Problem
with V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select
customer requests or orders while managing when and where to charge or
discharge. This involves navigating dynamic electricity prices, charging
station selection, and route constraints. We formulate the problem as a Mixed
Integer Programming (MIP) model and propose two near-optimal metaheuristic
algorithms: one evolutionary (EA) and the other based on large neighborhood
search (LNS). Experiments on real-world data show our methods can double driver
profits compared to baselines, while maintaining near-optimal performance on
small instances and excellent scalability on larger ones. Our work highlights a
promising path toward smarter, more profitable EV-based mobility systems that
actively support the energy grid.

</details>


### [15] [GymPN: A Library for Decision-Making in Process Management Systems](https://arxiv.org/abs/2506.20404)
*Riccardo Lo Bianco,Willem van Jaarsveld,Remco Dijkman*

Main category: cs.AI

TL;DR: GymPN is a software library using Deep Reinforcement Learning for optimal decision-making in business processes, addressing partial observability and multiple decisions.


<details>
  <summary>Details</summary>
Motivation: To support optimal task allocation decisions in organizations by overcoming limitations of prior work, such as partial process observability and modeling multiple decisions.

Method: Developed GymPN, a library based on Deep Reinforcement Learning, to model and solve business process decision-making problems.

Result: Evaluated on eight problem patterns, GymPN effectively models problems and learns optimal policies.

Conclusion: GymPN enhances realism in process decision representation and supports optimal decision-making.

Abstract: Process management systems support key decisions about the way work is
allocated in organizations. This includes decisions on which task to perform
next, when to execute the task, and who to assign the task to. Suitable
software tools are required to support these decisions in a way that is optimal
for the organization. This paper presents a software library, called GymPN,
that supports optimal decision-making in business processes using Deep
Reinforcement Learning. GymPN builds on previous work that supports task
assignment in business processes, introducing two key novelties: support for
partial process observability and the ability to model multiple decisions in a
business process. These novel elements address fundamental limitations of
previous work and thus enable the representation of more realistic process
decisions. We evaluate the library on eight typical business process
decision-making problem patterns, showing that GymPN allows for easy modeling
of the desired problems, as well as learning optimal decision policies.

</details>


### [16] [Mixtures of Neural Cellular Automata: A Stochastic Framework for Growth Modelling and Self-Organization](https://arxiv.org/abs/2506.20486)
*Salvatore Milite,Giulio Caravagna,Andrea Sottoriva*

Main category: cs.AI

TL;DR: MNCA introduces probabilistic rules and noise into Neural Cellular Automata (NCA) to better model stochastic biological systems, showing improved robustness and accuracy in simulations.


<details>
  <summary>Details</summary>
Motivation: The deterministic nature of NCAs limits their ability to capture real-world stochasticity, especially in biological systems.

Method: MNCA combines mixture models with NCAs, using probabilistic rule assignments and intrinsic noise to model diverse local behaviors.

Result: MNCA outperforms in tissue growth simulations, image morphogenesis robustness, and microscopy segmentation, offering interpretable rules.

Conclusion: MNCA is a promising tool for stochastic dynamical systems and self-growth process modeling.

Abstract: Neural Cellular Automata (NCAs) are a promising new approach to model
self-organizing processes, with potential applications in life science.
However, their deterministic nature limits their ability to capture the
stochasticity of real-world biological and physical systems.
  We propose the Mixture of Neural Cellular Automata (MNCA), a novel framework
incorporating the idea of mixture models into the NCA paradigm. By combining
probabilistic rule assignments with intrinsic noise, MNCAs can model diverse
local behaviors and reproduce the stochastic dynamics observed in biological
processes.
  We evaluate the effectiveness of MNCAs in three key domains: (1) synthetic
simulations of tissue growth and differentiation, (2) image morphogenesis
robustness, and (3) microscopy image segmentation. Results show that MNCAs
achieve superior robustness to perturbations, better recapitulate real
biological growth patterns, and provide interpretable rule segmentation. These
findings position MNCAs as a promising tool for modeling stochastic dynamical
systems and studying self-growth processes.

</details>


### [17] [Engineering Sentience](https://arxiv.org/abs/2506.20504)
*Konstantin Demin,Taylor Webb,Eric Elmoznino,Hakwan Lau*

Main category: cs.AI

TL;DR: The paper defines sentience for AI in functional, computational terms, emphasizing persistence and qualitative aspects of sensory signals, and suggests potential implementations.


<details>
  <summary>Details</summary>
Motivation: To provide a clear, implementable definition of sentience for AI, ensuring it captures subjective experience beyond mere perception.

Method: Proposes a functional notion of sentience requiring assertoric (persistent) and qualitative sensory signals, with sketches for implementation.

Result: A framework for designing sentient AI, with practical insights for avoiding inadvertent creation of sentient agents.

Conclusion: Defining functional sentience aids AI development and awareness of sentient AI creation.

Abstract: We spell out a definition of sentience that may be useful for designing and
building it in machines. We propose that for sentience to be meaningful for AI,
it must be fleshed out in functional, computational terms, in enough detail to
allow for implementation. Yet, this notion of sentience must also reflect
something essentially 'subjective', beyond just having the general capacity to
encode perceptual content. For this specific functional notion of sentience to
occur, we propose that certain sensory signals need to be both assertoric
(persistent) and qualitative. To illustrate the definition in more concrete
terms, we sketch out some ways for potential implementation, given current
technology. Understanding what it takes for artificial agents to be
functionally sentient can also help us avoid creating them inadvertently, or at
least, realize that we have created them in a timely manner.

</details>


### [18] [Case-based Reasoning Augmented Large Language Model Framework for Decision Making in Realistic Safety-Critical Driving Scenarios](https://arxiv.org/abs/2506.20531)
*Wenbin Gan,Minh-Son Dao,Koji Zettsu*

Main category: cs.AI

TL;DR: The paper introduces a Case-Based Reasoning Augmented Large Language Model (CBR-LLM) framework to enhance evasive maneuver decision-making in autonomous driving by combining semantic scene understanding with past driving cases.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) show promise for autonomous driving but face challenges in domain adaptation, contextual grounding, and experiential knowledge. The paper aims to bridge this gap.

Method: The CBR-LLM framework integrates semantic scene understanding from dashcam videos with retrieval of past driving cases to provide context-sensitive, human-aligned maneuver recommendations.

Result: Experiments show improved decision accuracy, justification quality, and alignment with human expert behavior. Risk-aware prompting and similarity-based case retrieval enhance performance.

Conclusion: The framework demonstrates robustness in real-world conditions, highlighting its potential as a trustworthy decision-support tool for intelligent driving systems.

Abstract: Driving in safety-critical scenarios requires quick, context-aware
decision-making grounded in both situational understanding and experiential
reasoning. Large Language Models (LLMs), with their powerful general-purpose
reasoning capabilities, offer a promising foundation for such decision-making.
However, their direct application to autonomous driving remains limited due to
challenges in domain adaptation, contextual grounding, and the lack of
experiential knowledge needed to make reliable and interpretable decisions in
dynamic, high-risk environments. To address this gap, this paper presents a
Case-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for
evasive maneuver decision-making in complex risk scenarios. Our approach
integrates semantic scene understanding from dashcam video inputs with the
retrieval of relevant past driving cases, enabling LLMs to generate maneuver
recommendations that are both context-sensitive and human-aligned. Experiments
across multiple open-source LLMs show that our framework improves decision
accuracy, justification quality, and alignment with human expert behavior.
Risk-aware prompting strategies further enhance performance across diverse risk
types, while similarity-based case retrieval consistently outperforms random
sampling in guiding in-context learning. Case studies further demonstrate the
framework's robustness in challenging real-world conditions, underscoring its
potential as an adaptive and trustworthy decision-support tool for intelligent
driving systems.

</details>


### [19] [Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges](https://arxiv.org/abs/2506.20598)
*Alexander D. Kalian,Jaewook Lee,Stefan P. Johannesson,Lennart Otte,Christer Hogstrand,Miao Guo*

Main category: cs.AI

TL;DR: A multi-agent AI framework for sustainable protein research, focusing on microbial sources, using RAG with two GPT-based agents for literature search and information extraction. Fine-tuning and prompt engineering improved performance, with fine-tuning yielding higher cosine similarity scores.


<details>
  <summary>Details</summary>
Motivation: Address the global demand for sustainable protein by leveraging AI to process and synthesize domain-specific scientific knowledge efficiently.

Method: Developed a RAG-oriented system with two GPT-based agents: one for literature search and another for information extraction. Explored fine-tuning and prompt engineering for optimization.

Result: Fine-tuning improved mean cosine similarity scores to ≥0.94, outperforming prompt engineering (≥0.89). A user interface was also developed.

Conclusion: The framework effectively supports sustainable protein research, with fine-tuning offering superior performance, though prompt engineering provides lower uncertainty.

Abstract: The global demand for sustainable protein sources has accelerated the need
for intelligent tools that can rapidly process and synthesise domain-specific
scientific knowledge. In this study, we present a proof-of-concept multi-agent
Artificial Intelligence (AI) framework designed to support sustainable protein
production research, with an initial focus on microbial protein sources. Our
Retrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based
LLM agents: (1) a literature search agent that retrieves relevant scientific
literature on microbial protein production for a specified microbial strain,
and (2) an information extraction agent that processes the retrieved content to
extract relevant biological and chemical information. Two parallel
methodologies, fine-tuning and prompt engineering, were explored for agent
optimisation. Both methods demonstrated effectiveness at improving the
performance of the information extraction agent in terms of transformer-based
cosine similarity scores between obtained and ideal outputs. Mean cosine
similarity scores were increased by up to 25%, while universally reaching mean
scores of $\geq 0.89$ against ideal output text. Fine-tuning overall improved
the mean scores to a greater extent (consistently of $\geq 0.94$) compared to
prompt engineering, although lower statistical uncertainties were observed with
the latter approach. A user interface was developed and published for enabling
the use of the multi-agent AI system, alongside preliminary exploration of
additional chemical safety-based search capabilities

</details>


### [20] [CogGen: A Learner-Centered Generative AI Architecture for Intelligent Tutoring with Programming Video](https://arxiv.org/abs/2506.20600)
*Wengxi Li,Roy Pea,Nick Haber,Hari Subramonyam*

Main category: cs.AI

TL;DR: CogGen is an AI architecture that converts programming videos into interactive, adaptive learning experiences using student modeling and generative AI tutoring based on the Cognitive Apprenticeship framework.


<details>
  <summary>Details</summary>
Motivation: To enhance video-based programming education by making it interactive and adaptive, bridging structured student modeling with AI tutoring.

Method: Three components: (1) video segmentation by learning goals, (2) conversational tutoring engine using Cognitive Apprenticeship strategies, and (3) student model with Bayesian Knowledge Tracing for adaptation.

Result: Effective video segmentation accuracy and strong pedagogical alignment across knowledge, method, action, and interaction layers. Ablation studies confirm each component's necessity.

Conclusion: CogGen advances AI-powered tutoring by integrating structured student modeling with interactive AI conversations, offering a scalable solution for programming education.

Abstract: We introduce CogGen, a learner-centered AI architecture that transforms
programming videos into interactive, adaptive learning experiences by
integrating student modeling with generative AI tutoring based on the Cognitive
Apprenticeship framework. The architecture consists of three components: (1)
video segmentation by learning goals, (2) a conversational tutoring engine
applying Cognitive Apprenticeship strategies, and (3) a student model using
Bayesian Knowledge Tracing to adapt instruction. Our technical evaluation
demonstrates effective video segmentation accuracy and strong pedagogical
alignment across knowledge, method, action, and interaction layers. Ablation
studies confirm the necessity of each component in generating effective
guidance. This work advances AI-powered tutoring by bridging structured student
modeling with interactive AI conversations, offering a scalable approach to
enhancing video-based programming education.

</details>


### [21] [AI Assistants to Enhance and Exploit the PETSc Knowledge Base](https://arxiv.org/abs/2506.20608)
*Barry Smith,Junchao Zhang,Hong Zhang,Lois Curfman McInnes,Murat Keceli,Archit Vasan,Satish Balay,Toby Isaac,Le Chen,Venkatram Vishwanath*

Main category: cs.AI

TL;DR: The paper discusses using LLMs to organize and utilize PETSc's fragmented knowledge base, introducing tools like RAG and chatbots to improve accessibility and documentation.


<details>
  <summary>Details</summary>
Motivation: PETSc's extensive but informal knowledge base is underutilized, hindering users and developers. The goal is to leverage LLMs to make this knowledge more accessible and actionable.

Method: The team built an LLM-powered system with RAG, reranking algorithms, and chatbots, evaluated using various LLMs and embedding models.

Result: Initial experiences show promise in enhancing PETSc's usability and documentation, with a focus on scalable Krylov solvers.

Conclusion: The framework aims to evolve into a robust platform for AI-driven scientific software support, accelerating research and development.

Abstract: Generative AI, especially through large language models (LLMs), is
transforming how technical knowledge can be accessed, reused, and extended.
PETSc, a widely used numerical library for high-performance scientific
computing, has accumulated a rich but fragmented knowledge base over its three
decades of development, spanning source code, documentation, mailing lists,
GitLab issues, Discord conversations, technical papers, and more. Much of this
knowledge remains informal and inaccessible to users and new developers. To
activate and utilize this knowledge base more effectively, the PETSc team has
begun building an LLM-powered system that combines PETSc content with custom
LLM tools -- including retrieval-augmented generation (RAG), reranking
algorithms, and chatbots -- to assist users, support developers, and propose
updates to formal documentation. This paper presents initial experiences
designing and evaluating these tools, focusing on system architecture, using
RAG and reranking for PETSc-specific information, evaluation methodologies for
various LLMs and embedding models, and user interface design. Leveraging the
Argonne Leadership Computing Facility resources, we analyze how LLM responses
can enhance the development and use of numerical software, with an initial
focus on scalable Krylov solvers. Our goal is to establish an extensible
framework for knowledge-centered AI in scientific software, enabling scalable
support, enriched documentation, and enhanced workflows for research and
development. We conclude by outlining directions for expanding this system into
a robust, evolving platform that advances software ecosystems to accelerate
scientific discovery.

</details>


### [22] [Towards Community-Driven Agents for Machine Learning Engineering](https://arxiv.org/abs/2506.20640)
*Sijie Li,Weiwei Sun,Shanda Li,Ameet Talwalkar,Yiming Yang*

Main category: cs.AI

TL;DR: MLE-Live is a framework for evaluating ML agents' ability to collaborate in a simulated research community. CoMind, a novel agent, excels in this context, outperforming most human competitors in Kaggle competitions.


<details>
  <summary>Details</summary>
Motivation: Existing ML agents work in isolation, missing the collaborative benefits of human research communities. MLE-Live and CoMind aim to bridge this gap.

Method: Introduces MLE-Live for live evaluation and CoMind, an agent designed to exchange insights and develop solutions collaboratively.

Result: CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2% of human competitors in Kaggle competitions.

Conclusion: The framework and agent demonstrate the potential of collaborative ML research, with CoMind showing superior performance in community-driven tasks.

Abstract: Large language model-based machine learning (ML) agents have shown great
promise in automating ML research. However, existing agents typically operate
in isolation on a given research problem, without engaging with the broader
research community, where human researchers often gain insights and contribute
by sharing knowledge. To bridge this gap, we introduce MLE-Live, a live
evaluation framework designed to assess an agent's ability to communicate with
and leverage collective knowledge from a simulated Kaggle research community.
Building on this framework, we propose CoMind, a novel agent that excels at
exchanging insights and developing novel solutions within a community context.
CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2%
human competitors on average across four ongoing Kaggle competitions. Our code
is released at https://github.com/comind-ml/CoMind.

</details>


### [23] [The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind](https://arxiv.org/abs/2506.20664)
*Andrei Lupu,Timon Willi,Jakob Foerster*

Main category: cs.AI

TL;DR: The paper introduces Decrypto, a game-based benchmark for evaluating multi-agent reasoning and theory of mind (ToM) in LLMs, addressing gaps in existing benchmarks. It finds current LLMs lag behind humans in these abilities.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for ToM and multi-agent reasoning in LLMs are limited by narrow scope, data leakage, and lack of interactivity, necessitating a new evaluation framework.

Method: Proposes Decrypto, a benchmark inspired by cognitive science and multi-agent reinforcement learning, designed to minimize confounding factors. Validates it through empirical evaluations of LLMs, robustness studies, and human-AI cross-play experiments.

Result: LLMs perform worse than humans and simple baselines in ToM tasks. Surprisingly, newer reasoning models underperform older ones in key ToM abilities.

Conclusion: Decrypto fills a critical gap in ToM evaluations and advances the development of better artificial agents.

Abstract: As Large Language Models (LLMs) gain agentic abilities, they will have to
navigate complex multi-agent scenarios, interacting with human users and other
agents in cooperative and competitive settings. This will require new reasoning
skills, chief amongst them being theory of mind (ToM), or the ability to reason
about the "mental" states of other agents. However, ToM and other multi-agent
abilities in LLMs are poorly understood, since existing benchmarks suffer from
narrow scope, data leakage, saturation, and lack of interactivity. We thus
propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM
drawing inspiration from cognitive science, computational pragmatics and
multi-agent reinforcement learning. It is designed to be as easy as possible in
all other dimensions, eliminating confounding factors commonly found in other
benchmarks. To our knowledge, it is also the first platform for designing
interactive ToM experiments.
  We validate the benchmark design through comprehensive empirical evaluations
of frontier LLMs, robustness studies, and human-AI cross-play experiments. We
find that LLM game-playing abilities lag behind humans and simple
word-embedding baselines. We then create variants of two classic cognitive
science experiments within Decrypto to evaluate three key ToM abilities.
Surprisingly, we find that state-of-the-art reasoning models are significantly
worse at those tasks than their older counterparts. This demonstrates that
Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and
paves the path towards better artificial agents.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [24] [A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior](https://arxiv.org/abs/2506.19999)
*Francesco Ignazio Re,Andreas Opedal,Glib Manaiev,Mario Giulianelli,Ryan Cotterell*

Main category: cs.LG

TL;DR: A probabilistic model using a marked spatio-temporal point process and Hawkes process is proposed to better capture the dynamics of reading fixations and saccades, outperforming baselines but showing limited improvement with surprisal theory.


<details>
  <summary>Details</summary>
Motivation: Standard models of reading behavior rely on aggregated eye-tracking data and strong assumptions, ignoring spatio-temporal dynamics. This paper aims to address this gap.

Method: The model uses a marked spatio-temporal point process for fixations (capturing duration, location, and timing) and a Hawkes process for saccades (modeling excitation effects). Fixation duration includes spillover effects via time-convolved predictors.

Result: The Hawkes process model fits human saccades better than baselines. Adding contextual surprisal as a predictor for fixation durations offers only marginal improvement, suggesting surprisal theory's limitations for fine-grained eye movements.

Conclusion: The proposed model better captures reading dynamics, but surprisal theory may not fully explain fine-grained eye movements during reading.

Abstract: Reading is a process that unfolds across space and time, alternating between
fixations where a reader focuses on a specific point in space, and saccades
where a reader rapidly shifts their focus to a new point. An ansatz of
psycholinguistics is that modeling a reader's fixations and saccades yields
insight into their online sentence processing. However, standard approaches to
such modeling rely on aggregated eye-tracking measurements and models that
impose strong assumptions, ignoring much of the spatio-temporal dynamics that
occur during reading. In this paper, we propose a more general probabilistic
model of reading behavior, based on a marked spatio-temporal point process,
that captures not only how long fixations last, but also where they land in
space and when they take place in time. The saccades are modeled using a Hawkes
process, which captures how each fixation excites the probability of a new
fixation occurring near it in time and space. The duration time of fixation
events is modeled as a function of fixation-specific predictors convolved
across time, thus capturing spillover effects. Empirically, our Hawkes process
model exhibits a better fit to human saccades than baselines. With respect to
fixation durations, we observe that incorporating contextual surprisal as a
predictor results in only a marginal improvement in the model's predictive
accuracy. This finding suggests that surprisal theory struggles to explain
fine-grained eye movements.

</details>


### [25] [Position: Machine Learning Conferences Should Establish a "Refutations and Critiques" Track](https://arxiv.org/abs/2506.19882)
*Rylan Schaeffer,Joshua Kazdan,Yegor Denisov-Blanch,Brando Miranda,Matthias Gerstgrasser,Susan Zhang,Andreas Haupt,Isha Gupta,Elyas Obbad,Jesse Dodge,Jessica Zosa Forde,Koustuv Sinha,Francesco Orabona,Sanmi Koyejo,David Donoho*

Main category: cs.LG

TL;DR: The paper proposes a "Refutations and Critiques" (R & C) Track at ML conferences to systematically correct errors in research, enhancing the field's self-correcting nature.


<details>
  <summary>Details</summary>
Motivation: Rapid advancements in ML research have led to flawed or fraudulent studies being accepted due to peer review fallibility, necessitating a mechanism for correction.

Method: The paper suggests designing a dedicated R & C Track, discussing its structure, review principles, and potential challenges, with an example from ICLR 2025.

Result: The proposed R & C Track would provide a reputable platform to critique and correct prior research, fostering a self-correcting ecosystem.

Conclusion: ML conferences should implement official mechanisms like the R & C Track to improve research integrity and self-correction.

Abstract: Science progresses by iteratively advancing and correcting humanity's
understanding of the world. In machine learning (ML) research, rapid
advancements have led to an explosion of publications, but have also led to
misleading, incorrect, flawed or perhaps even fraudulent studies being accepted
and sometimes highlighted at ML conferences due to the fallibility of peer
review. While such mistakes are understandable, ML conferences do not offer
robust processes to help the field systematically correct when such errors are
made.This position paper argues that ML conferences should establish a
dedicated "Refutations and Critiques" (R & C) Track. This R & C Track would
provide a high-profile, reputable platform to support vital research that
critically challenges prior research, thereby fostering a dynamic
self-correcting research ecosystem. We discuss key considerations including
track design, review principles, potential pitfalls, and provide an
illustrative example submission concerning a recent ICLR 2025 Oral. We conclude
that ML conferences should create official, reputable mechanisms to help ML
research self-correct.

</details>


### [26] [STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic Multi-Objective Learning](https://arxiv.org/abs/2506.19883)
*Zhuqing Liu,Chaosheng Dong,Michinari Momma,Simone Shao,Shaoyuan Xu,Yan Gao,Haibo Yang,Jia Liu*

Main category: cs.LG

TL;DR: The paper introduces STIMULUS, a novel multi-objective optimization algorithm, and its enhanced versions (STIMULUS-M, STIMULUS+, STIMULUS-M+) to improve convergence and sample complexity.


<details>
  <summary>Details</summary>
Motivation: Existing MOO methods have poor convergence rates and high sample complexity, limiting their practical applications.

Method: STIMULUS uses a recursive framework for stochastic gradient updates, while STIMULUS-M adds momentum. Adaptive batching is introduced in STIMULUS+/STIMULUS-M+ to reduce computational overhead.

Result: The algorithms achieve improved convergence rates (O(1/T) for non-convex, O(exp{-μT}) for strongly convex) and state-of-the-art sample complexities.

Conclusion: STIMULUS and its variants offer robust, efficient solutions for MOO problems with theoretical guarantees.

Abstract: Recently, multi-objective optimization (MOO) has gained attention for its
broad applications in ML, operations research, and engineering. However, MOO
algorithm design remains in its infancy and many existing MOO methods suffer
from unsatisfactory convergence rate and sample complexity performance. To
address this challenge, in this paper, we propose an algorithm called STIMULUS(
stochastic path-integrated multi-gradient recursive e\ulstimator), a new and
robust approach for solving MOO problems. Different from the traditional
methods, STIMULUS introduces a simple yet powerful recursive framework for
updating stochastic gradient estimates to improve convergence performance with
low sample complexity. In addition, we introduce an enhanced version of
STIMULUS, termed STIMULUS-M, which incorporates a momentum term to further
expedite convergence. We establish $O(1/T)$ convergence rates of the proposed
methods for non-convex settings and $O (\exp{-\mu T})$ for strongly convex
settings, where $T$ is the total number of iteration rounds. Additionally, we
achieve the state-of-the-art $O \left(n+\sqrt{n}\epsilon^{-1}\right)$ sample
complexities for non-convex settings and $O\left(n+ \sqrt{n} \ln
({\mu/\epsilon})\right)$ for strongly convex settings, where $\epsilon>0$ is a
desired stationarity error. Moreover, to alleviate the periodic full gradient
evaluation requirement in STIMULUS and STIMULUS-M, we further propose enhanced
versions with adaptive batching called STIMULUS+/ STIMULUS-M+ and provide their
theoretical analysis.

</details>


### [27] [FlightKooba: A Fast Interpretable FTP Model](https://arxiv.org/abs/2506.19885)
*Jing Lu,Xuan Wu,Yizhun Tian,Songhan Fan,Yali Fang*

Main category: cs.LG

TL;DR: FlightKooba combines HIPPO, Koopman theory, and state space equations to improve flight trajectory prediction by reducing computational load and enhancing interpretability.


<details>
  <summary>Details</summary>
Motivation: Current Koopman-based models for flight trajectory prediction are inefficient, lack interpretability, and have high computational costs.

Method: FlightKooba constructs Koopman operators directly from data using structural state space equations, reducing trainable parameters and training time.

Result: FlightKooba achieves faster training (comparable to Mamba without CUDA), reduces memory by over 50%, and cuts parameters tenfold.

Conclusion: FlightKooba offers an efficient, interpretable solution for Koopman operator computation, advancing time series forecasting and control.

Abstract: The Koopman theory is a powerful and effective modeling tool for converting
nonlinear systems into linear representations, and flight trajectory prediction
(FTP) is a complex nonlinear system. However, current models applying the
Koopman theory to FTP tasks are not very effective, model interpretability is
indeed an issue, and the Koopman operators are computationally intensive,
resulting in long training times. To address this issue, this paper proposes a
new modeling and control framework based on the HIPPO method, the Koopman
theory, and state space equations from cybernetics: FlightKooba. Inspired by
the idea of structural state space equations, FlightKooba directly constructs
the Koopman operators from data. This makes the framework highly interpretable
and significantly reduces the number of trainable parameters in the module,
thereby greatly reducing training time. Experiments have demonstrated the
superiority of the FlightKooba modeling method in terms of time and memory
consumption (training time comparable to the Mamba module without using
CUDA-level acceleration; memory reduced by more than 50% on most datasets, with
a tenfold reduction in the number of parameters), essentially completing the
FTP task. It provides a new method for the fast computation of the Koopman
operators, opening up new possibilities for the combination of time series
forecasting and control.

</details>


### [28] [Causal-Aware Intelligent QoE Optimization for VR Interaction with Adaptive Keyframe Extraction](https://arxiv.org/abs/2506.19890)
*Ziru Zhang,Jiadong Yu,Danny H. K. Tsang*

Main category: cs.LG

TL;DR: The paper proposes an intelligent framework combining adaptive keyframe extraction and causal-aware reinforcement learning to optimize QoE in multi-user VR, outperforming benchmarks in latency, fairness, and QoE.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook causal relationships between bandwidth, CPU frequency, and user perception, limiting QoE gains in VR interactions.

Method: The framework integrates adaptive keyframe extraction with causal-aware RL (PS-CDDPG), using a novel QoE metric and MIP optimization.

Result: Experiments show reduced latency, improved QoE, and maintained fairness compared to benchmarks.

Conclusion: The proposed framework effectively balances QoE optimization with fairness in multi-user VR, leveraging causal relationships for superior performance.

Abstract: The optimization of quality of experience (QoE) in multi-user virtual reality
(VR) interactions demands a delicate balance between ultra-low latency,
high-fidelity motion synchronization, and equitable resource allocation. While
adaptive keyframe extraction mitigates transmission overhead, existing
approaches often overlook the causal relationships among allocated bandwidth,
CPU frequency, and user perception, limiting QoE gains. This paper proposes an
intelligent framework to maximize QoE by integrating adaptive keyframe
extraction with causal-aware reinforcement learning (RL). First, a novel QoE
metric is formulated using the Weber-Fechner Law, combining perceptual
sensitivity, attention-driven priorities, and motion reconstruction accuracy.
The QoE optimization problem is then modeled as a mixed integer programming
(MIP) task, jointly optimizing keyframe ratios, bandwidth, and computational
resources under horizon-fairness constraints. We propose Partial State Causal
Deep Deterministic Policy Gradient (PS-CDDPG), which integrates the Deep
Deterministic Policy Gradient (DDPG) method with causal influence detection. By
leveraging causal information regarding how QoE is influenced and determined by
various actions, we explore actions guided by weights calculated from causal
inference (CI), which in turn improves training efficiency. Experiments
conducted with the CMU Motion Capture Database demonstrate that our framework
significantly reduces interactive latency, enhances QoE, and maintains
fairness, achieving superior performance compared to benchmark methods.

</details>


### [29] [Orthogonal Soft Pruning for Efficient Class Unlearning](https://arxiv.org/abs/2506.19891)
*Qinghui Gong,Xue Yang,Xiaohu Tang*

Main category: cs.LG

TL;DR: A novel class-aware soft pruning framework for machine unlearning achieves rapid, precise forgetting with minimal accuracy loss, leveraging orthogonal convolutional kernel regularization.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between unlearning speed and predictive accuracy in machine unlearning, ensuring compliance with privacy regulations like GDPR.

Method: Uses orthogonal convolutional kernel regularization to decorrelate filters and disentangle features, combined with activation difference analysis for class-specific channel identification.

Result: Achieves millisecond-level unlearning, complete forgetting of targeted classes, minimal accuracy loss, and reduced membership inference attack risks.

Conclusion: The framework offers an efficient, practical solution for real-time machine unlearning in MLaaS scenarios.

Abstract: Machine unlearning aims to selectively remove class-specific knowledge from
pretrained neural networks to satisfy privacy regulations such as the GDPR.
Existing methods typically face a trade-off between unlearning speed and
preservation of predictive accuracy, often incurring either high computational
overhead or significant performance degradation on retained classes. In this
paper, we propose a novel class-aware soft pruning framework leveraging
orthogonal convolutional kernel regularization to achieve rapid and precise
forgetting with millisecond-level response times. By enforcing orthogonality
constraints during training, our method decorrelates convolutional filters and
disentangles feature representations, while efficiently identifying
class-specific channels through activation difference analysis. Extensive
evaluations across multiple architectures and datasets demonstrate stable
pruning with near-instant execution, complete forgetting of targeted classes,
and minimal accuracy loss on retained data. Experiments on CIFAR-10, CIFAR-100,
and TinyImageNet confirm that our approach substantially reduces membership
inference attack risks and accelerates unlearning by orders of magnitude
compared to state-of-the-art baselines. This framework provides an efficient,
practical solution for real-time machine unlearning in Machine Learning as a
Service (MLaaS) scenarios.

</details>


### [30] [MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations](https://arxiv.org/abs/2506.20100)
*Vardhan Dongre,Chi Gui,Shubham Garg,Hooshang Nayyeri,Gokhan Tur,Dilek Hakkani-Tür,Vikram S. Adve*

Main category: cs.LG

TL;DR: MIRAGE is a new multimodal benchmark for expert-level reasoning in agriculture, featuring real-world user-expert interactions, image-based context, and diverse biological entities.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks for grounded reasoning, clarification strategies, and long-form generation in knowledge-intensive domains like agriculture.

Method: Curated from 35,000 real user-expert interactions, MIRAGE combines natural queries, expert responses, and images, covering diverse scenarios like crop health and pest diagnosis.

Result: MIRAGE includes 7,000+ unique biological entities and supports open-world settings, requiring models to infer gaps and handle rare entities.

Conclusion: MIRAGE provides a high-fidelity, taxonomically diverse benchmark for vision-language models in real-world, consultative settings.

Abstract: We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning
and decision-making in consultative interaction settings. Designed for the
agriculture domain, MIRAGE captures the full complexity of expert consultations
by combining natural user queries, expert-authored responses, and image-based
context, offering a high-fidelity benchmark for evaluating models on grounded
reasoning, clarification strategies, and long-form generation in a real-world,
knowledge-intensive domain. Grounded in over 35,000 real user-expert
interactions and curated through a carefully designed multi-step pipeline,
MIRAGE spans diverse crop health, pest diagnosis, and crop management
scenarios. The benchmark includes more than 7,000 unique biological entities,
covering plant species, pests, and diseases, making it one of the most
taxonomically diverse benchmarks available for vision-language models, grounded
in the real world. Unlike existing benchmarks that rely on well-specified user
inputs and closed-set taxonomies, MIRAGE features underspecified, context-rich
scenarios with open-world settings, requiring models to infer latent knowledge
gaps, handle rare entities, and either proactively guide the interaction or
respond. Project Page: https://mirage-benchmark.github.io

</details>


### [31] [Distillation-Enabled Knowledge Alignment for Generative Semantic Communications in AIGC Provisioning Tasks](https://arxiv.org/abs/2506.19893)
*Jingzhi Hu,Geoffrey Ye Li*

Main category: cs.LG

TL;DR: DeKA-g, a distillation-enabled knowledge alignment algorithm, improves GSC systems by aligning cloud and edge knowledge, enhancing efficiency and adaptability.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of aligning knowledge in cloud generative AI with edges/users and adapting to wireless channels in GSC systems.

Method: Proposes DeKA-g with two methods: MAKD (metaword-aided knowledge distillation) and VGSA (variable-rate grouped SNR adaptation).

Result: Improves alignment by 44%, adapts to compression rates 116% more efficiently, and enhances low-SNR performance by 28%.

Conclusion: DeKA-g effectively aligns knowledge and adapts to diverse conditions, significantly improving GSC system performance.

Abstract: Due to the surging amount of AI-generated content (AIGC), its provisioning to
edges and mobile users from the cloud incurs substantial traffic on networks.
Generative semantic communication (GSC) offers a promising solution by
transmitting highly compact information, i.e., prompt text and latent
representations, instead of high-dimensional AIGC data. However, GSC relies on
the alignment between the knowledge in the cloud generative AI (GAI) and that
possessed by the edges and users, and between the knowledge for wireless
transmission and that of actual channels, which remains challenging. In this
paper, we propose DeKA-g, a distillation-enabled knowledge alignment algorithm
for GSC systems. The core idea is to distill the generation knowledge from the
cloud-GAI into low-rank matrices, which can be incorporated by the edge and
used to adapt the transmission knowledge to diverse wireless channel
conditions. DeKA-g comprises two novel methods: metaword-aided knowledge
distillation (MAKD) and variable-rate grouped SNR adaptation (VGSA). For MAKD,
an optimized metaword is employed to enhance the efficiency of knowledge
distillation, while VGSA enables efficient adaptation to diverse compression
rates and SNR ranges. From simulation results, DeKA-g improves the alignment
between the edge-generated images and the cloud-generated ones by 44%.
Moreover, it adapts to compression rates with 116% higher efficiency than the
baseline and enhances the performance in low-SNR conditions by 28%.

</details>


### [32] [Explaining deep neural network models for electricity price forecasting with XAI](https://arxiv.org/abs/2506.19894)
*Antoine Pesenti,Aidan OSullivan*

Main category: cs.LG

TL;DR: The paper uses deep neural networks (DNN) and explainable AI (XAI) methods like SHAP and Gradient to forecast and analyze electricity market prices, enhancing understanding of market dynamics.


<details>
  <summary>Details</summary>
Motivation: Electricity markets are complex, and traditional econometric methods lack the power of DNNs. The goal is to improve understanding of price drivers using XAI.

Method: DNNs for price forecasting, combined with XAI methods (SHAP, Gradient) and visual techniques (heatmaps) to analyze feature contributions across five markets. Introduces SSHAP values and lines for better interpretability.

Result: Improved forecasting and interpretability of electricity market prices, with novel SSHAP concepts enhancing high-dimensional model representation.

Conclusion: The approach successfully combines DNN power with XAI to deepen understanding of electricity market dynamics, offering new tools for analysis.

Abstract: Electricity markets are highly complex, involving lots of interactions and
complex dependencies that make it hard to understand the inner workings of the
market and what is driving prices. Econometric methods have been developed for
this, white-box models, however, they are not as powerful as deep neural
network models (DNN). In this paper, we use a DNN to forecast the price and
then use XAI methods to understand the factors driving the price dynamics in
the market. The objective is to increase our understanding of how different
electricity markets work. To do that, we apply explainable methods such as SHAP
and Gradient, combined with visual techniques like heatmaps (saliency maps) to
analyse the behaviour and contributions of various features across five
electricity markets. We introduce the novel concepts of SSHAP values and SSHAP
lines to enhance the complex representation of high-dimensional tabular models.

</details>


### [33] [A Framework for Uncertainty Quantification Based on Nearest Neighbors Across Layers](https://arxiv.org/abs/2506.19895)
*Miguel N. Font,José L. Jorro-Aragoneses,Carlos M. Alaíz*

Main category: cs.LG

TL;DR: A post-hoc framework measures neural network uncertainty using retrieved training cases, introducing Decision Change and Layer Uncertainty metrics, improving uncertainty estimation over softmax-based methods.


<details>
  <summary>Details</summary>
Motivation: Neural networks can produce errors in high-risk domains; measuring uncertainty helps detect and mitigate these errors.

Method: Proposes a framework using retrieved training cases with similar activation vectors to compute Decision Change and Layer Uncertainty metrics.

Result: Metrics improve uncertainty estimation, especially in challenging tasks, outperforming softmax-based confidence on CIFAR-10 and MNIST datasets.

Conclusion: The framework enhances uncertainty measurement, aiding reliability in critical applications.

Abstract: Neural Networks have high accuracy in solving problems where it is difficult
to detect patterns or create a logical model. However, these algorithms
sometimes return wrong solutions, which become problematic in high-risk domains
like medical diagnosis or autonomous driving. One strategy to detect and
mitigate these errors is the measurement of the uncertainty over neural network
decisions. In this paper, we present a novel post-hoc framework for measuring
the uncertainty of a decision based on retrieved training cases that have a
similar activation vector to the query for each layer. Based on these retrieved
cases, we propose two new metrics: Decision Change and Layer Uncertainty, which
capture changes in nearest-neighbor class distributions across layers. We
evaluated our approach in a classification model for two datasets: CIFAR-10 and
MNIST. The results show that these metrics enhance uncertainty estimation,
especially in challenging classification tasks, outperforming softmax-based
confidence.

</details>


### [34] [A Comparative Analysis of Reinforcement Learning and Conventional Deep Learning Approaches for Bearing Fault Diagnosis](https://arxiv.org/abs/2506.19929)
*Efe Çakır,Patrick Dumond*

Main category: cs.LG

TL;DR: The paper explores using reinforcement learning (RL), specifically Deep Q-Networks (DQNs), for bearing fault classification, showing it matches traditional methods in performance but excels in adaptability with optimized rewards, though computational demands remain a challenge.


<details>
  <summary>Details</summary>
Motivation: Bearing faults cause operational disruptions and high costs. Current methods rely on vibration analysis and machine learning, which need labeled data and struggle in dynamic environments.

Method: The study uses reinforcement learning (RL), specifically Deep Q-Networks (DQNs), for bearing fault classification in machine condition monitoring.

Result: RL models match traditional supervised learning in controlled conditions but outperform in adaptability with optimized rewards, though they are computationally demanding.

Conclusion: RL has potential to complement traditional methods, offering adaptive diagnostic frameworks, but computational efficiency needs improvement.

Abstract: Bearing faults in rotating machinery can lead to significant operational
disruptions and maintenance costs. Modern methods for bearing fault diagnosis
rely heavily on vibration analysis and machine learning techniques, which often
require extensive labeled data and may not adapt well to dynamic environments.
This study explores the feasibility of reinforcement learning (RL),
specifically Deep Q-Networks (DQNs), for bearing fault classification tasks in
machine condition monitoring to enhance the accuracy and adaptability of
bearing fault diagnosis. The results demonstrate that while RL models developed
in this study can match the performance of traditional supervised learning
models under controlled conditions, they excel in adaptability when equipped
with optimized reward structures. However, their computational demands
highlight areas for further improvement. These findings demonstrate RL's
potential to complement traditional methods, paving the way for adaptive
diagnostic frameworks.

</details>


### [35] [Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture](https://arxiv.org/abs/2506.19935)
*Shuchen Xue,Tianyu Xie,Tianyang Hu,Zijin Feng,Jiacheng Sun,Kenji Kawaguchi,Zhenguo Li,Zhi-Ming Ma*

Main category: cs.LG

TL;DR: The paper compares autoregressive (AR) and masked diffusion models (MDMs) in a decoder-only framework to fairly evaluate their paradigms and architectural influences. It finds that decoder-only MDMs offer significant speedups and comparable performance despite modeling a larger space.


<details>
  <summary>Details</summary>
Motivation: To address the unfair comparison between AR and MDM paradigms due to their typical architectural differences (decoder-only vs. encoder-only), this research aims to decouple paradigm differences from architectural influences.

Method: The study evaluates MDMs within a decoder-only framework, comparing them as Any-Order AR (AO-AR) and standard AR. It investigates the impact of architectural choices (decoder-only vs. encoder-only) in MDMs.

Result: Decoder-only MDMs achieve dramatic generation speedups (~25×) and comparable perplexity with temperature annealing, despite modeling a larger space. The standard AO-AR objective may need refinement due to less informative token permutations.

Conclusion: The work provides insights for future model design by decoupling paradigm differences from architectural influences, showing the potential of decoder-only MDMs for speed and performance.

Abstract: Large language models (LLMs) predominantly use autoregressive (AR)
approaches, but masked diffusion models (MDMs) are emerging as viable
alternatives. A key challenge in comparing AR and MDM paradigms is their
typical architectural difference: AR models are often decoder-only, while MDMs
have largely been encoder-only. This practice of changing both the modeling
paradigm and architecture simultaneously makes direct comparisons unfair, as
it's hard to distinguish whether observed differences stem from the paradigm
itself or the architectural shift. This research evaluates MDMs within a
decoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or
AO-AR) and standard AR paradigms. Our investigation suggests that the standard
AO-AR objective, which averages over all token permutations, may benefit from
refinement, as many permutations appear less informative compared to the
language's inherent left-to-right structure. (2) Investigate architectural
influences (decoder-only vs. encoder-only) within MDMs. We demonstrate that
while encoder-only MDMs model a simpler conditional probability space,
decoder-only MDMs can achieve dramatic generation speedups ($\sim25\times$) and
comparable perplexity with temperature annealing despite modeling a vastly
larger space, highlighting key trade-offs. This work thus decouples core
paradigm differences from architectural influences, offering insights for
future model design. Code is available at https://github.com/scxue/AO-GPT-MDM.

</details>


### [36] [The Most Important Features in Generalized Additive Models Might Be Groups of Features](https://arxiv.org/abs/2506.19937)
*Tomas M. Bosschieter,Luis Franca,Jessica Wolk,Yiyuan Wu,Bella Mehta,Joseph Dehoney,Orsolya Kiss,Fiona C. Baker,Qingyu Zhao,Rich Caruana,Kilian M. Pohl*

Main category: cs.LG

TL;DR: The paper introduces a method to assess the importance of feature groups in GAMs, addressing the oversight of joint signals in interpretable ML. It avoids model retraining, supports posthoc grouping, and handles high-dimensional data.


<details>
  <summary>Details</summary>
Motivation: Current interpretable ML often neglects joint signals from feature groups, missing critical insights, especially in datasets with natural groupings like multimodal data.

Method: A novel approach for GAMs that efficiently evaluates group importance without retraining, allows posthoc and overlapping groups, and scales to high dimensions.

Result: Synthetic experiments validate the method, and case studies on depressive symptoms and health determinants show group analysis provides more accurate insights than single-feature analysis.

Conclusion: Analyzing feature groups offers a holistic view, improving interpretability and accuracy in applications like medical diagnostics.

Abstract: While analyzing the importance of features has become ubiquitous in
interpretable machine learning, the joint signal from a group of related
features is sometimes overlooked or inadvertently excluded. Neglecting the
joint signal could bypass a critical insight: in many instances, the most
significant predictors are not isolated features, but rather the combined
effect of groups of features. This can be especially problematic for datasets
that contain natural groupings of features, including multimodal datasets. This
paper introduces a novel approach to determine the importance of a group of
features for Generalized Additive Models (GAMs) that is efficient, requires no
model retraining, allows defining groups posthoc, permits overlapping groups,
and remains meaningful in high-dimensional settings. Moreover, this definition
offers a parallel with explained variation in statistics. We showcase
properties of our method on three synthetic experiments that illustrate the
behavior of group importance across various data regimes. We then demonstrate
the importance of groups of features in identifying depressive symptoms from a
multimodal neuroscience dataset, and study the importance of social
determinants of health after total hip arthroplasty. These two case studies
reveal that analyzing group importance offers a more accurate, holistic view of
the medical issues compared to a single-feature analysis.

</details>


### [37] [HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization](https://arxiv.org/abs/2506.19992)
*Gabor Petnehazi,Bernadett Aradi*

Main category: cs.LG

TL;DR: HERCULES is a hierarchical k-means clustering algorithm that integrates LLMs for interpretable summaries of clusters across text, images, and numeric data.


<details>
  <summary>Details</summary>
Motivation: The need for advanced tools to group complex datasets and provide human-understandable insights drives the development of HERCULES.

Method: HERCULES recursively applies k-means clustering, uses LLMs for semantic titles/descriptions, and offers two representation modes (direct and description).

Result: The algorithm enhances interpretability and supports hierarchical knowledge extraction from diverse data types.

Conclusion: HERCULES shows promise for extracting meaningful hierarchical insights from complex datasets.

Abstract: The explosive growth of complex datasets across various modalities
necessitates advanced analytical tools that not only group data effectively but
also provide human-understandable insights into the discovered structures. We
introduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using
LLMs for Efficient Summarization), a novel algorithm and Python package
designed for hierarchical k-means clustering of diverse data types, including
text, images, and numeric data (processed one modality per run). HERCULES
constructs a cluster hierarchy by recursively applying k-means clustering,
starting from individual data points at level 0. A key innovation is its deep
integration of Large Language Models (LLMs) to generate semantically rich
titles and descriptions for clusters at each level of the hierarchy,
significantly enhancing interpretability. The algorithm supports two main
representation modes: `direct' mode, which clusters based on original data
embeddings or scaled numeric features, and `description' mode, which clusters
based on embeddings derived from LLM-generated summaries. Users can provide a
`topic\_seed' to guide LLM-generated summaries towards specific themes. An
interactive visualization tool facilitates thorough analysis and understanding
of the clustering results. We demonstrate HERCULES's capabilities and discuss
its potential for extracting meaningful, hierarchical knowledge from complex
datasets.

</details>


### [38] [TRACED: Transition-aware Regret Approximation with Co-learnability for Environment Design](https://arxiv.org/abs/2506.19997)
*Geonwoo Cho,Jaegyun Im,Jihwan Lee,Hojun Yi,Sejin Kim,Sundong Kim*

Main category: cs.LG

TL;DR: TRACED improves zero-shot generalization in UED by combining transition prediction error and co-learnability, requiring fewer environment interactions.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generalizing deep reinforcement learning agents to unseen environments by refining regret approximation in UED.

Method: Introduces transition prediction error and co-learnability to measure learning potential and task relationships, forming TRACED.

Result: TRACED improves generalization and reduces required environment interactions by 2x compared to baselines.

Conclusion: Refined regret approximation and task relationship modeling enhance sample-efficient curriculum design in UED.

Abstract: Generalizing deep reinforcement learning agents to unseen environments
remains a significant challenge. One promising solution is Unsupervised
Environment Design (UED), a co-evolutionary framework in which a teacher
adaptively generates tasks with high learning potential, while a student learns
a robust policy from this evolving curriculum. Existing UED methods typically
measure learning potential via regret, the gap between optimal and current
performance, approximated solely by value-function loss. Building on these
approaches, we introduce the transition prediction error as an additional term
in our regret approximation. To capture how training on one task affects
performance on others, we further propose a lightweight metric called
co-learnability. By combining these two measures, we present Transition-aware
Regret Approximation with Co-learnability for Environment Design (TRACED).
Empirical evaluations show that TRACED yields curricula that improve zero-shot
generalization across multiple benchmarks while requiring up to 2x fewer
environment interactions than strong baselines. Ablation studies confirm that
the transition prediction error drives rapid complexity ramp-up and that
co-learnability delivers additional gains when paired with the transition
prediction error. These results demonstrate how refined regret approximation
and explicit modeling of task relationships can be leveraged for
sample-efficient curriculum design in UED.

</details>


### [39] [Neuromorphic Wireless Split Computing with Resonate-and-Fire Neurons](https://arxiv.org/abs/2506.20015)
*Dengyu Wu,Jiechen Chen,H. Vincent Poor,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.LG

TL;DR: The paper proposes a neuromorphic computing architecture using resonate-and-fire (RF) neurons for efficient spectral feature extraction in streaming signals, reducing energy consumption compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Conventional leaky integrate-and-fire (LIF) neurons fail to effectively capture spectral features in streaming signals, necessitating a more efficient approach for edge applications like wireless sensing and audio recognition.

Method: The study introduces a wireless split computing architecture with RF neurons, which resonate at tunable frequencies to extract spectral features directly in the time domain, avoiding costly pre-processing.

Result: The RF-SNN architecture achieves accuracy comparable to LIF-SNNs and ANNs while significantly reducing spike rates and energy consumption during inference and communication.

Conclusion: The RF neuron-based architecture offers a promising, energy-efficient solution for real-time time-series processing in edge applications.

Abstract: Neuromorphic computing offers an energy-efficient alternative to conventional
deep learning accelerators for real-time time-series processing. However, many
edge applications, such as wireless sensing and audio recognition, generate
streaming signals with rich spectral features that are not effectively captured
by conventional leaky integrate-and-fire (LIF) spiking neurons. This paper
investigates a wireless split computing architecture that employs
resonate-and-fire (RF) neurons with oscillatory dynamics to process time-domain
signals directly, eliminating the need for costly spectral pre-processing. By
resonating at tunable frequencies, RF neurons extract time-localized spectral
features while maintaining low spiking activity. This temporal sparsity
translates into significant savings in both computation and transmission
energy. Assuming an OFDM-based analog wireless interface for spike
transmission, we present a complete system design and evaluate its performance
on audio classification and modulation classification tasks. Experimental
results show that the proposed RF-SNN architecture achieves comparable accuracy
to conventional LIF-SNNs and ANNs, while substantially reducing spike rates and
total energy consumption during inference and communication.

</details>


### [40] [New Insights on Unfolding and Fine-tuning Quantum Federated Learning](https://arxiv.org/abs/2506.20016)
*Shanika Iroshi Nanayakkara,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: A deep unfolding-based approach for Quantum Federated Learning (QFL) dynamically optimizes hyperparameters to address client heterogeneity, achieving ~90% accuracy vs. traditional ~55%.


<details>
  <summary>Details</summary>
Motivation: Client heterogeneity in QFL leads to performance issues, requiring adaptive solutions for robust optimization.

Method: Leverages deep unfolding for autonomous hyperparameter optimization (e.g., learning rates, regularization) based on client-specific training behavior.

Result: Achieves ~90% accuracy on IBM quantum hardware and Qiskit simulators, outperforming traditional methods (~55%).

Conclusion: The framework enhances QFL applicability in critical domains like healthcare and genomics by mitigating overfitting and improving generalization.

Abstract: Client heterogeneity poses significant challenges to the performance of
Quantum Federated Learning (QFL). To overcome these limitations, we propose a
new approach leveraging deep unfolding, which enables clients to autonomously
optimize hyperparameters, such as learning rates and regularization factors,
based on their specific training behavior. This dynamic adaptation mitigates
overfitting and ensures robust optimization in highly heterogeneous
environments where standard aggregation methods often fail. Our framework
achieves approximately 90% accuracy, significantly outperforming traditional
methods, which typically yield around 55% accuracy, as demonstrated through
real-time training on IBM quantum hardware and Qiskit Aer simulators. By
developing self adaptive fine tuning, the proposed method proves particularly
effective in critical applications such as gene expression analysis and cancer
detection, enhancing diagnostic precision and predictive modeling within
quantum systems. Our results are attributed to convergence-aware, learnable
optimization steps intrinsic to the deep unfolded framework, which maintains
the generalization. Hence, this study addresses the core limitations of
conventional QFL, streamlining its applicability to any complex challenges such
as healthcare and genomic research.

</details>


### [41] [DIM-SUM: Dynamic IMputation for Smart Utility Management](https://arxiv.org/abs/2506.20023)
*Ryan Hildebrant,Rahul Bhope,Sharad Mehrotra,Christopher Tull,Nalini Venkatasubramanian*

Main category: cs.LG

TL;DR: DIM-SUM is a preprocessing framework for training robust time series imputation models, addressing real-world missing data challenges by combining pattern clustering and adaptive masking. It outperforms traditional methods and pre-trained models in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Real-world infrastructure monitoring datasets often have complex, heterogeneous missing data patterns, unlike artificially masked training data. Bridging this gap is crucial for robust imputation models.

Method: DIM-SUM uses pattern clustering and adaptive masking strategies with theoretical learning guarantees to handle diverse missing patterns in real data.

Result: Experiments on large datasets (e.g., California water districts, electricity data) show DIM-SUM outperforms traditional methods in accuracy and efficiency, even with less training data. It also beats pre-trained models with 2x higher accuracy and faster inference.

Conclusion: DIM-SUM effectively bridges the gap between artificial and real missing data patterns, offering a practical, efficient solution for robust time series imputation.

Abstract: Time series imputation models have traditionally been developed using
complete datasets with artificial masking patterns to simulate missing values.
However, in real-world infrastructure monitoring, practitioners often encounter
datasets where large amounts of data are missing and follow complex,
heterogeneous patterns. We introduce DIM-SUM, a preprocessing framework for
training robust imputation models that bridges the gap between artificially
masked training data and real missing patterns. DIM-SUM combines pattern
clustering and adaptive masking strategies with theoretical learning guarantees
to handle diverse missing patterns actually observed in the data. Through
extensive experiments on over 2 billion readings from California water
districts, electricity datasets, and benchmarks, we demonstrate that DIM-SUM
outperforms traditional methods by reaching similar accuracy with lower
processing time and significantly less training data. When compared against a
large pre-trained model, DIM-SUM averages 2x higher accuracy with significantly
less inference time.

</details>


### [42] [Elucidated Rolling Diffusion Models for Probabilistic Weather Forecasting](https://arxiv.org/abs/2506.20024)
*Salva Rühling Cachay,Miika Aittala,Karsten Kreis,Noah Brenowitz,Arash Vahdat,Morteza Mardani,Rose Yu*

Main category: cs.LG

TL;DR: ERDM integrates rolling forecasts with Elucidated Diffusion Models to better handle temporal dependencies and uncertainty growth in high-dimensional chaotic systems, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models struggle with complex temporal dependencies and uncertainty growth in chaotic systems, motivating the development of ERDM.

Method: ERDM adapts EDM components (noise schedule, network preconditioning, Heun sampler) to rolling forecasts, introduces a novel loss weighting scheme, efficient initialization, and a hybrid sequence architecture.

Result: ERDM outperforms baselines in 2D Navier-Stokes simulations and ERA5 weather forecasting.

Conclusion: ERDM provides a flexible framework for diffusion-based sequence generation, especially where escalating uncertainty is critical.

Abstract: Diffusion models are a powerful tool for probabilistic forecasting, yet most
applications in high-dimensional chaotic systems predict future snapshots
one-by-one. This common approach struggles to model complex temporal
dependencies and fails to explicitly account for the progressive growth of
uncertainty inherent to such systems. While rolling diffusion frameworks, which
apply increasing noise to forecasts at longer lead times, have been proposed to
address this, their integration with state-of-the-art, high-fidelity diffusion
techniques remains a significant challenge. We tackle this problem by
introducing Elucidated Rolling Diffusion Models (ERDM), the first framework to
successfully unify a rolling forecast structure with the principled, performant
design of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM
components-its noise schedule, network preconditioning, and Heun sampler-to the
rolling forecast setting. The success of this integration is driven by three
key contributions: (i) a novel loss weighting scheme that focuses model
capacity on the mid-range forecast horizons where determinism gives way to
stochasticity; (ii) an efficient initialization strategy using a pre-trained
EDM for the initial window; and (iii) a bespoke hybrid sequence architecture
for robust spatiotemporal feature extraction under progressive denoising. On 2D
Navier-Stokes simulations and ERA5 global weather forecasting at 1.5^\circ
resolution, ERDM consistently outperforms key diffusion-based baselines,
including conditional autoregressive EDM. ERDM offers a flexible and powerful
general framework for tackling diffusion-based sequence generation problems
where modeling escalating uncertainty is paramount. Code is available at:
https://github.com/salvaRC/erdm

</details>


### [43] [Thumb on the Scale: Optimal Loss Weighting in Last Layer Retraining](https://arxiv.org/abs/2506.20025)
*Nathan Stromberg,Christos Thrampoulidis,Lalitha Sankar*

Main category: cs.LG

TL;DR: Loss weighting effectiveness in last layer retraining (LLR) depends on model overparameterization, bridging underparameterized and overparameterized extremes.


<details>
  <summary>Details</summary>
Motivation: Addressing biases in machine learning models by exploring loss weighting effectiveness in the LLR regime, which lies between underparameterized and overparameterized settings.

Method: Analyzes last layer retraining (LLR) with limited, inseparable data, testing loss weighting while accounting for model overparameterization.

Result: Loss weighting remains effective in LLR, but its success hinges on considering the model's relative overparameterization.

Conclusion: Loss weighting is viable in LLR, provided it adapts to the model's overparameterization, offering a balanced approach between extremes.

Abstract: While machine learning models become more capable in discriminative tasks at
scale, their ability to overcome biases introduced by training data has come
under increasing scrutiny. Previous results suggest that there are two extremes
of parameterization with very different behaviors: the population
(underparameterized) setting where loss weighting is optimal and the separable
overparameterized setting where loss weighting is ineffective at ensuring equal
performance across classes. This work explores the regime of last layer
retraining (LLR) in which the unseen limited (retraining) data is frequently
inseparable and the model proportionately sized, falling between the two
aforementioned extremes. We show, in theory and practice, that loss weighting
is still effective in this regime, but that these weights \emph{must} take into
account the relative overparameterization of the model.

</details>


### [44] [Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning](https://arxiv.org/abs/2506.20031)
*Prithvi Poddar,Ehsan Tarkesh Esfahani,Karthik Dantu,Souma Chowdhury*

Main category: cs.LG

TL;DR: A framework for generating diverse courses of action (COAs) for multi-agent missions, using a graph-based approach and genetic algorithm, with task sequencing optimized by a graph neural network.


<details>
  <summary>Details</summary>
Motivation: Automated planning for multi-agent missions is needed due to environmental changes and varying agent capabilities, requiring diverse and adaptable COAs.

Method: A graph abstraction of tasks and COAs, genetic algorithm for task allocation, and graph neural network for task sequencing.

Result: Simulated tests show performance gains over random baselines, small optimality gaps, and feasible execution times (50 mins for 5 agents/100 tasks).

Conclusion: The framework effectively generates diverse and compatible COAs, addressing challenges in multi-agent mission planning.

Abstract: Operations in disaster response, search \& rescue, and military missions that
involve multiple agents demand automated processes to support the planning of
the courses of action (COA). Moreover, traverse-affecting changes in the
environment (rain, snow, blockades, etc.) may impact the expected performance
of a COA, making it desirable to have a pool of COAs that are diverse in task
distributions across agents. Further, variations in agent capabilities, which
could be human crews and/or autonomous systems, present practical opportunities
and computational challenges to the planning process. This paper presents a new
theoretical formulation and computational framework to generate such diverse
pools of COAs for operations with soft variations in agent-task compatibility.
Key to the problem formulation is a graph abstraction of the task space and the
pool of COAs itself to quantify its diversity. Formulating the COAs as a
centralized multi-robot task allocation problem, a genetic algorithm is used
for (order-ignoring) allocations of tasks to each agent that jointly maximize
diversity within the COA pool and overall compatibility of the agent-task
mappings. A graph neural network is trained using a policy gradient approach to
then perform single agent task sequencing in each COA, which maximizes
completion rates adaptive to task features. Our tests of the COA generation
process in a simulated environment demonstrate significant performance gain
over a random walk baseline, small optimality gap in task sequencing, and
execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task
operations.

</details>


### [45] [Verifiable Unlearning on Edge](https://arxiv.org/abs/2506.20037)
*Mohammad M Maheri,Alex Davidson,Hamed Haddadi*

Main category: cs.LG

TL;DR: A framework using zk-SNARKs ensures verifiable data unlearning on edge-device models while preserving privacy and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for verifiable removal of data samples on edge devices due to copyright, biases, or regulations.

Method: Leverages zero-knowledge proofs (zk-SNARKs) for verification, with algorithms designed for efficient unlearning and minimal overhead.

Result: Practical and effective verifiable unlearning with minimal impact on model performance.

Conclusion: The framework ensures privacy-preserving, verifiable unlearning across edge devices.

Abstract: Machine learning providers commonly distribute global models to edge devices,
which subsequently personalize these models using local data. However, issues
such as copyright infringements, biases, or regulatory requirements may require
the verifiable removal of certain data samples across all edge devices.
Ensuring that edge devices correctly execute such unlearning operations is
critical to maintaining integrity.
  In this work, we introduce a verification framework leveraging zero-knowledge
proofs, specifically zk-SNARKs, to confirm data unlearning on personalized
edge-device models without compromising privacy. We have developed algorithms
explicitly designed to facilitate unlearning operations that are compatible
with efficient zk-SNARK proof generation, ensuring minimal computational and
memory overhead suitable for constrained edge environments. Furthermore, our
approach carefully preserves personalized enhancements on edge devices,
maintaining model performance post-unlearning.
  Our results affirm the practicality and effectiveness of this verification
framework, demonstrating verifiable unlearning with minimal degradation in
personalization-induced performance improvements. Our methodology ensures
verifiable, privacy-preserving, and effective machine unlearning across edge
devices.

</details>


### [46] [Cross-Layer Discrete Concept Discovery for Interpreting Language Models](https://arxiv.org/abs/2506.20040)
*Ankur Garg,Xuemin Yu,Hassan Sajjad,Samira Ebrahimi Kahou*

Main category: cs.LG

TL;DR: The paper introduces CLVQVAE, a framework using vector quantization to map and collapse duplicated features in transformer layers into interpretable concept vectors, addressing challenges in analyzing cross-layer superposition.


<details>
  <summary>Details</summary>
Motivation: Current methods analyze neural representations at single layers, missing cross-layer superposition and redundancy in transformer models.

Method: Proposes CLVQVAE with top-k temperature-based sampling, EMA codebook updates, and scaled-spherical k-means++ for initialization to cluster by directional similarity.

Result: The framework collapses duplicated features into compact, interpretable concept vectors, improving analysis of feature evolution.

Conclusion: CLVQVAE effectively addresses limitations in understanding feature evolution across transformer layers by leveraging vector quantization and directional clustering.

Abstract: Uncovering emergent concepts across transformer layers remains a significant
challenge because the residual stream linearly mixes and duplicates
information, obscuring how features evolve within large language models.
Current research efforts primarily inspect neural representations at single
layers, thereby overlooking this cross-layer superposition and the redundancy
it introduces. These representations are typically either analyzed directly for
activation patterns or passed to probing classifiers that map them to a limited
set of predefined concepts. To address these limitations, we propose
\gls{clvqvae}, a framework that uses vector quantization to map representations
across layers and in the process collapse duplicated residual-stream features
into compact, interpretable concept vectors. Our approach uniquely combines
top-$k$ temperature-based sampling during quantization with EMA codebook
updates, providing controlled exploration of the discrete latent space while
maintaining code-book diversity. We further enhance the framework with
scaled-spherical k-means++ for codebook initialization, which clusters by
directional similarity rather than magnitude, better aligning with semantic
structure in word embedding space.

</details>


### [47] [LSH-DynED: A Dynamic Ensemble Framework with LSH-Based Undersampling for Evolving Multi-Class Imbalanced Classification](https://arxiv.org/abs/2506.20041)
*Soheil Abadifard,Fazli Can*

Main category: cs.LG

TL;DR: The paper introduces LSH-DynED, a novel method combining Locality Sensitive Hashing with Random Hyperplane Projections and Dynamic Ensemble Diversification to address multi-class imbalanced data streams, outperforming 15 state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of dynamic imbalance ratios in multi-class imbalanced data streams, with limited prior research compared to binary cases, motivates this study.

Method: LSH-RHP is integrated into the DynED framework for undersampling majority classes, creating balanced training sets and enhancing ensemble prediction.

Result: LSH-DynED outperforms 15 methods in Kappa and mG-Mean metrics, excelling in large-scale, high-dimensional datasets with class imbalances.

Conclusion: LSH-DynED is robust and adaptable for real-world multi-class imbalanced data streams, with potential for future research and reproducibility via GitHub.

Abstract: The classification of imbalanced data streams, which have unequal class
distributions, is a key difficulty in machine learning, especially when dealing
with multiple classes. While binary imbalanced data stream classification tasks
have received considerable attention, only a few studies have focused on
multi-class imbalanced data streams. Effectively managing the dynamic imbalance
ratio is a key challenge in this domain. This study introduces a novel, robust,
and resilient approach to address these challenges by integrating Locality
Sensitive Hashing with Random Hyperplane Projections (LSH-RHP) into the Dynamic
Ensemble Diversification (DynED) framework. To the best of our knowledge, we
present the first application of LSH-RHP for undersampling in the context of
imbalanced non-stationary data streams. The proposed method undersamples the
majority classes by utilizing LSH-RHP, provides a balanced training set, and
improves the ensemble's prediction performance. We conduct comprehensive
experiments on 23 real-world and ten semi-synthetic datasets and compare
LSH-DynED with 15 state-of-the-art methods. The results reveal that LSH-DynED
outperforms other approaches in terms of both Kappa and mG-Mean effectiveness
measures, demonstrating its capability in dealing with multi-class imbalanced
non-stationary data streams. Notably, LSH-DynED performs well in large-scale,
high-dimensional datasets with considerable class imbalances and demonstrates
adaptation and robustness in real-world circumstances. To motivate our design,
we review existing methods for imbalanced data streams, outline key challenges,
and offer guidance for future work. For the reproducibility of our results, we
have made our implementation available on GitHub.

</details>


### [48] [GNN's Uncertainty Quantification using Self-Distillation](https://arxiv.org/abs/2506.20046)
*Hirad Daneshvar,Reza Samavi*

Main category: cs.LG

TL;DR: A novel method using knowledge distillation is proposed to efficiently quantify predictive uncertainty in GNNs for healthcare, outperforming traditional Bayesian and ensemble methods in computational cost and precision.


<details>
  <summary>Details</summary>
Motivation: Quantifying predictive uncertainty in GNNs is crucial for trustworthiness in clinical settings, but existing methods like Bayesian and ensemble approaches are computationally expensive and lack model diversity capture.

Method: The method employs self-distillation, where the same network acts as both teacher and student, avoiding independent training of multiple networks. An uncertainty metric assigns weights to GNN classifiers to capture diversity.

Result: Experimental evaluation on MIMIC-IV and Enzymes datasets shows the method effectively captures uncertainty with performance comparable to MC Dropout and ensemble methods, while being more efficient.

Conclusion: The proposed self-distillation-based method offers a precise and efficient way to quantify GNN uncertainty, enhancing trustworthiness in healthcare applications.

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance in the
healthcare domain. However, what remained challenging is quantifying the
predictive uncertainty of GNNs, which is an important aspect of trustworthiness
in clinical settings. While Bayesian and ensemble methods can be used to
quantify uncertainty, they are computationally expensive. Additionally, the
disagreement metric used by ensemble methods to compute uncertainty cannot
capture the diversity of models in an ensemble network. In this paper, we
propose a novel method, based on knowledge distillation, to quantify GNNs'
uncertainty more efficiently and with higher precision. We apply
self-distillation, where the same network serves as both the teacher and
student models, thereby avoiding the need to train several networks
independently. To ensure the impact of self-distillation, we develop an
uncertainty metric that captures the diverse nature of the network by assigning
different weights to each GNN classifier. We experimentally evaluate the
precision, performance, and ability of our approach in distinguishing
out-of-distribution data on two graph datasets: MIMIC-IV and Enzymes. The
evaluation results demonstrate that the proposed method can effectively capture
the predictive uncertainty of the model while having performance similar to
that of the MC Dropout and ensemble methods. The code is publicly available at
https://github.com/tailabTMU/UQ_GNN.

</details>


### [49] [Universal pre-training by iterated random computation](https://arxiv.org/abs/2506.20057)
*Peter Bloem*

Main category: cs.LG

TL;DR: The paper explores using randomly generated data for model pre-training, supported by theoretical insights from algorithmic complexity. It shows synthetic data enables zero-shot learning and improves with scale, with finetuning enhancing performance.


<details>
  <summary>Details</summary>
Motivation: To theoretically and empirically validate the use of synthetic data for pre-training models, leveraging algorithmic complexity principles.

Method: Theoretical analysis of algorithmic complexity, empirical pre-training with synthetic data, and evaluation via zero-shot learning and finetuning on real-world datasets.

Result: Synthetic pre-training enables zero-shot learning, scales well, and finetuning improves convergence and generalization.

Conclusion: Randomly generated data is viable for pre-training, offering theoretical and practical benefits, especially when combined with finetuning.

Abstract: We investigate the use of randomly generated data for the sake of
pre-training a model. We justify this approach theoretically from the
perspective of algorithmic complexity, building on recent research that shows
that sequence models can be trained to approximate Solomonoff induction. We
derive similar, but complementary theoretical results. We show empirically that
synthetically generated data can be used to pre-train a model before the data
is seen. We replicate earlier results that models trained this way show
zero-shot in-context learning across a variety of datasets, and that this
performance improves with scale. We extend earlier results to real-world data,
and show that finetuning a model after pre-training offers faster convergence
and better generalization.

</details>


### [50] [Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models](https://arxiv.org/abs/2506.20061)
*Zhicheng Zhang,Ziyan Wang,Yali Du,Fei Fang*

Main category: cs.LG

TL;DR: The paper introduces a method using large language models (LLMs) to generate open-ended instructions from agent trajectories, reducing reliance on human-labeled data and improving instruction-following policies in reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The challenge of developing instruction-following policies in reinforcement learning due to sparse rewards and dependence on human-labeled datasets.

Method: Leveraging LLMs to relabel unsuccessful trajectories by identifying meaningful subtasks, enriching training data, and learning a unified policy.

Result: Improved sample efficiency, instruction coverage, and policy performance in the Craftax environment compared to baselines.

Conclusion: LLM-guided open-ended instruction relabeling effectively enhances instruction-following reinforcement learning.

Abstract: Developing effective instruction-following policies in reinforcement learning
remains challenging due to the reliance on extensive human-labeled instruction
datasets and the difficulty of learning from sparse rewards. In this paper, we
propose a novel approach that leverages the capabilities of large language
models (LLMs) to automatically generate open-ended instructions retrospectively
from previously collected agent trajectories. Our core idea is to employ LLMs
to relabel unsuccessful trajectories by identifying meaningful subtasks the
agent has implicitly accomplished, thereby enriching the agent's training data
and substantially alleviating reliance on human annotations. Through this
open-ended instruction relabeling, we efficiently learn a unified
instruction-following policy capable of handling diverse tasks within a single
policy. We empirically evaluate our proposed method in the challenging Craftax
environment, demonstrating clear improvements in sample efficiency, instruction
coverage, and overall policy performance compared to state-of-the-art
baselines. Our results highlight the effectiveness of utilizing LLM-guided
open-ended instruction relabeling to enhance instruction-following
reinforcement learning.

</details>


### [51] [Supervised Coupled Matrix-Tensor Factorization (SCMTF) for Computational Phenotyping of Patient Reported Outcomes in Ulcerative Colitis](https://arxiv.org/abs/2506.20065)
*Cristian Minoccheri,Sophia Tesic,Kayvan Najarian,Ryan Stidham*

Main category: cs.LG

TL;DR: The paper introduces a supervised coupled matrix-tensor factorization (SCMTF) method to integrate patient-reported outcomes (PROs) and lab data for predicting medication persistence in ulcerative colitis, achieving high AUC scores and interpretable phenotypes.


<details>
  <summary>Details</summary>
Motivation: Patient-reported symptoms in ulcerative colitis are often noisy and sparse, leading to their exclusion in phenotyping. This paper aims to leverage PROs using a novel method to improve phenotyping and prediction.

Method: The proposed SCMTF integrates temporal PROs, temporal labs, and static features within a deep learning framework to handle missing data and predict medication persistence.

Result: The model predicts medication changes 8 and 20 months ahead with AUCs of 0.853 and 0.803, respectively, and identifies interpretable phenotypes.

Conclusion: The study demonstrates the successful application of tensor-based phenotyping to UC and PROs, revealing their relevance in predicting medication persistence.

Abstract: Phenotyping is the process of distinguishing groups of patients to identify
different types of disease progression. A recent trend employs low-rank matrix
and tensor factorization methods for their capability of dealing with
multi-modal, heterogeneous, and missing data. Symptom quantification is crucial
for understanding patient experiences in inflammatory bowel disease, especially
in conditions such as ulcerative colitis (UC). However, patient-reported
symptoms are typically noisy, subjective, and significantly more sparse than
other data types. For this reason, they are usually not included in phenotyping
and other machine learning methods. This paper explores the application of
computational phenotyping to leverage Patient-Reported Outcomes (PROs) using a
novel supervised coupled matrix-tensor factorization (SCMTF) method, which
integrates temporal PROs and temporal labs with static features to predict
medication persistence in ulcerative colitis. This is the first tensor-based
method that is both supervised and coupled, it is the first application to the
UC domain, and the first application to PROs. We use a deep learning framework
that makes the model flexible and easy to train. The proposed method allows us
to handle the large amount of missing data in the PROs. The best model predicts
changes in medication 8 and 20 months in the future with AUCs of 0.853 and
0.803 on the test set respectively. We derive interpretable phenotypes
consisting of static features and temporal features (including their temporal
patterns). We show that low-rank matrix and tensor based phenotyping can be
successfully applied to the UC domain and to highly missing PRO data. We
identify phenotypes useful to predict medication persistence - these phenotypes
include several symptom variables, showing that PROs contain relevant
infromation that is usually discarded.

</details>


### [52] [A Survey of Predictive Maintenance Methods: An Analysis of Prognostics via Classification and Regression](https://arxiv.org/abs/2506.20090)
*Ainaz Jamshidi,Dongchan Kim,Muhammad Arif*

Main category: cs.LG

TL;DR: This review compares classification- and regression-based approaches in predictive maintenance (PdM), highlighting their strengths, challenges, and future trends.


<details>
  <summary>Details</summary>
Motivation: The lack of a standalone comparative study between regression- and classification-based PdM methods motivates this review to explore their differences and applications.

Method: The paper analyzes recent literature on PdM, focusing on regression (for RUL estimation) and classification (for failure probability forecasts).

Result: Key advancements, challenges (e.g., data imbalance), and trends (e.g., hybrid approaches) are identified, aiding in method selection.

Conclusion: The review guides researchers and practitioners in choosing PdM methods and suggests future work on practical tools and datasets.

Abstract: Predictive maintenance (PdM) has become a crucial element of modern
industrial practice. PdM plays a significant role in operational dependability
and cost management by decreasing unforeseen downtime and optimizing asset life
cycle management. Machine learning and deep learning have enabled more precise
forecasts of equipment failure and remaining useful life (RUL). Although many
studies have been conducted on PdM, there has not yet been a standalone
comparative study between regression- and classification-based approaches. In
this review, we look across a range of PdM methodologies, while focusing more
strongly on the comparative use of classification and regression methods in
prognostics. While regression-based methods typically provide estimates of RUL,
classification-based methods present a forecast of the probability of failure
across defined time intervals. Through a comprehensive analysis of recent
literature, we highlight key advancements, challenges-such as data imbalance
and high-dimensional feature spaces-and emerging trends, including hybrid
approaches and AI-enabled prognostic systems. This review aims to provide
researchers and practitioners with an awareness of the strengths and
compromises of various PdM methods and to help identify future research and
build more robust, directed adaptive maintenance systems. Future work may
include a systematic review of practical aspects such as public datasets,
benchmarking platforms, and open-source tools to support the advancement of PdM
research.

</details>


### [53] [MEL: Multi-level Ensemble Learning for Resource-Constrained Environments](https://arxiv.org/abs/2506.20094)
*Krishna Praneet Gudipaty,Walid A. Hanafy,Kaan Ozkara,Qianlin Liang,Jesse Milzman,Prashant Shenoy,Suhas Diggavi*

Main category: cs.LG

TL;DR: MEL is a framework for resilient edge inference using lightweight backup models that work collaboratively or independently, maintaining accuracy and fault tolerance.


<details>
  <summary>Details</summary>
Motivation: Edge environments are resource-constrained and prone to failures, requiring resilient inference without compromising latency or accuracy.

Method: Multi-Level Ensemble Learning (MEL) trains multiple lightweight models collaboratively, formulated as a multi-objective optimization problem to encourage diversity and standalone performance.

Result: MEL achieves performance comparable to original models, with a 40% size reduction, and retains 95.6% accuracy during failures.

Conclusion: MEL provides a flexible, fault-tolerant solution for edge inference, balancing accuracy and resilience.

Abstract: AI inference at the edge is becoming increasingly common for low-latency
services. However, edge environments are power- and resource-constrained, and
susceptible to failures. Conventional failure resilience approaches, such as
cloud failover or compressed backups, often compromise latency or accuracy,
limiting their effectiveness for critical edge inference services. In this
paper, we propose Multi-Level Ensemble Learning (MEL), a new framework for
resilient edge inference that simultaneously trains multiple lightweight backup
models capable of operating collaboratively, refining each other when multiple
servers are available, and independently under failures while maintaining good
accuracy. Specifically, we formulate our approach as a multi-objective
optimization problem with a loss formulation that inherently encourages
diversity among individual models to promote mutually refining representations,
while ensuring each model maintains good standalone performance. Empirical
evaluations across vision, language, and audio datasets show that MEL provides
performance comparable to original architectures while also providing fault
tolerance and deployment flexibility across edge platforms. Our results show
that our ensemble model, sized at 40\% of the original model, achieves similar
performance, while preserving 95.6\% of ensemble accuracy in the case of
failures when trained using MEL.

</details>


### [54] [High-Resolution Live Fuel Moisture Content (LFMC) Maps for Wildfire Risk from Multimodal Earth Observation Data](https://arxiv.org/abs/2506.20132)
*Patrick Alan Johnson,Gabriel Tseng,Yawen Zhang,Heather Heward,Virginia Sjahli,Favyen Bastani,Joseph Redmon,Patrick Beukema*

Main category: cs.LG

TL;DR: A pretrained multimodal earth-observation model improves Live Fuel Moisture Content (LFMC) mapping for wildfire risk, reducing RMSE by 20% and enabling rapid, large-scale updates.


<details>
  <summary>Details</summary>
Motivation: Wildfires are intensifying, and ground-based LFMC sampling is costly and sparse, necessitating better monitoring tools.

Method: Uses a pretrained multimodal earth-observation model to generate spatially complete LFMC maps, outperforming randomly initialized models.

Result: Achieves a 20% reduction in RMSE and demonstrates effectiveness in wildfire-impacted regions.

Conclusion: The automated pipeline provides scalable, accurate LFMC mapping for improved wildfire risk assessment and response.

Abstract: Wildfires are increasing in intensity and severity at an alarming rate.
Recent advances in AI and publicly available satellite data enable monitoring
critical wildfire risk factors globally, at high resolution and low latency.
Live Fuel Moisture Content (LFMC) is a critical wildfire risk factor and is
valuable for both wildfire research and operational response. However,
ground-based LFMC samples are both labor intensive and costly to acquire,
resulting in sparse and infrequent updates. In this work, we explore the use of
a pretrained, highly-multimodal earth-observation model for generating
large-scale spatially complete (wall-to-wall) LFMC maps. Our approach achieves
significant improvements over previous methods using randomly initialized
models (20 reduction in RMSE). We provide an automated pipeline that enables
rapid generation of these LFMC maps across the United States, and demonstrate
its effectiveness in two regions recently impacted by wildfire (Eaton and
Palisades).

</details>


### [55] [Causal discovery in deterministic discrete LTI-DAE systems](https://arxiv.org/abs/2506.20169)
*Bala Rajesh Konkathi,Arun K. Tangirala*

Main category: cs.LG

TL;DR: A method called Partition of Variables (PoV) is proposed for causal discovery in LTI-DAE systems, outperforming a prior DIPCA-based method by handling both dynamical and algebraic relations.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing methods in handling dynamical systems with algebraic relations (DAE systems), which are common in feedback-controlled or coupled systems.

Method: PoV uses DIPCA to identify algebraic and dynamical relations, then partitions variables via constraint matrix analysis to find causal drivers.

Result: PoV successfully identifies causal drivers in LTI-DAE systems, even in pure dynamical systems, outperforming the prior method.

Conclusion: PoV is a superior method for causal discovery in LTI-DAE systems, applicable to a broader range of scenarios.

Abstract: Discovering pure causes or driver variables in deterministic LTI systems is
of vital importance in the data-driven reconstruction of causal networks. A
recent work by Kathari and Tangirala, proposed in 2022, formulated the causal
discovery method as a constraint identification problem. The constraints are
identified using a dynamic iterative PCA (DIPCA)-based approach for dynamical
systems corrupted with Gaussian measurement errors. The DIPCA-based method
works efficiently for dynamical systems devoid of any algebraic relations.
However, several dynamical systems operate under feedback control and/or are
coupled with conservation laws, leading to differential-algebraic (DAE) or
mixed causal systems. In this work, a method, namely the partition of variables
(PoV), for causal discovery in LTI-DAE systems is proposed. This method is
superior to the method that was presented by Kathari and Tangirala (2022), as
PoV also works for pure dynamical systems, which are devoid of algebraic
equations. The proposed method identifies the causal drivers up to a minimal
subset. PoV deploys DIPCA to first determine the number of algebraic relations
($n_a$), the number of dynamical relations ($n_d$) and the constraint matrix.
Subsequently, the subsets are identified through an admissible partitioning of
the constraint matrix by finding the condition number of it. Case studies are
presented to demonstrate the effectiveness of the proposed method.

</details>


### [56] [Causal Operator Discovery in Partial Differential Equations via Counterfactual Physics-Informed Neural Networks](https://arxiv.org/abs/2506.20181)
*Ronald Katende*

Main category: cs.LG

TL;DR: A framework for discovering causal structure in PDEs using physics-informed neural networks and counterfactual perturbations, outperforming traditional methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To address limitations of classical methods like residual minimization or sparse regression in identifying causal structure in PDEs, by leveraging neural networks and functional interventions.

Method: Uses physics-informed neural networks and counterfactual perturbations, introducing causal sensitivity indices and structural deviation metrics to assess operator influence.

Result: Theoretically proven exact recovery of causal operator support under certain conditions; empirically validated on synthetic and real-world datasets, outperforming standard methods.

Conclusion: The framework makes causal PDE discovery tractable and interpretable, grounded in structural causal models and variational residual analysis.

Abstract: We develop a principled framework for discovering causal structure in partial
differential equations (PDEs) using physics-informed neural networks and
counterfactual perturbations. Unlike classical residual minimization or sparse
regression methods, our approach quantifies operator-level necessity through
functional interventions on the governing dynamics. We introduce causal
sensitivity indices and structural deviation metrics to assess the influence of
candidate differential operators within neural surrogates. Theoretically, we
prove exact recovery of the causal operator support under restricted isometry
or mutual coherence conditions, with residual bounds guaranteeing
identifiability. Empirically, we validate the framework on both synthetic and
real-world datasets across climate dynamics, tumor diffusion, and ocean flows.
Our method consistently recovers governing operators even under noise,
redundancy, and data scarcity, outperforming standard PINNs and DeepONets in
structural fidelity. This work positions causal PDE discovery as a tractable
and interpretable inference task grounded in structural causal models and
variational residual analysis.

</details>


### [57] [DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in LLMs](https://arxiv.org/abs/2506.20194)
*Ruokai Yin,Yuhang Li,Donghyun Lee,Priyadarshini Panda*

Main category: cs.LG

TL;DR: DuoGPT combines unstructured weight pruning and activation sparsity to reduce LLM deployment costs while maintaining accuracy, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: High memory and compute costs of LLMs make deployment challenging, and existing pruning methods often ignore runtime activation sparsity.

Method: DuoGPT integrates unstructured weight pruning with activation sparsity, using activation-aware calibration and dense model residuals for accuracy. It optimizes for GPU execution.

Result: DuoGPT achieves up to 9.17% higher accuracy than structured pruning methods at a 1.39× speedup over dense models.

Conclusion: DuoGPT effectively balances efficiency and accuracy, making LLM deployment more feasible.

Abstract: Large language models (LLMs) deliver strong performance but are difficult to
deploy due to high memory and compute costs. While pruning reduces these
demands, most methods ignore activation sparsity observed at runtime. We
reinterpret activation sparsity as dynamic structured weight sparsity and
propose DuoGPT, a unified framework that constructs dual-sparse (spMspV)
workloads by combining unstructured weight pruning with activation sparsity. To
preserve accuracy, we extend the Optimal Brain Compression (OBC) framework with
activation-aware calibration and introduce output residuals from the dense
model as correction terms. We further optimize the solution for efficient GPU
execution, enabling scalability to billion-parameter LLMs. Evaluations on
LLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured
pruning methods by up to 9.17% accuracy at an iso-speedup of 1.39$\times$
compared to the baseline dense model.

</details>


### [58] [Zero-Shot Attribution for Large Language Models: A Distribution Testing Approach](https://arxiv.org/abs/2506.20197)
*Clément L. Canonne,Yash Pote,Uddalok Sarkar*

Main category: cs.LG

TL;DR: The paper introduces Anubis, a zero-shot tool for attributing code generated by LLMs using hypothesis testing and density estimates, achieving high accuracy with minimal samples.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of attributing code generated by LLMs, leveraging hypothesis testing and density estimates for reliable attribution.

Method: Proposes Anubis, a tool that frames attribution as a distribution testing problem, using samples and density estimates from LLMs.

Result: Anubis achieves AUROC scores ≥0.9 in distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and Stable-Code with ≈2000 samples.

Conclusion: Anubis provides an effective solution for code attribution in LLMs, demonstrating high accuracy with limited data.

Abstract: A growing fraction of all code is sampled from Large Language Models (LLMs).
We investigate the problem of attributing code generated by language models
using hypothesis testing to leverage established techniques and guarantees.
Given a set of samples $S$ and a suspect model $\mathcal{L}^*$, our goal is to
assess the likelihood of $S$ originating from $\mathcal{L}^*$. Due to the curse
of dimensionality, this is intractable when only samples from the LLM are
given: to circumvent this, we use both samples and density estimates from the
LLM, a form of access commonly available.
  We introduce $\mathsf{Anubis}$, a zero-shot attribution tool that frames
attribution as a distribution testing problem. Our experiments on a benchmark
of code samples show that $\mathsf{Anubis}$ achieves high AUROC scores (
$\ge0.9$) when distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and
Stable-Code using only $\approx 2000$ samples.

</details>


### [59] [Affective Priming Score: A Data-Driven Method to Detect Priming in Sequential Datasets](https://arxiv.org/abs/2506.20204)
*Eduardo Gutierrez Maestro,Hadi Banaee,Amy Loutfi*

Main category: cs.LG

TL;DR: The paper introduces the Affective Priming Score (APS) to detect priming effects in physiological data, reducing misclassification in models by using priming-free sequences.


<details>
  <summary>Details</summary>
Motivation: Addressing the underexplored impact of priming on physiological data in affective computing, which can cause misclassifications in learning models.

Method: Proposes APS, a data-driven method to score priming effects, validated on SEED and SEED-VII datasets by comparing original and priming-free data.

Result: Misclassification rates drop significantly when using priming-free sequences, demonstrating APS's effectiveness.

Conclusion: APS mitigates priming effects at the data level, improving model robustness and informing better dataset design in affective computing.

Abstract: Affective priming exemplifies the challenge of ambiguity in affective
computing. While the community has largely addressed this issue from a
label-based perspective, identifying data points in the sequence affected by
the priming effect, the impact of priming on data itself, particularly in
physiological signals, remains underexplored. Data affected by priming can lead
to misclassifications when used in learning models. This study proposes the
Affective Priming Score (APS), a data-driven method to detect data points
influenced by the priming effect. The APS assigns a score to each data point,
quantifying the extent to which it is affected by priming. To validate this
method, we apply it to the SEED and SEED-VII datasets, which contain sufficient
transitions between emotional events to exhibit priming effects. We train
models with the same configuration using both the original data and
priming-free sequences. The misclassification rate is significantly reduced
when using priming-free sequences compared to the original data. This work
contributes to the broader challenge of ambiguity by identifying and mitigating
priming effects at the data level, enhancing model robustness, and offering
valuable insights for the design and collection of affective computing
datasets.

</details>


### [60] [Directed Link Prediction using GNN with Local and Global Feature Fusion](https://arxiv.org/abs/2506.20235)
*Yuyang Zhang,Xu Shen,Yu Xie,Ka-Chun Wong,Weidun Xie,Chengbin Peng*

Main category: cs.LG

TL;DR: A novel GNN framework combines feature embedding and community information for improved directed link prediction, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Enhance directed link prediction by integrating feature embedding with community information, addressing limitations of existing deep learning approaches.

Method: Proposes a GNN framework to fuse feature embedding and community data, and transforms input graphs into directed line graphs for better information aggregation.

Result: Outperforms state-of-the-art methods on benchmark datasets with varying training data percentages (30%-60%).

Conclusion: The hybrid feature approach and directed line graph transformation significantly improve directed link prediction performance.

Abstract: Link prediction is a classical problem in graph analysis with many practical
applications. For directed graphs, recently developed deep learning approaches
typically analyze node similarities through contrastive learning and aggregate
neighborhood information through graph convolutions. In this work, we propose a
novel graph neural network (GNN) framework to fuse feature embedding with
community information. We theoretically demonstrate that such hybrid features
can improve the performance of directed link prediction. To utilize such
features efficiently, we also propose an approach to transform input graphs
into directed line graphs so that nodes in the transformed graph can aggregate
more information during graph convolutions. Experiments on benchmark datasets
show that our approach outperforms the state-of-the-art in most cases when 30%,
40%, 50%, and 60% of the connected links are used as training data,
respectively.

</details>


### [61] [FedBKD: Distilled Federated Learning to Embrace Gerneralization and Personalization on Non-IID Data](https://arxiv.org/abs/2506.20245)
*Yushan Zhao,Jinyuan He,Donglai Chen,Weijie Luo,Chong Xie,Ri Zhang,Yonghong Chen,Yan Xu*

Main category: cs.LG

TL;DR: FedBKD is a novel data-free distillation framework for federated learning, addressing non-IID data challenges by using GANs for synthetic data and bidirectional knowledge distillation to improve both global and local models.


<details>
  <summary>Details</summary>
Motivation: To solve the challenges of non-IID data in federated learning without relying on public datasets, which risk data leakage, while improving both global and local model performance.

Method: Uses GANs to generate synthetic data, with local models as frozen discriminators, and employs bidirectional distillation between global and local models for knowledge sharing.

Result: Achieves state-of-the-art performance on 4 benchmarks under various non-IID settings.

Conclusion: FedBKD effectively balances generalization and personalization in FL, outperforming existing solutions without compromising data privacy.

Abstract: Federated learning (FL) is a decentralized collaborative machine learning
(ML) technique. It provides a solution to the issues of isolated data islands
and data privacy leakage in industrial ML practices. One major challenge in FL
is handling the non-identical and independent distributed (non-IID) data.
Current solutions either focus on constructing an all-powerful global model, or
customizing personalized local models. Few of them can provide both a
well-generalized global model and well-performed local models at the same time.
Additionally, many FL solutions to the non-IID problem are benefited from
introducing public datasets. However, this will also increase the risk of data
leakage. To tackle the problems, we propose a novel data-free distillation
framework, Federated Bidirectional Knowledge Distillation (FedBKD).
Specifically, we train Generative Adversarial Networks (GAN) for synthetic
data. During the GAN training, local models serve as discriminators and their
parameters are frozen. The synthetic data is then used for bidirectional
distillation between global and local models to achieve knowledge interactions
so that performances for both sides are improved. We conduct extensive
experiments on 4 benchmarks under different non-IID settings. The results show
that FedBKD achieves SOTA performances in every case.

</details>


### [62] [Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models](https://arxiv.org/abs/2506.20251)
*Kejia Chen,Jiawen Zhang,Jiacong Hu,Yu Wang,Jian Lou,Zunlei Feng,Mingli Song*

Main category: cs.LG

TL;DR: The paper evaluates safety risks in quantized LLMs and introduces Q-resafe, a framework to restore safety without compromising utility.


<details>
  <summary>Details</summary>
Motivation: Quantization of LLMs for resource-constrained environments may compromise safety, necessitating systematic evaluation and mitigation.

Method: Comprehensive safety evaluations of quantization techniques and calibration datasets, followed by the Q-resafe framework for safety restoration.

Result: Q-resafe effectively re-aligns safety of quantized LLMs with pre-quantization levels, even in challenging scenarios.

Conclusion: The proposed Q-resafe framework successfully mitigates safety vulnerabilities in quantized LLMs while preserving utility.

Abstract: Quantized large language models (LLMs) have gained increasing attention and
significance for enabling deployment in resource-constrained environments.
However, emerging studies on a few calibration dataset-free quantization
methods suggest that quantization may compromise the safety capabilities of
LLMs, underscoring the urgent need for systematic safety evaluations and
effective mitigation strategies. In this paper, we present comprehensive safety
evaluations across various mainstream quantization techniques and diverse
calibration datasets, utilizing widely accepted safety benchmarks. To address
the identified safety vulnerabilities, we propose a quantization-aware safety
patching framework, Q-resafe, to efficiently restore the safety capabilities of
quantized LLMs while minimizing any adverse impact on utility. Extensive
experimental results demonstrate that Q-resafe successfully re-aligns the
safety of quantized LLMs with their pre-quantization counterparts, even under
challenging evaluation scenarios. Project page is available at:
https://github.com/Thecommonirin/Qresafe.

</details>


### [63] [Counterfactual Influence as a Distributional Quantity](https://arxiv.org/abs/2506.20481)
*Matthieu Meeus,Igor Shilov,Georgios Kaissis,Yves-Alexandre de Montjoye*

Main category: cs.LG

TL;DR: The paper explores memorization in machine learning models beyond self-influence, emphasizing the role of near-duplicates and the full influence distribution.


<details>
  <summary>Details</summary>
Motivation: To address privacy and generalization concerns by understanding how memorization is influenced by all training samples, not just self-influence.

Method: Analyzes counterfactual influence as a distributional quantity, computing full influence distributions for a small language model and image classification (CIFAR-10).

Result: Self-influence underestimates memorization risks; near-duplicates reduce self-influence but remain extractable. Influence distributions reveal near-duplicates in CIFAR-10.

Conclusion: Memorization is better understood through the full influence distribution, highlighting complex interactions in training data.

Abstract: Machine learning models are known to memorize samples from their training
data, raising concerns around privacy and generalization. Counterfactual
self-influence is a popular metric to study memorization, quantifying how the
model's prediction for a sample changes depending on the sample's inclusion in
the training dataset. However, recent work has shown memorization to be
affected by factors beyond self-influence, with other training samples, in
particular (near-)duplicates, having a large impact. We here study memorization
treating counterfactual influence as a distributional quantity, taking into
account how all training samples influence how a sample is memorized. For a
small language model, we compute the full influence distribution of training
samples on each other and analyze its properties. We find that solely looking
at self-influence can severely underestimate tangible risks associated with
memorization: the presence of (near-)duplicates seriously reduces
self-influence, while we find these samples to be (near-)extractable. We
observe similar patterns for image classification, where simply looking at the
influence distributions reveals the presence of near-duplicates in CIFAR-10.
Our findings highlight that memorization stems from complex interactions across
training data and is better captured by the full influence distribution than by
self-influence alone.

</details>


### [64] [Time-series surrogates from energy consumers generated by machine learning approaches for long-term forecasting scenarios](https://arxiv.org/abs/2506.20253)
*Ben Gerhards,Nikita Popkov,Annekatrin König,Marcel Arpogaus,Bastian Schäfermeier,Leonie Riedl,Stephan Vogt,Philip Hehlert*

Main category: cs.LG

TL;DR: The paper evaluates data-driven methods for generating synthetic long-term energy consumption time series, comparing WGAN, DDPM, HMM, and MABF for accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: Long-term forecasting of individual power consumption is understudied, and high-fidelity synthetic data is needed for energy system applications.

Method: Comparative evaluation of WGAN, DDPM, HMM, and MABF using an open-source dataset from German households with 15min resolution.

Result: The study highlights strengths and limitations of each method for replicating temporal dynamics and long-range dependencies.

Conclusion: The framework aids in selecting suitable methods for synthetic data generation, balancing accuracy, privacy, and application needs.

Abstract: Forecasting attracts a lot of research attention in the electricity value
chain. However, most studies concentrate on short-term forecasting of
generation or consumption with a focus on systems and less on individual
consumers. Even more neglected is the topic of long-term forecasting of
individual power consumption.
  Here, we provide an in-depth comparative evaluation of data-driven methods
for generating synthetic time series data tailored to energy consumption
long-term forecasting. High-fidelity synthetic data is crucial for a wide range
of applications, including state estimations in energy systems or power grid
planning. In this study, we assess and compare the performance of multiple
state-of-the-art but less common techniques: a hybrid Wasserstein Generative
Adversarial Network (WGAN), Denoising Diffusion Probabilistic Model (DDPM),
Hidden Markov Model (HMM), and Masked Autoregressive Bernstein polynomial
normalizing Flows (MABF). We analyze the ability of each method to replicate
the temporal dynamics, long-range dependencies, and probabilistic transitions
characteristic of individual energy consumption profiles. Our comparative
evaluation highlights the strengths and limitations of: WGAN, DDPM, HMM and
MABF aiding in selecting the most suitable approach for state estimations and
other energy-related tasks. Our generation and analysis framework aims to
enhance the accuracy and reliability of synthetic power consumption data while
generating data that fulfills criteria like anonymisation - preserving privacy
concerns mitigating risks of specific profiling of single customers. This study
utilizes an open-source dataset from households in Germany with 15min time
resolution. The generated synthetic power profiles can readily be used in
applications like state estimations or consumption forecasting.

</details>


### [65] [Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards](https://arxiv.org/abs/2506.20520)
*Charles Arnal,Gaëtan Narozniak,Vivien Cabannes,Yunhao Tang,Julia Kempe,Remi Munos*

Main category: cs.LG

TL;DR: The paper explores an off-policy REINFORCE algorithm for aligning large language models (LLMs), balancing between off-policy RL and supervised fine-tuning. It shows that adjusting the baseline V can optimize performance, with theoretical and experimental validation.


<details>
  <summary>Details</summary>
Motivation: To address the suboptimal performance of off-policy RL methods in aligning LLMs, the study investigates a simple off-policy REINFORCE algorithm to bridge the gap between off-policy RL and supervised fine-tuning.

Method: The paper analyzes an off-policy REINFORCE algorithm with a tunable baseline V (A = r - V). It provides theoretical guarantees when V lower-bounds the expected reward and validates findings in a bandit setting and LLM fine-tuning.

Result: Theoretical analysis shows policy improvement when V lower-bounds expected reward. Experiments confirm that off-policy updates benefit more from positive rewards than negative ones.

Conclusion: The study demonstrates that adjusting the baseline V in off-policy REINFORCE can optimize LLM alignment, with practical benefits validated in experiments.

Abstract: Reinforcement learning (RL) is increasingly used to align large language
models (LLMs). Off-policy methods offer greater implementation simplicity and
data efficiency than on-policy techniques, but often result in suboptimal
performance. In this work, we study the intermediate range of algorithms
between off-policy RL and supervised fine-tuning by analyzing a simple
off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with
$r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$
emphasizes high-reward samples, while raising it penalizes low-reward ones more
heavily. We first provide a theoretical analysis of this off-policy REINFORCE
algorithm, showing that when the baseline $V$ lower-bounds the expected reward,
the algorithm enjoys a policy improvement guarantee. Our analysis reveals that
while on-policy updates can safely leverage both positive and negative signals,
off-policy updates benefit from focusing more on positive rewards than on
negative ones. We validate our findings experimentally in a controlled
stochastic bandit setting and through fine-tuning state-of-the-art LLMs on
reasoning tasks.

</details>


### [66] [Argumentative Ensembling for Robust Recourse under Model Multiplicity](https://arxiv.org/abs/2506.20260)
*Junqi Jiang,Antonio Rago,Francesco Leofante,Francesca Toni*

Main category: cs.LG

TL;DR: The paper addresses the challenge of providing robust counterfactual explanations (CEs) under model multiplicity (MM) in machine learning, proposing recourse-aware ensembling (RAE) with argumentative ensembling to ensure CE robustness.


<details>
  <summary>Details</summary>
Motivation: Model multiplicity complicates recourse recommendations via CEs, as CEs may not be valid across all competing models.

Method: Introduces RAE with a novel argumentative ensembling method, leveraging computational argumentation to resolve conflicts between models and CEs.

Result: The method guarantees CE robustness under MM, supports model preferences, and satisfies six desirable properties. Theoretical and empirical validation is provided.

Conclusion: Argumentative ensembling effectively addresses the problem of recourse under MM, offering customizable and robust solutions.

Abstract: In machine learning, it is common to obtain multiple equally performing
models for the same prediction task, e.g., when training neural networks with
different random seeds. Model multiplicity (MM) is the situation which arises
when these competing models differ in their predictions for the same input, for
which ensembling is often employed to determine an aggregation of the outputs.
Providing recourse recommendations via counterfactual explanations (CEs) under
MM thus becomes complex, since the CE may not be valid across all models, i.e.,
the CEs are not robust under MM. In this work, we formalise the problem of
providing recourse under MM, which we name recourse-aware ensembling (RAE). We
propose the idea that under MM, CEs for each individual model should be
considered alongside their predictions so that the aggregated prediction and
recourse are decided in tandem. Centred around this intuition, we introduce six
desirable properties for solutions to this problem. For solving RAE, we propose
a novel argumentative ensembling method which guarantees the robustness of CEs
under MM. Specifically, our method leverages computational argumentation to
explicitly represent the conflicts between models and counterfactuals regarding
prediction results and CE validity. It then uses argumentation semantics to
resolve the conflicts and obtain the final solution, in a manner which is
parametric to the chosen semantics. Our method also allows for the
specification of preferences over the models under MM, allowing further
customisation of the ensemble. In a comprehensive theoretical analysis, we
characterise the behaviour of argumentative ensembling with four different
argumentation semantics. We then empirically demonstrate the effectiveness of
our approach in satisfying desirable properties with eight instantiations of
our method. (Abstract is shortened for arXiv.)

</details>


### [67] [PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models](https://arxiv.org/abs/2506.20629)
*Soufiane Hayou,Nikhil Ghosh,Bin Yu*

Main category: cs.LG

TL;DR: PLoP (Precise LoRA Placement) is a lightweight method for automatically identifying optimal LoRA adapter placements in pretrained models, outperforming common strategies.


<details>
  <summary>Details</summary>
Motivation: Existing LoRA adapter placement strategies lack consensus and may not be optimal for all tasks. PLoP aims to automate and improve this process.

Method: PLoP uses intuitive theoretical analysis to determine the best module types (e.g., attention or MLP) for LoRA adapter placement, given a model and task.

Result: PLoP consistently outperforms or matches common placement strategies in supervised finetuning and reinforcement learning for reasoning tasks.

Conclusion: PLoP provides an efficient, automated solution for LoRA adapter placement, enhancing task-specific finetuning performance.

Abstract: Low-Rank Adaptation (LoRA) is a widely used finetuning method for large
models. Its small memory footprint allows practitioners to adapt large models
to specific tasks at a fraction of the cost of full finetuning. Different
modifications have been proposed to enhance its efficiency by, for example,
setting the learning rate, the rank, and the initialization. Another
improvement axis is adapter placement strategy: when using LoRA, practitioners
usually pick module types to adapt with LoRA, such as Query and Key modules.
Few works have studied the problem of adapter placement, with nonconclusive
results: original LoRA paper suggested placing adapters in attention modules,
while other works suggested placing them in the MLP modules. Through an
intuitive theoretical analysis, we introduce PLoP (Precise LoRA Placement), a
lightweight method that allows automatic identification of module types where
LoRA adapters should be placed, given a pretrained model and a finetuning task.
We demonstrate that PLoP consistently outperforms, and in the worst case
competes, with commonly used placement strategies through comprehensive
experiments on supervised finetuning and reinforcement learning for reasoning.

</details>


### [68] [Distilling A Universal Expert from Clustered Federated Learning](https://arxiv.org/abs/2506.20285)
*Zeqi Leng,Chunxu Zhang,Guodong Long,Riting Xia,Bo Yang*

Main category: cs.LG

TL;DR: The paper introduces a novel Federated Learning (FL) framework that distills a universal expert model from clustered federated learning (CFL) to capture shared knowledge across clients, improving performance by balancing personalized and shared knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing CFL methods often ignore shared information across clusters, which is valuable for generalizable knowledge in FL systems.

Method: The framework involves three iterative steps: local model training, cluster-specific model aggregation, and universal expert distillation.

Result: The proposed method outperforms traditional gradient-based aggregation, handling model heterogeneity better and reducing conflicts among cluster-specific experts.

Conclusion: The framework advances CFL by effectively balancing personalized and shared knowledge, demonstrating superior performance in various scenarios.

Abstract: Clustered Federated Learning (CFL) addresses the challenges posed by non-IID
data by training multiple group- or cluster-specific expert models. However,
existing methods often overlook the shared information across clusters, which
represents the generalizable knowledge valuable to all participants in the
Federated Learning (FL) system. To overcome this limitation, this paper
introduces a novel FL framework that distills a universal expert model from the
knowledge of multiple clusters. This universal expert captures globally shared
information across all clients and is subsequently distributed to each client
as the initialization for the next round of model training. The proposed FL
framework operates in three iterative steps: (1) local model training at each
client, (2) cluster-specific model aggregation, and (3) universal expert
distillation. This three-step learning paradigm ensures the preservation of
fine-grained non-IID characteristics while effectively incorporating shared
knowledge across clusters. Compared to traditional gradient-based aggregation
methods, the distillation-based model aggregation introduces greater
flexibility in handling model heterogeneity and reduces conflicts among
cluster-specific experts. Extensive experimental results demonstrate the
superior performance of the proposed method across various scenarios,
highlighting its potential to advance the state of CFL by balancing
personalized and shared knowledge more effectively.

</details>


### [69] [Learning Moderately Input-Sensitive Functions: A Case Study in QR Code Decoding](https://arxiv.org/abs/2506.20305)
*Kazuki Yoda,Kazuhiko Kawamoto,Hiroshi Kera*

Main category: cs.LG

TL;DR: The paper explores learning-based QR code decoding using Transformers, showing they can decode beyond error-correction limits by focusing on data bits, not error-correction bits.


<details>
  <summary>Details</summary>
Motivation: To investigate learning functions of medium sensitivity, bridging input-insensitive tasks (e.g., image classification) and highly input-sensitive tasks (e.g., arithmetic).

Method: Uses Transformers to decode QR codes, testing their ability to learn and generalize from English-rich data to other languages and random strings.

Result: Transformers successfully decode QR codes beyond theoretical limits, focusing on data bits and ignoring error-correction bits, demonstrating a unique decoding mechanism.

Conclusion: Transformers can effectively learn medium-sensitivity tasks like QR decoding, offering insights into their adaptability and potential for similar applications.

Abstract: The hardness of learning a function that attains a target task relates to its
input-sensitivity. For example, image classification tasks are
input-insensitive as minor corruptions should not affect the classification
results, whereas arithmetic and symbolic computation, which have been recently
attracting interest, are highly input-sensitive as each input variable connects
to the computation results. This study presents the first learning-based Quick
Response (QR) code decoding and investigates learning functions of medium
sensitivity. Our experiments reveal that Transformers can successfully decode
QR codes, even beyond the theoretical error-correction limit, by learning the
structure of embedded texts. They generalize from English-rich training data to
other languages and even random strings. Moreover, we observe that the
Transformer-based QR decoder focuses on data bits while ignoring
error-correction bits, suggesting a decoding mechanism distinct from standard
QR code readers.

</details>


### [70] [Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration](https://arxiv.org/abs/2506.20307)
*Heyang Zhao,Xingrui Yu,David M. Bossens,Ivor W. Tsang,Quanquan Gu*

Main category: cs.LG

TL;DR: ILDE is a novel imitation learning algorithm that uses double exploration (optimistic policy optimization and curiosity-driven exploration) to improve convergence and achieve beyond-expert performance with fewer demonstrations.


<details>
  <summary>Details</summary>
Motivation: Challenges in accurately learning expert policies from limited demonstrations and the need for exploration to achieve beyond-expert performance.

Method: ILDE combines optimistic policy optimization (rewarding uncertain state-action pairs) and curiosity-driven exploration (exploring states deviating from demonstrations).

Result: ILDE outperforms state-of-the-art methods in sample efficiency and achieves beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations.

Conclusion: ILDE is theoretically justified as an uncertainty-regularized policy optimization method with sublinear regret growth.

Abstract: Imitation learning is a central problem in reinforcement learning where the
goal is to learn a policy that mimics the expert's behavior. In practice, it is
often challenging to learn the expert policy from a limited number of
demonstrations accurately due to the complexity of the state space. Moreover,
it is essential to explore the environment and collect data to achieve
beyond-expert performance. To overcome these challenges, we propose a novel
imitation learning algorithm called Imitation Learning with Double Exploration
(ILDE), which implements exploration in two aspects: (1) optimistic policy
optimization via an exploration bonus that rewards state-action pairs with high
uncertainty to potentially improve the convergence to the expert policy, and
(2) curiosity-driven exploration of the states that deviate from the
demonstration trajectories to potentially yield beyond-expert performance.
Empirically, we demonstrate that ILDE outperforms the state-of-the-art
imitation learning algorithms in terms of sample efficiency and achieves
beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations
than in previous work. We also provide a theoretical justification of ILDE as
an uncertainty-regularized policy optimization method with optimistic
exploration, leading to a regret growing sublinearly in the number of episodes.

</details>


### [71] [Comparative Analysis of Deep Learning Models for Crop Disease Detection: A Transfer Learning Approach](https://arxiv.org/abs/2506.20323)
*Saundarya Subramaniam,Shalini Majumdar,Shantanu Nadar,Kaustubh Kulkarni*

Main category: cs.LG

TL;DR: AI-driven crop disease detection system using deep learning models (EfficientNet, ResNet101, MobileNetV2, custom CNN) achieves 95.76% validation accuracy, aiding rural farmers.


<details>
  <summary>Details</summary>
Motivation: Assist farmers in rural areas with limited resources by improving crop disease detection and management.

Method: Comparative analysis of deep learning models (EfficientNet, ResNet101, MobileNetV2, custom CNN) for transfer learning efficacy.

Result: Custom CNN achieved 95.76% validation accuracy, demonstrating effective disease classification.

Conclusion: Transfer learning can reshape agricultural practices, enhance crop health, and support sustainable farming in rural areas.

Abstract: This research presents the development of an Artificial Intelligence (AI) -
driven crop disease detection system designed to assist farmers in rural areas
with limited resources. We aim to compare different deep learning models for a
comparative analysis, focusing on their efficacy in transfer learning. By
leveraging deep learning models, including EfficientNet, ResNet101,
MobileNetV2, and our custom CNN, which achieved a validation accuracy of
95.76%, the system effectively classifies plant diseases. This research
demonstrates the potential of transfer learning in reshaping agricultural
practices, improving crop health management, and supporting sustainable farming
in rural environments.

</details>


### [72] [Permutation Equivariant Neural Controlled Differential Equations for Dynamic Graph Representation Learning](https://arxiv.org/abs/2506.20324)
*Torben Berndt,Benjamin Walker,Tiexin Qin,Jan Stühmer,Andrey Kormilitzin*

Main category: cs.LG

TL;DR: The paper introduces Permutation Equivariant Neural Graph CDEs, a more efficient variant of Graph Neural CDEs, reducing parameters while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address the complexity of dynamic graphs by improving the efficiency and generalizability of Graph Neural CDEs.

Method: Projects Graph Neural CDEs onto permutation equivariant function spaces, reducing parameters without losing representational power.

Result: Empirical tests show improved performance in interpolation and extrapolation tasks.

Conclusion: The proposed method enhances efficiency and generalization in modeling dynamic graphs.

Abstract: Dynamic graphs exhibit complex temporal dynamics due to the interplay between
evolving node features and changing network structures. Recently, Graph Neural
Controlled Differential Equations (Graph Neural CDEs) successfully adapted
Neural CDEs from paths on Euclidean domains to paths on graph domains. Building
on this foundation, we introduce Permutation Equivariant Neural Graph CDEs,
which project Graph Neural CDEs onto permutation equivariant function spaces.
This significantly reduces the model's parameter count without compromising
representational power, resulting in more efficient training and improved
generalisation. We empirically demonstrate the advantages of our approach
through experiments on simulated dynamical systems and real-world tasks,
showing improved performance in both interpolation and extrapolation scenarios.

</details>


### [73] [Producer-Fairness in Sequential Bundle Recommendation](https://arxiv.org/abs/2506.20329)
*Alexandre Rio,Marta Soare,Sihem Amer-Yahia*

Main category: cs.LG

TL;DR: The paper addresses fairness in sequential bundle recommendations, proposing methods to balance item group exposure (producer-fairness) and bundle quality, with real-time solutions and adaptive heuristics.


<details>
  <summary>Details</summary>
Motivation: Real-world scenarios require fair exposure of item groups across users in sequential bundle recommendations while maintaining high-quality bundles.

Method: The paper formalizes producer-fairness, proposes an exact solution for small instances, and examines three heuristics: quality-first, fairness-first, and an adaptive variant balancing fairness and quality.

Result: Experiments on three datasets show the efficacy of the solutions in achieving fair bundle recommendations without compromising quality.

Conclusion: The proposed methods effectively balance fairness and quality in sequential bundle recommendations, with adaptive heuristics offering flexibility.

Abstract: We address fairness in the context of sequential bundle recommendation, where
users are served in turn with sets of relevant and compatible items. Motivated
by real-world scenarios, we formalize producer-fairness, that seeks to achieve
desired exposure of different item groups across users in a recommendation
session. Our formulation combines naturally with building high quality bundles.
Our problem is solved in real time as users arrive. We propose an exact
solution that caters to small instances of our problem. We then examine two
heuristics, quality-first and fairness-first, and an adaptive variant that
determines on-the-fly the right balance between bundle fairness and quality.
Our experiments on three real-world datasets underscore the strengths and
limitations of each solution and demonstrate their efficacy in providing fair
bundle recommendations without compromising bundle quality.

</details>


### [74] [On the ability of Deep Neural Networks to Learn Granger Causality in Multi-Variate Time Series Data](https://arxiv.org/abs/2506.20347)
*Malik Shahid Sultan,Hernando Ombao*

Main category: cs.LG

TL;DR: The paper introduces a novel deep learning approach to Granger Causality (GC) estimation, treating it as a prediction problem rather than variable selection, and shows that well-regularized models can learn true GC structure without explicit sparsity constraints.


<details>
  <summary>Details</summary>
Motivation: Traditional linear VAR models for GC have limited practical applicability due to restrictive assumptions. While DNNs have been used for GC, they treat it as variable selection, missing the predictive essence of GC.

Method: The authors propose modeling time series jointly with a deep learning model and infer GC by comparing model uncertainty or residual distributions when specific components are dropped. They also study input layer dropout's impact.

Result: A well-regularized deep learning model can learn the true GC structure from data without explicit sparsity terms in the loss function.

Conclusion: The paper demonstrates that GC can be effectively learned through prediction-focused deep learning, bypassing traditional variable selection approaches.

Abstract: Granger Causality (GC) offers an elegant statistical framework to study the
association between multivariate time series data. Linear Vector Autoregressive
models (VAR) though have nice interpretation properties but have limited
practical application due to underlying assumptions on the kind of associations
that can be captured by these models. Numerous attempts have already been made
in the literature that exploit the functional approximation power of Deep
Neural Networks (DNNs) for the task of GC estimation. These methods however
treat GC as a variable selection problem. We present a novel paradigm for
approaching GC. We present this idea that GC is essentially linked with
prediction and if a deep learning model is used to model the time series
collectively or jointly, a well regularized model may learn the true granger
causal structure from the data, given that there is enough training data. We
propose to uncover the learned GC structure by comparing the model uncertainty
or distribution of the residuals when the past of everything is used as
compared to the one where a specific time series component is dropped from the
model. We also compare the effect of input layer dropout on the ability of a
neural network to learn granger causality from the data. We show that a well
regularized model infact can learn the true GC structure from the data without
explicitly adding terms in the loss function that guide the model to select
variables or perform sparse regression.

</details>


### [75] [DipSVD: Dual-importance Protected SVD for Efficient LLM Compression](https://arxiv.org/abs/2506.20353)
*Xuan Ding,Rui Sun,Yunjian Zhang,Xiu Yan,Yueqi Zhou,Kaihao Huang,Suzhong Fu,Chuanlong Xie,Yao Zhu*

Main category: cs.LG

TL;DR: DipSVD enhances SVD-based LLM compression by protecting critical matrix components locally and globally, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the oversight of critical component protection in existing SVD-based compression methods, which leads to inferior model performance.

Method: Proposes a dual-level importance protection mechanism: local (channel-weighted data whitening) and global (heuristic or optimization-based layer compression balancing).

Result: DipSVD achieves superior performance, especially at high compression ratios, across multiple benchmarks.

Conclusion: The dual-level protection mechanism effectively improves SVD-based compression, making it more robust for LLM deployment.

Abstract: The ever-increasing computational demands and deployment costs of large
language models (LLMs) have spurred numerous compressing methods. Compared to
quantization and unstructured pruning, SVD compression offers superior hardware
compatibility and theoretical guarantees. However, existing SVD-based methods
focus on the overall discrepancy between the original and compressed matrices
while overlooking the protection of critical components within the matrix,
which leads to inferior performance in the compressed models. This paper
proposes a dual-level importance protection mechanism to enhance SVD-based
compression methods: (1) local importance protection: preserving the most
critical singular vectors within each weight matrix through channel-weighted
data whitening; and (2) global importance protection: enabling less important
layers to bear a greater portion of the compression burden through either a
heuristic or optimization-based approach, thereby minimizing the impact of
compression on critical layers. Extensive experiments demonstrate that DipSVD
outperforms existing SVD-based compression approaches across multiple
benchmarks, achieving superior model performance especially at high model
compression ratios.

</details>


### [76] [A foundation model with multi-variate parallel attention to generate neuronal activity](https://arxiv.org/abs/2506.20354)
*Francesco Carzaniga,Michael Hersche,Abu Sebastian,Kaspar Schindler,Abbas Rahimi*

Main category: cs.LG

TL;DR: The paper introduces MVPA, a self-attention mechanism for heterogeneous time-series data, and MVPFormer, a generative model for iEEG, achieving expert-level seizure detection and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of learning from multi-variate time-series with varying channel configurations, especially in clinical iEEG data.

Method: Proposes MVPA, disentangling content, temporal, and spatial attention, and builds MVPFormer, a generative model trained on the SWEC iEEG dataset.

Result: MVPFormer generalizes well across subjects, excels in seizure detection, and outperforms state-of-the-art models on multiple datasets.

Conclusion: MVPA is a versatile attention mechanism, and MVPFormer is a groundbreaking open-source iEEG model with top-tier clinical performance.

Abstract: Learning from multi-variate time-series with heterogeneous channel
configurations remains a fundamental challenge for deep neural networks (DNNs),
particularly in clinical domains such as intracranial electroencephalography
(iEEG), where channel setups vary widely across subjects. In this work, we
introduce multi-variate parallel attention (MVPA), a novel self-attention
mechanism that disentangles content, temporal, and spatial attention, enabling
flexible, generalizable, and efficient modeling of time-series data with
varying channel counts and configurations. We use MVPA to build MVPFormer, a
generative foundation model for human electrophysiology, trained to predict the
evolution of iEEG signals across diverse subjects. To support this and future
effort by the community, we release the SWEC iEEG dataset, the largest publicly
available iEEG dataset to date, comprising nearly 10,000 hours of recordings
from heterogeneous clinical sources. MVPFormer leverages MVPA to achieve strong
generalization across subjects, demonstrating expert-level performance in
seizure detection and outperforming state-of-the-art Transformer baselines on
our SWEC, the MAYO, and the FNUSA dataset. We further validate MVPA on standard
time-series forecasting and classification tasks, where it matches or exceeds
existing attention-based models. Together, our contributions establish MVPA as
a general-purpose attention mechanism for heterogeneous time-series and
MVPFormer as the first open-source, open-weights, and open-data iEEG foundation
model with state-of-the-art clinical performance. The code is available at
https://github.com/IBM/multi-variate-parallel-transformer. The SWEC iEEG
dataset is available at
https://mb-neuro.medical-blocks.ch/public_access/databases/ieeg/swec_ieeg.

</details>


### [77] [Towards Interpretable and Efficient Feature Selection in Trajectory Datasets: A Taxonomic Approach](https://arxiv.org/abs/2506.20359)
*Chanuka Don Samarasinghage,Dhruv Gulabani*

Main category: cs.LG

TL;DR: The paper introduces a taxonomy-based feature selection method for trajectory analysis to address high-dimensionality issues, improving efficiency, interpretability, and predictive performance.


<details>
  <summary>Details</summary>
Motivation: High-dimensionality in trajectory data reduces model accuracy and interpretability, necessitating effective feature selection methods.

Method: A taxonomy-based approach categorizes features into geometric (curvature, indentation) and kinematic (speed, acceleration) groups.

Result: The method achieves comparable or superior predictive performance, reduces feature selection time, and enhances interpretability.

Conclusion: Taxonomy-based feature selection improves efficiency, reduces complexity, and supports explainable AI in trajectory analysis.

Abstract: Trajectory analysis is not only about obtaining movement data, but it is also
of paramount importance in understanding the pattern in which an object moves
through space and time, as well as in predicting its next move. Due to the
significant interest in the area, data collection has improved substantially,
resulting in a large number of features becoming available for training and
predicting models. However, this introduces a high-dimensionality-induced
feature explosion problem, which reduces the efficiency and interpretability of
the data, thereby reducing the accuracy of machine learning models. To overcome
this issue, feature selection has become one of the most prevalent tools. Thus,
the objective of this paper was to introduce a taxonomy-based feature selection
method that categorizes features based on their internal structure. This
approach classifies the data into geometric and kinematic features, further
categorizing them into curvature, indentation, speed, and acceleration. The
comparative analysis indicated that a taxonomy-based approach consistently
achieved comparable or superior predictive performance. Furthermore, due to the
taxonomic grouping, which reduces combinatorial space, the time taken to select
features was drastically reduced. The taxonomy was also used to gain insights
into what feature sets each dataset was more sensitive to. Overall, this study
provides robust evidence that a taxonomy-based feature selection method can add
a layer of interpretability, reduce dimensionality and computational
complexity, and contribute to high-level decision-making. It serves as a step
toward providing a methodological framework for researchers and practitioners
dealing with trajectory datasets and contributing to the broader field of
explainable artificial intelligence.

</details>


### [78] [Self-Supervised Graph Learning via Spectral Bootstrapping and Laplacian-Based Augmentations](https://arxiv.org/abs/2506.20362)
*Lorenzo Bini,Stephane Marchand-Maillet*

Main category: cs.LG

TL;DR: LaplaceGNN is a self-supervised graph learning framework that avoids negative sampling by using spectral bootstrapping and Laplacian-based signals, achieving efficient and scalable performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to simplify and improve self-supervised graph learning by eliminating the need for negative sampling and handcrafted augmentations, focusing instead on spectral techniques.

Method: LaplaceGNN integrates Laplacian-based signals and uses spectral augmentations precomputed via max-min centrality-guided optimization. It employs an adversarial bootstrapped training scheme for robust feature learning.

Result: Experiments show LaplaceGNN outperforms state-of-the-art self-supervised graph methods, demonstrating its efficiency and scalability.

Conclusion: LaplaceGNN offers a simpler, more efficient self-supervised alternative for graph neural networks, with strong performance across diverse domains.

Abstract: We present LaplaceGNN, a novel self-supervised graph learning framework that
bypasses the need for negative sampling by leveraging spectral bootstrapping
techniques. Our method integrates Laplacian-based signals into the learning
process, allowing the model to effectively capture rich structural
representations without relying on contrastive objectives or handcrafted
augmentations. By focusing on positive alignment, LaplaceGNN achieves linear
scaling while offering a simpler, more efficient, self-supervised alternative
for graph neural networks, applicable across diverse domains. Our contributions
are twofold: we precompute spectral augmentations through max-min
centrality-guided optimization, enabling rich structural supervision without
relying on handcrafted augmentations, then we integrate an adversarial
bootstrapped training scheme that further strengthens feature learning and
robustness. Our extensive experiments on different benchmark datasets show that
LaplaceGNN achieves superior performance compared to state-of-the-art
self-supervised graph methods, offering a promising direction for efficiently
learning expressive graph representations.

</details>


### [79] [TESSERA: Temporal Embeddings of Surface Spectra for Earth Representation and Analysis](https://arxiv.org/abs/2506.20380)
*Zhengpeng Feng,Sadiq Jaffer,Jovana Knezevic,Silja Sormunen,Robin Young,Madeline Lisaius,Markus Immitzer,James Ball,Clement Atzberger,David A. Coomes,Anil Madhavapeddy,Andrew Blake,Srinivasan Keshav*

Main category: cs.LG

TL;DR: TESSERA is a Remote Sensing Foundation Model (RSFM) using Self-Supervised Learning (SSL) to create global representations at 10m scale from satellite data, outperforming traditional and foundation models in diverse tasks.


<details>
  <summary>Details</summary>
Motivation: To democratize access to high-performance, high-resolution satellite data representations for Earth observation applications like climate modeling and conservation.

Method: Combines optical (Sentinel-2) and SAR (Sentinel-1) data via parallel Transformer encoders, fused with an MLP to generate global representations (2017-2024).

Result: Sets a new state-of-the-art benchmark, outperforming traditional and leading geospatial foundation models in five diverse tasks.

Conclusion: TESSERA's open-source approach and robust performance advance remote sensing applications, making high-resolution representations widely accessible.

Abstract: Satellite remote sensing (RS) enables a wide array of downstream Earth
observation (EO) applications, including climate modeling, carbon accounting,
and strategies for conservation and sustainable land use. We present TESSERA, a
novel Remote Sensing Foundation Model (RSFM) that uses Self-Supervised Learning
(SSL) to generate global, robust representations at 10m scale from pixel-level
satellite time series data. TESSERA combines information from only optical and
SAR data streams using two parallel Transformer-based encoders: one dedicated
to Sentinel-1 SAR polarizations and another to Sentinel-2 MSI data (10 selected
spectral bands) to create representations that are then fused using a
multilayer perceptron (MLP), resulting in a global representation map covering
the years 2017 to 2024. Our precomputed representations set a new
state-of-the-art performance benchmark and our open-source approach
democratizes access to high-performance, high-resolution representations. We
benchmark the performance of TESSERA in five diverse tasks, comparing our work
with state-of-the-art task-specific models and other foundation models. Our
results show that TESSERA outperforms both traditional RS baselines and the
leading geospatial foundation models in these diverse downstream tasks.

</details>


### [80] [Client Clustering Meets Knowledge Sharing: Enhancing Privacy and Robustness in Personalized Peer-to-Peer Learning](https://arxiv.org/abs/2506.20413)
*Mohammad Mahdi Maheri,Denys Herasymuk,Hamed Haddadi*

Main category: cs.LG

TL;DR: P4 is a decentralized, privacy-preserving method for personalized learning in IoT, ensuring robustness against attacks and efficient knowledge transfer.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like privacy, efficiency, and resilience in decentralized AI for IoT.

Method: Uses a lightweight algorithm to detect client similarity, forms groups, and employs differentially private knowledge distillation.

Result: Achieves 5-30% higher accuracy than peers, robust against 30% malicious clients, and adds minimal overhead (~7s).

Conclusion: P4 effectively balances personalization, privacy, and robustness in resource-constrained IoT settings.

Abstract: The growing adoption of Artificial Intelligence (AI) in Internet of Things
(IoT) ecosystems has intensified the need for personalized learning methods
that can operate efficiently and privately across heterogeneous,
resource-constrained devices. However, enabling effective personalized learning
in decentralized settings introduces several challenges, including efficient
knowledge transfer between clients, protection of data privacy, and resilience
against poisoning attacks. In this paper, we address these challenges by
developing P4 (Personalized, Private, Peer-to-Peer) -- a method designed to
deliver personalized models for resource-constrained IoT devices while ensuring
differential privacy and robustness against poisoning attacks. Our solution
employs a lightweight, fully decentralized algorithm to privately detect client
similarity and form collaborative groups. Within each group, clients leverage
differentially private knowledge distillation to co-train their models,
maintaining high accuracy while ensuring robustness to the presence of
malicious clients. We evaluate P4 on popular benchmark datasets using both
linear and CNN-based architectures across various heterogeneity settings and
attack scenarios. Experimental results show that P4 achieves 5% to 30% higher
accuracy than leading differentially private peer-to-peer approaches and
maintains robustness with up to 30% malicious clients. Additionally, we
demonstrate its practicality by deploying it on resource-constrained devices,
where collaborative training between two clients adds only ~7 seconds of
overhead.

</details>


### [81] [Off-Policy Evaluation and Learning for the Future under Non-Stationarity](https://arxiv.org/abs/2506.20417)
*Tatsuhiro Shimizu,Kazuki Kawamura,Takanori Muroi,Yusuke Narita,Kei Tateno,Takuma Udagawa,Yuta Saito*

Main category: cs.LG

TL;DR: The paper introduces a novel estimator, OPFV, for future off-policy evaluation and learning in non-stationary environments, leveraging time-series structures to reduce bias and improve accuracy.


<details>
  <summary>Details</summary>
Motivation: The challenge of estimating and optimizing policy values in non-stationary environments, where future data is unobserved, motivates the need for a method that avoids restrictive assumptions and leverages time-related structures.

Method: The proposed OPFV estimator uses importance weighting to exploit time-series patterns (e.g., seasonality) in historical data for accurate future policy evaluation. It also extends to a policy-gradient method for proactive learning.

Result: Empirical results demonstrate OPFV's superiority over existing methods in estimating and optimizing future policy values under non-stationarity.

Conclusion: OPFV effectively addresses the limitations of existing methods by leveraging time-series structures, offering a robust solution for future off-policy evaluation and learning.

Abstract: We study the novel problem of future off-policy evaluation (F-OPE) and
learning (F-OPL) for estimating and optimizing the future value of policies in
non-stationary environments, where distributions vary over time. In e-commerce
recommendations, for instance, our goal is often to estimate and optimize the
policy value for the upcoming month using data collected by an old policy in
the previous month. A critical challenge is that data related to the future
environment is not observed in the historical data. Existing methods assume
stationarity or depend on restrictive reward-modeling assumptions, leading to
significant bias. To address these limitations, we propose a novel estimator
named \textit{\textbf{O}ff-\textbf{P}olicy Estimator for the \textbf{F}uture
\textbf{V}alue (\textbf{\textit{OPFV}})}, designed for accurately estimating
policy values at any future time point. The key feature of OPFV is its ability
to leverage the useful structure within time-series data. While future data
might not be present in the historical log, we can leverage, for example,
seasonal, weekly, or holiday effects that are consistent in both the historical
and future data. Our estimator is the first to exploit these time-related
structures via a new type of importance weighting, enabling effective F-OPE.
Theoretical analysis identifies the conditions under which OPFV becomes
low-bias. In addition, we extend our estimator to develop a new policy-gradient
method to proactively learn a good future policy using only historical data.
Empirical results show that our methods substantially outperform existing
methods in estimating and optimizing the future policy value under
non-stationarity for various experimental setups.

</details>


### [82] [Tackling Data Heterogeneity in Federated Learning through Knowledge Distillation with Inequitable Aggregation](https://arxiv.org/abs/2506.20431)
*Xing Ma*

Main category: cs.LG

TL;DR: Proposes KDIA, a knowledge distillation method for federated learning with few participating clients, improving accuracy and efficiency under severe heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation in federated learning due to client label skew, data quantity skew, and limited client participation.

Method: Uses teacher-student inequitable aggregation (KDIA) with weighted teacher models and self-knowledge distillation, plus auxiliary IID data generation.

Result: Achieves better accuracy with fewer training rounds, especially under severe heterogeneity.

Conclusion: KDIA effectively leverages knowledge from all clients, enhancing federated learning performance in challenging scenarios.

Abstract: Federated learning aims to train a global model in a distributed environment
that is close to the performance of centralized training. However, issues such
as client label skew, data quantity skew, and other heterogeneity problems
severely degrade the model's performance. Most existing methods overlook the
scenario where only a small portion of clients participate in training within a
large-scale client setting, whereas our experiments show that this scenario
presents a more challenging federated learning task. Therefore, we propose a
Knowledge Distillation with teacher-student Inequitable Aggregation (KDIA)
strategy tailored to address the federated learning setting mentioned above,
which can effectively leverage knowledge from all clients. In KDIA, the student
model is the average aggregation of the participating clients, while the
teacher model is formed by a weighted aggregation of all clients based on three
frequencies: participation intervals, participation counts, and data volume
proportions. During local training, self-knowledge distillation is performed.
Additionally, we utilize a generator trained on the server to generate
approximately independent and identically distributed (IID) data features
locally for auxiliary training. We conduct extensive experiments on the
CIFAR-10/100/CINIC-10 datasets and various heterogeneous settings to evaluate
KDIA. The results show that KDIA can achieve better accuracy with fewer rounds
of training, and the improvement is more significant under severe
heterogeneity.

</details>


### [83] [Méthode de quadrature pour les PINNs fondée théoriquement sur la hessienne des résiduels](https://arxiv.org/abs/2506.20441)
*Antoine Caradot,Rémi Emonet,Amaury Habrard,Abdel-Rahim Mezidi,Marc Sebban*

Main category: cs.LG

TL;DR: A new quadrature method for selecting collocation points in PINNs, guided by the Hessian of the function, is proposed.


<details>
  <summary>Details</summary>
Motivation: Improving the efficiency of PINNs by refining the selection of collocation points beyond uniform sampling.

Method: Proposes a Hessian-based quadrature method to adaptively select collocation points during PINN training.

Result: Enhanced accuracy and efficiency in training PINNs by optimizing collocation point selection.

Conclusion: The Hessian-guided method improves PINN performance by better approximating integrals and refining point selection.

Abstract: Physics-informed Neural Networks (PINNs) have emerged as an efficient way to
learn surrogate neural solvers of PDEs by embedding the physical model in the
loss function and minimizing its residuals using automatic differentiation at
so-called collocation points. Originally uniformly sampled, the choice of the
latter has been the subject of recent advances leading to adaptive sampling
refinements. In this paper, we propose a new quadrature method for
approximating definite integrals based on the hessian of the considered
function, and that we leverage to guide the selection of the collocation points
during the training process of PINNs.

</details>


### [84] [Automatic Demonstration Selection for LLM-based Tabular Data Classification](https://arxiv.org/abs/2506.20451)
*Shuchu Han,Wolfgang Bruckner*

Main category: cs.LG

TL;DR: An algorithm for selecting the optimal number of demonstrations in ICL for tabular data, considering data distribution, prompt template, and LLM, validated against random selection.


<details>
  <summary>Details</summary>
Motivation: Determining the ideal number of demonstrations in ICL for tabular data classification is challenging, requiring consideration of data, prompt, and LLM.

Method: Uses Spectral Graph Theory to quantify demonstration similarities, constructs a similarity graph, and analyzes Laplacian eigenvalues to find the minimum representative demonstrations.

Result: Validated through experiments, outperforming random selection on diverse datasets and LLMs.

Conclusion: The proposed method effectively automates demonstration selection, improving ICL performance for tabular data.

Abstract: A fundamental question in applying In-Context Learning (ICL) for tabular data
classification is how to determine the ideal number of demonstrations in the
prompt. This work addresses this challenge by presenting an algorithm to
automatically select a reasonable number of required demonstrations. Our method
distinguishes itself by integrating not only the tabular data's distribution
but also the user's selected prompt template and the specific Large Language
Model (LLM) into its estimation. Rooted in Spectral Graph Theory, our proposed
algorithm defines a novel metric to quantify the similarities between different
demonstrations. We then construct a similarity graph and analyze the
eigenvalues of its Laplacian to derive the minimum number of demonstrations
capable of representing the data within the LLM's intrinsic representation
space. We validate the efficacy of our approach through experiments comparing
its performance against conventional random selection algorithms on diverse
datasets and LLMs.

</details>


### [85] [Multimodal Representation Learning and Fusion](https://arxiv.org/abs/2506.20494)
*Qihang Jin,Enze Ge,Yuhang Xie,Hongying Luo,Junhao Song,Ziqian Bi,Chia Xin Liang,Jibin Guan,Joe Yeong,Junfeng Hao*

Main category: cs.LG

TL;DR: Multi-modal learning combines data from sources like images, text, and audio to enhance AI understanding, using techniques like representation learning, alignment, and fusion. Challenges include data format diversity and adversarial attacks. Future directions focus on unsupervised learning, AutoML, and better benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve AI's ability to interpret and reason by leveraging multiple data modalities for richer representations.

Method: Uses representation learning, alignment methods, and fusion strategies via deep learning models.

Result: Enhances AI's interpretation, reasoning, and decision-making in real-world scenarios.

Conclusion: Multi-modal learning holds promise for advancing AI in fields like computer vision and healthcare, aiming for human-like understanding.

Abstract: Multi-modal learning is a fast growing area in artificial intelligence. It
tries to help machines understand complex things by combining information from
different sources, like images, text, and audio. By using the strengths of each
modality, multi-modal learning allows AI systems to build stronger and richer
internal representations. These help machines better interpretation, reasoning,
and making decisions in real-life situations. This field includes core
techniques such as representation learning (to get shared features from
different data types), alignment methods (to match information across
modalities), and fusion strategies (to combine them by deep learning models).
Although there has been good progress, some major problems still remain. Like
dealing with different data formats, missing or incomplete inputs, and
defending against adversarial attacks. Researchers now are exploring new
methods, such as unsupervised or semi-supervised learning, AutoML tools, to
make models more efficient and easier to scale. And also more attention on
designing better evaluation metrics or building shared benchmarks, make it
easier to compare model performance across tasks and domains. As the field
continues to grow, multi-modal learning is expected to improve many areas:
computer vision, natural language processing, speech recognition, and
healthcare. In the future, it may help to build AI systems that can understand
the world in a way more like humans, flexible, context aware, and able to deal
with real-world complexity.

</details>


### [86] [Collaborative Batch Size Optimization for Federated Learning](https://arxiv.org/abs/2506.20511)
*Arno Geimer,Karthick Panner Selvam,Beltran Fiz Pontiveros*

Main category: cs.LG

TL;DR: Optimizing local batch sizes in Federated Learning via greedy randomized search improves convergence speed without compromising performance.


<details>
  <summary>Details</summary>
Motivation: Improve local training efficiency in FL by optimizing hardware usage, addressing improper configurations due to decentralized training.

Method: Use greedy randomized search to optimize local batch sizes, leveraging FL's parallel processing.

Result: Faster convergence compared to default settings, nearly matching performance of optimized local parameters.

Conclusion: Proposed method effectively enhances FL training efficiency without requiring parameter optimization.

Abstract: Federated Learning (FL) is a decentralized collaborative Machine Learning
framework for training models without collecting data in a centralized
location. It has seen application across various disciplines, from helping
medical diagnoses in hospitals to detecting fraud in financial transactions. In
this paper, we focus on improving the local training process through hardware
usage optimization. While participants in a federation might share the hardware
they are training on, since there is no information exchange between them,
their training process can be hindered by an improper training configuration.
Taking advantage of the parallel processing inherent to Federated Learning, we
use a greedy randomized search to optimize local batch sizes for the best
training settings across all participants. Our results show that against
default parameter settings, our method improves convergence speed while staying
nearly on par with the case where local parameters are optimized.

</details>


### [87] [WallStreetFeds: Client-Specific Tokens as Investment Vehicles in Federated Learning](https://arxiv.org/abs/2506.20518)
*Arno Geimer,Beltran Fiz Pontiveros,Radu State*

Main category: cs.LG

TL;DR: A novel framework for Federated Learning (FL) introduces client-specific tokens and DeFi platforms to improve reward distribution and investment in FL.


<details>
  <summary>Details</summary>
Motivation: Existing FL incentive schemes lack flexible and scalable reward distribution methods, especially for for-profit FL.

Method: Proposes a framework using client-specific tokens, DeFi platforms, and automated market makers (AMMs) for reward distribution and investment.

Result: The framework aims to enhance flexibility and scalability in FL reward systems and enable third-party investment.

Conclusion: The proposed framework addresses limitations in current FL incentive schemes by integrating decentralized finance solutions.

Abstract: Federated Learning (FL) is a collaborative machine learning paradigm which
allows participants to collectively train a model while training data remains
private. This paradigm is especially beneficial for sectors like finance, where
data privacy, security and model performance are paramount. FL has been
extensively studied in the years following its introduction, leading to, among
others, better performing collaboration techniques, ways to defend against
other clients trying to attack the model, and contribution assessment methods.
An important element in for-profit Federated Learning is the development of
incentive methods to determine the allocation and distribution of rewards for
participants. While numerous methods for allocation have been proposed and
thoroughly explored, distribution frameworks remain relatively understudied. In
this paper, we propose a novel framework which introduces client-specific
tokens as investment vehicles within the FL ecosystem. Our framework aims to
address the limitations of existing incentive schemes by leveraging a
decentralized finance (DeFi) platform and automated market makers (AMMs) to
create a more flexible and scalable reward distribution system for
participants, and a mechanism for third parties to invest in the federation
learning process.

</details>


### [88] [Industrial Energy Disaggregation with Digital Twin-generated Dataset and Efficient Data Augmentation](https://arxiv.org/abs/2506.20525)
*Christian Internò,Andrea Castellani,Sebastian Schmitt,Fabio Stella,Barbara Hammer*

Main category: cs.LG

TL;DR: The paper introduces SIDED, a synthetic industrial dataset for NILM, and AMDA, a data augmentation method, to improve energy disaggregation in industrial settings.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity and privacy issues in industrial NILM due to the lack of high-quality datasets and complex energy consumption patterns.

Method: Generated SIDED using Digital Twin simulations and proposed AMDA for data augmentation to enhance model generalization.

Result: AMDA-augmented models achieved a Normalized Disaggregation Error of 0.093, outperforming non-augmented (0.451) and randomly augmented (0.290) models.

Conclusion: SIDED and AMDA effectively improve NILM model performance and generalization for industrial energy disaggregation.

Abstract: Industrial Non-Intrusive Load Monitoring (NILM) is limited by the scarcity of
high-quality datasets and the complex variability of industrial energy
consumption patterns. To address data scarcity and privacy issues, we introduce
the Synthetic Industrial Dataset for Energy Disaggregation (SIDED), an
open-source dataset generated using Digital Twin simulations. SIDED includes
three types of industrial facilities across three different geographic
locations, capturing diverse appliance behaviors, weather conditions, and load
profiles. We also propose the Appliance-Modulated Data Augmentation (AMDA)
method, a computationally efficient technique that enhances NILM model
generalization by intelligently scaling appliance power contributions based on
their relative impact. We show in experiments that NILM models trained with
AMDA-augmented data significantly improve the disaggregation of energy
consumption of complex industrial appliances like combined heat and power
systems. Specifically, in our out-of-sample scenarios, models trained with AMDA
achieved a Normalized Disaggregation Error of 0.093, outperforming models
trained without data augmentation (0.451) and those trained with random data
augmentation (0.290). Data distribution analyses confirm that AMDA effectively
aligns training and test data distributions, enhancing model generalization.

</details>


### [89] [Physics-Informed Machine Learning Regulated by Finite Element Analysis for Simulation Acceleration of Laser Powder Bed Fusion](https://arxiv.org/abs/2506.20537)
*R. Sharma,M. Raissi,Y. B. Guo*

Main category: cs.LG

TL;DR: The paper introduces FEA-PINN, a hybrid modeling framework combining Physics-Informed Neural Networks (PINN) with corrective FEA simulations to efficiently predict thermal fields in LPBF, maintaining FEA accuracy while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Traditional FEA for LPBF simulation is computationally expensive, necessitating a more efficient method without sacrificing accuracy.

Method: The FEA-PINN framework integrates PINN with dynamic material updating and corrective FEA simulations to address residual accumulation and enforce physical consistency.

Result: FEA-PINN achieves FEA-level accuracy with significantly lower computational cost, validated using benchmark data and single-track LPBF scanning.

Conclusion: FEA-PINN offers an efficient and accurate alternative to traditional FEA for LPBF thermal field prediction, addressing computational challenges.

Abstract: Efficient simulation of Laser Powder Bed Fusion (LPBF) is crucial for process
prediction due to the lasting issue of high computation cost using traditional
numerical methods such as finite element analysis (FEA). This study presents an
efficient modeling framework termed FEA-Regulated Physics-Informed Neural
Network (FEA-PINN) to accelerate the thermal field prediction in a LPBF process
while maintaining the FEA accuracy. A novel dynamic material updating strategy
is developed to capture the dynamic phase change of powder-liquid-solid in the
PINN model. The PINN model incorporates temperature-dependent material
properties and phase change behavior using the apparent heat capacity method.
While the PINN model demonstrates high accuracy with a small training data and
enables generalization of new process parameters via transfer learning, it
faces the challenge of high computation cost in time-dependent problems due to
the residual accumulation. To overcome this issue, the FEA-PINN framework
integrates corrective FEA simulations during inference to enforce physical
consistency and reduce error drift. A comparative analysis shows that FEA-PINN
achieves equivalent accuracy to FEA while significantly reducing computational
cost. The framework has been validated using the benchmark FEA data and
demonstrated through single-track scanning in LPBF.

</details>


### [90] [Demonstration of effective UCB-based routing in skill-based queues on real-world data](https://arxiv.org/abs/2506.20543)
*Sanne van Kempen,Jaron Sanders,Fiona Sloothaak,Maarten G. Wolf*

Main category: cs.LG

TL;DR: The paper explores reinforcement learning for optimal routing in skill-based queueing systems, demonstrating its adaptability and superiority over static policies. It introduces a heuristic for delay reduction and multi-objective optimization, with insights on parameter tuning and error sensitivity.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency in queueing systems like data centers and cloud networks by leveraging reinforcement learning for dynamic, optimal customer routing.

Method: Uses a reinforcement learning algorithm tested on real-world data, augmented with a heuristic routing rule and multi-objective optimization.

Result: The algorithm outperforms static policies, adapts to changes, and balances multiple objectives like payoff, server fairness, and waiting times.

Conclusion: The approach is practical for real-world implementation, with insights on tuning and error sensitivity for adaptive routing in complex systems.

Abstract: This paper is about optimally controlling skill-based queueing systems such
as data centers, cloud computing networks, and service systems. By means of a
case study using a real-world data set, we investigate the practical
implementation of a recently developed reinforcement learning algorithm for
optimal customer routing. Our experiments show that the algorithm efficiently
learns and adapts to changing environments and outperforms static benchmark
policies, indicating its potential for live implementation. We also augment the
real-world applicability of this algorithm by introducing a new heuristic
routing rule to reduce delays. Moreover, we show that the algorithm can
optimize for multiple objectives: next to payoff maximization, secondary
objectives such as server load fairness and customer waiting time reduction can
be incorporated. Tuning parameters are used for balancing inherent performance
trade--offs. Lastly, we investigate the sensitivity to estimation errors and
parameter tuning, providing valuable insights for implementing adaptive routing
algorithms in complex real-world queueing systems.

</details>


### [91] [Benchmarking Unsupervised Strategies for Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2506.20574)
*Laura Boggia,Rafael Teixeira de Lima,Bogdan Malaescu*

Main category: cs.LG

TL;DR: The paper explores transformer-based methods, specifically iTransformer, for anomaly detection in multivariate time series, analyzing parameters, label extraction, training impacts, and model comparisons.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection in multivariate time series is crucial but challenging due to unknown anomalies and complex interdependencies.

Method: Investigates iTransformer for anomaly detection, studying parameters, label extraction, training impacts, and loss functions.

Result: Provides insights into parameter influences, label extraction methods, and comparative performance of transformer models.

Conclusion: Transformer-based approaches, particularly iTransformer, show promise for effective anomaly detection in multivariate time series.

Abstract: Anomaly detection in multivariate time series is an important problem across
various fields such as healthcare, financial services, manufacturing or physics
detector monitoring. Accurately identifying when unexpected errors or faults
occur is essential, yet challenging, due to the unknown nature of anomalies and
the complex interdependencies between time series dimensions. In this paper, we
investigate transformer-based approaches for time series anomaly detection,
focusing on the recently proposed iTransformer architecture. Our contributions
are fourfold: (i) we explore the application of the iTransformer to time series
anomaly detection, and analyse the influence of key parameters such as window
size, step size, and model dimensions on performance; (ii) we examine methods
for extracting anomaly labels from multidimensional anomaly scores and discuss
appropriate evaluation metrics for such labels; (iii) we study the impact of
anomalous data present during training and assess the effectiveness of
alternative loss functions in mitigating their influence; and (iv) we present a
comprehensive comparison of several transformer-based models across a diverse
set of datasets for time series anomaly detection.

</details>


### [92] [Exploring Graph-Transformer Out-of-Distribution Generalization Abilities](https://arxiv.org/abs/2506.20575)
*Itay Niv,Neta Rabin*

Main category: cs.LG

TL;DR: The paper explores the effectiveness of graph-transformer (GT) and hybrid GT-MPNN backbones for out-of-distribution (OOD) generalization in graph neural networks, revealing their superior performance over traditional MPNNs. It also introduces a novel post-training analysis method for evaluating generalization.


<details>
  <summary>Details</summary>
Motivation: Current graph neural networks assume training and testing data share the same distribution, which is unrealistic. The paper aims to address OOD generalization challenges, focusing on backbone architectures like GT and hybrid models.

Method: The study systematically evaluates GT and hybrid backbones in OOD settings, adapting domain generalization (DG) algorithms for GTs. A novel post-training analysis method examines clustering structure for domain alignment and class separation.

Result: GT and hybrid GT-MPNN backbones outperform MPNNs in OOD generalization, even without specialized DG algorithms. The post-training analysis provides deeper insights into generalization abilities.

Conclusion: Graph-transformers show promise for robust real-world graph learning, setting a new direction for OOD generalization research. The proposed analysis method offers broader applicability beyond graph learning.

Abstract: Deep learning on graphs has shown remarkable success across numerous
applications, including social networks, bio-physics, traffic networks, and
recommendation systems. Regardless of their successes, current methods
frequently depend on the assumption that training and testing data share the
same distribution, a condition rarely met in real-world scenarios. While
graph-transformer (GT) backbones have recently outperformed traditional
message-passing neural networks (MPNNs) in multiple in-distribution (ID)
benchmarks, their effectiveness under distribution shifts remains largely
unexplored.
  In this work, we address the challenge of out-of-distribution (OOD)
generalization for graph neural networks, with a special focus on the impact of
backbone architecture. We systematically evaluate GT and hybrid backbones in
OOD settings and compare them to MPNNs. To do so, we adapt several leading
domain generalization (DG) algorithms to work with GTs and assess their
performance on a benchmark designed to test a variety of distribution shifts.
Our results reveal that GT and hybrid GT-MPNN backbones consistently
demonstrate stronger generalization ability compared to MPNNs, even without
specialized DG algorithms.
  Additionally, we propose a novel post-training analysis approach that
compares the clustering structure of the entire ID and OOD test datasets,
specifically examining domain alignment and class separation. Demonstrating its
model-agnostic design, this approach not only provided meaningful insights into
GT and MPNN backbones. It also shows promise for broader applicability to DG
problems beyond graph learning, offering a deeper perspective on generalization
abilities that goes beyond standard accuracy metrics. Together, our findings
highlight the promise of graph-transformers for robust, real-world graph
learning and set a new direction for future research in OOD generalization.

</details>


### [93] [The kernel of graph indices for vector search](https://arxiv.org/abs/2506.20584)
*Mariano Tepper,Ted Willke*

Main category: cs.LG

TL;DR: The paper introduces Support Vector Graph (SVG), a machine learning-based graph index for vector search in metric and non-metric spaces, with formal navigability guarantees. It generalizes popular indices like HNSW and DiskANN and proposes SVG-L0 for bounded out-degree.


<details>
  <summary>Details</summary>
Motivation: Existing graph indices for vector search rely on Euclidean space assumptions. The work aims to extend these principles to metric and non-metric spaces using machine learning.

Method: The authors propose SVG, leveraging kernel methods for graph connectivity, and SVG-L0, incorporating an ℓ0 sparsity constraint for bounded out-degree.

Result: SVG provides formal navigability guarantees in diverse spaces, generalizes existing indices, and SVG-L0 offers a principled approach to out-degree control with self-tuning properties.

Conclusion: The work advances graph-based vector search by unifying and extending existing methods with machine learning, offering theoretical and practical improvements.

Abstract: The most popular graph indices for vector search use principles from
computational geometry to build the graph. Hence, their formal graph
navigability guarantees are only valid in Euclidean space. In this work, we
show that machine learning can be used to build graph indices for vector search
in metric and non-metric vector spaces (e.g., for inner product similarity).
From this novel perspective, we introduce the Support Vector Graph (SVG), a new
type of graph index that leverages kernel methods to establish the graph
connectivity and that comes with formal navigability guarantees valid in metric
and non-metric vector spaces. In addition, we interpret the most popular graph
indices, including HNSW and DiskANN, as particular specializations of SVG and
show that new indices can be derived from the principles behind this
specialization. Finally, we propose SVG-L0 that incorporates an $\ell_0$
sparsity constraint into the SVG kernel method to build graphs with a bounded
out-degree. This yields a principled way of implementing this practical
requirement, in contrast to the traditional heuristic of simply truncating the
out edges of each node. Additionally, we show that SVG-L0 has a self-tuning
property that avoids the heuristic of using a set of candidates to find the
out-edges of each node and that keeps its computational complexity in check.

</details>


### [94] [H-FEX: A Symbolic Learning Method for Hamiltonian Systems](https://arxiv.org/abs/2506.20607)
*Jasen Lai,Senwei Liang,Chunmei Wang*

Main category: cs.LG

TL;DR: H-FEX, a symbolic learning method, accurately learns complex Hamiltonian functions while preserving energy conservation, outperforming traditional data-driven approaches.


<details>
  <summary>Details</summary>
Motivation: Existing data-driven methods struggle to accurately capture complex Hamiltonian functions and conserve energy, limiting their effectiveness for dynamical systems.

Method: Proposes H-FEX, a symbolic learning method with novel interaction nodes to effectively capture intricate interaction terms in Hamiltonian systems.

Result: H-FEX successfully recovers Hamiltonian functions for complex systems, accurately capturing dynamics and preserving energy over long time horizons.

Conclusion: H-FEX is a promising framework for discovering closed-form expressions of complex dynamical systems, addressing limitations of current methods.

Abstract: Hamiltonian systems describe a broad class of dynamical systems governed by
Hamiltonian functions, which encode the total energy and dictate the evolution
of the system. Data-driven approaches, such as symbolic regression and neural
network-based methods, provide a means to learn the governing equations of
dynamical systems directly from observational data of Hamiltonian systems.
However, these methods often struggle to accurately capture complex Hamiltonian
functions while preserving energy conservation. To overcome this limitation, we
propose the Finite Expression Method for learning Hamiltonian Systems (H-FEX),
a symbolic learning method that introduces novel interaction nodes designed to
capture intricate interaction terms effectively. Our experiments, including
those on highly stiff dynamical systems, demonstrate that H-FEX can recover
Hamiltonian functions of complex systems that accurately capture system
dynamics and preserve energy over long time horizons. These findings highlight
the potential of H-FEX as a powerful framework for discovering closed-form
expressions of complex dynamical systems.

</details>


### [95] [Lost in Retraining: Roaming the Parameter Space of Exponential Families Under Closed-Loop Learning](https://arxiv.org/abs/2506.20623)
*Fariba Jangjoo,Matteo Marsili,Yasser Roudi*

Main category: cs.LG

TL;DR: Closed-loop learning in exponential family models converges to biased absorbing states, but bias can be mitigated with data pollution, MAP estimation, or regularization.


<details>
  <summary>Details</summary>
Motivation: Address the potential risks of large neural networks being trained on self-generated data, focusing on exponential family models.

Method: Derive equations of motion for parameters, analyze maximum likelihood estimation, and explore mitigation strategies like data pollution, MAP estimation, and regularization.

Result: Closed-loop learning converges to biased absorbing states, but bias can be prevented with specific interventions.

Conclusion: Mitigation strategies like data pollution or regularization are necessary to avoid bias amplification in closed-loop learning.

Abstract: Closed-loop learning is the process of repeatedly estimating a model from
data generated from the model itself. It is receiving great attention due to
the possibility that large neural network models may, in the future, be
primarily trained with data generated by artificial neural networks themselves.
We study this process for models that belong to exponential families, deriving
equations of motions that govern the dynamics of the parameters. We show that
maximum likelihood estimation of the parameters endows sufficient statistics
with the martingale property and that as a result the process converges to
absorbing states that amplify initial biases present in the data. However, we
show that this outcome may be prevented by polluting the data with an
infinitesimal fraction of data points generated from a fixed model, by relying
on maximum a posteriori estimation or by introducing regularisation.
Furthermore, we show that the asymptotic behavior of the dynamics is not
reparametrisation invariant.

</details>


### [96] [Efficient Federated Learning with Encrypted Data Sharing for Data-Heterogeneous Edge Devices](https://arxiv.org/abs/2506.20644)
*Hangyu Li,Hongyue Wu,Guodong Fan,Zhen Zhang,Shizhan Chen,Zhiyong Feng*

Main category: cs.LG

TL;DR: FedEDS is a federated learning scheme for edge devices that uses encrypted data sharing to improve convergence speed and model performance by addressing data heterogeneity and network issues.


<details>
  <summary>Details</summary>
Motivation: Current federated learning methods overlook network topology, physical distance, and data heterogeneity, leading to latency and performance degradation in edge devices.

Method: FedEDS trains a data encryptor using client models and stochastic layers, generates encrypted data for sharing, and uses both local and shared encrypted data to train models.

Result: FedEDS accelerates convergence and mitigates data heterogeneity, enhancing model performance in edge device applications.

Conclusion: FedEDS effectively addresses federated learning challenges on edge devices, proving its suitability for rapid convergence and improved performance.

Abstract: As privacy protection gains increasing importance, more models are being
trained on edge devices and subsequently merged into the central server through
Federated Learning (FL). However, current research overlooks the impact of
network topology, physical distance, and data heterogeneity on edge devices,
leading to issues such as increased latency and degraded model performance. To
address these issues, we propose a new federated learning scheme on edge
devices that called Federated Learning with Encrypted Data Sharing(FedEDS).
FedEDS uses the client model and the model's stochastic layer to train the data
encryptor. The data encryptor generates encrypted data and shares it with other
clients. The client uses the corresponding client's stochastic layer and
encrypted data to train and adjust the local model. FedEDS uses the client's
local private data and encrypted shared data from other clients to train the
model. This approach accelerates the convergence speed of federated learning
training and mitigates the negative impact of data heterogeneity, making it
suitable for application services deployed on edge devices requiring rapid
convergence. Experiments results show the efficacy of FedEDS in promoting model
performance.

</details>


### [97] [Mastering Multiple-Expert Routing: Realizable $H$-Consistency and Strong Guarantees for Learning to Defer](https://arxiv.org/abs/2506.20650)
*Anqi Mao,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: The paper introduces novel surrogate loss functions and algorithms for learning to defer with multiple experts, addressing consistency properties in single-stage and two-stage scenarios.


<details>
  <summary>Details</summary>
Motivation: The challenge of optimally assigning input instances to experts, balancing accuracy and computational cost, is critical in fields like natural language generation, image processing, and medical diagnostics.

Method: The paper proposes new surrogate loss functions and efficient algorithms with theoretical guarantees, covering realizable H-consistency, H-consistency bounds, and Bayes-consistency for single-stage and two-stage deferral.

Result: Theoretical guarantees are provided for both scenarios, and experiments show improved performance of the proposed surrogate losses compared to baselines.

Conclusion: The paper advances the understanding and practical application of deferral learning with multiple experts, offering robust theoretical and empirical results.

Abstract: The problem of learning to defer with multiple experts consists of optimally
assigning input instances to experts, balancing the trade-off between their
accuracy and computational cost. This is a critical challenge in natural
language generation, but also in other fields such as image processing, and
medical diagnostics. Recent studies have proposed surrogate loss functions to
optimize deferral, but challenges remain in ensuring their consistency
properties. This paper introduces novel surrogate loss functions and efficient
algorithms with strong theoretical learning guarantees. We address open
questions regarding realizable $H$-consistency, $H$-consistency bounds, and
Bayes-consistency for both single-stage (jointly learning predictor and
deferral function) and two-stage (learning only the deferral function with a
fixed expert) learning scenarios. For single-stage deferral, we introduce a
family of new realizable $H$-consistent surrogate losses and further prove
$H$-consistency for a selected member. For two-stage deferral, we derive new
surrogate losses that achieve realizable $H$-consistency, $H$-consistency
bounds, and Bayes-consistency for the two-expert scenario and, under natural
assumptions, multiple-expert scenario. Additionally, we provide enhanced
theoretical guarantees under low-noise assumptions for both scenarios. Finally,
we report the results of experiments using our proposed surrogate losses,
comparing their performance against existing baselines.

</details>


### [98] [Hear No Evil: Detecting Gradient Leakage by Malicious Servers in Federated Learning](https://arxiv.org/abs/2506.20651)
*Fei Wang,Baochun Li*

Main category: cs.LG

TL;DR: The paper analyzes malicious gradient leakage attacks in federated learning (FL), showing they are limited in practice and detectable. It proposes a lightweight client-side detection mechanism for added security.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate the risk of sensitive data leakage in FL due to malicious server manipulation of gradient updates.

Method: Comprehensive analysis of gradient leakage attacks and model manipulation techniques, plus a proposed client-side detection mechanism.

Result: Attacks are not both highly effective and stealthy in realistic FL settings. Detection is feasible with minimal overhead.

Conclusion: Malicious gradient leakage attacks are limited in practice, and basic monitoring or lightweight detection can defend against them effectively.

Abstract: Recent work has shown that gradient updates in federated learning (FL) can
unintentionally reveal sensitive information about a client's local data. This
risk becomes significantly greater when a malicious server manipulates the
global model to provoke information-rich updates from clients. In this paper,
we adopt a defender's perspective to provide the first comprehensive analysis
of malicious gradient leakage attacks and the model manipulation techniques
that enable them. Our investigation reveals a core trade-off: these attacks
cannot be both highly effective in reconstructing private data and sufficiently
stealthy to evade detection -- especially in realistic FL settings that
incorporate common normalization techniques and federated averaging.
  Building on this insight, we argue that malicious gradient leakage attacks,
while theoretically concerning, are inherently limited in practice and often
detectable through basic monitoring. As a complementary contribution, we
propose a simple, lightweight, and broadly applicable client-side detection
mechanism that flags suspicious model updates before local training begins,
despite the fact that such detection may not be strictly necessary in realistic
FL settings. This mechanism further underscores the feasibility of defending
against these attacks with minimal overhead, offering a deployable safeguard
for privacy-conscious federated learning systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [99] [Computer Vision based Automated Quantification of Agricultural Sprayers Boom Displacement](https://arxiv.org/abs/2506.19939)
*Aryan Singh Dalal,Sidharth Rai,Rahul Singh,Treman Singh Kaloya,Rahul Harsha Cheppally,Ajay Sharda*

Main category: cs.CV

TL;DR: A computer vision system using YOLO models was developed to quantify spray boom movement in agricultural sprayers, achieving high accuracy and potential for design improvements.


<details>
  <summary>Details</summary>
Motivation: Spray boom instability causes application rate errors, but lacks quantitative data for systematic solutions.

Method: Developed a computer vision system with YOLO V7, V8, and V11 models to track boom movement, validated by an inclinometer sensor.

Result: The system detected targets with >90% accuracy and estimated distances within 0.026 m of sensor data.

Conclusion: The system effectively quantifies boom movement, aiding in design improvements for greater sprayer stability and accuracy.

Abstract: Application rate errors when using self-propelled agricultural sprayers for
agricultural production remain a concern. Among other factors, spray boom
instability is one of the major contributors to application errors. Spray
booms' width of 38m, combined with 30 kph driving speeds, varying terrain, and
machine dynamics when maneuvering complex field boundaries, make controls of
these booms very complex. However, there is no quantitative knowledge on the
extent of boom movement to systematically develop a solution that might include
boom designs and responsive boom control systems. Therefore, this study was
conducted to develop an automated computer vision system to quantify the boom
movement of various agricultural sprayers. A computer vision system was
developed to track a target on the edge of the sprayer boom in real time. YOLO
V7, V8, and V11 neural network models were trained to track the boom's
movements in field operations to quantify effective displacement in the
vertical and transverse directions. An inclinometer sensor was mounted on the
boom to capture boom angles and validate the neural network model output. The
results showed that the model could detect the target with more than 90 percent
accuracy, and distance estimates of the target on the boom were within 0.026 m
of the inclinometer sensor data. This system can quantify the boom movement on
the current sprayer and potentially on any other sprayer with minor
modifications. The data can be used to make design improvements to make sprayer
booms more stable and achieve greater application accuracy.

</details>


### [100] [EBC-ZIP: Improving Blockwise Crowd Counting with Zero-Inflated Poisson Regression](https://arxiv.org/abs/2506.19955)
*Yiming Ma,Victor Sanchez,Tanaya Guha*

Main category: cs.CV

TL;DR: EBC-ZIP introduces a Zero-Inflated Poisson (ZIP) regression framework for crowd counting, addressing imbalance in sparse density maps and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing crowd counting methods ignore extreme sparsity in density maps and use loss functions ill-suited for count data, leading to biased estimations.

Method: EBC-ZIP replaces traditional regression loss with ZIP's negative log-likelihood, improving handling of zero-heavy distributions while preserving count accuracy.

Result: EBC-ZIP outperforms the Enhanced Block Classification (EBC) framework and achieves state-of-the-art results on four benchmarks.

Conclusion: EBC-ZIP provides a principled probabilistic approach for crowd counting, effectively addressing sparsity and improving accuracy.

Abstract: Density map estimation has become the mainstream paradigm in crowd counting.
However, most existing methods overlook the extreme sparsity of ground-truth
density maps. In real-world crowd scenes, the vast majority of spatial regions
(often over 95%) contain no people, leading to heavily imbalanced count
distributions. Ignoring this imbalance can bias models toward overestimating
dense regions and underperforming in sparse areas. Furthermore, most loss
functions used in density estimation are majorly based on MSE and implicitly
assume Gaussian distributions, which are ill-suited for modeling discrete,
non-negative count data. In this paper, we propose EBC-ZIP, a crowd counting
framework that models the spatial distribution of counts using a Zero-Inflated
Poisson (ZIP) regression formulation. Our approach replaces the traditional
regression loss with the negative log-likelihood of the ZIP distribution,
enabling better handling of zero-heavy distributions while preserving count
accuracy. Built upon the recently proposed Enhanced Block Classification (EBC)
framework, EBC-ZIP inherits EBC's advantages in preserving the discreteness of
targets and ensuring training stability, while further improving performance
through a more principled probabilistic loss. We also evaluate EBC-ZIP with
backbones of varying computational complexity to assess its scalability.
Extensive experiments on four crowd counting benchmarks demonstrate that
EBC-ZIP consistently outperforms EBC and achieves state-of-the-art results.

</details>


### [101] [ToSA: Token Merging with Spatial Awareness](https://arxiv.org/abs/2506.20066)
*Hsiang-Wei Huang,Wenhao Chai,Kuang-Ming Chen,Cheng-Yen Yang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: ToSA introduces a token merging method for Vision Transformers (ViT) that combines semantic and spatial awareness, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing token merging methods rely on feature similarity, ignoring spatial information, which is crucial in early ViT layers where visual tokens lack strong features.

Method: ToSA uses depth images to create pseudo spatial tokens, integrating spatial awareness into the merging process for better scene structure preservation.

Result: ToSA outperforms previous methods in benchmarks for visual and embodied question answering while reducing ViT runtime.

Conclusion: ToSA is an efficient solution for ViT acceleration, balancing semantic and spatial information for improved performance.

Abstract: Token merging has emerged as an effective strategy to accelerate Vision
Transformers (ViT) by reducing computational costs. However, existing methods
primarily rely on the visual token's feature similarity for token merging,
overlooking the potential of integrating spatial information, which can serve
as a reliable criterion for token merging in the early layers of ViT, where the
visual tokens only possess weak visual information. In this paper, we propose
ToSA, a novel token merging method that combines both semantic and spatial
awareness to guide the token merging process. ToSA leverages the depth image as
input to generate pseudo spatial tokens, which serve as auxiliary spatial
information for the visual token merging process. With the introduced spatial
awareness, ToSA achieves a more informed merging strategy that better preserves
critical scene structure. Experimental results demonstrate that ToSA
outperforms previous token merging methods across multiple benchmarks on visual
and embodied question answering while largely reducing the runtime of the ViT,
making it an efficient solution for ViT acceleration. The code will be
available at: https://github.com/hsiangwei0903/ToSA

</details>


### [102] [BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos](https://arxiv.org/abs/2506.20103)
*Jiahao Lin,Weixuan Peng,Bojia Zi,Yifeng Gao,Xianbiao Qi,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: The paper introduces BrokenVideos, a benchmark dataset for detecting and localizing visual artifacts in AI-generated videos, improving artifact detection models.


<details>
  <summary>Details</summary>
Motivation: Current AI-generated videos suffer from visual artifacts, but existing datasets lack fine-grained spatial annotations for artifact localization.

Method: The authors created BrokenVideos, a dataset of 3,254 AI-generated videos with pixel-level annotations for visual corruption, validated by human inspection.

Result: Training artifact detection models and MLLMs on BrokenVideos enhances their ability to localize corrupted regions.

Conclusion: BrokenVideos provides a critical benchmark for advancing research on artifact localization in generative video models.

Abstract: Recent advances in deep generative models have led to significant progress in
video generation, yet the fidelity of AI-generated videos remains limited.
Synthesized content often exhibits visual artifacts such as temporally
inconsistent motion, physically implausible trajectories, unnatural object
deformations, and local blurring that undermine realism and user trust.
Accurate detection and spatial localization of these artifacts are crucial for
both automated quality control and for guiding the development of improved
generative models. However, the research community currently lacks a
comprehensive benchmark specifically designed for artifact localization in AI
generated videos. Existing datasets either restrict themselves to video or
frame level detection or lack the fine-grained spatial annotations necessary
for evaluating localization methods. To address this gap, we introduce
BrokenVideos, a benchmark dataset of 3,254 AI-generated videos with
meticulously annotated, pixel-level masks highlighting regions of visual
corruption. Each annotation is validated through detailed human inspection to
ensure high quality ground truth. Our experiments show that training state of
the art artifact detection models and multi modal large language models (MLLMs)
on BrokenVideos significantly improves their ability to localize corrupted
regions. Through extensive evaluation, we demonstrate that BrokenVideos
establishes a critical foundation for benchmarking and advancing research on
artifact localization in generative video models. The dataset is available at:
https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.

</details>


### [103] [From 2D to 3D Cognition: A Brief Survey of General World Models](https://arxiv.org/abs/2506.20134)
*Ningwei Xie,Zizi Tian,Lei Yang,Xiao-Ping Zhang,Meng Guo,Jie Li*

Main category: cs.CV

TL;DR: The paper reviews the evolution of world models from 2D perception to 3D cognition, highlighting key technologies and cognitive capabilities, and discusses applications and future challenges.


<details>
  <summary>Details</summary>
Motivation: The lack of systematic analysis in the field of 3D cognitive world models motivates this survey to categorize techniques and clarify their roles.

Method: The survey introduces a conceptual framework, reviews advancements in 3D representations and world knowledge, and dissects core cognitive capabilities like 3D scene generation, reasoning, and interaction.

Result: The paper identifies applications in embodied AI, autonomous driving, digital twins, and gaming/VR, and outlines challenges in data, modeling, and deployment.

Conclusion: Future directions focus on advancing robust and generalizable 3D world models by addressing identified challenges.

Abstract: World models have garnered increasing attention in the development of
artificial general intelligence (AGI), serving as computational frameworks for
learning representations of the external world and forecasting future states.
While early efforts focused on 2D visual perception and simulation, recent
3D-aware generative world models have demonstrated the ability to synthesize
geometrically consistent, interactive 3D environments, marking a shift toward
3D spatial cognition. Despite rapid progress, the field lacks systematic
analysis to categorize emerging techniques and clarify their roles in advancing
3D cognitive world models. This survey addresses this need by introducing a
conceptual framework, providing a structured and forward-looking review of
world models transitioning from 2D perception to 3D cognition. Within this
framework, we highlight two key technological drivers, particularly advances in
3D representations and the incorporation of world knowledge, as fundamental
pillars. Building on these, we dissect three core cognitive capabilities that
underpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning,
and 3D spatial interaction. We further examine the deployment of these
capabilities in real-world applications, including embodied AI, autonomous
driving, digital twin, and gaming/VR. Finally, we identify challenges across
data, modeling, and deployment, and outline future directions for advancing
more robust and generalizable 3D world models.

</details>


### [104] [EAR: Erasing Concepts from Unified Autoregressive Models](https://arxiv.org/abs/2506.20151)
*Haipeng Fan,Shiyuan Zhang,Baohunesitu,Zihang Guo,Huaiwen Zhang*

Main category: cs.CV

TL;DR: The paper introduces EAR, a fine-tuning method for concept erasure in AR models, using WGA and TLM strategies, and proposes the ECGVF benchmark for rigorous evaluation.


<details>
  <summary>Details</summary>
Motivation: AR models struggle with removing undesired concepts without degrading generation quality, prompting the need for effective erasure methods.

Method: EAR employs Windowed Gradient Accumulation (WGA) and Thresholded Loss Masking (TLM) for fine-tuning, alongside the ECGVF benchmark for evaluation.

Result: EAR shows significant improvements in erasure effectiveness and utility preservation on the Janus-Pro AR model.

Conclusion: EAR provides a robust solution for concept erasure in AR models, validated by the ECGVF benchmark.

Abstract: Autoregressive (AR) models have achieved unified and strong performance
across both visual understanding and image generation tasks. However, removing
undesired concepts from AR models while maintaining overall generation quality
remains an open challenge. In this paper, we propose Erasure Autoregressive
Model (EAR), a fine-tuning method for effective and utility-preserving concept
erasure in AR models. Specifically, we introduce Windowed Gradient Accumulation
(WGA) strategy to align patch-level decoding with erasure objectives, and
Thresholded Loss Masking (TLM) strategy to protect content unrelated to the
target concept during fine-tuning. Furthermore, we propose a novel benchmark,
Erase Concept Generator and Visual Filter (ECGVF), aim at provide a more
rigorous and comprehensive foundation for evaluating concept erasure in AR
models. Specifically, we first employ structured templates across diverse large
language models (LLMs) to pre-generate a large-scale corpus of
target-replacement concept prompt pairs. Subsequently, we generate images from
these prompts and subject them to rigorous filtering via a visual classifier to
ensure concept fidelity and alignment. Extensive experimental results conducted
on the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR
achieves marked improvements in both erasure effectiveness and model utility
preservation. Code is available at: https://github.com/immc-lab/ear/

</details>


### [105] [Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep Neural Network Acceleration](https://arxiv.org/abs/2506.20152)
*Deepak Ghimire,Kilho Lee,Seong-heum Kim*

Main category: cs.CV

TL;DR: LAASP introduces an efficient, loss-aware structured pruning method for neural networks, combining pruning and training into one cycle, outperforming state-of-the-art methods in accuracy and FLOP reduction.


<details>
  <summary>Details</summary>
Motivation: To streamline neural network pruning by eliminating the separate training stage and automating criteria selection, improving efficiency and accuracy.

Method: Adopts a pruning-while-training approach, automatically selecting pruning criteria and layer-specific rates based on network loss, with brief retraining to mitigate accuracy drops.

Result: Achieves significant FLOP reduction (52% for ResNet56/110 on CIFAR-10, 42% for ResNet50 on ImageNet) with minimal accuracy loss (0.33% drop in top-5 accuracy).

Conclusion: LAASP is an effective, automated pruning method that enhances efficiency and accuracy, suitable for resource-limited edge devices.

Abstract: Structured pruning is a well-established technique for compressing neural
networks, making it suitable for deployment in resource-limited edge devices.
This paper presents an efficient Loss-Aware Automatic Selection of Structured
Pruning Criteria (LAASP) for slimming and accelerating deep neural networks.
The majority of pruning methodologies employ a sequential process consisting of
three stages: 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed
pruning technique adopts a pruning-while-training approach that eliminates the
first stage and integrates the second and third stages into a single cycle. The
automatic selection of magnitude or similarity-based filter pruning criteria
from a specified pool of criteria and the specific pruning layer at each
pruning iteration is guided by the network's overall loss on a small subset of
the training data. To mitigate the abrupt accuracy drop due to pruning, the
network is retrained briefly after each reduction of a predefined number of
floating-point operations (FLOPs). The optimal pruning rates for each layer in
the network are automatically determined, eliminating the need for manual
allocation of fixed or variable pruning rates for each layer. Experiments on
the VGGNet and ResNet models on the CIFAR-10 and ImageNet benchmark datasets
demonstrate the effectiveness of the proposed method. In particular, the
ResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the
top-1 accuracy compared to state-of-the-art methods while reducing the network
FLOPs by 52\%. Furthermore, the ResNet50 model on the ImageNet dataset reduces
FLOPs by more than 42\% with a negligible 0.33\% drop in top-5 accuracy. The
source code of this paper is publicly available online -
https://github.com/ghimiredhikura/laasp.

</details>


### [106] [Towards Efficient Exemplar Based Image Editing with Multimodal VLMs](https://arxiv.org/abs/2506.20155)
*Avadhoot Jadhav,Ashutosh Srivastava,Abhinav Java,Silky Singh,Tarun Ram Menta,Surgan Jandial,Balaji Krishnamurthy*

Main category: cs.CV

TL;DR: A method for exemplar-based image editing using pretrained diffusion models and VLMs, outperforming baselines while being faster.


<details>
  <summary>Details</summary>
Motivation: Text alone is insufficient for ambiguous image edits; exemplar pairs better express such edits.

Method: Leverages pretrained text-to-image diffusion models and multimodal VLMs in an optimization-free pipeline.

Result: Outperforms baselines on multiple edit types and is ~4x faster.

Conclusion: The approach efficiently transfers edits from exemplar pairs to content images without optimization.

Abstract: Text-to-Image Diffusion models have enabled a wide array of image editing
applications. However, capturing all types of edits through text alone can be
challenging and cumbersome. The ambiguous nature of certain image edits is
better expressed through an exemplar pair, i.e., a pair of images depicting an
image before and after an edit respectively. In this work, we tackle
exemplar-based image editing -- the task of transferring an edit from an
exemplar pair to a content image(s), by leveraging pretrained text-to-image
diffusion models and multimodal VLMs. Even though our end-to-end pipeline is
optimization-free, our experiments demonstrate that it still outperforms
baselines on multiple types of edits while being ~4x faster.

</details>


### [107] [Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2506.20168)
*Zhentao He,Can Zhang,Ziheng Wu,Zhenghao Chen,Yufei Zhan,Yifan Li,Zhao Zhang,Xian Wang,Minghui Qiu*

Main category: cs.CV

TL;DR: The paper introduces KIE-HVQA, a benchmark for evaluating OCR hallucination in degraded documents, and proposes a GRPO-based framework to mitigate hallucinations by incorporating visual uncertainty awareness.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal models struggle with visual degradation, leading to unreliable outputs and hallucinations. The paper aims to address this gap.

Method: Proposes KIE-HVQA benchmark and a GRPO-based framework with a novel reward mechanism for vision-faithful reasoning.

Result: The 7B-parameter model achieves a 22% improvement in hallucination-free accuracy over GPT-4o on KIE-HVQA without performance drop in standard tasks.

Conclusion: The approach effectively mitigates hallucinations in degraded visual inputs while maintaining robustness in standard tasks.

Abstract: Recent advancements in multimodal large language models have enhanced
document understanding by integrating textual and visual information. However,
existing models exhibit incompleteness within their paradigm in real-world
scenarios, particularly under visual degradation. In such conditions, the
current response paradigm often fails to adequately perceive visual degradation
and ambiguity, leading to overreliance on linguistic priors or misaligned
visual-textual reasoning. This difficulty in recognizing uncertainty frequently
results in the generation of hallucinatory content, especially when a precise
answer is not feasible. To better demonstrate and analyze this phenomenon and
problem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR
hallucination in degraded document understanding. This dataset includes test
samples spanning identity cards and invoices, with simulated real-world
degradations for OCR reliability. This setup allows for evaluating models'
capacity, under degraded input, to distinguish reliable visual information and
answer accordingly, thereby highlighting the challenge of avoiding
hallucination on uncertain data. To achieve vision-faithful reasoning and
thereby avoid the aforementioned issues, we further introduce a GRPO-based
framework featuring a novel reward mechanism. By incorporating a self-awareness
of visual uncertainty and an analysis method that initiates refusal to answer
to increase task difficulty within our supervised fine-tuning and reinforcement
learning framework, we successfully mitigated hallucinations in ambiguous
regions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model
achieves a 22\% absolute improvement in hallucination-free accuracy over GPT-4o
on KIE-HVQA and there is no significant performance drop in standard tasks,
highlighting both effectiveness and robustness.

</details>


### [108] [Towards Scalable and Generalizable Earth Observation Data Mining via Foundation Model Composition](https://arxiv.org/abs/2506.20174)
*Man Duc Chuc*

Main category: cs.CV

TL;DR: Foundation models pretrained on remote sensing and general vision datasets can be combined to improve Earth Observation tasks, matching or outperforming larger models with less computational cost.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored strategy of reusing and combining existing pretrained models for Earth Observation tasks, rather than developing large models from scratch.

Method: Evaluated models like Prithvi, Hiera, and DOFA on the GEO-Bench benchmark across 11 datasets, using feature-level ensembling and knowledge distillation.

Result: Feature-level ensembling of smaller pretrained models matched or exceeded larger models' performance with less training time and resources. Knowledge distillation further enabled compact model deployment.

Conclusion: Combining pretrained models and knowledge distillation offers a practical, efficient approach for deploying foundation models in Earth Observation applications.

Abstract: Foundation models are rapidly transforming Earth Observation data mining by
enabling generalizable and scalable solutions for key tasks such as scene
classification and semantic segmentation. While most efforts in the geospatial
domain have focused on developing large models trained from scratch using
massive Earth Observation datasets, an alternative strategy that remains
underexplored is the reuse and combination of existing pretrained models. In
this study, we investigate whether foundation models pretrained on remote
sensing and general vision datasets can be effectively combined to improve
performance across a diverse set of key Earth Observation tasks. Using the
GEO-Bench benchmark, we evaluate several prominent models, including Prithvi,
Hiera, and DOFA, on eleven datasets covering a range of spatial resolutions,
sensor modalities, and task types. The results show that feature-level
ensembling of smaller pretrained models can match or exceed the performance of
much larger models, while requiring less training time and computational
resources. Moreover, the study highlights the potential of applying knowledge
distillation to transfer the strengths of ensembles into more compact models,
offering a practical path for deploying foundation models in real-world Earth
Observation applications.

</details>


### [109] [Progressive Alignment Degradation Learning for Pansharpening](https://arxiv.org/abs/2506.20179)
*Enzhe Zhao,Zhichang Guo,Yao Li,Fanghui Song,Boying Wu*

Main category: cs.CV

TL;DR: The paper critiques the Wald protocol's limitations in pansharpening and introduces PADM and HFreqdiff for better degradation modeling and high-frequency detail integration, improving performance.


<details>
  <summary>Details</summary>
Motivation: The Wald protocol's inaccurate degradation approximation limits deep pansharpening models' generalization, prompting the need for adaptive degradation learning and better detail integration.

Method: Proposes PADM (Progressive Alignment Degradation Module) with PAlignNet and PDegradeNet for adaptive degradation learning, and HFreqdiff for high-frequency detail embedding using CFB and BACM modules.

Result: The method enhances spatial sharpness and quality, outperforming state-of-the-art techniques in experiments and ablation studies.

Conclusion: The proposed innovations address Wald protocol limitations, improving pansharpening performance through adaptive degradation learning and high-frequency detail integration.

Abstract: Deep learning-based pansharpening has been shown to effectively generate
high-resolution multispectral (HRMS) images. To create supervised ground-truth
HRMS images, synthetic data generated using the Wald protocol is commonly
employed. This protocol assumes that networks trained on artificial
low-resolution data will perform equally well on high-resolution data. However,
well-trained models typically exhibit a trade-off in performance between
reduced-resolution and full-resolution datasets. In this paper, we delve into
the Wald protocol and find that its inaccurate approximation of real-world
degradation patterns limits the generalization of deep pansharpening models. To
address this issue, we propose the Progressive Alignment Degradation Module
(PADM), which uses mutual iteration between two sub-networks, PAlignNet and
PDegradeNet, to adaptively learn accurate degradation processes without relying
on predefined operators. Building on this, we introduce HFreqdiff, which embeds
high-frequency details into a diffusion framework and incorporates CFB and BACM
modules for frequency-selective detail extraction and precise reverse process
learning. These innovations enable effective integration of high-resolution
panchromatic and multispectral images, significantly enhancing spatial
sharpness and quality. Experiments and ablation studies demonstrate the
proposed method's superior performance compared to state-of-the-art techniques.

</details>


### [110] [UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2506.20214)
*Yanzhe Chen,Huasong Zhong,Yan Li,Zhenheng Yang*

Main category: cs.CV

TL;DR: UniCode$^2$ introduces a cascaded codebook framework for stable and semantically aligned visual tokenization, improving multimodal understanding and generation in MLLMs.


<details>
  <summary>Details</summary>
Motivation: Existing codebook-based methods either lack fine-grained semantics or face instability when scaled up, limiting their effectiveness in multimodal tasks.

Method: UniCode$^2$ clusters millions of SigLIP sequence embeddings to create a 500K-entry codebook, using a cascaded design with a frozen anchor codebook and a trainable refinement codebook.

Result: The framework achieves stable training, high token utilization, and seamless integration with pretrained diffusion decoders, enabling high-quality visual synthesis.

Conclusion: UniCode$^2$ demonstrates the feasibility of scaling visual token spaces without compromising stability, semantics, or modularity, performing well across diverse benchmarks.

Abstract: Unified multimodal large language models (MLLMs) have shown promise in
jointly advancing multimodal understanding and generation, with visual
codebooks discretizing images into tokens for autoregressive modeling. Existing
codebook-based methods either rely on small vocabularies (~16K entries) that
lack fine-grained semantics or naively scale up, resulting in low token
utilization and unstable training. We propose UniCode$^2$, a cascaded codebook
framework enabling large-scale, semantically aligned, and stable visual
tokenization. By clustering millions of SigLIP sequence embeddings, we build a
500K-entry codebook that preserves vision-language alignment while expanding
capacity. Stability is ensured via a cascaded design: a frozen codebook anchors
the embedding space, and a trainable codebook refines task-specific semantics.
This decoupling promotes high utilization and robust learning. Moreover, the
alignment of our visual tokens with textual semantics enables seamless
integration with pretrained diffusion decoders, supporting high-quality visual
synthesis with minimal adaptation. UniCode^2 delivers strong performance across
diverse benchmarks, demonstrating the viability of scaling visual token spaces
without sacrificing stability, semantics, or modularity.

</details>


### [111] [Dynamic Bandwidth Allocation for Hybrid Event-RGB Transmission](https://arxiv.org/abs/2506.20222)
*Pujing Yang,Guangyi Zhang,Yunlong Cai,Lei Yu,Guanding Yu*

Main category: cs.CV

TL;DR: A transmission scheme for hybrid RGB and event camera systems optimizes bandwidth by eliminating redundancy and enabling real-time deblurring.


<details>
  <summary>Details</summary>
Motivation: Hybrid RGB and event camera systems face challenges in transmitting large volumes of data efficiently.

Method: A joint event-image transmission framework uses Bayesian modeling and the information bottleneck method to disentangle shared and domain-specific information, optimizing bandwidth allocation based on scene dynamics.

Result: The scheme achieves superior reconstruction quality and enhanced deblurring performance compared to conventional systems.

Conclusion: The proposed framework effectively optimizes bandwidth and improves performance in hybrid camera systems.

Abstract: Event cameras asynchronously capture pixel-level intensity changes with
extremely low latency. They are increasingly used in conjunction with RGB
cameras for a wide range of vision-related applications. However, a major
challenge in these hybrid systems lies in the transmission of the large volume
of triggered events and RGB images. To address this, we propose a transmission
scheme that retains efficient reconstruction performance of both sources while
accomplishing real-time deblurring in parallel. Conventional RGB cameras and
event cameras typically capture the same scene in different ways, often
resulting in significant redundant information across their outputs. To address
this, we develop a joint event and image (E-I) transmission framework to
eliminate redundancy and thereby optimize channel bandwidth utilization. Our
approach employs Bayesian modeling and the information bottleneck method to
disentangle the shared and domain-specific information within the E-I inputs.
This disentangled information bottleneck framework ensures both the compactness
and informativeness of extracted shared and domain-specific information.
Moreover, it adaptively allocates transmission bandwidth based on scene
dynamics, i.e., more symbols are allocated to events for dynamic details or to
images for static information. Simulation results demonstrate that the proposed
scheme not only achieves superior reconstruction quality compared to
conventional systems but also delivers enhanced deblurring performance.

</details>


### [112] [Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and Task-graph Guided Refinement](https://arxiv.org/abs/2506.20254)
*Kun Yuan,Tingxuan Chen,Shi Li,Joel L. Lavanchy,Christian Heiliger,Ege Özsoy,Yiming Huang,Long Bai,Nassir Navab,Vinkle Srivastav,Hongliang Ren,Nicolas Padoy*

Main category: cs.CV

TL;DR: SPA is a lightweight framework for surgical workflow understanding, adapting foundation models to institutional settings with minimal annotation, achieving state-of-the-art performance in few-shot phase recognition.


<details>
  <summary>Details</summary>
Motivation: The challenge of developing generalizable models for surgical workflows due to heterogeneous settings and domain shifts motivates the need for adaptable solutions like SPA.

Method: SPA uses few-shot spatial adaptation, diffusion modeling for temporal consistency, and dynamic test-time adaptation to align multi-modal embeddings with institution-specific scenes and phases.

Result: SPA outperforms full-shot models with just 32-shot labeled data, achieving top performance in cross-institutional and cross-procedural phase recognition.

Conclusion: SPA provides a versatile, lightweight solution for surgical workflow understanding, enabling rapid customization with minimal annotation and robust performance under domain shifts.

Abstract: The complexity and diversity of surgical workflows, driven by heterogeneous
operating room settings, institutional protocols, and anatomical variability,
present a significant challenge in developing generalizable models for
cross-institutional and cross-procedural surgical understanding. While recent
surgical foundation models pretrained on large-scale vision-language data offer
promising transferability, their zero-shot performance remains constrained by
domain shifts, limiting their utility in unseen surgical environments. To
address this, we introduce Surgical Phase Anywhere (SPA), a lightweight
framework for versatile surgical workflow understanding that adapts foundation
models to institutional settings with minimal annotation. SPA leverages
few-shot spatial adaptation to align multi-modal embeddings with
institution-specific surgical scenes and phases. It also ensures temporal
consistency through diffusion modeling, which encodes task-graph priors derived
from institutional procedure protocols. Finally, SPA employs dynamic test-time
adaptation, exploiting the mutual agreement between multi-modal phase
prediction streams to adapt the model to a given test video in a
self-supervised manner, enhancing the reliability under test-time distribution
shifts. SPA is a lightweight adaptation framework, allowing hospitals to
rapidly customize phase recognition models by defining phases in natural
language text, annotating a few images with the phase labels, and providing a
task graph defining phase transitions. The experimental results show that the
SPA framework achieves state-of-the-art performance in few-shot surgical phase
recognition across multiple institutions and procedures, even outperforming
full-shot models with 32-shot labeled data. Code is available at
https://github.com/CAMMA-public/SPA

</details>


### [113] [A Transformer Based Handwriting Recognition System Jointly Using Online and Offline Features](https://arxiv.org/abs/2506.20255)
*Ayush Lodh,Ritabrata Chakraborty,Shivakumara Palaiahnakote,Umapada Pal*

Main category: cs.CV

TL;DR: The paper introduces an end-to-end network for handwriting recognition by fusing offline images and online stroke data early in a shared latent space, achieving state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: Handwriting recognition systems typically use only one modality (either rasterized glyphs or pen trajectory), missing complementary cues from both.

Method: An end-to-end network performs early fusion of offline images and online stroke data using a patch encoder and lightweight transformer, with learnable latent queries attending to both modalities.

Result: The method achieves state-of-the-art accuracy on IAMOn-DB and VNOn-DB, exceeding previous bests by up to 1%.

Conclusion: Early fusion of complementary modalities enhances representation learning, improving accuracy and writer independence.

Abstract: We posit that handwriting recognition benefits from complementary cues
carried by the rasterized complex glyph and the pen's trajectory, yet most
systems exploit only one modality. We introduce an end-to-end network that
performs early fusion of offline images and online stroke data within a shared
latent space. A patch encoder converts the grayscale crop into fixed-length
visual tokens, while a lightweight transformer embeds the $(x, y, \text{pen})$
sequence. Learnable latent queries attend jointly to both token streams,
yielding context-enhanced stroke embeddings that are pooled and decoded under a
cross-entropy loss objective. Because integration occurs before any high-level
classification, temporal cues reinforce each other during representation
learning, producing stronger writer independence. Comprehensive experiments on
IAMOn-DB and VNOn-DB demonstrate that our approach achieves state-of-the-art
accuracy, exceeding previous bests by up to 1\%. Our study also shows
adaptation of this pipeline with gesturification on the ISI-Air dataset. Our
code can be found here.

</details>


### [114] [Hierarchical Mask-Enhanced Dual Reconstruction Network for Few-Shot Fine-Grained Image Classification](https://arxiv.org/abs/2506.20263)
*Ning Luo,Meiyin Hu,Huan Wan,Yanyan Yang,Zhuohang Jiang,Xin Wei*

Main category: cs.CV

TL;DR: HMDRN improves few-shot fine-grained image classification by combining dual-layer feature reconstruction with mask-enhanced processing, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for FS-FGIC lose spatial information or fail to utilize hierarchical features, limiting performance.

Method: HMDRN integrates dual-layer feature reconstruction and fusion with a mask-enhanced transformer module to focus on discriminative regions.

Result: HMDRN outperforms state-of-the-art methods on three datasets, with ablation studies confirming its effectiveness.

Conclusion: HMDRN enhances inter-class discrimination and reduces intra-class variations, validated by superior feature reconstruction.

Abstract: Few-shot fine-grained image classification (FS-FGIC) presents a significant
challenge, requiring models to distinguish visually similar subclasses with
limited labeled examples. Existing methods have critical limitations:
metric-based methods lose spatial information and misalign local features,
while reconstruction-based methods fail to utilize hierarchical feature
information and lack mechanisms to focus on discriminative regions. We propose
the Hierarchical Mask-enhanced Dual Reconstruction Network (HMDRN), which
integrates dual-layer feature reconstruction with mask-enhanced feature
processing to improve fine-grained classification. HMDRN incorporates a
dual-layer feature reconstruction and fusion module that leverages
complementary visual information from different network hierarchies. Through
learnable fusion weights, the model balances high-level semantic
representations from the last layer with mid-level structural details from the
penultimate layer. Additionally, we design a spatial binary mask-enhanced
transformer self-reconstruction module that processes query features through
adaptive thresholding while maintaining complete support features, enhancing
focus on discriminative regions while filtering background noise. Extensive
experiments on three challenging fine-grained datasets demonstrate that HMDRN
consistently outperforms state-of-the-art methods across Conv-4 and ResNet-12
backbone architectures. Comprehensive ablation studies validate the
effectiveness of each proposed component, revealing that dual-layer
reconstruction enhances inter-class discrimination while mask-enhanced
transformation reduces intra-class variations. Visualization results provide
evidence of HMDRN's superior feature reconstruction capabilities.

</details>


### [115] [Forensic Study of Paintings Through the Comparison of Fabrics](https://arxiv.org/abs/2506.20272)
*Juan José Murillo-Fuentes,Pablo M. Olmos,Laura Alba-Carcelén*

Main category: cs.CV

TL;DR: A deep learning-based method for comparing textile similarity in artworks, bypassing traditional thread density maps, validated on Prado Museum canvases.


<details>
  <summary>Details</summary>
Motivation: Traditional thread density map matching fails for non-contiguous canvases, necessitating a new approach for authentication and conservation.

Method: A Siamese deep learning model compares canvas images, using feature representations and aggregated predictions for robust similarity scores.

Result: The method effectively compares plain weave canvases with similar thread densities, proving feasible and accurate.

Conclusion: The approach opens new possibilities for analyzing masterpieces, enhancing authentication and conservation efforts.

Abstract: The study of canvas fabrics in works of art is a crucial tool for
authentication, attribution and conservation. Traditional methods are based on
thread density map matching, which cannot be applied when canvases do not come
from contiguous positions on a roll. This paper presents a novel approach based
on deep learning to assess the similarity of textiles. We introduce an
automatic tool that evaluates the similarity between canvases without relying
on thread density maps. A Siamese deep learning model is designed and trained
to compare pairs of images by exploiting the feature representations learned
from the scans. In addition, a similarity estimation method is proposed,
aggregating predictions from multiple pairs of cloth samples to provide a
robust similarity score. Our approach is applied to canvases from the Museo
Nacional del Prado, corroborating the hypothesis that plain weave canvases,
widely used in painting, can be effectively compared even when their thread
densities are similar. The results demonstrate the feasibility and accuracy of
the proposed method, opening new avenues for the analysis of masterpieces.

</details>


### [116] [From Ideal to Real: Unified and Data-Efficient Dense Prediction for Real-World Scenarios](https://arxiv.org/abs/2506.20279)
*Changliang Xia,Chengyou Jia,Zhuohang Dang,Minnan Luo*

Main category: cs.CV

TL;DR: The paper introduces DenseWorld, a benchmark for 25 real-world dense prediction tasks, and proposes DenseDiT, a method leveraging generative models' visual priors with minimal additional parameters, outperforming baselines with far less training data.


<details>
  <summary>Details</summary>
Motivation: Existing dense prediction methods lack generalization to real-world scenarios due to idealized conditions and scarce real-world data.

Method: DenseDiT combines a parameter-reuse mechanism and lightweight branches for multi-scale context integration, adding less than 0.1% parameters.

Result: DenseDiT achieves superior performance on DenseWorld, using less than 0.01% of baseline training data.

Conclusion: DenseDiT demonstrates practical value for real-world dense prediction tasks, outperforming existing methods with minimal data and parameters.

Abstract: Dense prediction tasks hold significant importance of computer vision, aiming
to learn pixel-wise annotated label for an input image. Despite advances in
this field, existing methods primarily focus on idealized conditions, with
limited generalization to real-world scenarios and facing the challenging
scarcity of real-world data. To systematically study this problem, we first
introduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction
tasks that correspond to urgent real-world applications, featuring unified
evaluation across tasks. Then, we propose DenseDiT, which maximally exploits
generative models' visual priors to perform diverse real-world dense prediction
tasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism
and two lightweight branches that adaptively integrate multi-scale context,
working with less than 0.1% additional parameters. Evaluations on DenseWorld
reveal significant performance drops in existing general and specialized
baselines, highlighting their limited real-world generalization. In contrast,
DenseDiT achieves superior results using less than 0.01% training data of
baselines, underscoring its practical value for real-world deployment. Our
data, and checkpoints and codes are available at
https://xcltql666.github.io/DenseDiTProj

</details>


### [117] [Breaking Spatial Boundaries: Spectral-Domain Registration Guided Hyperspectral and Multispectral Blind Fusion](https://arxiv.org/abs/2506.20293)
*Kunjing Yang,Libin Zheng,Minru Bai,Ting Lu,Leyuan Fang*

Main category: cs.CV

TL;DR: The paper proposes a spectral-domain approach for registering and fusing hyperspectral (HSI) and multispectral (MSI) images, using a lightweight Spectral Prior Learning network and blind sparse fusion method, improving accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for registering HSIs and MSIs rely on spatial transformations, which are inefficient and perform poorly due to resolution differences. The paper aims to address these limitations by focusing on the spectral domain.

Method: A Spectral Prior Learning (SPL) network extracts spectral features and enhances MSI resolution. Subspace representation and cyclic training improve spectral accuracy. A blind sparse fusion (BSF) method with group sparsity regularization is proposed, solved using the Proximal Alternating Optimization (PAO) algorithm.

Result: The method effectively registers and fuses HSIs and MSIs, reducing computational complexity and improving spectral accuracy. It also enhances classification performance in experiments.

Conclusion: The proposed spectral-domain approach outperforms traditional spatial methods in registration and fusion, offering efficiency and improved results for remote sensing applications.

Abstract: The blind fusion of unregistered hyperspectral images (HSIs) and
multispectral images (MSIs) has attracted growing attention recently. To
address the registration challenge, most existing methods employ spatial
transformations on the HSI to achieve alignment with the MSI. However, due to
the substantial differences in spatial resolution of the images, the
performance of these methods is often unsatisfactory. Moreover, the
registration process tends to be time-consuming when dealing with large-sized
images in remote sensing. To address these issues, we propose tackling the
registration problem from the spectral domain. Initially, a lightweight
Spectral Prior Learning (SPL) network is developed to extract spectral features
from the HSI and enhance the spectral resolution of the MSI. Following this,
the obtained image undergoes spatial downsampling to produce the registered
HSI. In this process, subspace representation and cyclic training strategy are
employed to improve spectral accuracy of the registered HSI obtained. Next, we
propose a blind sparse fusion (BSF) method, which utilizes group sparsity
regularization to equivalently promote the low-rankness of the image. This
approach not only circumvents the need for rank estimation, but also reduces
computational complexity. Then, we employ the Proximal Alternating Optimization
(PAO) algorithm to solve the BSF model, and present its convergence analysis.
Finally, extensive numerical experiments on simulated and real datasets are
conducted to verify the effectiveness of our method in registration and fusion.
We also demonstrate its efficacy in enhancing classification performance.

</details>


### [118] [Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations](https://arxiv.org/abs/2506.20294)
*Shunqi Mao,Wei Guo,Chaoyi Zhang,Weidong Cai*

Main category: cs.CV

TL;DR: Ctrl-Z Sampling is a novel method to escape local optima in diffusion models by injecting noise and reverting to noisier states, improving generation quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion models often converge to locally coherent but globally inconsistent outputs due to latent space complexity.

Method: Ctrl-Z Sampling identifies local maxima, injects noise, and reverts to noisier states, evaluating trajectories for improvement.

Result: The method improves generation quality with a 7.6X increase in function evaluations.

Conclusion: Ctrl-Z Sampling enhances alignment and visual quality in diffusion models without requiring framework changes.

Abstract: Diffusion models have shown strong performance in conditional generation by
progressively denoising Gaussian noise toward a target data distribution. This
denoising process can be interpreted as a form of hill climbing in a learned
latent space, where the model iteratively refines the sample toward regions of
higher probability. However, diffusion models often converge to local optima
that are locally visually coherent yet globally inconsistent or conditionally
misaligned, due to latent space complexity and suboptimal initialization. Prior
efforts attempted to address this by strengthening guidance signals or
manipulating the initial noise distribution. We introduce Controlled Random
Zigzag Sampling (Ctrl-Z Sampling), a novel sampling strategy designed to detect
and escape such local maxima during conditional generation. The method first
identifies potential local maxima using a reward model. Upon detection, it
injects noise and reverts to a previous, noisier state to escape the current
optimization plateau. The reward model then evaluates candidate trajectories,
accepting only those that offer improvement, while progressively deeper retreat
enables stronger escapes when nearby alternatives fail. This controlled random
zigzag process allows dynamic alternation between forward refinement and
backward exploration, enhancing both alignment and visual quality in the
generated outputs. The proposed Ctrl-Z Sampling is model-agnostic and
compatible with existing diffusion frameworks. Experimental results show that
Ctrl-Z Sampling substantially improves generation quality with only around 7.6X
increase in function evaluations.

</details>


### [119] [TDiR: Transformer based Diffusion for Image Restoration Tasks](https://arxiv.org/abs/2506.20302)
*Abbas Anwar,Mohammad Shullar,Ali Arshad Nasir,Mudassir Masood,Saeed Anwar*

Main category: cs.CV

TL;DR: A transformer-based diffusion model improves degraded image quality, outperforming existing methods in underwater enhancement, denoising, and deraining.


<details>
  <summary>Details</summary>
Motivation: Degraded images hinder downstream tasks; the model aims to restore quality for better usability.

Method: Transformer-based diffusion model for image restoration tasks.

Result: Outperforms existing deep learning methods in enhancing, denoising, and deraining images.

Conclusion: Diffusion models with transformers effectively improve degraded images, enhancing their utility in high-fidelity tasks.

Abstract: Images captured in challenging environments often experience various forms of
degradation, including noise, color cast, blur, and light scattering. These
effects significantly reduce image quality, hindering their applicability in
downstream tasks such as object detection, mapping, and classification. Our
transformer-based diffusion model was developed to address image restoration
tasks, aiming to improve the quality of degraded images. This model was
evaluated against existing deep learning methodologies across multiple quality
metrics for underwater image enhancement, denoising, and deraining on publicly
available datasets. Our findings demonstrate that the diffusion model, combined
with transformers, surpasses current methods in performance. The results of our
model highlight the efficacy of diffusion models and transformers in improving
the quality of degraded images, consequently expanding their utility in
downstream tasks that require high-fidelity visual data.

</details>


### [120] [Radiomic fingerprints for knee MR images assessment](https://arxiv.org/abs/2506.20306)
*Yaxi Chen,Simin Ni,Shaheer U. Saeed,Aleksandra Ivanova,Rikin Hargunani,Jie Huang,Chaozong Liu,Yipeng Hu*

Main category: cs.CV

TL;DR: The paper proposes a dynamic radiomic fingerprint framework for knee MRI interpretation, improving accuracy and interpretability over traditional radiomic signatures and matching deep learning performance.


<details>
  <summary>Details</summary>
Motivation: Current radiomic approaches use fixed, population-level features, limiting performance and individual pathological representation. The authors aim to enhance generalization and interpretability by dynamically selecting features per patient.

Method: A deep learning model dynamically selects radiomic features (fingerprints) for each patient from a large pool, training alongside a low-dimensional logistic regression for classification.

Result: The framework achieves comparable or superior diagnostic accuracy to state-of-the-art deep learning models across tasks like knee abnormalities, ACL tears, and meniscus tears.

Conclusion: The radiomic fingerprint framework improves diagnostic accuracy while maintaining interpretability, offering clinical insights and potential biomarker discovery.

Abstract: Accurate interpretation of knee MRI scans relies on expert clinical judgment,
often with high variability and limited scalability. Existing radiomic
approaches use a fixed set of radiomic features (the signature), selected at
the population level and applied uniformly to all patients. While
interpretable, these signatures are often too constrained to represent
individual pathological variations. As a result, conventional radiomic-based
approaches are found to be limited in performance, compared with recent
end-to-end deep learning (DL) alternatives without using interpretable radiomic
features. We argue that the individual-agnostic nature in current radiomic
selection is not central to its intepretability, but is responsible for the
poor generalization in our application. Here, we propose a novel radiomic
fingerprint framework, in which a radiomic feature set (the fingerprint) is
dynamically constructed for each patient, selected by a DL model. Unlike the
existing radiomic signatures, our fingerprints are derived on a per-patient
basis by predicting the feature relevance in a large radiomic feature pool, and
selecting only those that are predictive of clinical conditions for individual
patients. The radiomic-selecting model is trained simultaneously with a
low-dimensional (considered relatively explainable) logistic regression for
downstream classification. We validate our methods across multiple diagnostic
tasks including general knee abnormalities, anterior cruciate ligament (ACL)
tears, and meniscus tears, demonstrating comparable or superior diagnostic
accuracy relative to state-of-the-art end-to-end DL models. More importantly,
we show that the interpretability inherent in our approach facilitates
meaningful clinical insights and potential biomarker discovery, with detailed
discussion, quantitative and qualitative analysis of real-world clinical cases
to evidence these advantages.

</details>


### [121] [On the Burstiness of Faces in Set](https://arxiv.org/abs/2506.20312)
*Jiong Wang*

Main category: cs.CV

TL;DR: The paper addresses burstiness in set-based face recognition (SFR), where certain faces dominate sets, harming performance. It proposes detection methods and solutions to mitigate this issue, improving recognition accuracy.


<details>
  <summary>Details</summary>
Motivation: Burstiness in SFR causes poor generalization and interferes with evaluation. The study aims to detect and suppress bursty faces to enhance performance.

Method: Three strategies (Quickshift++, feature self-similarity, generalized max-pooling) detect bursty faces. Solutions adjust sampling ratios and introduce quality-aware GMP for evaluation.

Result: Experiments show burstiness is common in SFR, and suppressing it significantly boosts recognition performance.

Conclusion: Detecting and mitigating burstiness in SFR improves generalization and evaluation accuracy, validated by benchmarks.

Abstract: Burstiness, a phenomenon observed in text and image retrieval, refers to that
particular elements appear more times in a set than a statistically independent
model assumes. We argue that in the context of set-based face recognition
(SFR), burstiness exists widely and degrades the performance in two aspects:
Firstly, the bursty faces, where faces with particular attributes %exist
frequently in a face set, dominate the training instances and dominate the
training face sets and lead to poor generalization ability to unconstrained
scenarios. Secondly, the bursty faces %dominating the evaluation sets interfere
with the similarity comparison in set verification and identification when
evaluation. To detect the bursty faces in a set, we propose three strategies
based on Quickshift++, feature self-similarity, and generalized max-pooling
(GMP). We apply the burst detection results on training and evaluation stages
to enhance the sampling ratios or contributions of the infrequent faces. When
evaluation, we additionally propose the quality-aware GMP that enables
awareness of the face quality and robustness to the low-quality faces for the
original GMP. We give illustrations and extensive experiments on the SFR
benchmarks to demonstrate that burstiness is widespread and suppressing
burstiness considerably improves the recognition performance.

</details>


### [122] [From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents](https://arxiv.org/abs/2506.20326)
*Sergio Torres Aguilar*

Main category: cs.CV

TL;DR: Benchmarking five object detection models on historical documents shows Transformer-based models excel in structured layouts, while CNN-OBB models outperform in complex, diverse datasets.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of different object detection architectures for robust Document Layout Analysis (DLA) in historical documents with varying complexities.

Method: Benchmarked five state-of-the-art object detection models (Co-DETR, Grounding DINO, YOLO variants) on three datasets (e-NDP, CATMuS, HORAE) representing different levels of document complexity.

Result: Co-DETR performed best on structured layouts (e-NDP, 0.752 mAP), while YOLOv11x-OBB excelled in complex datasets (CATMuS, 0.564; HORAE, 0.568). Oriented Bounding Boxes (OBB) proved essential for accuracy.

Conclusion: Transformers are ideal for structured layouts, but CNN-OBB models generalize better for complex documents. OBB is crucial for historical manuscript analysis.

Abstract: Robust Document Layout Analysis (DLA) is critical for the automated
processing and understanding of historical documents with complex page
organizations. This paper benchmarks five state-of-the-art object detection
architectures on three annotated datasets representing a spectrum of
codicological complexity: The e-NDP, a corpus of Parisian medieval registers
(1326-1504); CATMuS, a diverse multiclass dataset derived from various medieval
and modern sources (ca.12th-17th centuries) and HORAE, a corpus of decorated
books of hours (ca.13th-16th centuries). We evaluate two Transformer-based
models (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and
YOLO-World). Our findings reveal significant performance variations dependent
on model architecture, data set characteristics, and bounding box
representation. In the e-NDP dataset, Co-DETR achieves state-of-the-art results
(0.752 mAP@.50:.95), closely followed by YOLOv11X-OBB (0.721). Conversely, on
the more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB
significantly outperforms all other models (0.564 and 0.568, respectively).
This study unequivocally demonstrates that using Oriented Bounding Boxes (OBB)
is not a minor refinement but a fundamental requirement for accurately modeling
the non-Cartesian nature of historical manuscripts. We conclude that a key
trade-off exists between the global context awareness of Transformers, ideal
for structured layouts, and the superior generalization of CNN-OBB models for
visually diverse and complex documents.

</details>


### [123] [Feature Hallucination for Self-supervised Action Recognition](https://arxiv.org/abs/2506.20342)
*Lei Wang,Piotr Koniusz*

Main category: cs.CV

TL;DR: A deep translational action recognition framework enhances accuracy by predicting action concepts and auxiliary features, integrating novel descriptors and multimodal cues, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Human action recognition requires high-level semantic reasoning and multimodal feature integration, beyond raw pixel analysis.

Method: Proposes a framework with hallucination streams for missing cues, novel descriptors (ODF, SDF), and aleatoric uncertainty modeling. Integrates auxiliary modalities like optical flow, skeleton data, and audio.

Result: Achieves state-of-the-art performance on benchmarks (Kinetics-400, Kinetics-600, Something-Something V2).

Conclusion: The framework effectively captures fine-grained action dynamics by leveraging multimodal features and robust uncertainty handling.

Abstract: Understanding human actions in videos requires more than raw pixel analysis;
it relies on high-level semantic reasoning and effective integration of
multimodal features. We propose a deep translational action recognition
framework that enhances recognition accuracy by jointly predicting action
concepts and auxiliary features from RGB video frames. At test time,
hallucination streams infer missing cues, enriching feature representations
without increasing computational overhead. To focus on action-relevant regions
beyond raw pixels, we introduce two novel domain-specific descriptors. Object
Detection Features (ODF) aggregate outputs from multiple object detectors to
capture contextual cues, while Saliency Detection Features (SDF) highlight
spatial and intensity patterns crucial for action recognition. Our framework
seamlessly integrates these descriptors with auxiliary modalities such as
optical flow, Improved Dense Trajectories, skeleton data, and audio cues. It
remains compatible with state-of-the-art architectures, including I3D,
AssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE
V2 and InternVideo2. To handle uncertainty in auxiliary features, we
incorporate aleatoric uncertainty modeling in the hallucination step and
introduce a robust loss function to mitigate feature noise. Our multimodal
self-supervised action recognition framework achieves state-of-the-art
performance on multiple benchmarks, including Kinetics-400, Kinetics-600, and
Something-Something V2, demonstrating its effectiveness in capturing
fine-grained action dynamics.

</details>


### [124] [InvZW: Invariant Feature Learning via Noise-Adversarial Training for Robust Image Zero-Watermarking](https://arxiv.org/abs/2506.20370)
*Abdullah All Tanvir,Xin Zhong*

Main category: cs.CV

TL;DR: A deep learning framework for robust image zero-watermarking using distortion-invariant feature learning, leaving the original image unaltered and achieving state-of-the-art robustness.


<details>
  <summary>Details</summary>
Motivation: To develop a zero-watermarking method that preserves the original image while ensuring robustness against distortions.

Method: Combines noise-adversarial learning for invariant feature extraction and a learning-based multibit zero-watermarking scheme.

Result: Achieves superior robustness in feature stability and watermark recovery, outperforming existing methods.

Conclusion: The framework excels in generalization and robustness, making it a leading solution for zero-watermarking.

Abstract: This paper introduces a novel deep learning framework for robust image
zero-watermarking based on distortion-invariant feature learning. As a
zero-watermarking scheme, our method leaves the original image unaltered and
learns a reference signature through optimization in the feature space. The
proposed framework consists of two key modules. In the first module, a feature
extractor is trained via noise-adversarial learning to generate representations
that are both invariant to distortions and semantically expressive. This is
achieved by combining adversarial supervision against a distortion
discriminator and a reconstruction constraint to retain image content. In the
second module, we design a learning-based multibit zero-watermarking scheme
where the trained invariant features are projected onto a set of trainable
reference codes optimized to match a target binary message. Extensive
experiments on diverse image datasets and a wide range of distortions show that
our method achieves state-of-the-art robustness in both feature stability and
watermark recovery. Comparative evaluations against existing self-supervised
and deep watermarking techniques further highlight the superiority of our
framework in generalization and robustness.

</details>


### [125] [Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking](https://arxiv.org/abs/2506.20381)
*Ben Kang,Xin Chen,Jie Zhao,Chunjuan Bo,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: HiT and DyHiT are efficient transformer-based visual trackers designed for resource-constrained devices, achieving high speed and performance. HiT introduces a Bridge Module and dual-image position encoding, while DyHiT dynamically adapts to scene complexity. A training-free acceleration method further boosts speed without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Transformer-based trackers are powerful but slow on resource-constrained devices. The goal is to develop efficient models (HiT, DyHiT) and acceleration methods to address this limitation.

Method: HiT uses a Bridge Module and dual-image position encoding. DyHiT employs dynamic routing to adapt to scene complexity. A training-free acceleration method is introduced for speedup.

Result: HiT achieves 61 fps (64.6% AUC on LaSOT). DyHiT reaches 111 fps (62.4% AUC). The acceleration method speeds up SeqTrack-B256 by 2.68x (69.9% AUC).

Conclusion: HiT and DyHiT offer efficient, high-performance tracking solutions. The dynamic routing and acceleration methods enhance practicality for resource-constrained devices.

Abstract: Transformer-based visual trackers have demonstrated significant advancements
due to their powerful modeling capabilities. However, their practicality is
limited on resource-constrained devices because of their slow processing
speeds. To address this challenge, we present HiT, a novel family of efficient
tracking models that achieve high performance while maintaining fast operation
across various devices. The core innovation of HiT lies in its Bridge Module,
which connects lightweight transformers to the tracking framework, enhancing
feature representation quality. Additionally, we introduce a dual-image
position encoding approach to effectively encode spatial information. HiT
achieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson
AGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark,
outperforming all previous efficient trackers.Building on HiT, we propose
DyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by
selecting routes with varying computational requirements. DyHiT uses search
area features extracted by the backbone network and inputs them into an
efficient dynamic router to classify tracking scenarios. Based on the
classification, DyHiT applies a divide-and-conquer strategy, selecting
appropriate routes to achieve a superior trade-off between accuracy and speed.
The fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while
maintaining an AUC of 62.4% on LaSOT.Furthermore, we introduce a training-free
acceleration method based on the dynamic routing architecture of DyHiT. This
method significantly improves the execution speed of various high-performance
trackers without sacrificing accuracy. For instance, our acceleration method
enables the state-of-the-art tracker SeqTrack-B256 to achieve a 2.68 times
speedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of
69.9% on the LaSOT.

</details>


### [126] [A Novel Large Vision Foundation Model (LVFM)-based Approach for Generating High-Resolution Canopy Height Maps in Plantations for Precision Forestry Management](https://arxiv.org/abs/2506.20388)
*Shen Tan,Xin Zhang,Liangxiu Han,Huaguo Huang,Han Wang*

Main category: cs.CV

TL;DR: A novel model using a Large Vision Foundation Model (LVFM) generates high-resolution canopy height maps (CHMs) from RGB imagery, outperforming existing methods and enabling accurate aboveground biomass (AGB) monitoring for carbon sequestration.


<details>
  <summary>Details</summary>
Motivation: Accurate, cost-effective monitoring of plantation AGB is vital for carbon sequestration initiatives like China's CCER program, but traditional lidar-based methods are expensive, and deep learning with RGB imagery struggles with canopy height feature extraction.

Method: The model combines a feature extractor, a self-supervised feature enhancement module, and a height estimator, tested using 1-meter Google Earth imagery in Beijing's Fangshan District.

Result: The model achieved a mean absolute error of 0.09 m, RMSE of 0.24 m, and correlation of 0.78 against lidar-based CHMs, with over 90% success in tree detection and high AGB estimation accuracy.

Conclusion: This scalable approach is promising for evaluating carbon sequestration in plantations and natural forests, offering a cost-effective alternative to lidar.

Abstract: Accurate, cost-effective monitoring of plantation aboveground biomass (AGB)
is crucial for supporting local livelihoods and carbon sequestration
initiatives like the China Certified Emission Reduction (CCER) program.
High-resolution canopy height maps (CHMs) are essential for this, but standard
lidar-based methods are expensive. While deep learning with RGB imagery offers
an alternative, accurately extracting canopy height features remains
challenging. To address this, we developed a novel model for high-resolution
CHM generation using a Large Vision Foundation Model (LVFM). Our model
integrates a feature extractor, a self-supervised feature enhancement module to
preserve spatial details, and a height estimator. Tested in Beijing's Fangshan
District using 1-meter Google Earth imagery, our model outperformed existing
methods, including conventional CNNs. It achieved a mean absolute error of 0.09
m, a root mean square error of 0.24 m, and a correlation of 0.78 against
lidar-based CHMs. The resulting CHMs enabled over 90% success in individual
tree detection, high accuracy in AGB estimation, and effective tracking of
plantation growth, demonstrating strong generalization to non-training areas.
This approach presents a promising, scalable tool for evaluating carbon
sequestration in both plantations and natural forests.

</details>


### [127] [Med-Art: Diffusion Transformer for 2D Medical Text-to-Image Generation](https://arxiv.org/abs/2506.20449)
*Changlu Guo,Anders Nymark Christensen,Morten Rieger Hannemose*

Main category: cs.CV

TL;DR: Med-Art is a framework for medical image generation with limited data, leveraging vision-language models and a pre-trained Diffusion Transformer (DiT) model, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Address challenges in medical image generation, such as small datasets and scarce medical textual data.

Method: Uses Med-Art framework with vision-language models and adapts PixArt-α (DiT-based). Introduces Hybrid-Level Diffusion Fine-tuning (HLDF) for pixel-level losses.

Result: State-of-the-art performance on two medical datasets (FID, KID, classification metrics).

Conclusion: Med-Art effectively addresses data scarcity and quality issues in medical image generation.

Abstract: Text-to-image generative models have achieved remarkable breakthroughs in
recent years. However, their application in medical image generation still
faces significant challenges, including small dataset sizes, and scarcity of
medical textual data. To address these challenges, we propose Med-Art, a
framework specifically designed for medical image generation with limited data.
Med-Art leverages vision-language models to generate visual descriptions of
medical images which overcomes the scarcity of applicable medical textual data.
Med-Art adapts a large-scale pre-trained text-to-image model, PixArt-$\alpha$,
based on the Diffusion Transformer (DiT), achieving high performance under
limited data. Furthermore, we propose an innovative Hybrid-Level Diffusion
Fine-tuning (HLDF) method, which enables pixel-level losses, effectively
addressing issues such as overly saturated colors. We achieve state-of-the-art
performance on two medical image datasets, measured by FID, KID, and downstream
classification performance.

</details>


### [128] [HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based Diffusion Sampling](https://arxiv.org/abs/2506.20452)
*Tobias Vontobel,Seyedmorteza Sadat,Farnood Salehi,Romann M. Weber*

Main category: cs.CV

TL;DR: HiWave is a training-free, zero-shot method for ultra-high-resolution image synthesis using pretrained diffusion models, enhancing visual fidelity and coherence without retraining.


<details>
  <summary>Details</summary>
Motivation: Training diffusion models at high resolutions is computationally expensive, and existing zero-shot methods often produce artifacts like object duplication and spatial incoherence.

Method: HiWave uses a two-stage pipeline: generating a base image, then applying patch-wise DDIM inversion and a wavelet-based detail enhancer to preserve global coherence and enrich fine details.

Result: HiWave outperforms prior methods, reducing artifacts and achieving superior perceptual quality, with users preferring it in over 80% of comparisons.

Conclusion: HiWave is effective for high-quality, ultra-high-resolution image synthesis without retraining or architectural changes.

Abstract: Diffusion models have emerged as the leading approach for image synthesis,
demonstrating exceptional photorealism and diversity. However, training
diffusion models at high resolutions remains computationally prohibitive, and
existing zero-shot generation techniques for synthesizing images beyond
training resolutions often produce artifacts, including object duplication and
spatial incoherence. In this paper, we introduce HiWave, a training-free,
zero-shot approach that substantially enhances visual fidelity and structural
coherence in ultra-high-resolution image synthesis using pretrained diffusion
models. Our method employs a two-stage pipeline: generating a base image from
the pretrained model followed by a patch-wise DDIM inversion step and a novel
wavelet-based detail enhancer module. Specifically, we first utilize inversion
methods to derive initial noise vectors that preserve global coherence from the
base image. Subsequently, during sampling, our wavelet-domain detail enhancer
retains low-frequency components from the base image to ensure structural
consistency, while selectively guiding high-frequency components to enrich fine
details and textures. Extensive evaluations using Stable Diffusion XL
demonstrate that HiWave effectively mitigates common visual artifacts seen in
prior methods, achieving superior perceptual quality. A user study confirmed
HiWave's performance, where it was preferred over the state-of-the-art
alternative in more than 80% of comparisons, highlighting its effectiveness for
high-quality, ultra-high-resolution image synthesis without requiring
retraining or architectural modifications.

</details>


### [129] [A Deep Learning Approach to Identify Rock Bolts in Complex 3D Point Clouds of Underground Mines Captured Using Mobile Laser Scanners](https://arxiv.org/abs/2506.20464)
*Dibyayan Patra,Pasindu Ranasinghe,Bikram Banerjee,Simit Raval*

Main category: cs.CV

TL;DR: The paper proposes DeepBolt, a two-stage deep learning architecture for automated rock bolt detection in 3D point clouds, outperforming existing methods in precision and recall.


<details>
  <summary>Details</summary>
Motivation: Manual rock bolt surveying in underground mines is challenging due to low light and time constraints, necessitating automated solutions. Existing methods lack robustness in noisy, complex environments.

Method: DeepBolt, a novel two-stage deep learning architecture, addresses class imbalance and efficiently identifies rock bolts in large-scale 3D point clouds.

Result: DeepBolt achieves 96.41% precision and 96.96% recall, surpassing state-of-the-art models by up to 42.5% in IoU for rock bolt points.

Conclusion: DeepBolt is robust and effective for rock bolt identification in complex underground environments, offering significant improvements over existing techniques.

Abstract: Rock bolts are crucial components of the subterranean support systems in
underground mines that provide adequate structural reinforcement to the rock
mass to prevent unforeseen hazards like rockfalls. This makes frequent
assessments of such bolts critical for maintaining rock mass stability and
minimising risks in underground mining operations. Where manual surveying of
rock bolts is challenging due to the low light conditions in the underground
mines and the time-intensive nature of the process, automated detection of rock
bolts serves as a plausible solution. To that end, this study focuses on the
automatic identification of rock bolts within medium to large-scale 3D point
clouds obtained from underground mines using mobile laser scanners. Existing
techniques for automated rock bolt identification primarily rely on feature
engineering and traditional machine learning approaches. However, such
techniques lack robustness as these point clouds present several challenges due
to data noise, varying environments, and complex surrounding structures.
Moreover, the target rock bolts are extremely small objects within large-scale
point clouds and are often partially obscured due to the application of
reinforcement shotcrete. Addressing these challenges, this paper proposes an
approach termed DeepBolt, which employs a novel two-stage deep learning
architecture specifically designed for handling severe class imbalance for the
automatic and efficient identification of rock bolts in complex 3D point
clouds. The proposed method surpasses state-of-the-art semantic segmentation
models by up to 42.5% in Intersection over Union (IoU) for rock bolt points.
Additionally, it outperforms existing rock bolt identification techniques,
achieving a 96.41% precision and 96.96% recall in classifying rock bolts,
demonstrating its robustness and effectiveness in complex underground
environments.

</details>


### [130] [AI-assisted radiographic analysis in detecting alveolar bone-loss severity and patterns](https://arxiv.org/abs/2506.20522)
*Chathura Wimalasiri,Piumal Rathnayake,Shamod Wijerathne,Sumudu Rasnayaka,Dhanushka Leuke Bandara,Roshan Ragel,Vajira Thambawita,Isuru Nawinne*

Main category: cs.CV

TL;DR: An AI-based deep learning framework using YOLOv8 and Keypoint R-CNN models to automatically detect and quantify alveolar bone loss and its patterns in IOPA radiographs, achieving high accuracy and offering a rapid, objective tool for periodontal assessment.


<details>
  <summary>Details</summary>
Motivation: Accurate assessment of bone loss severity and pattern is critical for diagnosing and treating periodontitis, but current methods rely on subjective manual evaluation.

Method: Combines YOLOv8 for tooth detection, Keypoint R-CNN for anatomical landmarks, and YOLOv8x-seg for bone level segmentation and geometric analysis to classify bone loss patterns.

Result: Achieved high accuracy in detecting bone loss severity (ICC up to 0.80) and classifying patterns (87% accuracy) on a dataset of 1000 radiographs.

Conclusion: The AI framework provides a rapid, objective, and reproducible tool for periodontal assessment, improving early diagnosis and personalized treatment planning.

Abstract: Periodontitis, a chronic inflammatory disease causing alveolar bone loss,
significantly affects oral health and quality of life. Accurate assessment of
bone loss severity and pattern is critical for diagnosis and treatment
planning. In this study, we propose a novel AI-based deep learning framework to
automatically detect and quantify alveolar bone loss and its patterns using
intraoral periapical (IOPA) radiographs. Our method combines YOLOv8 for tooth
detection with Keypoint R-CNN models to identify anatomical landmarks, enabling
precise calculation of bone loss severity. Additionally, YOLOv8x-seg models
segment bone levels and tooth masks to determine bone loss patterns (horizontal
vs. angular) via geometric analysis. Evaluated on a large, expertly annotated
dataset of 1000 radiographs, our approach achieved high accuracy in detecting
bone loss severity (intra-class correlation coefficient up to 0.80) and bone
loss pattern classification (accuracy 87%). This automated system offers a
rapid, objective, and reproducible tool for periodontal assessment, reducing
reliance on subjective manual evaluation. By integrating AI into dental
radiographic analysis, our framework has the potential to improve early
diagnosis and personalized treatment planning for periodontitis, ultimately
enhancing patient care and clinical outcomes.

</details>


### [131] [Pay Less Attention to Deceptive Artifacts: Robust Detection of Compressed Deepfakes on Online Social Networks](https://arxiv.org/abs/2506.20548)
*Manyi Li,Renshuai Tao,Yufan Liu,Chuangchuang Tan,Haotong Qin,Bing Li,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: PLADA is a novel framework for deepfake detection, addressing block effects from compression in OSNs and improving detection with limited paired data.


<details>
  <summary>Details</summary>
Motivation: Deepfake images on OSNs are hard to detect due to compression artifacts (block effects) and lack of paired data. Existing methods ignore these challenges.

Method: PLADA includes Block Effect Eraser (B2E) for handling compression artifacts and Open Data Aggregation (ODA) for processing paired/unpaired data.

Result: PLADA outperforms state-of-the-art methods across 26 datasets, even with limited paired data and compression.

Conclusion: PLADA introduces block effects as a key factor in deepfake detection, offering a robust solution for real-world OSN scenarios.

Abstract: With the rapid advancement of deep learning, particularly through generative
adversarial networks (GANs) and diffusion models (DMs), AI-generated images, or
``deepfakes", have become nearly indistinguishable from real ones. These images
are widely shared across Online Social Networks (OSNs), raising concerns about
their misuse. Existing deepfake detection methods overlook the ``block effects"
introduced by compression in OSNs, which obscure deepfake artifacts, and
primarily focus on raw images, rarely encountered in real-world scenarios. To
address these challenges, we propose PLADA (Pay Less Attention to Deceptive
Artifacts), a novel framework designed to tackle the lack of paired data and
the ineffective use of compressed images. PLADA consists of two core modules:
Block Effect Eraser (B2E), which uses a dual-stage attention mechanism to
handle block effects, and Open Data Aggregation (ODA), which processes both
paired and unpaired data to improve detection. Extensive experiments across 26
datasets demonstrate that PLADA achieves a remarkable balance in deepfake
detection, outperforming SoTA methods in detecting deepfakes on OSNs, even with
limited paired data and compression. More importantly, this work introduces the
``block effect" as a critical factor in deepfake detection, providing a robust
solution for open-world scenarios. Our code is available at
https://github.com/ManyiLee/PLADA.

</details>


### [132] [Lightweight Multi-Frame Integration for Robust YOLO Object Detection in Videos](https://arxiv.org/abs/2506.20550)
*Yitong Quan,Benjamin Kiefer,Martin Messmer,Andreas Zell*

Main category: cs.CV

TL;DR: A simple method to improve video object detection by stacking consecutive frames as input to a YOLO-based detector, leveraging temporal context without complex modifications.


<details>
  <summary>Details</summary>
Motivation: Single-frame detectors ignore temporal context, while existing video methods add complexity. Transient challenges like motion blur degrade performance.

Method: Stack multiple consecutive frames as input to a YOLO detector, supervising only the target frame's output.

Result: Improves robustness, especially for lightweight models, narrowing the gap between compact and heavy networks.

Conclusion: The method effectively leverages temporal information with minimal architectural changes, demonstrated on MOT20Det and BOAT360 datasets.

Abstract: Modern image-based object detection models, such as YOLOv7, primarily process
individual frames independently, thus ignoring valuable temporal context
naturally present in videos. Meanwhile, existing video-based detection methods
often introduce complex temporal modules, significantly increasing model size
and computational complexity. In practical applications such as surveillance
and autonomous driving, transient challenges including motion blur, occlusions,
and abrupt appearance changes can severely degrade single-frame detection
performance. To address these issues, we propose a straightforward yet highly
effective strategy: stacking multiple consecutive frames as input to a
YOLO-based detector while supervising only the output corresponding to a single
target frame. This approach leverages temporal information with minimal
modifications to existing architectures, preserving simplicity, computational
efficiency, and real-time inference capability. Extensive experiments on the
challenging MOT20Det and our BOAT360 datasets demonstrate that our method
improves detection robustness, especially for lightweight models, effectively
narrowing the gap between compact and heavy detection networks. Additionally,
we contribute the BOAT360 benchmark dataset, comprising annotated fisheye video
sequences captured from a boat, to support future research in multi-frame video
object detection in challenging real-world scenarios.

</details>


### [133] [AdvMIM: Adversarial Masked Image Modeling for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2506.20563)
*Lei Zhu,Jun Zhou,Rick Siow Mong Goh,Yong Liu*

Main category: cs.CV

TL;DR: Proposes adversarial masked image modeling to enhance transformer performance in semi-supervised medical image segmentation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Transformers require large labeled datasets, limiting their use in semi-supervised scenarios with scarce annotations. Existing methods struggle to train transformers effectively with limited labeled data.

Method: Introduces adversarial masked image modeling to create an auxiliary masked domain, using original and pseudo-labels to increase supervision. Includes theoretical analysis and adversarial training to reduce domain gaps.

Result: Outperforms existing methods on three public medical image segmentation datasets.

Conclusion: The method effectively leverages transformers in semi-supervised settings, demonstrating significant performance improvements.

Abstract: Vision Transformer has recently gained tremendous popularity in medical image
segmentation task due to its superior capability in capturing long-range
dependencies. However, transformer requires a large amount of labeled data to
be effective, which hinders its applicability in annotation scarce
semi-supervised learning scenario where only limited labeled data is available.
State-of-the-art semi-supervised learning methods propose combinatorial
CNN-Transformer learning to cross teach a transformer with a convolutional
neural network, which achieves promising results. However, it remains a
challenging task to effectively train the transformer with limited labeled
data. In this paper, we propose an adversarial masked image modeling method to
fully unleash the potential of transformer for semi-supervised medical image
segmentation. The key challenge in semi-supervised learning with transformer
lies in the lack of sufficient supervision signal. To this end, we propose to
construct an auxiliary masked domain from original domain with masked image
modeling and train the transformer to predict the entire segmentation mask with
masked inputs to increase supervision signal. We leverage the original labels
from labeled data and pseudo-labels from unlabeled data to learn the masked
domain. To further benefit the original domain from masked domain, we provide a
theoretical analysis of our method from a multi-domain learning perspective and
devise a novel adversarial training loss to reduce the domain gap between the
original and masked domain, which boosts semi-supervised learning performance.
We also extend adversarial masked image modeling to CNN network. Extensive
experiments on three public medical image segmentation datasets demonstrate the
effectiveness of our method, where our method outperforms existing methods
significantly. Our code is publicly available at
https://github.com/zlheui/AdvMIM.

</details>


### [134] [Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization](https://arxiv.org/abs/2506.20567)
*Zhiwang Zhang,Dong Xu,Wanli Ouyang,Chuanqi Tan*

Main category: cs.CV

TL;DR: Proposes a division-and-summarization (DaS) framework for dense video captioning, using event proposals and hierarchical LSTM with attention to generate descriptive sentences.


<details>
  <summary>Details</summary>
Motivation: To address dense video captioning by summarizing rich semantic descriptions from partitioned video segments into coherent sentences.

Method: Partitions videos into event proposals, extracts visual features, uses captioning for segments, and employs a two-stage LSTM with hierarchical attention for summarization.

Result: Demonstrates effectiveness on the ActivityNet Captions dataset.

Conclusion: The DaS framework effectively combines visual and semantic information for dense video captioning.

Abstract: In this work, we propose a division-and-summarization (DaS) framework for
dense video captioning. After partitioning each untrimmed long video as
multiple event proposals, where each event proposal consists of a set of short
video segments, we extract visual feature (e.g., C3D feature) from each segment
and use the existing image/video captioning approach to generate one sentence
description for this segment. Considering that the generated sentences contain
rich semantic descriptions about the whole event proposal, we formulate the
dense video captioning task as a visual cue aided sentence summarization
problem and propose a new two stage Long Short Term Memory (LSTM) approach
equipped with a new hierarchical attention mechanism to summarize all generated
sentences as one descriptive sentence with the aid of visual features.
Specifically, the first-stage LSTM network takes all semantic words from the
generated sentences and the visual features from all segments within one event
proposal as the input, and acts as the encoder to effectively summarize both
semantic and visual information related to this event proposal. The
second-stage LSTM network takes the output from the first-stage LSTM network
and the visual features from all video segments within one event proposal as
the input, and acts as the decoder to generate one descriptive sentence for
this event proposal. Our comprehensive experiments on the ActivityNet Captions
dataset demonstrate the effectiveness of our newly proposed DaS framework for
dense video captioning.

</details>


### [135] [Causal Representation Learning with Observational Grouping for CXR Classification](https://arxiv.org/abs/2506.20582)
*Rajat Rasal,Avinash Kori,Ben Glocker*

Main category: cs.CV

TL;DR: The paper introduces a method for learning identifiable causal representations in medical imaging to improve disease classification in chest X-rays by grouping observations.


<details>
  <summary>Details</summary>
Motivation: To enhance the generalisability and robustness of task-specific latent features in medical imaging by uncovering true causal relationships.

Method: An end-to-end framework that groups observations to enforce invariance with respect to race, sex, and imaging views.

Result: Causal representations improve generalisability and robustness across multiple classification tasks.

Conclusion: Grouping observations to learn identifiable causal representations is effective for improving disease classification in medical imaging.

Abstract: Identifiable causal representation learning seeks to uncover the true causal
relationships underlying a data generation process. In medical imaging, this
presents opportunities to improve the generalisability and robustness of
task-specific latent features. This work introduces the concept of grouping
observations to learn identifiable representations for disease classification
in chest X-rays via an end-to-end framework. Our experiments demonstrate that
these causal representations improve generalisability and robustness across
multiple classification tasks when grouping is used to enforce invariance w.r.t
race, sex, and imaging views.

</details>


### [136] [Dense Video Captioning using Graph-based Sentence Summarization](https://arxiv.org/abs/2506.20583)
*Zhiwang Zhang,Dong Xu,Wanli Ouyang,Luping Zhou*

Main category: cs.CV

TL;DR: The paper proposes a graph-based partition-and-summarization (GPaS) framework for dense video captioning, addressing scene evolution in long event proposals by splitting and summarizing descriptions.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to adequately explore scene evolution in long event proposals, leading to suboptimal performance when scenes and objects change over time.

Method: The GPaS framework splits event proposals into shorter segments for finer captioning and summarizes these into one sentence using a GCN-LSTM interaction module to exploit semantic word relationships.

Result: The approach outperforms state-of-the-art methods on ActivityNet Captions and YouCook II datasets.

Conclusion: The GPaS framework effectively addresses scene evolution in dense video captioning, improving performance through partition and summarization.

Abstract: Recently, dense video captioning has made attractive progress in detecting
and captioning all events in a long untrimmed video. Despite promising results
were achieved, most existing methods do not sufficiently explore the scene
evolution within an event temporal proposal for captioning, and therefore
perform less satisfactorily when the scenes and objects change over a
relatively long proposal. To address this problem, we propose a graph-based
partition-and-summarization (GPaS) framework for dense video captioning within
two stages. For the ``partition" stage, a whole event proposal is split into
short video segments for captioning at a finer level. For the ``summarization"
stage, the generated sentences carrying rich description information for each
segment are summarized into one sentence to describe the whole event. We
particularly focus on the ``summarization" stage, and propose a framework that
effectively exploits the relationship between semantic words for summarization.
We achieve this goal by treating semantic words as nodes in a graph and
learning their interactions by coupling Graph Convolutional Network (GCN) and
Long Short Term Memory (LSTM), with the aid of visual cues. Two schemes of
GCN-LSTM Interaction (GLI) modules are proposed for seamless integration of GCN
and LSTM. The effectiveness of our approach is demonstrated via an extensive
comparison with the state-of-the-arts methods on the two benchmarks ActivityNet
Captions dataset and YouCook II dataset.

</details>


### [137] [Learning-Based Distance Estimation for 360° Single-Sensor Setups](https://arxiv.org/abs/2506.20586)
*Yitong Quan,Benjamin Kiefer,Martin Messmer,Andreas Zell*

Main category: cs.CV

TL;DR: A neural network-based method for monocular distance estimation using a 360° fisheye lens camera outperforms traditional geometric techniques, offering robustness and adaptability.


<details>
  <summary>Details</summary>
Motivation: Traditional geometric methods struggle with lens distortions and environmental variability in omnidirectional imaging, necessitating a more robust solution.

Method: Proposes a learning-based approach using a neural network to infer object distances directly from raw omnidirectional inputs, bypassing the need for precise lens calibration.

Result: The method outperforms traditional geometry-based techniques and other learning baselines in accuracy and robustness across three diverse 360° datasets.

Conclusion: Deep learning shows promise for real-time omnidirectional distance estimation, making it suitable for low-cost robotics, autonomous navigation, and surveillance applications.

Abstract: Accurate distance estimation is a fundamental challenge in robotic
perception, particularly in omnidirectional imaging, where traditional
geometric methods struggle with lens distortions and environmental variability.
In this work, we propose a neural network-based approach for monocular distance
estimation using a single 360{\deg} fisheye lens camera. Unlike classical
trigonometric techniques that rely on precise lens calibration, our method
directly learns and infers the distance of objects from raw omnidirectional
inputs, offering greater robustness and adaptability across diverse conditions.
We evaluate our approach on three 360{\deg} datasets (LOAF, ULM360, and a newly
captured dataset Boat360), each representing distinct environmental and sensor
setups. Our experimental results demonstrate that the proposed learning-based
model outperforms traditional geometry-based methods and other learning
baselines in both accuracy and robustness. These findings highlight the
potential of deep learning for real-time omnidirectional distance estimation,
making our approach particularly well-suited for low-cost applications in
robotics, autonomous navigation, and surveillance.

</details>


### [138] [TRIM: A Self-Supervised Video Summarization Framework Maximizing Temporal Relative Information and Representativeness](https://arxiv.org/abs/2506.20588)
*Pritam Mishra,Coloma Ballester,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: A self-supervised video summarization model using Markov process-driven loss metrics and a two-stage learning paradigm achieves state-of-the-art performance without relying on attention or supervised annotations.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of current methods (dependency on annotations or attention-based models) and improve cross-domain applicability.

Method: Introduces a self-supervised model with Markov process-driven loss metrics and a two-stage learning paradigm, avoiding attention, RNNs, or transformers.

Result: Outperforms unsupervised methods on SUMME and TVSUM datasets and rivals supervised models.

Conclusion: Demonstrates the potential of efficient, annotation-free architectures, challenging reliance on complex models.

Abstract: The increasing ubiquity of video content and the corresponding demand for
efficient access to meaningful information have elevated video summarization
and video highlights as a vital research area. However, many state-of-the-art
methods depend heavily either on supervised annotations or on attention-based
models, which are computationally expensive and brittle in the face of
distribution shifts that hinder cross-domain applicability across datasets. We
introduce a pioneering self-supervised video summarization model that captures
both spatial and temporal dependencies without the overhead of attention, RNNs,
or transformers. Our framework integrates a novel set of Markov process-driven
loss metrics and a two-stage self supervised learning paradigm that ensures
both performance and efficiency. Our approach achieves state-of-the-art
performance on the SUMME and TVSUM datasets, outperforming all existing
unsupervised methods. It also rivals the best supervised models, demonstrating
the potential for efficient, annotation-free architectures. This paves the way
for more generalizable video summarization techniques and challenges the
prevailing reliance on complex architectures.

</details>


### [139] [WonderFree: Enhancing Novel View Quality and Cross-View Consistency for 3D Scene Exploration](https://arxiv.org/abs/2506.20590)
*Chaojun Ni,Jie Li,Haoyun Li,Hengyu Liu,Xiaofeng Wang,Zheng Zhu,Guosheng Zhao,Boyuan Wang,Chenxin Li,Guan Huang,Wenjun Mei*

Main category: cs.CV

TL;DR: WonderFree enables interactive 3D scene generation from a single image, addressing challenges like novel view quality and cross-view consistency with WorldRestorer and ConsistView.


<details>
  <summary>Details</summary>
Motivation: Current 3D generation methods lack explorability and struggle with rendering quality beyond the original viewpoint, especially in unseen areas.

Method: Proposes WonderFree, combining WorldRestorer for novel view quality and ConsistView for cross-view consistency, with automated data collection.

Result: Improves rendering quality and coherence, with a 77.20% user preference over WonderWorld, validated by CLIP metrics.

Conclusion: WonderFree offers a seamless, immersive 3D exploration experience, with plans to release code, model, and data.

Abstract: Interactive 3D scene generation from a single image has gained significant
attention due to its potential to create immersive virtual worlds. However, a
key challenge in current 3D generation methods is the limited explorability,
which cannot render high-quality images during larger maneuvers beyond the
original viewpoint, particularly when attempting to move forward into unseen
areas. To address this challenge, we propose WonderFree, the first model that
enables users to interactively generate 3D worlds with the freedom to explore
from arbitrary angles and directions. Specifically, we decouple this challenge
into two key subproblems: novel view quality, which addresses visual artifacts
and floating issues in novel views, and cross-view consistency, which ensures
spatial consistency across different viewpoints. To enhance rendering quality
in novel views, we introduce WorldRestorer, a data-driven video restoration
model designed to eliminate floaters and artifacts. In addition, a data
collection pipeline is presented to automatically gather training data for
WorldRestorer, ensuring it can handle scenes with varying styles needed for 3D
scene generation. Furthermore, to improve cross-view consistency, we propose
ConsistView, a multi-view joint restoration mechanism that simultaneously
restores multiple perspectives while maintaining spatiotemporal coherence.
Experimental results demonstrate that WonderFree not only enhances rendering
quality across diverse viewpoints but also significantly improves global
coherence and consistency. These improvements are confirmed by CLIP-based
metrics and a user study showing a 77.20% preference for WonderFree over
WonderWorld enabling a seamless and immersive 3D exploration experience. The
code, model, and data will be publicly available.

</details>


### [140] [MMSearch-R1: Incentivizing LMMs to Search](https://arxiv.org/abs/2506.20670)
*Jinming Wu,Zihao Deng,Wei Li,Yiding Liu,Bo You,Bo Li,Zejun Ma,Ziwei Liu*

Main category: cs.CV

TL;DR: MMSearch-R1 is a reinforcement learning framework for LMMs to perform efficient, on-demand multimodal searches, outperforming RAG-based methods with fewer search calls.


<details>
  <summary>Details</summary>
Motivation: Existing methods like RAG and search agents are inefficient for dynamic real-world information, necessitating a flexible, outcome-driven approach.

Method: The framework uses reinforcement learning with image/text search tools, guided by outcome-based rewards and penalties, trained on a curated multimodal VQA dataset.

Result: MMSearch-R1 outperforms same-size RAG models and matches larger ones while reducing search calls by 30%.

Conclusion: The framework advances multimodal search research by demonstrating efficient, on-demand search behavior.

Abstract: Robust deployment of large multimodal models (LMMs) in real-world scenarios
requires access to external knowledge sources, given the complexity and dynamic
nature of real-world information. Existing approaches such as
retrieval-augmented generation (RAG) and prompt engineered search agents rely
on rigid pipelines, often leading to inefficient or excessive search behaviors.
We present MMSearch-R1, the first end-to-end reinforcement learning framework
that enables LMMs to perform on-demand, multi-turn search in real-world
Internet environments. Our framework integrates both image and text search
tools, allowing the model to reason about when and how to invoke them guided by
an outcome-based reward with a search penalty. To support training, We collect
a multimodal search VQA dataset through a semi-automated pipeline that covers
diverse visual and textual knowledge needs and curate a search-balanced subset
with both search-required and search-free samples, which proves essential for
shaping efficient and on-demand search behavior. Extensive experiments on
knowledge-intensive and info-seeking VQA tasks show that our model not only
outperforms RAG-based baselines of the same model size, but also matches the
performance of a larger RAG-based model while reducing search calls by over
30%. We further analyze key empirical findings to offer actionable insights for
advancing research in multimodal search.

</details>


### [141] [SFNet: Fusion of Spatial and Frequency-Domain Features for Remote Sensing Image Forgery Detection](https://arxiv.org/abs/2506.20599)
*Ji Qi,Xinchang Zhang,Dingqi Ye,Yongjia Ruan,Xin Guo,Shaowen Wang,Haifeng Li*

Main category: cs.CV

TL;DR: SFNet is a novel forgery detection framework for fake remote sensing imagery (RSI) that combines spatial and frequency domain features, outperforming existing methods by 4%-15.18% in accuracy.


<details>
  <summary>Details</summary>
Motivation: The rise of sophisticated generative AI makes fake RSI harder to detect, risking misinformation. Existing methods, relying on single visual cues, fail to generalize across diverse RSI data.

Method: SFNet uses two feature extractors for spatial and frequency domains, aligns and fuses them via domain feature mapping and refinement modules (CBAM attention).

Result: SFNet improves accuracy by 4%-15.18% over state-of-the-art methods and shows robust generalization across three datasets.

Conclusion: SFNet effectively addresses the limitations of single-cue detectors by leveraging multi-domain features, enhancing forgery detection in diverse RSI.

Abstract: The rapid advancement of generative artificial intelligence is producing fake
remote sensing imagery (RSI) that is increasingly difficult to detect,
potentially leading to erroneous intelligence, fake news, and even conspiracy
theories. Existing forgery detection methods typically rely on single visual
features to capture predefined artifacts, such as spatial-domain cues to detect
forged objects like roads or buildings in RSI, or frequency-domain features to
identify artifacts from up-sampling operations in adversarial generative
networks (GANs). However, the nature of artifacts can significantly differ
depending on geographic terrain, land cover types, or specific features within
the RSI. Moreover, these complex artifacts evolve as generative models become
more sophisticated. In short, over-reliance on a single visual cue makes
existing forgery detectors struggle to generalize across diverse remote sensing
data. This paper proposed a novel forgery detection framework called SFNet,
designed to identify fake images in diverse remote sensing data by leveraging
spatial and frequency domain features. Specifically, to obtain rich and
comprehensive visual information, SFNet employs two independent feature
extractors to capture spatial and frequency domain features from input RSIs. To
fully utilize the complementary domain features, the domain feature mapping
module and the hybrid domain feature refinement module(CBAM attention) of SFNet
are designed to successively align and fuse the multi-domain features while
suppressing redundant information. Experiments on three datasets show that
SFNet achieves an accuracy improvement of 4%-15.18% over the state-of-the-art
RS forgery detection methods and exhibits robust generalization capabilities.
The code is available at https://github.com/GeoX-Lab/RSTI/tree/main/SFNet.

</details>


### [142] [Video Perception Models for 3D Scene Synthesis](https://arxiv.org/abs/2506.20601)
*Rui Huang,Guangyao Zhai,Zuria Bauer,Marc Pollefeys,Federico Tombari,Leonidas Guibas,Gao Huang,Francis Engelmann*

Main category: cs.CV

TL;DR: VIPScene leverages video generation models for 3D scene synthesis, addressing limitations of LLMs and image-based methods by ensuring coherence and consistency.


<details>
  <summary>Details</summary>
Motivation: Automating 3D scene synthesis benefits fields like architecture and gaming, but current methods (LLMs, image generation) lack spatial reasoning and multi-view consistency.

Method: VIPScene integrates video generation, 3D reconstruction, and open-vocabulary perception models, using text/image prompts for flexible, realistic scene synthesis. Introduces FPVScore for evaluation.

Result: VIPScene outperforms existing methods, achieving high realism and structural consistency across diverse scenarios.

Conclusion: VIPScene offers a novel, effective approach to 3D scene synthesis, with potential for broad application.

Abstract: Traditionally, 3D scene synthesis requires expert knowledge and significant
manual effort. Automating this process could greatly benefit fields such as
architectural design, robotics simulation, virtual reality, and gaming. Recent
approaches to 3D scene synthesis often rely on the commonsense reasoning of
large language models (LLMs) or strong visual priors of modern image generation
models. However, current LLMs demonstrate limited 3D spatial reasoning ability,
which restricts their ability to generate realistic and coherent 3D scenes.
Meanwhile, image generation-based methods often suffer from constraints in
viewpoint selection and multi-view inconsistencies. In this work, we present
Video Perception models for 3D Scene synthesis (VIPScene), a novel framework
that exploits the encoded commonsense knowledge of the 3D physical world in
video generation models to ensure coherent scene layouts and consistent object
placements across views. VIPScene accepts both text and image prompts and
seamlessly integrates video generation, feedforward 3D reconstruction, and
open-vocabulary perception models to semantically and geometrically analyze
each object in a scene. This enables flexible scene synthesis with high realism
and structural consistency. For more precise analysis, we further introduce
First-Person View Score (FPVScore) for coherence and plausibility evaluation,
utilizing continuous first-person perspective to capitalize on the reasoning
ability of multimodal large language models. Extensive experiments show that
VIPScene significantly outperforms existing methods and generalizes well across
diverse scenarios. The code will be released.

</details>


### [143] [Shape2Animal: Creative Animal Generation from Natural Silhouettes](https://arxiv.org/abs/2506.20616)
*Quoc-Duy Tran,Anh-Tuan Vo,Dinh-Khoi Vo,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: Shape2Animal is a framework that mimics human pareidolia by transforming natural object silhouettes into animal forms using vision-language models and text-to-image diffusion.


<details>
  <summary>Details</summary>
Motivation: To replicate the human ability to perceive meaningful patterns (pareidolia) in ambiguous stimuli like clouds or stones.

Method: Uses open-vocabulary segmentation to extract silhouettes, interprets animal concepts with vision-language models, and synthesizes images via text-to-image diffusion, blending them into scenes.

Result: Demonstrated robustness and creative potential on diverse real-world inputs.

Conclusion: Shape2Animal opens new opportunities for visual storytelling, education, digital art, and interactive media.

Abstract: Humans possess a unique ability to perceive meaningful patterns in ambiguous
stimuli, a cognitive phenomenon known as pareidolia. This paper introduces
Shape2Animal framework to mimics this imaginative capacity by reinterpreting
natural object silhouettes, such as clouds, stones, or flames, as plausible
animal forms. Our automated framework first performs open-vocabulary
segmentation to extract object silhouette and interprets semantically
appropriate animal concepts using vision-language models. It then synthesizes
an animal image that conforms to the input shape, leveraging text-to-image
diffusion model and seamlessly blends it into the original scene to generate
visually coherent and spatially consistent compositions. We evaluated
Shape2Animal on a diverse set of real-world inputs, demonstrating its
robustness and creative potential. Our Shape2Animal can offer new opportunities
for visual storytelling, educational content, digital art, and interactive
media design. Our project page is here: https://shape2image.github.io

</details>


### [144] [Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects](https://arxiv.org/abs/2506.20638)
*Clément Forray,Pauline Delporte,Nicolas Delaygue,Florence Genin,Dawa Derksen*

Main category: cs.CV

TL;DR: The paper explores using Neural Radiance Fields (NeRF) for 3D reconstruction of space objects from simulated images, focusing on joint optimization of camera poses and NeRF.


<details>
  <summary>Details</summary>
Motivation: Improving Space Situational Awareness (SSA) for applications like debris removal and anomaly detection by leveraging 3D models of non-cooperative space objects.

Method: Uses NeRF for 3D reconstruction, addressing challenges like monochromatic images and unknown object orientation. Focuses on joint optimization of camera poses and NeRF, with regularization to prevent large pose deviations.

Result: Most accurate 3D reconstruction achieved by training with successive images one-by-one and optimizing uniform rotation for camera poses.

Conclusion: Joint optimization of camera poses and NeRF, with regularization, enhances 3D reconstruction accuracy for space objects under challenging conditions.

Abstract: Obtaining a better knowledge of the current state and behavior of objects
orbiting Earth has proven to be essential for a range of applications such as
active debris removal, in-orbit maintenance, or anomaly detection. 3D models
represent a valuable source of information in the field of Space Situational
Awareness (SSA). In this work, we leveraged Neural Radiance Fields (NeRF) to
perform 3D reconstruction of non-cooperative space objects from simulated
images. This scenario is challenging for NeRF models due to unusual camera
characteristics and environmental conditions : mono-chromatic images, unknown
object orientation, limited viewing angles, absence of diffuse lighting etc. In
this work we focus primarly on the joint optimization of camera poses alongside
the NeRF. Our experimental results show that the most accurate 3D
reconstruction is achieved when training with successive images one-by-one. We
estimate camera poses by optimizing an uniform rotation and use regularization
to prevent successive poses from being too far apart.

</details>


### [145] [Disentangled representations of microscopy images](https://arxiv.org/abs/2506.20649)
*Jacopo Dapueto,Vito Paolo Pastore,Nicoletta Noceti,Francesca Odone*

Main category: cs.CV

TL;DR: A Disentangled Representation Learning (DRL) method is proposed to improve interpretability in microscopy image classification, balancing accuracy and interpretability across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: The need for interpretable deep learning methods in microscopy image analysis, given the increasing volume of images and the challenge of interpretability in neural networks.

Method: A DRL framework, leveraging synthetic data to learn representations, applied to microscopy image classification across three domains: plankton, yeast vacuoles, and human cells.

Result: The DRL framework achieves a good trade-off between accuracy and interpretability in microscopy image classification.

Conclusion: DRL enhances interpretability in microscopy image analysis while maintaining performance, addressing a critical challenge in the field.

Abstract: Microscopy image analysis is fundamental for different applications, from
diagnosis to synthetic engineering and environmental monitoring. Modern
acquisition systems have granted the possibility to acquire an escalating
amount of images, requiring a consequent development of a large collection of
deep learning-based automatic image analysis methods. Although deep neural
networks have demonstrated great performance in this field, interpretability,
an essential requirement for microscopy image analysis, remains an open
challenge.
  This work proposes a Disentangled Representation Learning (DRL) methodology
to enhance model interpretability for microscopy image classification.
Exploiting benchmark datasets from three different microscopic image domains
(plankton, yeast vacuoles, and human cells), we show how a DRL framework, based
on transferring a representation learnt from synthetic data, can provide a good
trade-off between accuracy and interpretability in this domain.

</details>


### [146] [IPFormer: Visual 3D Panoptic Scene Completion with Context-Adaptive Instance Proposals](https://arxiv.org/abs/2506.20671)
*Markus Gross,Aya Fahmy,Danit Niwattananan,Dominik Muhle,Rui Song,Daniel Cremers,Henri Meeß*

Main category: cs.CV

TL;DR: IPFormer introduces context-adaptive instance proposals for vision-based 3D Panoptic Scene Completion (PSC), outperforming state-of-the-art methods in panoptic metrics and runtime efficiency.


<details>
  <summary>Details</summary>
Motivation: Current PSC methods lack dynamic adaptation to observed scenes, especially in vision-based approaches, limiting performance.

Method: IPFormer adaptively initializes queries as panoptic instance proposals from image context and refines them via attention-based encoding/decoding.

Result: IPFormer achieves superior PQ$^\dagger$ and PQ-All metrics, 14× runtime reduction, and notable improvements in Thing-metrics.

Conclusion: Context-adaptive instance proposals significantly enhance vision-based PSC, setting a new benchmark for the field.

Abstract: Semantic Scene Completion (SSC) has emerged as a pivotal approach for jointly
learning scene geometry and semantics, enabling downstream applications such as
navigation in mobile robotics. The recent generalization to Panoptic Scene
Completion (PSC) advances the SSC domain by integrating instance-level
information, thereby enhancing object-level sensitivity in scene understanding.
While PSC was introduced using LiDAR modality, methods based on camera images
remain largely unexplored. Moreover, recent Transformer-based SSC approaches
utilize a fixed set of learned queries to reconstruct objects within the scene
volume. Although these queries are typically updated with image context during
training, they remain static at test time, limiting their ability to
dynamically adapt specifically to the observed scene. To overcome these
limitations, we propose IPFormer, the first approach that leverages
context-adaptive instance proposals at train and test time to address
vision-based 3D Panoptic Scene Completion. Specifically, IPFormer adaptively
initializes these queries as panoptic instance proposals derived from image
context and further refines them through attention-based encoding and decoding
to reason about semantic instance-voxel relationships. Experimental results
show that our approach surpasses state-of-the-art methods in overall panoptic
metrics PQ$^\dagger$ and PQ-All, matches performance in individual metrics, and
achieves a runtime reduction exceeding 14$\times$. Furthermore, our ablation
studies reveal that dynamically deriving instance proposals from image context,
as opposed to random initialization, leads to a 3.62% increase in PQ-All and a
remarkable average improvement of 18.65% in combined Thing-metrics. These
results highlight our introduction of context-adaptive instance proposals as a
pioneering effort in addressing vision-based 3D Panoptic Scene Completion.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [147] [CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation](https://arxiv.org/abs/2506.19952)
*Deepon Halder,Thanmay Jayakumar,Raj Dabre*

Main category: cs.CL

TL;DR: CycleDistill uses LLMs to generate synthetic parallel corpora from monolingual data, improving MT quality for low-resource languages without needing extensive parallel data.


<details>
  <summary>Details</summary>
Motivation: Parallel corpora are scarce for low-resource languages, limiting MT quality. LLMs can help bridge this gap.

Method: CycleDistill iteratively generates synthetic parallel data via few-shot MT and fine-tunes the model, using monolingual corpora.

Result: Achieves 20-30 chrF point improvements over few-shot baselines for Indian languages. Softmax activations offer mild gains.

Conclusion: CycleDistill effectively leverages LLMs and monolingual data to enhance MT for low-resource languages.

Abstract: Large language models (LLMs), despite their ability to perform few-shot
machine translation (MT), often lag behind dedicated MT systems trained on
parallel corpora, which are crucial for high quality machine translation (MT).
However, parallel corpora are often scarce or non-existent for low-resource
languages. In this paper, we propose CycleDistill, a bootstrapping approach
leveraging LLMs and few-shot translation to obtain high-quality MT systems.
CycleDistill involves iteratively generating synthetic parallel corpora from
monolingual corpora via zero- or few-shot MT, which is then used to fine-tune
the model that was used for generating said data for MT. CycleDistill does not
need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments
focusing on three Indian languages, by relying solely on monolingual corpora,
it can achieve high-quality machine translation, improving upon a few-shot
baseline model by over 20-30 chrF points on average in the first iteration. We
also study the effect of leveraging softmax activations during the distillation
process and observe mild improvements in translation quality.

</details>


### [148] [Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs](https://arxiv.org/abs/2506.19967)
*Travis Thompson,Seung-Hwan Lim,Paul Liu,Ruoying He,Dongkuan Xu*

Main category: cs.CL

TL;DR: Inference-Scaled GraphRAG improves LLM reasoning on knowledge graphs by combining sequential and parallel scaling, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: LLMs underperform on knowledge-intensive tasks due to limited structured context and multi-hop reasoning. Existing RAG methods fail to capture relational structure in knowledge graphs.

Method: Introduces Inference-Scaled GraphRAG, using inference-time compute scaling with sequential (deep chain-of-thought traversal) and parallel (majority voting over trajectories) approaches.

Result: Significant improvement in multi-hop question answering on GRBench, outperforming GraphRAG and prior baselines.

Conclusion: Inference-time scaling is a practical, architecture-agnostic solution for structured knowledge reasoning with LLMs.

Abstract: Large Language Models (LLMs) have achieved impressive capabilities in
language understanding and generation, yet they continue to underperform on
knowledge-intensive reasoning tasks due to limited access to structured context
and multi-hop information. Retrieval-Augmented Generation (RAG) partially
mitigates this by grounding generation in retrieved context, but conventional
RAG and GraphRAG methods often fail to capture relational structure across
nodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel
framework that enhances LLM-based graph reasoning by applying inference-time
compute scaling. Our method combines sequential scaling with deep
chain-of-thought graph traversal, and parallel scaling with majority voting
over sampled trajectories within an interleaved reasoning-execution loop.
Experiments on the GRBench benchmark demonstrate that our approach
significantly improves multi-hop question answering performance, achieving
substantial gains over both traditional GraphRAG and prior graph traversal
baselines. These findings suggest that inference-time scaling is a practical
and architecture-agnostic solution for structured knowledge reasoning with LLMs

</details>


### [149] [Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation](https://arxiv.org/abs/2506.19998)
*Xinyi Ni,Haonan Jian,Qiuyang Wang,Vedanshi Chetan Shah,Pengyu Hong*

Main category: cs.CL

TL;DR: Doc2Agent is a scalable pipeline for building agents that generate and refine executable tools from API documentation, improving performance and reducing costs.


<details>
  <summary>Details</summary>
Motivation: Most API-based agents rely on curated toolsets, failing to address the complexity of real-world APIs. Building agents for arbitrary domains is challenging due to unstructured documentation and parameter inference.

Method: Doc2Agent generates executable tools from API documentation and iteratively refines them using a code agent.

Result: Achieved a 55% performance improvement and 90% lower cost on WebArena benchmark. Demonstrated adaptability in glycomaterial science.

Conclusion: Doc2Agent provides a generalizable solution for scalable tool agent creation from unstructured API documentation.

Abstract: REST APIs play important roles in enriching the action space of web agents,
yet most API-based agents rely on curated and uniform toolsets that do not
reflect the complexity of real-world APIs. Building tool-using agents for
arbitrary domains remains a major challenge, as it requires reading
unstructured API documentation, testing APIs and inferring correct parameters.
We propose Doc2Agent, a scalable pipeline to build agents that can call
Python-based tools generated from API documentation. Doc2Agent generates
executable tools from API documentations and iteratively refines them using a
code agent. We evaluate our approach on real-world APIs, WebArena APIs, and
research APIs, producing validated tools. We achieved a 55\% relative
performance improvement with 90\% lower cost compared to direct API calling on
WebArena benchmark. A domain-specific agent built for glycomaterial science
further demonstrates the pipeline's adaptability to complex, knowledge-rich
tasks. Doc2Agent offers a generalizable solution for building tool agents from
unstructured API documentation at scale.

</details>


### [150] [A Modular Multitask Reasoning Framework Integrating Spatio-temporal Models and LLMs](https://arxiv.org/abs/2506.20073)
*Kethmi Hirushini Hettige,Jiahao Ji,Cheng Long,Shili Xiang,Gao Cong,Jingyuan Wang*

Main category: cs.CL

TL;DR: STReason integrates LLMs with spatio-temporal models for multi-task inference, outperforming baselines in complex reasoning tasks without task-specific finetuning.


<details>
  <summary>Details</summary>
Motivation: Existing spatio-temporal models lack multi-task inference and explanatory outputs, limiting real-world applicability.

Method: STReason uses in-context learning to decompose queries into modular programs, generating solutions and rationales.

Result: STReason outperforms advanced LLM baselines, excels in complex scenarios, and reduces expert workload.

Conclusion: STReason offers a promising direction for generalizable spatio-temporal reasoning systems.

Abstract: Spatio-temporal data mining plays a pivotal role in informed decision making
across diverse domains. However, existing models are often restricted to narrow
tasks, lacking the capacity for multi-task inference and complex long-form
reasoning that require generation of in-depth, explanatory outputs. These
limitations restrict their applicability to real-world, multi-faceted decision
scenarios. In this work, we introduce STReason, a novel framework that
integrates the reasoning strengths of large language models (LLMs) with the
analytical capabilities of spatio-temporal models for multi-task inference and
execution. Without requiring task-specific finetuning, STReason leverages
in-context learning to decompose complex natural language queries into modular,
interpretable programs, which are then systematically executed to generate both
solutions and detailed rationales. To facilitate rigorous evaluation, we
construct a new benchmark dataset and propose a unified evaluation framework
with metrics specifically designed for long-form spatio-temporal reasoning.
Experimental results show that STReason significantly outperforms advanced LLM
baselines across all metrics, particularly excelling in complex,
reasoning-intensive spatio-temporal scenarios. Human evaluations further
validate STReason's credibility and practical utility, demonstrating its
potential to reduce expert workload and broaden the applicability to real-world
spatio-temporal tasks. We believe STReason provides a promising direction for
developing more capable and generalizable spatio-temporal reasoning systems.

</details>


### [151] [SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization](https://arxiv.org/abs/2506.20081)
*Dhruv Gupta,Gayathri Ganesh Lakshmy,Yiqing Xie*

Main category: cs.CL

TL;DR: The paper analyzes code retrieval in RACG, revealing biases towards surface-level features and well-documented code. It proposes SACL to mitigate these issues, improving retrieval and code generation performance.


<details>
  <summary>Details</summary>
Motivation: Current code retrievers rely too much on superficial textual features and biased documentation, limiting their effectiveness.

Method: Systematically mask features while preserving functionality, then introduce SACL to enrich textual and structural knowledge with semantics.

Result: SACL improves retrieval (e.g., 12.8% Recall@1 on HumanEval) and code generation (e.g., 4.88% Pass@1 on HumanEval).

Conclusion: SACL effectively reduces bias and enhances retrieval and code generation, demonstrating its practical value.

Abstract: Retrieval-Augmented Code Generation (RACG) is a critical technique for
enhancing code generation by retrieving relevant information. In this work, we
conduct an in-depth analysis of code retrieval by systematically masking
specific features while preserving code functionality. Our discoveries include:
(1) although trained on code, current retrievers heavily rely on surface-level
textual features (e.g., docstrings, identifier names), and (2) they exhibit a
strong bias towards well-documented code, even if the documentation is
irrelevant.Based on our discoveries, we propose SACL, a framework that enriches
textual information and reduces bias by augmenting code or structural knowledge
with semantic information. Extensive experiments show that SACL substantially
improves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on HumanEval /
MBPP / SWE-Bench-Lite), which also leads to better code generation performance
(e.g., by 4.88% Pass@1 on HumanEval).

</details>


### [152] [Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder](https://arxiv.org/abs/2506.20083)
*Yingji Zhang,Danilo S. Carvalho,André Freitas*

Main category: cs.CL

TL;DR: The paper explores integrating compositional and symbolic properties into distributional semantic spaces to improve Transformer-based LMs, focusing on semantic representation learning to bridge symbolic and distributional semantics. It reviews VAE, VQVAE, and SAE architectures for their latent geometries and semantic interpretability.


<details>
  <summary>Details</summary>
Motivation: To enhance interpretability, controllability, compositionality, and generalization of Transformer-based LMs by merging compositional and symbolic properties with distributional semantics.

Method: Reviews and compares three autoencoder architectures (VAE, VQVAE, SAE) to analyze their latent space geometries in relation to semantic structure and interpretability.

Result: Identifies how each architecture (VAE, VQVAE, SAE) shapes latent space geometry and impacts semantic interpretability, bridging symbolic and distributional semantics.

Conclusion: Semantic representation learning offers a promising direction to unify symbolic and distributional semantics, improving LM capabilities through structured latent spaces.

Abstract: Integrating compositional and symbolic properties into current distributional
semantic spaces can enhance the interpretability, controllability,
compositionality, and generalisation capabilities of Transformer-based
auto-regressive language models (LMs). In this survey, we offer a novel
perspective on latent space geometry through the lens of compositional
semantics, a direction we refer to as \textit{semantic representation
learning}. This direction enables a bridge between symbolic and distributional
semantics, helping to mitigate the gap between them. We review and compare
three mainstream autoencoder architectures-Variational AutoEncoder (VAE),
Vector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE)-and examine the
distinctive latent geometries they induce in relation to semantic structure and
interpretability.

</details>


### [153] [ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA with Large-Scale Multitask Dataset](https://arxiv.org/abs/2506.20093)
*Yilin Wang,Peixuan Lei,Jie Song,Yuzhe Hao,Tao Chen,Yuxuan Zhang,Lei Jia,Yuanxiang Li,Zhongyu Wei*

Main category: cs.CL

TL;DR: The paper introduces Time-Series QA and EngineMT-QA dataset, proposing ITFormer to integrate time-series and natural language for improved QA accuracy with minimal added parameters.


<details>
  <summary>Details</summary>
Motivation: The challenge of integrating high-dimensional time-series data with natural language for dynamic tasks motivates the creation of a new QA task and dataset.

Method: Proposes ITFormer, a framework combining time-series encoders with frozen LLMs to align and fuse temporal-textual features.

Result: ITFormer achieves significant QA accuracy improvements with less than 1% additional trainable parameters.

Conclusion: The work establishes a scalable paradigm for temporal-textual integration, enabling new multi-modal AI research and applications.

Abstract: Time-series data are critical in diverse applications, such as industrial
monitoring, medical diagnostics, and climate research. However, effectively
integrating these high-dimensional temporal signals with natural language for
dynamic, interactive tasks remains a significant challenge. To address this, we
introduce the Time-Series Question Answering (Time-Series QA) task and release
EngineMT-QA, the first large-scale, multi-task, temporal-textual QA dataset
designed to capture complex interactions between time-series signals and
natural language. Building on this resource, we propose the Instruct Time
Transformer (ITFormer), a novel framework that bridges time-series encoders
with frozen large language models (LLMs). ITFormer effectively extracts,
aligns, and fuses temporal and textual features, achieving a strong improvement
in QA accuracy over strong baselines with fewer than 1\% additional trainable
parameters. By combining computational efficiency with robust cross-modal
modeling, our work establishes a adaptable paradigm for integrating temporal
data with natural language, paving the way for new research and applications in
multi-modal AI. More details about the project, including datasets and code,
are available at: https://pandalin98.github.io/itformer_site/

</details>


### [154] [A Multi-Pass Large Language Model Framework for Precise and Efficient Radiology Report Error Detection](https://arxiv.org/abs/2506.20112)
*Songsoo Kim,Seungtae Lee,See Young Lee,Joonho Kim,Keechan Kan,Dukyong Yoon*

Main category: cs.CL

TL;DR: A three-pass LLM framework improves PPV and reduces costs for radiology report proofreading, outperforming simpler methods.


<details>
  <summary>Details</summary>
Motivation: To enhance the positive predictive value (PPV) of LLM-based proofreading in radiology reports and reduce operational costs, given the low error prevalence.

Method: Tested three LLM frameworks on 1,000 radiology reports: single-prompt detector, extractor plus detector, and extractor, detector, and false-positive verifier. Metrics included PPV, aTPR, and operational costs.

Result: Framework 3 significantly improved PPV (0.159 vs. 0.063/0.079) and reduced costs (USD 5.58 vs. 9.72/6.85 per 1,000 reports), with stable aTPR.

Conclusion: The three-pass LLM framework is effective for AI-assisted radiology report quality assurance, balancing performance and cost.

Abstract: Background: The positive predictive value (PPV) of large language model
(LLM)-based proofreading for radiology reports is limited due to the low error
prevalence. Purpose: To assess whether a three-pass LLM framework enhances PPV
and reduces operational costs compared with baseline approaches. Materials and
Methods: A retrospective analysis was performed on 1,000 consecutive radiology
reports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III
database. Two external datasets (CheXpert and Open-i) were validation sets.
Three LLM frameworks were tested: (1) single-prompt detector; (2) extractor
plus detector; and (3) extractor, detector, and false-positive verifier.
Precision was measured by PPV and absolute true positive rate (aTPR).
Efficiency was calculated from model inference charges and reviewer
remuneration. Statistical significance was tested using cluster bootstrap,
exact McNemar tests, and Holm-Bonferroni correction. Results: Framework PPV
increased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118,
Framework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs.
baselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per
1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and
USD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively.
Human-reviewed reports decreased from 192 to 88. External validation supported
Framework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR
(0.007). Conclusion: A three-pass LLM framework significantly enhanced PPV and
reduced operational costs, maintaining detection performance, providing an
effective strategy for AI-assisted radiology report quality assurance.

</details>


### [155] [Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability Estimation in Constructed-Response Tests](https://arxiv.org/abs/2506.20119)
*Masaki Uto,Yuma Ito*

Main category: cs.CL

TL;DR: A novel method combines automated scoring with IRT to accurately estimate learner abilities while reducing manual grading workload.


<details>
  <summary>Details</summary>
Motivation: The need to assess higher-order skills like expressive abilities and logical thinking is growing, but manual grading of constructed-response tests is labor-intensive and costly.

Method: The study proposes using automated scoring technologies to impute missing scores in IRT, improving accuracy for sparse or heterogeneous data.

Result: The method achieves high accuracy in ability estimation and significantly reduces the manual grading burden.

Conclusion: Automated scoring integrated with IRT offers an efficient solution for accurate ability assessment with less manual effort.

Abstract: Evaluating the abilities of learners is a fundamental objective in the field
of education. In particular, there is an increasing need to assess higher-order
abilities such as expressive skills and logical thinking. Constructed-response
tests such as short-answer and essay-based questions have become widely used as
a method to meet this demand. Although these tests are effective, they require
substantial manual grading, making them both labor-intensive and costly. Item
response theory (IRT) provides a promising solution by enabling the estimation
of ability from incomplete score data, where human raters grade only a subset
of answers provided by learners across multiple test items. However, the
accuracy of ability estimation declines as the proportion of missing scores
increases. Although data augmentation techniques for imputing missing scores
have been explored in order to address this limitation, they often struggle
with inaccuracy for sparse or heterogeneous data. To overcome these challenges,
this study proposes a novel method for imputing missing scores by leveraging
automated scoring technologies for accurate IRT-based ability estimation. The
proposed method achieves high accuracy in ability estimation while markedly
reducing manual grading workload.

</details>


### [156] [CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG Evaluation](https://arxiv.org/abs/2506.20128)
*Aashiq Muhamed*

Main category: cs.CL

TL;DR: The paper introduces CCRS, a novel suite of five metrics for evaluating RAG systems, leveraging a pretrained LLM for efficient, zero-shot assessment. It outperforms existing methods in discriminative power and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for RAG systems are either simplistic (lexical overlap) or inefficient (multi-stage pipelines). CCRS aims to address these gaps by providing a comprehensive yet practical framework.

Method: CCRS uses a pretrained LLM as a zero-shot judge to evaluate five metrics: Contextual Coherence, Question Relevance, Information Density, Answer Correctness, and Information Recall. It is tested on the BioASQ dataset with six RAG configurations.

Result: CCRS effectively discriminates between RAG system performances, showing Mistral-7B's superiority over Llama variants. It matches or surpasses RAGChecker in discriminative power while being more efficient.

Conclusion: CCRS offers a practical, efficient, and comprehensive framework for evaluating RAG systems, enabling iterative improvements without computational bottlenecks.

Abstract: RAG systems enhance LLMs by incorporating external knowledge, which is
crucial for domains that demand factual accuracy and up-to-date information.
However, evaluating the multifaceted quality of RAG outputs, spanning aspects
such as contextual coherence, query relevance, factual correctness, and
informational completeness, poses significant challenges. Existing evaluation
methods often rely on simple lexical overlap metrics, which are inadequate for
capturing these nuances, or involve complex multi-stage pipelines with
intermediate steps like claim extraction or require finetuning specialized
judge models, hindering practical efficiency. To address these limitations, we
propose CCRS (Contextual Coherence and Relevance Score), a novel suite of five
metrics that utilizes a single, powerful, pretrained LLM as a zero-shot,
end-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance
(QR), Information Density (ID), Answer Correctness (AC), and Information Recall
(IR). We apply CCRS to evaluate six diverse RAG system configurations on the
challenging BioASQ dataset. Our analysis demonstrates that CCRS effectively
discriminates between system performances, confirming, for instance, that the
Mistral-7B reader outperforms Llama variants. We provide a detailed analysis of
CCRS metric properties, including score distributions, convergent/discriminant
validity, tie rates, population statistics, and discriminative power. Compared
to the complex RAGChecker framework, CCRS offers comparable or superior
discriminative power for key aspects like recall and faithfulness, while being
significantly more computationally efficient. CCRS thus provides a practical,
comprehensive, and efficient framework for evaluating and iteratively improving
RAG systems.

</details>


### [157] [AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control](https://arxiv.org/abs/2506.20160)
*Ruosen Li,Ziming Luo,Quan Zhang,Ruochen Li,Ben Zhou,Ali Payani,Xinya Du*

Main category: cs.CL

TL;DR: AALC, a lightweight reward method, balances accuracy and brevity in large reasoning models, reducing response length by 50% while maintaining accuracy and refining reasoning patterns.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models incur high latency and cost due to lengthy chain-of-thoughts without proportional accuracy gains.

Method: AALC integrates an accuracy-aware length reward into reinforcement learning, delaying length penalty until target performance is met.

Result: Reduces response length by over 50% while maintaining or improving accuracy, curbing redundant reasoning patterns.

Conclusion: Reward-based strategies like AALC can guide models toward efficient, generalizable reasoning, though interpretability may decrease.

Abstract: Large reasoning models (LRMs) achieve impressive reasoning capabilities by
generating lengthy chain-of-thoughts, but this "overthinking" incurs high
latency and cost without commensurate accuracy gains. In this work, we
introduce AALC, a lightweight, accuracy-aware length reward integrated into
reinforcement learning that dynamically balances correctness and brevity during
training. By incorporating validation accuracy into the reward and employing a
smooth, dynamically scheduled length penalty, AALC delays length penalty until
target performance is met. Through extensive experiments across standard and
out-of-distribution math benchmarks, we show that our approach reduces response
length by over 50% while maintaining or even improving the original accuracy.
Furthermore, qualitative analysis reveals that our method curbs redundant
reasoning patterns such as excessive subgoal setting and verification, leading
to structurally refined outputs rather than naive truncation. We also identify
that efficiency gains are accompanied by reduced interpretability: models
trained with AALC omit some narrative framing and explanatory context. These
findings highlight the potential of reward-based strategies to guide LRMs
toward more efficient, generalizable reasoning paths.

</details>


### [158] [SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs](https://arxiv.org/abs/2506.20167)
*Fengze Li,Yue Wang,Yangle Liu,Ming Huang,Dou Hong,Jieming Ma*

Main category: cs.CL

TL;DR: SEED integrates structural encoding with LLMs for multivariate time series forecasting, bridging the gap between numerical patterns and semantic reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing models lack the ability to combine structural dependencies with semantic-level reasoning or task adaptation, limiting unified prediction systems.

Method: SEED uses a token-aware encoder, projection module, semantic reprogramming, and frozen LLM for embedding-driven decoding.

Result: SEED consistently outperforms baselines and addresses the structural-semantic modeling gap.

Conclusion: SEED successfully integrates structural and semantic modeling for improved multivariate time series forecasting.

Abstract: Multivariate time series forecasting requires models to simultaneously
capture variable-wise structural dependencies and generalize across diverse
tasks. While structural encoders are effective in modeling feature
interactions, they lack the capacity to support semantic-level reasoning or
task adaptation. Conversely, large language models (LLMs) possess strong
generalization capabilities but remain incompatible with raw time series
inputs. This gap limits the development of unified, transferable prediction
systems. Therefore, we introduce SEED, a structural encoder for
embedding-driven decoding, which integrates four stages: a token-aware encoder
for patch extraction, a projection module that aligns patches with language
model embeddings, a semantic reprogramming mechanism that maps patches to
task-aware prototypes, and a frozen language model for prediction. This modular
architecture decouples representation learning from inference, enabling
efficient alignment between numerical patterns and semantic reasoning.
Empirical results demonstrate that the proposed method achieves consistent
improvements over strong baselines, and comparative studies on various datasets
confirm SEED's role in addressing the structural-semantic modeling gap.

</details>


### [159] [COIN: Uncertainty-Guarding Selective Question Answering for Foundation Models with Provable Risk Guarantees](https://arxiv.org/abs/2506.20178)
*Zhiyuan Wang,Jinhao Duan,Qingni Wang,Xiaofeng Zhu,Tianlong Chen,Xiaoshuang Shi,Kaidi Xu*

Main category: cs.CL

TL;DR: COIN is a framework for uncertainty quantification in foundation models, ensuring FDR control while maximizing answer retention.


<details>
  <summary>Details</summary>
Motivation: Address limitations of heuristic UQ methods and SCP frameworks by providing formal guarantees for FDR in selective prediction.

Method: COIN calibrates thresholds using empirical error rates and confidence intervals (e.g., Clopper-Pearson) to filter answers under FDR constraints.

Result: COIN achieves robust FDR control, high answer retention, and efficiency with limited calibration data.

Conclusion: COIN is adaptable and extensible, improving uncertainty quantification for diverse text generation tasks.

Abstract: Uncertainty quantification (UQ) for foundation models is essential to
identify and mitigate potential hallucinations in automatically generated text.
However, heuristic UQ approaches lack formal guarantees for key metrics such as
the false discovery rate (FDR) in selective prediction. Previous work adopts
the split conformal prediction (SCP) framework to ensure desired coverage of
admissible answers by constructing prediction sets, but these sets often
contain incorrect candidates, limiting their practical utility. To address
this, we propose COIN, an uncertainty-guarding selection framework that
calibrates statistically valid thresholds to filter a single generated answer
per question under user-specified FDR constraints. COIN estimates the empirical
error rate on a calibration set and applies confidence interval methods such as
Clopper-Pearson to establish a high-probability upper bound on the true error
rate (i.e., FDR). This enables the selection of the largest uncertainty
threshold that ensures FDR control on test data while significantly increasing
sample retention. We demonstrate COIN's robustness in risk control, strong
test-time power in retaining admissible answers, and predictive efficiency
under limited calibration data across both general and multimodal text
generation tasks. Furthermore, we show that employing alternative upper bound
constructions and UQ strategies can further boost COIN's power performance,
which underscores its extensibility and adaptability to diverse application
scenarios.

</details>


### [160] [How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?](https://arxiv.org/abs/2506.20199)
*Mengqi Wang,Tiantian Feng,Shrikanth Narayanan*

Main category: cs.CL

TL;DR: The study explores improving conversational emotion recognition (CER) using LLMs by focusing on high-quality example retrieval in in-context learning (ICL), with augmented retrieval outperforming other methods.


<details>
  <summary>Details</summary>
Motivation: Creating high-accuracy applications with LLMs for subjective tasks like emotion recognition is challenging, prompting investigation into better CER approaches.

Method: Proposed strategies include random and augmented example retrieval in ICL, analyzing conversational context's impact on CER accuracy, tested on IEMOCAP, MELD, and EmoryNLP datasets.

Result: Augmented example retrieval consistently outperformed other techniques, emphasizing the value of coherent targeted examples and paraphrasing.

Conclusion: Augmented retrieval is key for enhancing CER in LLMs, demonstrating the importance of high-quality example selection and enhancement.

Abstract: Large language models (LLMs) have enabled a wide variety of real-world
applications in various domains. However, creating a high-performing
application with high accuracy remains challenging, particularly for subjective
tasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this
study investigates approaches to improving conversational emotion recognition
(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples
in in-context learning (ICL) to enhance CER. We propose various strategies
based on random and augmented example retrieval and also analyze the impact of
conversational context on CER accuracy. Experiments were conducted on the three
datasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented
example retrieval consistently outperforms other techniques under investigation
across all datasets, highlighting the importance of retrieving coherent
targeted examples and enhancing them through paraphrasing.

</details>


### [161] [Intrinsic vs. Extrinsic Evaluation of Czech Sentence Embeddings: Semantic Relevance Doesn't Help with MT Evaluation](https://arxiv.org/abs/2506.20203)
*Petra Barančíková,Ondřej Bojar*

Main category: cs.CL

TL;DR: The paper compares Czech-specific and multilingual sentence embedding models using intrinsic and extrinsic evaluations, revealing a disconnect between semantic similarity performance and downstream task results.


<details>
  <summary>Details</summary>
Motivation: To understand how well sentence embeddings capture linguistic phenomena and perform in practical tasks like machine translation evaluation.

Method: Intrinsic evaluation uses Costra and STS benchmarks; extrinsic evaluation involves fine-tuning models for COMET-based translation evaluation.

Result: Models excelling in semantic similarity tests don't always perform well in translation tasks, while others with over-smoothed embeddings achieve better results after fine-tuning.

Conclusion: The findings stress the need for research into 'operationalizable semantics' in embeddings and better downstream task datasets.

Abstract: In this paper, we compare Czech-specific and multilingual sentence embedding
models through intrinsic and extrinsic evaluation paradigms. For intrinsic
evaluation, we employ Costra, a complex sentence transformation dataset, and
several Semantic Textual Similarity (STS) benchmarks to assess the ability of
the embeddings to capture linguistic phenomena such as semantic similarity,
temporal aspects, and stylistic variations. In the extrinsic evaluation, we
fine-tune each embedding model using COMET-based metrics for machine
translation evaluation.
  Our experiments reveal an interesting disconnect: models that excel in
intrinsic semantic similarity tests do not consistently yield superior
performance on downstream translation evaluation tasks. Conversely, models with
seemingly over-smoothed embedding spaces can, through fine-tuning, achieve
excellent results. These findings highlight the complex relationship between
semantic property probes and downstream task, emphasizing the need for more
research into 'operationalizable semantics' in sentence embeddings, or more
in-depth downstream tasks datasets (here translation evaluation)

</details>


### [162] [Perspectives in Play: A Multi-Perspective Approach for More Inclusive NLP Systems](https://arxiv.org/abs/2506.20209)
*Benedetta Muscato,Lucia Passaro,Gizem Gezici,Fosca Giannotti*

Main category: cs.CL

TL;DR: The paper proposes a multi-perspective approach using soft labels in NLP to capture human disagreements, outperforming traditional aggregation methods in subjective tasks like hate speech and stance detection.


<details>
  <summary>Details</summary>
Motivation: Traditional NLP methods aggregate annotators' viewpoints into a single ground truth, often underrepresenting minority perspectives. This study aims to address this by valuing diverse individual opinions.

Method: The study introduces a multi-perspective approach with soft labels, tested on subjective tasks (hate speech, irony, abusive language, stance detection). Performance is measured using JSD and F1 scores.

Result: The multi-perspective approach better approximates human label distributions (lower JSD) and achieves higher F1 scores, though it shows lower confidence in highly subjective tasks like irony and stance detection.

Conclusion: The approach promotes inclusivity and pluralism in NLP models, with XAI revealing insights into model uncertainty and predictions.

Abstract: In the realm of Natural Language Processing (NLP), common approaches for
handling human disagreement consist of aggregating annotators' viewpoints to
establish a single ground truth. However, prior studies show that disregarding
individual opinions can lead can lead to the side effect of underrepresenting
minority perspectives, especially in subjective tasks, where annotators may
systematically disagree because of their preferences. Recognizing that labels
reflect the diverse backgrounds, life experiences, and values of individuals,
this study proposes a new multi-perspective approach using soft labels to
encourage the development of the next generation of perspective aware models,
more inclusive and pluralistic. We conduct an extensive analysis across diverse
subjective text classification tasks, including hate speech, irony, abusive
language, and stance detection, to highlight the importance of capturing human
disagreements, often overlooked by traditional aggregation methods. Results
show that the multi-perspective approach not only better approximates human
label distributions, as measured by Jensen-Shannon Divergence (JSD), but also
achieves superior classification performance (higher F1 scores), outperforming
traditional approaches. However, our approach exhibits lower confidence in
tasks like irony and stance detection, likely due to the inherent subjectivity
present in the texts. Lastly, leveraging Explainable AI (XAI), we explore model
uncertainty and uncover meaningful insights into model predictions.

</details>


### [163] [Enhancing Large Language Models through Structured Reasoning](https://arxiv.org/abs/2506.20241)
*Yubo Dong,Hehe Fan*

Main category: cs.CL

TL;DR: The paper introduces a structured reasoning approach to enhance LLMs, addressing their limitations in complex reasoning tasks by converting unstructured data into structured formats and using novel algorithms for training.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex reasoning due to reliance on implicit statistical relationships, lacking structured knowledge representation.

Method: Convert unstructured data to structured formats, train LLMs via Supervised Fine-Tuning, and enhance reasoning with Group Relative Policy Optimization (GRPO) using MAX-Flow and LCS algorithms.

Result: Improved reasoning effectiveness, reduced computational complexity, and robust performance across scenarios in the DeepSeek-R1-Distill-Qwen-1.5B model.

Conclusion: Structured reasoning integration significantly enhances LLM performance, validating the proposed approach.

Abstract: Recent Large Language Models (LLMs) have significantly advanced natural
language processing and automated decision-making. However, these models still
encounter difficulties when performing complex reasoning tasks involving
logical deduction and systematic planning, primarily due to their reliance on
implicit statistical relationships without structured knowledge
representation.Inspired by cognitive science and neurosymbolic AI, we introduce
a novel approach to enhance LLMs through explicit structured reasoning. First,
we convert unstructured data into structured formats by explicitly annotating
reasoning steps. We then employ this structured dataset to train LLMs through
Supervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning
capabilities of LLMs using Group Relative Policy Optimization (GRPO),
incorporating two innovative algorithms--MAX-Flow and Longest Common
Subsequence (LCS)--which notably improve reasoning effectiveness and reduce
computational complexity. Experimental results from fine-tuning a
DeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust
performance across various scenarios, and improved compatibility with
optimization techniques, validating the efficacy of structured reasoning
integration in LLMs.

</details>


### [164] [CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment](https://arxiv.org/abs/2506.20243)
*Papa Séga Wade,Mihai Andries,Ioannis Kanellos,Thierry Moudenc*

Main category: cs.CL

TL;DR: A chunk-based approach using self-supervised learning models (Wav2Vec2, HuBERT, WavLM) and a CNN-BiLSTM framework improves fluency assessment in non-native speakers by analyzing speech rhythm, pauses, and disfluencies.


<details>
  <summary>Details</summary>
Motivation: Automatic fluency assessment (AFA) is challenging, especially for non-native speakers, due to difficulties in capturing speech rhythm, pauses, and disfluencies.

Method: The method segments speech into breath-group chunks using Silero-VAD, fuses SSL embeddings with a weighted mechanism, and enriches them with fluency markers. A CNN-BiLSTM captures dependencies across chunks.

Result: The approach improves F1-score by 2.8 and Pearson correlation by 6.2 points on Speechocean762, and 4.2 F1-score and 4.0 Pearson points on Avalinguo, outperforming baselines.

Conclusion: Chunk-based multi-SSL fusion enhances fluency evaluation, but future work should address generalization to dialects with irregular prosody.

Abstract: Automatic fluency assessment (AFA) remains challenging, particularly in
capturing speech rhythm, pauses, and disfluencies in non-native speakers. We
introduce a chunk-based approach integrating self-supervised learning (SSL)
models (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths
in phonetic, prosodic, and noisy speech modeling, with a hierarchical
CNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero
voice activity detection (Silero-VAD), enabling fine-grained temporal analysis
while mitigating over-segmentation artifacts. SSL embeddings are fused via a
learnable weighted mechanism, balancing acoustic and linguistic features, and
enriched with chunk-level fluency markers (e.g., speech rate, pause durations,
n-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies
across chunks. Evaluated on Avalinguo and Speechocean762, our approach improves
F1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines
on Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on
Avalinguo, surpassing Pyannote.audio-based segmentation baselines. These
findings highlight chunk-based multi-SSL fusion for robust fluency evaluation,
though future work should explore generalization to dialects with irregular
prosody.

</details>


### [165] [Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and Large Language Models](https://arxiv.org/abs/2506.20269)
*Kai-Robin Lange,Tobias Schmidt,Matthias Reccius,Henrik Müller,Michael Roos,Carsten Jentsch*

Main category: cs.CL

TL;DR: The paper proposes combining Large Language Models (LLMs) with topic models to dynamically track narrative shifts over time, addressing scalability and cost issues of LLMs alone.


<details>
  <summary>Details</summary>
Motivation: To investigate how media narratives evolve over time, overcoming the limitations of current methods like LLMs, which are costly and computationally intensive for large corpora.

Method: A hybrid approach using topic models for large-scale analysis and LLMs for detailed narrative extraction, applied to Wall Street Journal articles (2009-2023).

Result: LLMs effectively detect narrative shifts when they exist but struggle to differentiate between content and narrative shifts.

Conclusion: The hybrid method offers a scalable solution for tracking narrative evolution, though further refinement is needed to distinguish content from narrative changes.

Abstract: With rapidly evolving media narratives, it has become increasingly critical
to not just extract narratives from a given corpus but rather investigate, how
they develop over time. While popular narrative extraction methods such as
Large Language Models do well in capturing typical narrative elements or even
the complex structure of a narrative, applying them to an entire corpus comes
with obstacles, such as a high financial or computational cost. We propose a
combination of the language understanding capabilities of Large Language Models
with the large scale applicability of topic models to dynamically model
narrative shifts across time using the Narrative Policy Framework. We apply a
topic model and a corresponding change point detection method to find changes
that concern a specific topic of interest. Using this model, we filter our
corpus for documents that are particularly representative of that change and
feed them into a Large Language Model that interprets the change that happened
in an automated fashion and distinguishes between content and narrative shifts.
We employ our pipeline on a corpus of The Wall Street Journal news paper
articles from 2009 to 2023. Our findings indicate that a Large Language Model
can efficiently extract a narrative shift if one exists at a given point in
time, but does not perform as well when having to decide whether a shift in
content or a narrative shift took place.

</details>


### [166] [Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content](https://arxiv.org/abs/2506.20331)
*Rian Touchent,Nathan Godey,Eric de la Clergerie*

Main category: cs.CL

TL;DR: Biomed-Enriched is a biomedical text dataset from PubMed, annotated for type, domain, and educational quality, enabling refined subsets for NLP tasks. It improves model performance in biomedical and clinical tasks.


<details>
  <summary>Details</summary>
Motivation: Clinical text is hard to access due to privacy constraints. The dataset provides an open, large-scale alternative for biomedical and clinical NLP.

Method: A two-stage annotation process: a large language model annotates 400K paragraphs, then a small model propagates labels across PMC-OA. Subsets are created via quality filtering and domain upsampling.

Result: Curated subsets improve model performance (e.g., ~5% boost on MMLU ProfMed). Combinations of techniques enable faster convergence and efficiency.

Conclusion: Biomed-Enriched is a valuable resource for biomedical NLP, offering efficient pretraining strategies and performance gains.

Abstract: We introduce Biomed-Enriched, a biomedical text dataset constructed from
PubMed via a two-stage annotation process. In the first stage, a large language
model annotates 400K paragraphs from PubMed scientific articles, assigning
scores for their type (review, study, clinical case, other), domain (clinical,
biomedical, other), and educational quality. The educational quality score
(rated 1 to 5) estimates how useful a paragraph is for college-level learning.
These annotations are then used to fine-tune a small language model, which
propagates the labels across the full PMC-OA corpus. The resulting metadata
allows us to extract refined subsets, including 2M clinical case paragraphs
with over 450K high-quality ones from articles with commercial-use licenses,
and to construct several variants via quality filtering and domain upsampling.
Clinical text is typically difficult to access due to privacy constraints, as
hospital records cannot be publicly shared. Hence, our dataset provides an
alternative large-scale, openly available collection of clinical cases from
PubMed, making it a valuable resource for biomedical and clinical NLP.
Preliminary continual-pretraining experiments with OLMo2 suggest these curated
subsets enable targeted improvements, with clinical upsampling boosting
performance by ~5% on MMLU ProfMed and educational quality filtering improving
MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster
convergence, reaching same performance with a third of training tokens,
indicating potential for more efficient and effective biomedical pretraining
strategies.

</details>


### [167] [TAPS: Tool-Augmented Personalisation via Structured Tagging](https://arxiv.org/abs/2506.20409)
*Ekaterina Taktasheva,Jeff Dalton*

Main category: cs.CL

TL;DR: The paper introduces TAPS, a method to enhance personalized tool use in LLMs by leveraging structured tagging and uncertainty-based detection, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing approaches neglect personalization in tool-augmented LLMs, limiting their effectiveness in goal-oriented tasks.

Method: TAPS uses a structured tagging tool and an uncertainty-based tool detector to integrate user preferences.

Result: TAPS improves LLMs' ability to personalize tool use, setting a new benchmark for open-source models on the NLSI task.

Conclusion: Personalization is crucial for tool-augmented LLMs, and TAPS effectively addresses this gap, advancing the field.

Abstract: Recent advancements in tool-augmented large language models have enabled them
to interact with external tools, enhancing their ability to perform complex
user tasks. However, existing approaches overlook the role of personalisation
in guiding tool use. This work investigates how user preferences can be
effectively integrated into goal-oriented dialogue agents. Through extensive
analysis, we identify key weaknesses in the ability of LLMs to personalise tool
use. To this end, we introduce \name, a novel solution that enhances
personalised tool use by leveraging a structured tagging tool and an
uncertainty-based tool detector. TAPS significantly improves the ability of
LLMs to incorporate user preferences, achieving the new state-of-the-art for
open source models on the NLSI task.

</details>


### [168] [An Agentic System for Rare Disease Diagnosis with Traceable Reasoning](https://arxiv.org/abs/2506.20430)
*Weike Zhao,Chaoyi Wu,Yanjie Fan,Xiaoman Zhang,Pengcheng Qiu,Yuze Sun,Xiao Zhou,Yanfeng Wang,Ya Zhang,Yongguo Yu,Kun Sun,Weidi Xie*

Main category: cs.CL

TL;DR: DeepRare is an LLM-powered rare disease diagnosis system that outperforms existing methods, achieving high accuracy and recall scores.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of timely and accurate diagnosis for rare diseases due to clinical heterogeneity and low prevalence.

Method: A modular system with a central host, specialized agent servers, and access to up-to-date medical knowledge, processing heterogeneous clinical inputs.

Result: Achieves 100% accuracy for 1,013 diseases, outperforms 15 other methods, and excels in multi-modal input scenarios.

Conclusion: DeepRare is a scalable, accurate, and user-friendly solution for rare disease diagnosis.

Abstract: Rare diseases collectively affect over 300 million individuals worldwide, yet
timely and accurate diagnosis remains a pervasive challenge. This is largely
due to their clinical heterogeneity, low individual prevalence, and the limited
familiarity most clinicians have with rare conditions. Here, we introduce
DeepRare, the first rare disease diagnosis agentic system powered by a large
language model (LLM), capable of processing heterogeneous clinical inputs. The
system generates ranked diagnostic hypotheses for rare diseases, each
accompanied by a transparent chain of reasoning that links intermediate
analytic steps to verifiable medical evidence.
  DeepRare comprises three key components: a central host with a long-term
memory module; specialized agent servers responsible for domain-specific
analytical tasks integrating over 40 specialized tools and web-scale,
up-to-date medical knowledge sources, ensuring access to the most current
clinical information. This modular and scalable design enables complex
diagnostic reasoning while maintaining traceability and adaptability. We
evaluate DeepRare on eight datasets. The system demonstrates exceptional
diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013
diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15
methods, like traditional bioinformatics diagnostic tools, LLMs, and other
agentic systems, achieving an average Recall@1 score of 57.18% and surpassing
the second-best method (Reasoning LLM) by a substantial margin of 23.79
percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at
Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of
reasoning chains by clinical experts achieves 95.40% agreements. Furthermore,
the DeepRare system has been implemented as a user-friendly web application
http://raredx.cn/doctor.

</details>


### [169] [Probing AI Safety with Source Code](https://arxiv.org/abs/2506.20471)
*Ujwal Narayan,Shreyas Chaudhari,Ashwin Kalyan,Tanmay Rajpurohit,Karthik Narasimhan,Ameet Deshpande,Vishvak Murahari*

Main category: cs.CL

TL;DR: The paper introduces Code of Thought (CoDoT), a prompting strategy to evaluate LLM safety, revealing significant toxicity increases in state-of-the-art models like GPT-4 Turbo and DeepSeek R1.


<details>
  <summary>Details</summary>
Motivation: The widespread use of LLMs in safety-critical applications necessitates better alignment with human values, as current models often fail to ensure safety.

Method: CoDoT converts natural language prompts into simple code to test LLM safety, e.g., transforming "Make the statement more toxic" into "make_more_toxic({text})".

Result: CoDoT exposes safety failures: GPT-4 Turbo's toxicity rises 16.5x, DeepSeek R1 fails 100%, and average toxicity increases 300% across seven LLMs. Recursive CoDoT doubles toxicity.

Conclusion: CoDoT highlights the urgent need for safety evaluations in LLMs to ensure safety and capabilities progress together.

Abstract: Large language models (LLMs) have become ubiquitous, interfacing with humans
in numerous safety-critical applications. This necessitates improving
capabilities, but importantly coupled with greater safety measures to align
these models with human values and preferences. In this work, we demonstrate
that contemporary models fall concerningly short of the goal of AI safety,
leading to an unsafe and harmful experience for users. We introduce a prompting
strategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT
converts natural language inputs to simple code that represents the same
intent. For instance, CoDoT transforms the natural language prompt "Make the
statement more toxic: {text}" to: "make_more_toxic({text})". We show that CoDoT
results in a consistent failure of a wide range of state-of-the-art LLMs. For
example, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of
the time, and toxicity increases 300% on average across seven modern LLMs.
Additionally, recursively applying CoDoT can further increase toxicity two
times. Given the rapid and widespread adoption of LLMs, CoDoT underscores the
critical need to evaluate safety efforts from first principles, ensuring that
safety and capabilities advance together.

</details>


### [170] [Time is On My Side: Dynamics of Talk-Time Sharing in Video-chat Conversations](https://arxiv.org/abs/2506.20474)
*Kaixiang Zhang,Justine Zhang,Cristian Danescu-Niculescu-Mizil*

Main category: cs.CL

TL;DR: A computational framework quantifies talk-time distribution and dynamics in conversations, revealing preferences for balanced talk-time and varied perceptions of dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand how talk-time is shared in conversations and how its distribution and dynamics affect speaker perceptions.

Method: Developed a computational framework to analyze talk-time distribution and dynamics, applied to a dataset of video-chats between strangers.

Result: Balanced conversations are preferred, especially by those talking less. Different dynamics, even with the same balance, are perceived differently.

Conclusion: The framework provides tools for designing communication platforms, emphasizing the importance of talk-time dynamics in perception.

Abstract: An intrinsic aspect of every conversation is the way talk-time is shared
between multiple speakers. Conversations can be balanced, with each speaker
claiming a similar amount of talk-time, or imbalanced when one talks
disproportionately. Such overall distributions are the consequence of
continuous negotiations between the speakers throughout the conversation: who
should be talking at every point in time, and for how long?
  In this work we introduce a computational framework for quantifying both the
conversation-level distribution of talk-time between speakers, as well as the
lower-level dynamics that lead to it. We derive a typology of talk-time sharing
dynamics structured by several intuitive axes of variation. By applying this
framework to a large dataset of video-chats between strangers, we confirm that,
perhaps unsurprisingly, different conversation-level distributions of talk-time
are perceived differently by speakers, with balanced conversations being
preferred over imbalanced ones, especially by those who end up talking less.
Then we reveal that -- even when they lead to the same level of overall balance
-- different types of talk-time sharing dynamics are perceived differently by
the participants, highlighting the relevance of our newly introduced typology.
Finally, we discuss how our framework offers new tools to designers of
computer-mediated communication platforms, for both human-human and human-AI
communication.

</details>


### [171] [Knowledge-Aware Diverse Reranking for Cross-Source Question Answering](https://arxiv.org/abs/2506.20476)
*Tong Zhou*

Main category: cs.CL

TL;DR: Team Marikarp's winning solution for SIGIR 2025 LiveRAG used a knowledge-aware diverse reranking RAG pipeline to outperform competitors on a diverse evaluation set.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of retrieving question-relevant documents from a large corpus (15M documents) for diverse topics, question types, and audiences.

Method: A knowledge-aware diverse reranking RAG pipeline was developed and tested on the competition's evaluation set.

Result: Achieved first place in the SIGIR 2025 LiveRAG competition.

Conclusion: The proposed pipeline effectively handles diverse retrieval tasks, demonstrating its superiority in the competition.

Abstract: This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG
competition. The competition's evaluation set, automatically generated by
DataMorgana from internet corpora, encompassed a wide range of target topics,
question types, question formulations, audience types, and knowledge
organization methods. It offered a fair evaluation of retrieving
question-relevant supporting documents from a 15M documents subset of the
FineWeb corpus. Our proposed knowledge-aware diverse reranking RAG pipeline
achieved first place in the competition.

</details>


### [172] [GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching](https://arxiv.org/abs/2506.20480)
*Guinan Su,Li Shen,Lu Yin,Shiwei Liu,Yanwu Yang,Jonas Geiping*

Main category: cs.CL

TL;DR: A novel strategy for compressing large language models (LLMs) by combining or merging layers from finetuned variants, achieving competitive pruning results.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of deploying large LLMs by reducing their size while preserving performance.

Method: Poses the problem as zero-order optimization, supporting layer removal, selection, and merging.

Result: Compressed models retain ~97.3% performance while removing ~25% parameters, outperforming prior methods.

Conclusion: The approach effectively balances model size reduction with performance retention, offering a practical solution for LLM deployment.

Abstract: Large language models (LLMs) have shown remarkable capabilities in language
understanding and generation. However, such impressive capability typically
comes with a substantial model size, which presents significant challenges in
deployment and inference. While structured pruning of model parameters offers a
promising way to reduce computational costs at deployment time, current methods
primarily focus on single model pruning. In this work, we develop a novel
strategy to compress models by strategically combining or merging layers from
finetuned model variants, which preserves the original model's abilities by
aggregating capabilities accentuated in different finetunes. We pose the
optimal tailoring of these LLMs as a zero-order optimization problem, adopting
a search space that supports three different operations: (1) Layer removal, (2)
Layer selection from different candidate models, and (3) Layer merging. Our
experiments demonstrate that this approach leads to competitive model pruning,
for example, for the Llama2-13B model families, our compressed models maintain
approximately 97.3\% of the original performance while removing $\sim25\%$ of
parameters, significantly outperforming previous state-of-the-art methods. The
code is available at https://github.com/Guinan-Su/auto-merge-llm.

</details>


### [173] [ReCode: Updating Code API Knowledge with Reinforcement Learning](https://arxiv.org/abs/2506.20495)
*Haoze Wu,Yunzhi Yao,Wenhao Yu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: ReCode is a framework using rule-based reinforcement learning to improve LLMs' adaptation to API updates, enhancing code generation in dynamic environments without compromising general abilities.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with adapting to frequent API updates due to outdated training data, limiting reliable code generation in dynamic settings.

Method: ReCode trains LLMs on a dataset of 2,000 entries for version migration, using a modified string similarity metric as reinforcement learning reward.

Result: ReCode significantly improves LLMs' performance in dynamic API scenarios, outperforming larger models and maintaining general code generation capabilities.

Conclusion: ReCode effectively addresses LLMs' adaptation to API changes, offering a scalable solution with consistent improvements across models and reinforcement learning algorithms.

Abstract: Large Language Models (LLMs) exhibit remarkable code generation capabilities
but falter when adapting to frequent updates in external library APIs. This
critical limitation, stemming from reliance on outdated API knowledge from
their training data, even with access to current documentation, impedes
reliable code generation in dynamic environments. To tackle this issue, we
propose ReCode (rule-based Reinforcement learning for Code Update), a novel
framework that mimics human programmer adaptation to API changes. Specifically,
we construct a dataset of approximately 2,000 data entries to train the LLMs to
perform version migration based on updated information. Then, we introduce a
modified string similarity metric for code evaluation as the reward for
reinforcement learning. Our experiments demonstrate that ReCode substantially
boosts LLMs' code generation performance in dynamic API scenarios, especially
on the unseen CodeUpdateArena task. Crucially, compared to supervised
fine-tuning, ReCode has less impact on LLMs' general code generation abilities.
We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and
DAPO), all achieving consistent improvements. Notably, after training,
Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned
model and the reasoning model with the same architecture. Code is available at
https://github.com/zjunlp/ReCode.

</details>


### [174] [OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling](https://arxiv.org/abs/2506.20512)
*Zengzhi Wang,Fan Zhou,Xuefeng Li,Pengfei Liu*

Main category: cs.CL

TL;DR: The paper explores how mid-training strategies affect reinforcement learning (RL) performance in language models like Qwen and Llama, identifying key factors like high-quality math corpora and QA-style data. It introduces a two-stage training strategy, Stable-then-Decay, resulting in the OctoThinker model family.


<details>
  <summary>Details</summary>
Motivation: Understanding what makes base language models suitable for RL is crucial for developing next-generation foundation models.

Method: Investigates mid-training strategies, focusing on Qwen and Llama, and introduces a two-stage Stable-then-Decay approach.

Result: High-quality math corpora and QA-style data improve RL performance, while long-CoT reasoning enhances depth but risks verbosity and instability. Scaling mid-training boosts downstream RL.

Conclusion: The Stable-then-Decay strategy produces RL-compatible models like OctoThinker, closing gaps with RL-friendly families like Qwen. The work aims to guide future pre-training strategies for RL-era foundation models.

Abstract: Different base language model families, such as Llama and Qwen, exhibit
divergent behaviors during post-training with reinforcement learning (RL),
especially on reasoning-intensive tasks. What makes a base language model
suitable for reinforcement learning? Gaining deeper insight into this question
is essential for developing RL-scalable foundation models of the next
generation. In this work, we investigate how mid-training strategies shape RL
dynamics, focusing on two representative model families: Qwen and Llama. Our
study reveals that (1) high-quality mathematical corpora, such as
MegaMath-Web-Pro, significantly improve both base model and RL performance,
while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further
adding QA-style data, particularly long chain-of-thought (CoT) reasoning
examples, enhances RL outcomes, and instruction data further unlocks this
effect; (3) while long-CoT improves reasoning depth, it can also induce
verbosity of model responses and unstability of RL training, underscoring the
importance of data formatting; (4) scaling mid-training consistently leads to
stronger downstream RL performance. Building on these insights, we introduce a
two-stage mid-training strategy, Stable-then-Decay, in which base models are
first trained on 200B tokens with a constant learning rate, followed by 20B
tokens across three CoT-focused branches with learning rate decay. This yields
OctoThinker, a family of models demonstrating strong RL compatibility and
closing the performance gap with more RL-friendly model families, i.e., Qwen.
We hope our work will help shape pre-training strategies for foundation models
in the RL era. To support further research, we release our open-source models
along with a curated math reasoning-intensive corpus of over 70 billion tokens
(i.e., MegaMath-Web-Pro-Max).

</details>


### [175] [When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs](https://arxiv.org/abs/2506.20544)
*Ammar Khairi,Daniel D'souza,Ye Shen,Julia Kreutzer,Sara Hooker*

Main category: cs.CL

TL;DR: The paper explores robust scaling of inference-time compute for multilingual, multi-task generative tasks, proposing novel sampling and selection strategies that outperform existing methods, especially in underrepresented languages.


<details>
  <summary>Details</summary>
Motivation: To generalize inference-time compute improvements across diverse domains and languages, addressing the limitations of current methods focused on English and narrow domains.

Method: Proposes adapted sampling and selection strategies for multilingual and multi-task settings, evaluating them against existing methods.

Result: Achieves significant performance gains (e.g., +6.8 win-rate for 8B models, +9.0 for 111B models) on multilingual benchmarks compared to single-sample decoding.

Conclusion: Highlights the necessity of language- and task-aware inference-time compute methods to democratize performance improvements, especially for underrepresented languages.

Abstract: Recent advancements in large language models (LLMs) have shifted focus toward
scaling inference-time compute, improving performance without retraining the
model. A common approach is to sample multiple outputs in parallel, and select
one of these as the final output. However, work to date has focused on English
and a handful of domains such as math and code. In contrast, we are most
interested in techniques that generalize across open-ended tasks, formally
verifiable tasks, and across languages. In this work, we study how to robustly
scale inference-time compute for open-ended generative tasks in a multilingual,
multi-task setting.
  Our findings show that both sampling strategy based on temperature variation
and selection strategy must be adapted to account for diverse domains and
varied language settings. We evaluate existing selection methods, revealing
that strategies effective in English often fail to generalize across languages.
We propose novel sampling and selection strategies specifically adapted for
multilingual and multi-task inference scenarios, and show they yield notable
gains across languages and tasks. In particular, our combined sampling and
selection methods lead to an average +6.8 jump in win-rates for our 8B models
on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At
larger scale, Command-A (111B model) equipped with our methods, shows +9.0
improvement in win-rates on the same benchmark with just five samples against
single-sample decoding, a substantial increase at minimal cost. Our results
underscore the need for language- and task-aware approaches to inference-time
compute, aiming to democratize performance improvements in underrepresented
languages.

</details>


### [176] [Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm](https://arxiv.org/abs/2506.20606)
*Baixiang Huang,Zhen Tan,Haoran Wang,Zijie Liu,Dawei Li,Ali Payani,Huan Liu,Tianlong Chen,Kai Shu*

Main category: cs.CL

TL;DR: The paper introduces Behavior Editing, a method to steer LLM-based agents' ethical behavior using model editing, and evaluates it with BehaviorBench, a multi-tier benchmark grounded in moral theories.


<details>
  <summary>Details</summary>
Motivation: Deploying LLM-based agents in high-stakes domains poses safety and ethical risks, necessitating methods to control their behavior to prevent harm.

Method: Behavior Editing frames agent behavior steering as a model editing task, tested using BehaviorBench, a benchmark with scenarios of increasing complexity.

Result: Behavior Editing effectively steers agent behavior locally and globally, promoting ethical or harmful actions, as validated by evaluations on frontier LLMs.

Conclusion: Behavior Editing offers a promising but risky paradigm for controlling agent behavior, with potential for both ethical and malicious applications.

Abstract: Agents based on Large Language Models (LLMs) have demonstrated strong
capabilities across a wide range of tasks. However, deploying LLM-based agents
in high-stakes domains comes with significant safety and ethical risks.
Unethical behavior by these agents can directly result in serious real-world
consequences, including physical harm and financial loss. To efficiently steer
the ethical behavior of agents, we frame agent behavior steering as a model
editing task, which we term Behavior Editing. Model editing is an emerging area
of research that enables precise and efficient modifications to LLMs while
preserving their overall capabilities. To systematically study and evaluate
this approach, we introduce BehaviorBench, a multi-tier benchmark grounded in
psychological moral theories. This benchmark supports both the evaluation and
editing of agent behaviors across a variety of scenarios, with each tier
introducing more complex and ambiguous scenarios. We first demonstrate that
Behavior Editing can dynamically steer agents toward the target behavior within
specific scenarios. Moreover, Behavior Editing enables not only
scenario-specific local adjustments but also more extensive shifts in an
agent's global moral alignment. We demonstrate that Behavior Editing can be
used to promote ethical and benevolent behavior or, conversely, to induce
harmful or malicious behavior. Through comprehensive evaluations on agents
based on frontier LLMs, BehaviorBench shows the effectiveness of Behavior
Editing across different models and scenarios. Our findings offer key insights
into a new paradigm for steering agent behavior, highlighting both the promise
and perils of Behavior Editing.

</details>


### [177] [DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation](https://arxiv.org/abs/2506.20639)
*Shansan Gong,Ruixiang Zhang,Huangjie Zheng,Jiatao Gu,Navdeep Jaitly,Lingpeng Kong,Yizhe Zhang*

Main category: cs.CL

TL;DR: DiffuCoder, a 7B dLLM, is trained for code generation, revealing unique decoding behaviors and improved performance via coupled-GRPO, a novel RL method.


<details>
  <summary>Details</summary>
Motivation: To explore and enhance the potential of dLLMs for code generation by analyzing their denoising processes and developing effective RL training methods.

Method: Train a 7B dLLM (DiffuCoder) on 130B tokens of code, analyze its decoding behavior, and propose coupled-GRPO for RL training to reduce variance and improve efficiency.

Result: DiffuCoder shows improved performance (+4.4% on EvalPlus) and reduced reliance on AR causal decoding, with diverse generation order.

Conclusion: The study provides insights into dLLM generation and introduces an effective RL framework for code generation.

Abstract: Diffusion large language models (dLLMs) are compelling alternatives to
autoregressive (AR) models because their denoising models operate over the
entire sequence. The global planning and iterative refinement features of dLLMs
are particularly useful for code generation. However, current training and
inference mechanisms for dLLMs in coding are still under-explored. To demystify
the decoding behavior of dLLMs and unlock their potential for coding, we
systematically investigate their denoising processes and reinforcement learning
(RL) methods. We train a 7B dLLM, \textbf{DiffuCoder}, on 130B tokens of code.
Using this model as a testbed, we analyze its decoding behavior, revealing how
it differs from that of AR models: (1) dLLMs can decide how causal their
generation should be without relying on semi-AR decoding, and (2) increasing
the sampling temperature diversifies not only token choices but also their
generation order. This diversity creates a rich search space for RL rollouts.
For RL training, to reduce the variance of token log-likelihood estimates and
maintain training efficiency, we propose \textbf{coupled-GRPO}, a novel
sampling scheme that constructs complementary mask noise for completions used
in training. In our experiments, coupled-GRPO significantly improves
DiffuCoder's performance on code generation benchmarks (+4.4\% on EvalPlus) and
reduces reliance on AR causal during decoding. Our work provides deeper insight
into the machinery of dLLM generation and offers an effective, diffusion-native
RL training framework. https://github.com/apple/ml-diffucoder.

</details>


### [178] [Memento: Note-Taking for Your Future Self](https://arxiv.org/abs/2506.20642)
*Chao Wan,Albert Gong,Mihir Mishra,Carl-Leander Henneking,Claas Beger,Kilian Q. Weinberger*

Main category: cs.CL

TL;DR: Memento is a prompting strategy that decomposes complex questions, dynamically builds a fact database, and combines facts to solve multi-hop QA tasks, significantly outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) struggle with tasks requiring tight coupling of reasoning and retrieval, such as multi-hop question answering.

Method: Memento decomposes questions into steps, constructs a dynamic fact database using LLMs, and integrates facts to solve questions.

Result: Memento doubles CoT performance on PhantomWiki, improves CoT-RAG by 20+ F1 points on 2WikiMultiHopQA, and boosts ReAct by 3+ F1 points on MuSiQue.

Conclusion: Memento effectively enhances LLM performance in multi-hop QA by integrating reasoning and retrieval dynamically.

Abstract: Large language models (LLMs) excel at reasoning-only tasks, but struggle when
reasoning must be tightly coupled with retrieval, as in multi-hop question
answering. To overcome these limitations, we introduce a prompting strategy
that first decomposes a complex question into smaller steps, then dynamically
constructs a database of facts using LLMs, and finally pieces these facts
together to solve the question. We show how this three-stage strategy, which we
call Memento, can boost the performance of existing prompting strategies across
diverse settings. On the 9-step PhantomWiki benchmark, Memento doubles the
performance of chain-of-thought (CoT) when all information is provided in
context. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento
improves over vanilla CoT-RAG by more than 20 F1 percentage points and over the
multi-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the
challenging MuSiQue dataset, Memento improves ReAct by more than 3 F1
percentage points, demonstrating its utility in agentic settings.

</details>


### [179] [Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs](https://arxiv.org/abs/2506.20666)
*Sonia K. Murthy,Rosie Zhao,Jennifer Hu,Sham Kakade,Markus Wulfmeier,Peng Qian,Tomer Ullman*

Main category: cs.CL

TL;DR: The paper explores how LLMs handle human-like value trade-offs, such as balancing truth and politeness, using a cognitive model of polite speech. It evaluates these trade-offs in different model settings and training dynamics.


<details>
  <summary>Details</summary>
Motivation: Current tools for interpreting value trade-offs in LLMs are limited, despite their importance in human decision-making and language use. The study aims to bridge this gap by applying a cognitive model.

Method: The study uses a cognitive model of polite speech to analyze LLMs, evaluating value trade-offs in two settings: reasoning effort in black-box models and RL post-training dynamics in open-source models.

Result: Findings show higher informational utility than social utility in reasoning models, and early training shifts in utility values with lasting effects from base models and pretraining data.

Conclusion: The method provides insights into LLM behavior, aiding hypothesis formation, training regime design, and better control of value trade-offs during model training.

Abstract: Navigating everyday social situations often requires juggling conflicting
goals, such as conveying a harsh truth, maintaining trust, all while still
being mindful of another person's feelings. These value trade-offs are an
integral part of human decision-making and language use, however, current tools
for interpreting such dynamic and multi-faceted notions of values in LLMs are
limited. In cognitive science, so-called "cognitive models" provide formal
accounts of these trade-offs in humans, by modeling the weighting of a
speaker's competing utility functions in choosing an action or utterance. In
this work, we use a leading cognitive model of polite speech to interpret the
extent to which LLMs represent human-like trade-offs. We apply this lens to
systematically evaluate value trade-offs in two encompassing model settings:
degrees of reasoning "effort" in frontier black-box models, and RL
post-training dynamics of open-source models. Our results highlight patterns of
higher informational utility than social utility in reasoning models, and in
open-source models shown to be stronger in mathematical reasoning. Our findings
from LLMs' training dynamics suggest large shifts in utility values early on in
training with persistent effects of the choice of base model and pretraining
data, compared to feedback dataset or alignment method. We show that our method
is responsive to diverse aspects of the rapidly evolving LLM landscape, with
insights for forming hypotheses about other high-level behaviors, shaping
training regimes for reasoning models, and better controlling trade-offs
between values during model training.

</details>

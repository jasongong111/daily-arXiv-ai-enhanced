<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 44]
- [cs.LG](#cs.LG) [Total: 162]
- [cs.CV](#cs.CV) [Total: 162]
- [cs.CL](#cs.CL) [Total: 63]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)
*Daewon Choi,Jimin Lee,Jihoon Tack,Woomin Song,Saket Dingliwal,Sai Muralidhar Jayanthi,Bhavana Ganesh,Jinwoo Shin,Aram Galstyan,Sravan Babu Bodapati*

Main category: cs.AI

TL;DR: The paper proposes a method to improve reasoning in large language models by removing redundant tokens in reasoning paths, enhancing accuracy without training.


<details>
  <summary>Details</summary>
Motivation: Large language models exhibit redundancy in reasoning paths, leading to scattered attention and incorrect answers. Addressing this can improve performance.

Method: Identify redundancy using token-level attention scores to an end-of-thinking token, prune low-contributing reasoning chunks, and resume generation after removing redundant tokens.

Result: Significant accuracy improvement on reasoning benchmarks, especially in mathematical competitions like AIME and AMC.

Conclusion: Removing reasoning redundancy enhances model performance, particularly in complex tasks, without additional training.

Abstract: Recent large language models have shown promising capabilities in long-form
reasoning, following structured chains of thought before arriving at a final
answer. However, we observe that these reasoning paths tend to include
substantial redundancy; analyzing attention patterns reveals that attention
scores are widely scattered, particularly incorrect answers exhibit greater
attention sparsity. In this paper, we demonstrate that deliberately removing
this redundancy in the reasoning process significantly improves performance
through clear thinking, i.e., removing distraction. Specifically, we
systematically identify reasoning redundancy by measuring token-level attention
scores to a special end-of-thinking token, which is appended to an explicit
instruction inserted to conclude each intermediate reasoning step. Furthermore,
we propose structure-aware pruning that prioritizes removing tokens in
low-contributing reasoning chunks over individual tokens. After evicting
redundant tokens, we remove the injected end-of-thinking instruction, then
resume the reasoning generation. We demonstrate that our method significantly
improves overall accuracy across reasoning-intensive benchmarks without any
training involved. In particular, our method shows strong performance on
challenging mathematical competition benchmarks such as AIME and AMC, where
reasoning redundancy is more prevalent.

</details>


### [2] [A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data](https://arxiv.org/abs/2507.08875)
*Fuh-Hwa Franklin Liu,Su-Chuan Shih*

Main category: cs.AI

TL;DR: A novel MCA approach combining two Virtual Gap Analysis (VGA) models is proposed to improve efficiency and fairness in multi-criteria assessments, addressing challenges like subjective judgment and homogeneity assumptions.


<details>
  <summary>Details</summary>
Motivation: To tackle the limitations of existing MCA methods (DEA, SFA, MCDM) by addressing subjective judgment, variability in criteria, and the homogeneity assumption, ensuring more comprehensive and dependable evaluations.

Method: Combines two VGA models rooted in linear programming to evaluate Decision-Making Units (DMUs) with both quantitative and qualitative criteria.

Result: The proposed method demonstrates accuracy and transparency in two numerical examples, improving efficiency and fairness in evaluations.

Conclusion: The approach offers a robust and adaptive solution for MCA, encouraging advancements in automated decision and decision support systems.

Abstract: Modern methods for multi-criteria assessment (MCA), such as Data Envelopment
Analysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria
Decision-Making (MCDM), are utilized to appraise a collection of
Decision-Making Units (DMUs), also known as alternatives, based on several
criteria. These methodologies inherently rely on assumptions and can be
influenced by subjective judgment to effectively tackle the complex evaluation
challenges in various fields. In real-world scenarios, it is essential to
incorporate both quantitative and qualitative criteria as they consist of
cardinal and ordinal data. Despite the inherent variability in the criterion
values of different alternatives, the homogeneity assumption is often employed,
significantly affecting evaluations. To tackle these challenges and determine
the most appropriate alternative, we propose a novel MCA approach that combines
two Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear
programming, is pivotal in the MCA methodology. This approach improves
efficiency and fairness, ensuring that evaluations are both comprehensive and
dependable, thus offering a strong and adaptive solution. Two comprehensive
numerical examples demonstrate the accuracy and transparency of our proposed
method. The goal is to encourage continued advancement and stimulate progress
in automated decision systems and decision support systems.

</details>


### [3] [Multi-Actor Generative Artificial Intelligence as a Game Engine](https://arxiv.org/abs/2507.08892)
*Alexander Sasha Vezhnevets,Jayd Matyas,Logan Cross,Davide Paglieri,Minsuk Chang,William A. Cunningham,Simon Osindero,William S. Isaac,Joel Z. Leibo*

Main category: cs.AI

TL;DR: The paper proposes using tabletop role-playing game (TTRPG) principles and the Entity-Component pattern to create a flexible framework for generative AI in multi-actor environments, demonstrated by the Concordia library.


<details>
  <summary>Details</summary>
Motivation: To address the diverse needs of generative AI applications (Simulationist, Dramatist, Evaluationist) by providing a flexible and modular scenario definition framework.

Method: Inspired by TTRPGs, the approach uses the Entity-Component pattern, where a configurable Game Master (GM) entity is composed of reusable components, separating implementation from design.

Result: The Concordia library exemplifies this philosophy, enabling users to configure scenarios tailored to their goals while ensuring modularity and scalability.

Conclusion: The proposed framework supports rapid iteration and scalability, making it adaptable for various generative AI use cases.

Abstract: Generative AI can be used in multi-actor environments with purposes ranging
from social science modeling to interactive narrative and AI evaluation.
Supporting this diversity of use cases -- which we classify as Simulationist,
Dramatist, and Evaluationist -- demands a flexible scenario definition
framework. We argue here that a good approach is to take inspiration from
tabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible
for the environment and generates all parts of the story not directly
determined by the voluntary actions of player characters. We argue that the
Entity-Component architectural pattern is useful here. In such a system, the GM
is not a hardcoded computer game but is itself a configurable entity, composed
of components just like any other actor. By design, the approach allows for a
separation between the underlying implementation details handled by an
engineer, the creation of reusable components, and their composition and
configuration managed by a designer who constructs entities from the
components. This separation of concerns is instrumental for achieving rapid
iteration, maintaining modularity, and ultimately to ensure scalability. We
describe the ongoing evolution of the Concordia library in terms of this
philosophy, demonstrating how it allows users to effectively configure
scenarios that align with their specific goals.

</details>


### [4] [BioAnalyst: A Foundation Model for Biodiversity](https://arxiv.org/abs/2507.09080)
*Athanasios Trantas,Martino Mensio,Stylianos Stasinos,Sebastian Gribincea,Taimur Khan,Damian Podareanu,Aliene van der Veen*

Main category: cs.AI

TL;DR: BioAnalyst, a transformer-based AI Foundation Model, is introduced for biodiversity analysis and conservation, outperforming existing methods in data-scarce scenarios.


<details>
  <summary>Details</summary>
Motivation: Biodiversity loss threatens ecological balance, requiring advanced tools like AI for monitoring and conservation.

Method: BioAnalyst uses a transformer architecture pre-trained on multi-modal datasets (species records, remote sensing, climate data) and is fine-tunable for tasks like species distribution modeling.

Result: BioAnalyst achieves higher accuracy in ecological forecasting, especially in data-scarce cases, setting a new benchmark.

Conclusion: Openly releasing BioAnalyst aims to enhance collaborative biodiversity modeling and address ecological challenges with AI.

Abstract: The accelerating loss of biodiversity presents critical challenges for
ecological research and conservation strategies. The preservation of
biodiversity is paramount for maintaining ecological balance and ensuring the
sustainability of ecosystems. However, biodiversity faces numerous threats,
including habitat loss, climate change, and the proliferation of invasive
species. Addressing these and other ecology-related challenges, both at local
and global scales, requires comprehensive monitoring, predictive and
conservation planning capabilities. Artificial Intelligence (AI) Foundation
Models (FMs) have gained significant momentum in numerous scientific domains by
leveraging vast datasets to learn general-purpose representations adaptable to
various downstream tasks. This paradigm holds immense promise for biodiversity
conservation. In response, we introduce BioAnalyst, the first Foundation Model
tailored for biodiversity analysis and conservation planning. BioAnalyst
employs a transformer-based architecture, pre-trained on extensive multi-modal
datasets encompassing species occurrence records, remote sensing indicators,
climate and environmental variables. BioAnalyst is designed for adaptability,
allowing for fine-tuning of a range of downstream tasks, such as species
distribution modelling, habitat suitability assessments, invasive species
detection, and population trend forecasting. We evaluate the model's
performance on two downstream use cases, demonstrating its generalisability
compared to existing methods, particularly in data-scarce scenarios for two
distinct use-cases, establishing a new accuracy baseline for ecological
forecasting. By openly releasing BioAnalyst and its fine-tuning workflows to
the scientific community, we aim to foster collaborative efforts in
biodiversity modelling and advance AI-driven solutions to pressing ecological
challenges.

</details>


### [5] [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity](https://arxiv.org/abs/2507.09089)
*Joel Becker,Nate Rush,Elizabeth Barnes,David Rein*

Main category: cs.AI

TL;DR: AI tools unexpectedly increased task completion time by 19% for experienced developers, contrary to predictions of faster work.


<details>
  <summary>Details</summary>
Motivation: To study the real-world impact of AI tools on software development productivity.

Method: A randomized controlled trial (RCT) with 16 developers completing 246 tasks, comparing AI-allowed and AI-disallowed conditions.

Result: AI tools slowed developers by 19%, contradicting expert predictions of 38-39% faster completion.

Conclusion: The slowdown effect is robust, suggesting AI tools may not universally boost productivity as assumed.

Abstract: Despite widespread adoption, the impact of AI tools on software development
in the wild remains understudied. We conduct a randomized controlled trial
(RCT) to understand how AI tools at the February-June 2025 frontier affect the
productivity of experienced open-source developers. 16 developers with moderate
AI experience complete 246 tasks in mature projects on which they have an
average of 5 years of prior experience. Each task is randomly assigned to allow
or disallow usage of early 2025 AI tools. When AI tools are allowed, developers
primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.
Before starting tasks, developers forecast that allowing AI will reduce
completion time by 24%. After completing the study, developers estimate that
allowing AI reduced completion time by 20%. Surprisingly, we find that allowing
AI actually increases completion time by 19%--AI tooling slowed developers
down. This slowdown also contradicts predictions from experts in economics (39%
shorter) and ML (38% shorter). To understand this result, we collect and
evaluate evidence for 20 properties of our setting that a priori could
contribute to the observed slowdown effect--for example, the size and quality
standards of projects, or prior developer experience with AI tooling. Although
the influence of experimental artifacts cannot be entirely ruled out, the
robustness of the slowdown effect across our analyses suggests it is unlikely
to primarily be a function of our experimental design.

</details>


### [6] [Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System](https://arxiv.org/abs/2507.09179)
*Ronghua Shi,Yiou Liu,Xinyu Ying,Yang Tan,Yuchun Feng,Lynn Ai,Bill Shi,Xuhui Wang,Zhuang Liu*

Main category: cs.AI

TL;DR: A MARL framework for detecting DeFi market manipulation, using adversarial game dynamics, innovative learning methods, and multi-modal data integration.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of centralized oversight in DeFi, which enables market manipulation like pump-and-dump schemes.

Method: Proposes a MARL framework with GRPO, a theory-based reward function, and multi-modal agent pipeline for detection.

Result: Achieves high detection accuracy and causal attribution, validated on real-world data and simulations.

Conclusion: Bridges multi-agent systems with financial surveillance, offering a decentralized solution for market intelligence.

Abstract: Decentralized finance (DeFi) has introduced a new era of permissionless
financial innovation but also led to unprecedented market manipulation. Without
centralized oversight, malicious actors coordinate shilling campaigns and
pump-and-dump schemes across various platforms. We propose a Multi-Agent
Reinforcement Learning (MARL) framework for decentralized manipulation
detection, modeling the interaction between manipulators and detectors as a
dynamic adversarial game. This framework identifies suspicious patterns using
delayed token price reactions as financial indicators.Our method introduces
three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance
learning stability in sparse-reward and partially observable settings; (2) a
theory-based reward function inspired by rational expectations and information
asymmetry, differentiating price discovery from manipulation noise; and (3) a
multi-modal agent pipeline that integrates LLM-based semantic features, social
graph signals, and on-chain market data for informed decision-making.The
framework is integrated within the Symphony system, a decentralized multi-agent
architecture enabling peer-to-peer agent execution and trust-aware learning
through distributed logs, supporting chain-verifiable evaluation. Symphony
promotes adversarial co-evolution among strategic actors and maintains robust
manipulation detection without centralized oracles, enabling real-time
surveillance across global DeFi ecosystems.Trained on 100,000 real-world
discourse episodes and validated in adversarial simulations, Hide-and-Shill
achieves top performance in detection accuracy and causal attribution. This
work bridges multi-agent systems with financial surveillance, advancing a new
paradigm for decentralized market intelligence. All resources are available at
the Hide-and-Shill GitHub repository to promote open research and
reproducibility.

</details>


### [7] [When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents](https://arxiv.org/abs/2507.09329)
*Matous Kozak,Roshanak Zilouchian Moghaddam,Siva Sivaraman*

Main category: cs.AI

TL;DR: The paper evaluates the security risks of LLM-based coding agents, finding 21% of actions insecure, and proposes mitigation strategies with varying success.


<details>
  <summary>Details</summary>
Motivation: To understand and address the security implications of deploying LLM-based coding agents in software development.

Method: Systematic security evaluation of 12,000 actions across five models on 93 real-world tasks, identifying vulnerabilities and testing mitigations.

Result: 21% of actions were insecure, with information exposure (CWE-200) most common. GPT-4.1 showed 96.8% mitigation success.

Conclusion: Highlights the need for security-aware design in next-gen coding agents and provides a framework for evaluation.

Abstract: LLM-based coding agents are rapidly being deployed in software development,
yet their security implications remain poorly understood. These agents, while
capable of accelerating software development, may inadvertently introduce
insecure practices. We conducted the first systematic security evaluation of
autonomous coding agents, analyzing over 12,000 actions across five
state-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world
software setup tasks. Our findings reveal significant security concerns: 21% of
agent trajectories contained insecure actions, with models showing substantial
variation in security behavior. We developed a high-precision detection system
that identified four major vulnerability categories, with information exposure
(CWE-200) being the most prevalent one. We also evaluated mitigation strategies
including feedback mechanisms and security reminders with various effectiveness
between models. GPT-4.1 demonstrated exceptional security awareness with 96.8%
mitigation success. Our work provides the first comprehensive framework for
evaluating coding agent security and highlights the need for security-aware
design of next generation LLM-based coding agents.

</details>


### [8] [A Taxonomy of Omnicidal Futures Involving Artificial Intelligence](https://arxiv.org/abs/2507.09369)
*Andrew Critch,Jacob Tsimerman*

Main category: cs.AI

TL;DR: A taxonomy of AI-driven omnicidal events is presented to highlight catastrophic risks and advocate for preventive measures.


<details>
  <summary>Details</summary>
Motivation: To raise awareness and public support for avoiding potential AI-driven human extinction scenarios.

Method: Presents a taxonomy and examples of omnicidal events caused by AI.

Result: Identifies possibilities of catastrophic risks from AI, emphasizing their avoidability.

Conclusion: Public awareness can help support preventive actions against AI-driven existential threats.

Abstract: This report presents a taxonomy and examples of potential omnicidal events
resulting from AI: scenarios where all or almost all humans are killed. These
events are not presented as inevitable, but as possibilities that we can work
to avoid. Insofar as large institutions require a degree of public support in
order to take certain actions, we hope that by presenting these possibilities
in public, we can help to support preventive measures against catastrophic
risks from AI.

</details>


### [9] [EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique](https://arxiv.org/abs/2507.09374)
*Chenglin Zhu,Tao Zhang,Chong Li,Mingan Lin,Zenan Zhou,Jian Xie*

Main category: cs.AI

TL;DR: EduFlow is an end-to-end framework for improving scientific reasoning in MLLMs, featuring EduPRM for step-wise critique and EduMCTS for adaptive search, validated by enhanced reasoning consistency.


<details>
  <summary>Details</summary>
Motivation: MLLMs perform poorly on scientific tasks due to inadequate reasoning patterns, incoherent multi-step inference, and lack of self-correction. EduFlow addresses these gaps.

Method: EduFlow integrates data selection, MCTS-based trajectory construction, model training, and output optimization. EduPRM critiques steps with tags, and EduMCTS adapts search for educational reasoning.

Result: EduFlow improves reasoning consistency and coherence, demonstrated through extensive experiments. EduMCTS-160K dataset was created for validation.

Conclusion: EduFlow effectively enhances MLLMs' scientific reasoning, with plans to release code, data, and models.

Abstract: Multimodal large language models (MLLMs) still perform poorly on scientific
tasks, particularly those requiring multi-step and interpretable reasoning.
Their limitations include insufficient scientific reasoning patterns, lack of
global coherence in multi-step inference, and the absence of reflective
self-correction, making them unreliable in structured scientific contexts. We
introduce EduFlow, the first end-to-end framework that covers the full pipeline
of educational scientific reasoning, including data selection, MCTS-based
trajectory construction, model training, and output optimization. At its core
is EduPRM, a process-aware reward model that critiques reasoning steps with
tags and justifications. EduPRM is trained via curriculum learning on three
complementary supervision sources: MCTS-guided trajectories, error-injected
critiques, and teacher-student dialogues, enabling dynamic adaptation to
multi-stage problem solving and iterative refinement during inference. We
further propose EduMCTS, a domain-adapted search framework that introduces
bootstrapping actions specifically designed for educational reasoning, such as
a self-reflection mechanism that promotes reflective error correction. It
further leverages EduPRM's fine-grained feedback to guide the search toward
higher-quality reasoning trajectories. By applying self-consistency and
rejection sampling, we constructed EduMCTS-160K, a large-scale dataset of
educational reasoning trajectories. Extensive experiments demonstrate that
EduFlow enhances reasoning consistency and coherence. Code, data, and models
will be released.

</details>


### [10] [Knowledge Conceptualization Impacts RAG Efficacy](https://arxiv.org/abs/2507.09389)
*Chris Davis Jaldi,Anmol Saini,Elham Ghiasi,O. Divine Eziolise,Cogan Shimizu*

Main category: cs.AI

TL;DR: The paper explores merging explainability and adaptability in AI, focusing on transferable and interpretable neurosymbolic systems, specifically Agentic Retrieval-Augmented Generation systems.


<details>
  <summary>Details</summary>
Motivation: To investigate how knowledge representation impacts AI agents (like LLMs) in querying triplestores, balancing explainability and adaptability.

Method: Systematic evaluation of different knowledge conceptualizations and representations, focusing on structure and complexity.

Result: Findings show that knowledge representation impacts AI agent performance in querying triplestores.

Conclusion: The study highlights the trade-offs and implications of knowledge representation for interpretable and adaptable AI systems.

Abstract: Explainability and interpretability are cornerstones of frontier and
next-generation artificial intelligence (AI) systems. This is especially true
in recent systems, such as large language models (LLMs), and more broadly,
generative AI. On the other hand, adaptability to new domains, contexts, or
scenarios is also an important aspect for a successful system. As such, we are
particularly interested in how we can merge these two efforts, that is,
investigating the design of transferable and interpretable neurosymbolic AI
systems. Specifically, we focus on a class of systems referred to as ''Agentic
Retrieval-Augmented Generation'' systems, which actively select, interpret, and
query knowledge sources in response to natural language prompts. In this paper,
we systematically evaluate how different conceptualizations and representations
of knowledge, particularly the structure and complexity, impact an AI agent (in
this case, an LLM) in effectively querying a triplestore. We report our
results, which show that there are impacts from both approaches, and we discuss
their impact and implications.

</details>


### [11] [LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing](https://arxiv.org/abs/2507.09407)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: LLM-Stackelberg games integrate LLMs into leader-follower interactions, replacing classical assumptions with probabilistic reasoning and adaptation. Two equilibria concepts are introduced, applied to a spearphishing case study, demonstrating the framework's utility in cybersecurity and beyond.


<details>
  <summary>Details</summary>
Motivation: To model strategic interactions with bounded rationality and asymmetric information using LLMs, moving beyond classical Stackelberg assumptions.

Method: Define reasoning and behavioral equilibrium, and conjectural reasoning equilibrium, using structured prompts and probabilistic LLM behaviors. Tested in a spearphishing deception game.

Result: The framework captures cognitive richness and adversarial potential, proving useful for domains like cybersecurity and misinformation.

Conclusion: LLM-Stackelberg games offer a powerful paradigm for decision-making in complex, adaptive environments.

Abstract: We introduce the framework of LLM-Stackelberg games, a class of sequential
decision-making models that integrate large language models (LLMs) into
strategic interactions between a leader and a follower. Departing from
classical Stackelberg assumptions of complete information and rational agents,
our formulation allows each agent to reason through structured prompts,
generate probabilistic behaviors via LLMs, and adapt their strategies through
internal cognition and belief updates. We define two equilibrium concepts:
reasoning and behavioral equilibrium, which aligns an agent's internal
prompt-based reasoning with observable behavior, and conjectural reasoning
equilibrium, which accounts for epistemic uncertainty through parameterized
models over an opponent's response. These layered constructs capture bounded
rationality, asymmetric information, and meta-cognitive adaptation. We
illustrate the framework through a spearphishing case study, where a sender and
a recipient engage in a deception game using structured reasoning prompts. This
example highlights the cognitive richness and adversarial potential of
LLM-mediated interactions. Our results show that LLM-Stackelberg games provide
a powerful paradigm for modeling decision-making in domains such as
cybersecurity, misinformation, and recommendation systems.

</details>


### [12] [GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective](https://arxiv.org/abs/2507.09495)
*Hang Wang,Junshan Zhang*

Main category: cs.AI

TL;DR: The paper proposes a shift from reactive to proactive multi-agent reinforcement learning using generative AI, enabling anticipatory decision-making and improved coordination.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent reinforcement learning struggles with joint action spaces, non-stationarity, and partial observability, limiting adaptability to novel scenarios.

Method: Advocates using generative AI-based reinforcement learning, where agents act as generative models to predict and synthesize multi-agent dynamics for proactive decisions.

Result: Generative-RL agents can model environment evolution, predict behaviors, and coordinate actions strategically, enhancing adaptability and collaboration.

Conclusion: This paradigm shift promises breakthroughs in distributed intelligence, solving coordination challenges in autonomous systems and human-AI collaboration.

Abstract: Multi-agent reinforcement learning faces fundamental challenges that
conventional approaches have failed to overcome: exponentially growing joint
action spaces, non-stationary environments where simultaneous learning creates
moving targets, and partial observability that constrains coordination. Current
methods remain reactive, employing stimulus-response mechanisms that fail when
facing novel scenarios. We argue for a transformative paradigm shift from
reactive to proactive multi-agent intelligence through generative AI-based
reinforcement learning. This position advocates reconceptualizing agents not as
isolated policy optimizers, but as sophisticated generative models capable of
synthesizing complex multi-agent dynamics and making anticipatory decisions
based on predictive understanding of future interactions. Rather than
responding to immediate observations, generative-RL agents can model
environment evolution, predict other agents' behaviors, generate coordinated
action sequences, and engage in strategic reasoning accounting for long-term
dynamics. This approach leverages pattern recognition and generation
capabilities of generative AI to enable proactive decision-making, seamless
coordination through enhanced communication, and dynamic adaptation to evolving
scenarios. We envision this paradigm shift will unlock unprecedented
possibilities for distributed intelligence, moving beyond individual
optimization toward emergent collective behaviors representing genuine
collaborative intelligence. The implications extend across autonomous systems,
robotics, and human-AI collaboration, promising solutions to coordination
challenges intractable under traditional reactive frameworks.

</details>


### [13] [Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.09534)
*Guanquan Wang,Takuya Hiraoka,Yoshimasa Tsuruoka*

Main category: cs.AI

TL;DR: CTP is a fast, single-step trajectory planning method using CTM, outperforming diffusion-based methods in efficiency and performance on D4RL tasks.


<details>
  <summary>Details</summary>
Motivation: Address high computational costs of iterative sampling in diffusion-based planning methods.

Method: Leverages Consistency Trajectory Model (CTM) for efficient, single-step trajectory optimization.

Result: Outperforms existing methods on D4RL, achieving higher returns with fewer steps and 120x speedup.

Conclusion: CTP is practical and effective for high-performance, low-latency offline planning.

Abstract: This paper introduces Consistency Trajectory Planning (CTP), a novel offline
model-based reinforcement learning method that leverages the recently proposed
Consistency Trajectory Model (CTM) for efficient trajectory optimization. While
prior work applying diffusion models to planning has demonstrated strong
performance, it often suffers from high computational costs due to iterative
sampling procedures. CTP supports fast, single-step trajectory generation
without significant degradation in policy quality. We evaluate CTP on the D4RL
benchmark and show that it consistently outperforms existing diffusion-based
planning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves
higher normalized returns while using significantly fewer denoising steps. In
particular, CTP achieves comparable performance with over $120\times$ speedup
in inference time, demonstrating its practicality and effectiveness for
high-performance, low-latency offline planning.

</details>


### [14] [Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling](https://arxiv.org/abs/2507.09540)
*Ali Safa,Farida Mohsen,Ali Al-Zawqari*

Main category: cs.AI

TL;DR: A novel framework using Metropolis-Hastings sampling trains Spiking Neural Networks (SNNs) for RL tasks, outperforming traditional methods in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: SNNs are energy-efficient but challenging to train for RL due to non-differentiable spike-based communication.

Method: Uses Metropolis-Hastings sampling to propose and accept parameter updates based on rewards, avoiding gradient-based methods.

Result: Outperforms Deep Q-Learning and prior SNN-based RL methods in reward maximization and resource efficiency on AcroBot and CartPole benchmarks.

Conclusion: The MH-based approach effectively trains SNNs for RL, offering a viable alternative to gradient-based methods.

Abstract: Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient
alternatives to traditional Deep Neural Networks (DNNs) for real-time control
systems. However, their training presents several challenges, particularly for
reinforcement learning (RL) tasks, due to the non-differentiable nature of
spike-based communication. In this work, we introduce what is, to our
knowledge, the first framework that employs Metropolis-Hastings (MH) sampling,
a Bayesian inference technique, to train SNNs for dynamical agent control in RL
environments without relying on gradient-based methods. Our approach
iteratively proposes and probabilistically accepts network parameter updates
based on accumulated reward signals, effectively circumventing the limitations
of backpropagation while enabling direct optimization on neuromorphic
platforms. We evaluated this framework on two standard control benchmarks:
AcroBot and CartPole. The results demonstrate that our MH-based approach
outperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL
approaches in terms of maximizing the accumulated reward while minimizing
network resources and training episodes.

</details>


### [15] [eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation](https://arxiv.org/abs/2507.09588)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.AI

TL;DR: eSapiens is an AIaaS platform integrating proprietary data, workflows, and LLMs, offering businesses control, security, and automation. It excels in retrieval and generation tasks, improving precision and factual alignment.


<details>
  <summary>Details</summary>
Motivation: To provide businesses with an in-house AI solution for secure, efficient, and high-impact automation while retaining knowledge and data control.

Method: Combines structured document ingestion, hybrid vector retrieval, no-code orchestration via LangChain, and supports top LLMs. Features THOR Agent for SQL-style queries and insights.

Result: Achieves 91.3% retrieval precision (Top-3) and up to 23% better factual alignment in generation tasks.

Conclusion: eSapiens effectively enables trustworthy, auditable AI workflows for high-stakes domains like legal and finance.

Abstract: We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a
business-oriented trifecta: proprietary data, operational workflows, and any
major agnostic Large Language Model (LLM). eSapiens gives businesses full
control over their AI assets, keeping everything in-house for AI knowledge
retention and data security. eSapiens AI Agents (Sapiens) empower your team by
providing valuable insights and automating repetitive tasks, enabling them to
focus on high-impact work and drive better business outcomes.
  The system integrates structured document ingestion, hybrid vector retrieval,
and no-code orchestration via LangChain, and supports top LLMs including
OpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which
handles structured SQL-style queries and generates actionable insights over
enterprise databases.
  To evaluate the system, we conduct two experiments. First, a retrieval
benchmark on legal corpora reveals that a chunk size of 512 tokens yields the
highest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation
quality test using TRACe metrics across five LLMs shows that eSapiens delivers
more context-consistent outputs with up to a 23% improvement in factual
alignment.
  These results demonstrate the effectiveness of eSapiens in enabling
trustworthy, auditable AI workflows for high-stakes domains like legal and
finance.

</details>


### [16] [The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development](https://arxiv.org/abs/2507.09611)
*Jenis Winsta*

Main category: cs.AI

TL;DR: The paper reviews AI's overlooked environmental and ethical challenges, focusing on energy consumption, e-waste, compute inequality, and cybersecurity energy demands.


<details>
  <summary>Details</summary>
Motivation: To highlight systemic issues in AI's rapid expansion, such as high emissions, hardware turnover, and global disparities, advocating for responsible development.

Method: Draws from recent studies and institutional reports to analyze AI's environmental and ethical impacts.

Result: Identifies key research gaps and systemic issues, emphasizing the need for sustainable and equitable AI practices.

Conclusion: AI progress must align with ethical responsibility and environmental stewardship for a sustainable future.

Abstract: Artificial intelligence (AI) has made remarkable progress in recent years,
yet its rapid expansion brings overlooked environmental and ethical challenges.
This review explores four critical areas where AI's impact extends beyond
performance: energy consumption, electronic waste (e-waste), inequality in
compute access, and the hidden energy burden of cybersecurity systems. Drawing
from recent studies and institutional reports, the paper highlights systemic
issues such as high emissions from model training, rising hardware turnover,
global infrastructure disparities, and the energy demands of securing AI. By
connecting these concerns, the review contributes to Responsible AI discourse
by identifying key research gaps and advocating for sustainable, transparent,
and equitable development practices. Ultimately, it argues that AI's progress
must align with ethical responsibility and environmental stewardship to ensure
a more inclusive and sustainable technological future.

</details>


### [17] [Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs](https://arxiv.org/abs/2507.09617)
*Margherita Martorana,Francesca Urgese,Mark Adamik,Ilaria Tiddi*

Main category: cs.AI

TL;DR: A neurosymbolic framework combines multimodal language models with Knowledge Graphs (KGs) and ontologies to enhance interoperability in personal service robots, showing GPT-o1 and LLaMA 4 Maverick as top performers.


<details>
  <summary>Details</summary>
Motivation: Current robotic systems rely on proprietary, hard-coded solutions, limiting adaptability and scalability. Combining multimodal language models with structured KGs and ontologies aims to address these limitations.

Method: The proposed framework integrates robot perception data, ontologies, and five multimodal models (three LLaMA and two GPT models) to generate ontology-compliant KGs. Different neural-symbolic interaction modes are tested.

Result: GPT-o1 and LLaMA 4 Maverick outperform other models, but newer models don't always yield better results, emphasizing the importance of integration strategy.

Conclusion: The neurosymbolic framework successfully combines perceptual and symbolic strengths, though model choice and integration strategy are critical for generating effective KGs.

Abstract: Personal service robots are deployed to support daily living in domestic
environments, particularly for elderly and individuals requiring assistance.
These robots must perceive complex and dynamic surroundings, understand tasks,
and execute context-appropriate actions. However, current systems rely on
proprietary, hard-coded solutions tied to specific hardware and software,
resulting in siloed implementations that are difficult to adapt and scale
across platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to
enable interoperability across systems, through structured and standardized
representations of knowledge and reasoning. However, symbolic systems such as
KGs and ontologies struggle with raw and noisy sensory input. In contrast,
multimodal language models are well suited for interpreting input such as
images and natural language, but often lack transparency, consistency, and
knowledge grounding. In this work, we propose a neurosymbolic framework that
combines the perceptual strengths of multimodal language models with the
structured representations provided by KGs and ontologies, with the aim of
supporting interoperability in robotic applications. Our approach generates
ontology-compliant KGs that can inform robot behavior in a platform-independent
manner. We evaluated this framework by integrating robot perception data,
ontologies, and five multimodal models (three LLaMA and two GPT models), using
different modes of neural-symbolic interaction. We assess the consistency and
effectiveness of the generated KGs across multiple runs and configurations, and
perform statistical analyzes to evaluate performance. Results show that GPT-o1
and LLaMA 4 Maverick consistently outperform other models. However, our
findings also indicate that newer models do not guarantee better results,
highlighting the critical role of the integration strategy in generating
ontology-compliant KGs.

</details>


### [18] [humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems](https://arxiv.org/abs/2507.09626)
*Rodion Nazarov,Anthony Quinn,Robert Shorten,Jakub Marecek*

Main category: cs.AI

TL;DR: A PyTorch toolkit for stochastic control in AI systems ensures fairness and robustness in multi-agent interactions.


<details>
  <summary>Details</summary>
Motivation: Regulating AI systems interacting with multiple agents requires fairness and robustness guarantees, which are complex to model.

Method: Uses stochastic control techniques in a PyTorch-based toolkit to model AI systems and multi-agent interactions.

Result: Provides a priori guarantees for fairness and robustness in closed-loop multi-agent systems.

Conclusion: The toolkit simplifies fairness guarantees for AI systems in multi-agent settings.

Abstract: Artificial intelligence (AI) systems often interact with multiple agents. The
regulation of such AI systems often requires that {\em a priori\/} guarantees
of fairness and robustness be satisfied. With stochastic models of agents'
responses to the outputs of AI systems, such {\em a priori\/} guarantees
require non-trivial reasoning about the corresponding stochastic systems. Here,
we present an open-source PyTorch-based toolkit for the use of stochastic
control techniques in modelling interconnections of AI systems and properties
of their repeated uses. It models robustness and fairness desiderata in a
closed-loop fashion, and provides {\em a priori\/} guarantees for these
interconnections. The PyTorch-based toolkit removes much of the complexity
associated with the provision of fairness guarantees for closed-loop models of
multi-agent systems.

</details>


### [19] [Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey](https://arxiv.org/abs/2507.09662)
*Jason Zhu,Hongyu Li*

Main category: cs.AI

TL;DR: Survey on concise and adaptive thinking for large reasoning models (LRMs) to address inefficiencies in lengthy reasoning chains.


<details>
  <summary>Details</summary>
Motivation: LRMs generate redundant reasoning for trivial tasks, wasting resources and increasing response time, hindering practical use.

Method: Overview of methodologies, benchmarks, and challenges for adaptive reasoning between fast and slow thinking.

Result: Identifies need for adaptive reasoning to optimize LRM efficiency.

Conclusion: Aims to guide researchers in developing adaptive thinking solutions for better LRM utilization.

Abstract: Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have
demonstrated impressive performance on complex reasoning tasks like mathematics
and programming with long Chain-of-Thought (CoT) reasoning sequences
(slow-thinking), compared with traditional large language models
(fast-thinking). However, these reasoning models also face a huge challenge
that generating unnecessarily lengthy and redundant reasoning chains even for
trivial questions. This phenomenon leads to a significant waste of inference
resources, increases the response time for simple queries, and hinders the
practical application of LRMs in real-world products. To this end, it is
crucial to shorten lengthy reasoning chains and learn adaptive reasoning
between fast and slow thinking based on input difficulty. In this survey, we
provide a comprehensive overview of recent progress in concise and adaptive
thinking for efficient reasoning of LRMs, including methodologies, benchmarks,
and challenges for future exploration. We hope this survey can help researchers
quickly understand the landscape of this field and inspire novel adaptive
thinking ideas to facilitate better usage of LRMs.

</details>


### [20] [Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations](https://arxiv.org/abs/2507.09742)
*Xiaofeng Xiao,Bo Shen,Xubo Yue*

Main category: cs.AI

TL;DR: The paper proposes a causality-informed deep Q-network (Causal DQ) for optimal sensor placement in anomaly detection, addressing limitations of existing methods by integrating causal analysis without impractical interventions.


<details>
  <summary>Details</summary>
Motivation: The growing need for real-time monitoring in AI-driven manufacturing, coupled with limited resources, necessitates optimal sensor placement strategies that detect anomalies efficiently. Existing methods often ignore causality or rely on impractical interventions.

Method: The authors introduce Causal DQ, a deep Q-network that integrates causal information during training to improve convergence and theoretical error bounds.

Result: Causal DQ reduces anomaly detection time significantly across various settings, proving effective for large-scale, real-world data streams.

Conclusion: The approach not only enhances sensor placement for anomaly detection but also offers broader applications in causality-informed reinforcement learning for engineering.

Abstract: Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume
of data streams requiring real-time monitoring continues to grow. However, due
to limited resources, it is impractical to place sensors at every location to
detect unexpected shifts. Therefore, it is necessary to develop an optimal
sensor placement strategy that enables partial observability of the system
while detecting anomalies as quickly as possible. Numerous approaches have been
proposed to address this challenge; however, most existing methods consider
only variable correlations and neglect a crucial factor: Causality. Moreover,
although a few techniques incorporate causal analysis, they rely on
interventions-artificially creating anomalies-to identify causal effects, which
is impractical and might lead to catastrophic losses. In this paper, we
introduce a causality-informed deep Q-network (Causal DQ) approach for
partially observable sensor placement in anomaly detection. By integrating
causal information at each stage of Q-network training, our method achieves
faster convergence and tighter theoretical error bounds. Furthermore, the
trained causal-informed Q-network significantly reduces the detection time for
anomalies under various settings, demonstrating its effectiveness for sensor
placement in large-scale, real-world data streams. Beyond the current
implementation, our technique's fundamental insights can be applied to various
reinforcement learning problems, opening up new possibilities for real-world
causality-informed machine learning methods in engineering applications.

</details>


### [21] [Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations](https://arxiv.org/abs/2507.09751)
*Bradley P. Allen,Prateek Chhikara,Thomas Macaulay Ferguson,Filip Ilievski,Paul Groth*

Main category: cs.AI

TL;DR: A method integrates LLMs into paraconsistent logic to address their logical inconsistency, leveraging their knowledge while preserving soundness and completeness.


<details>
  <summary>Details</summary>
Motivation: LLMs show logical inconsistency despite strong language capabilities, needing a way to harness their knowledge for formal reasoning.

Method: Integrate LLM into the interpretation function of paraconsistent logic's formal semantics.

Result: Feasibility demonstrated via evaluation on factuality benchmarks, preserving logic properties.

Conclusion: The method provides a neuro-symbolic framework for consistent reasoning with LLMs.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but they exhibit problems with
logical consistency in the output they generate. How can we harness LLMs'
broad-coverage parametric knowledge in formal reasoning despite their
inconsistency? We present a method for directly integrating an LLM into the
interpretation function of the formal semantics for a paraconsistent logic. We
provide experimental evidence for the feasibility of the method by evaluating
the function using datasets created from several short-form factuality
benchmarks. Unlike prior work, our method offers a theoretical framework for
neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the
underlying logic's soundness and completeness properties.

</details>


### [22] [Technical Requirements for Halting Dangerous AI Activities](https://arxiv.org/abs/2507.09801)
*Peter Barnett,Aaron Scher,David Abecassis*

Main category: cs.AI

TL;DR: Proposes technical interventions for a coordinated halt on dangerous AI development to mitigate risks like misuse and geopolitical instability.


<details>
  <summary>Details</summary>
Motivation: Address the unprecedented risks posed by rapid AI development, including loss of control and power concentration.

Method: Outline key technical interventions to enable a coordinated halt on dangerous AI activities.

Result: Demonstrates how these interventions can restrict dangerous AI and support governance plans.

Conclusion: Technical interventions can form a foundation for AI governance to prevent worst-case outcomes.

Abstract: The rapid development of AI systems poses unprecedented risks, including loss
of control, misuse, geopolitical instability, and concentration of power. To
navigate these risks and avoid worst-case outcomes, governments may proactively
establish the capability for a coordinated halt on dangerous AI development and
deployment. In this paper, we outline key technical interventions that could
allow for a coordinated halt on dangerous AI activities. We discuss how these
interventions may contribute to restricting various dangerous AI activities,
and show how these interventions can form the technical foundation for
potential AI governance plans.

</details>


### [23] [Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation](https://arxiv.org/abs/2507.09850)
*Wei Du,Branislav Kisacanin,George Armstrong,Shubham Toshniwal,Ivan Moshkov,Alexan Ayrapetyan,Sadegh Mahdavi,Dan Zhao,Shizhe Diao,Dragan Masulovic,Marius Stanean,Advaith Avadhanam,Max Wang,Ashmit Dutta,Shitij Govil,Sri Yanamandara,Mihir Tandon,Sriram Ananthakrishnan,Vedant Rathi,David Zhang,Joonseok Kang,Leon Luo,Titu Andreescu,Boris Ginsburg,Igor Gitman*

Main category: cs.AI

TL;DR: Light fine-tuning with 20 long CoT examples from a reasoning model significantly boosts a base model's performance, surpassing larger models. Human or non-reasoning model CoT data falls short, highlighting unique qualities in expert traces.


<details>
  <summary>Details</summary>
Motivation: To explore if minimal tuning or prompting can induce long CoT reasoning in base models, leveraging small high-quality datasets.

Method: Light fine-tuning of a base model using 20 long CoT examples from a reasoning model, compared with CoT data from non-reasoning models and humans.

Result: The fine-tuned model outperforms a much larger model, while human or non-reasoning CoT data fails to match reasoning model traces.

Conclusion: Small, high-quality reasoning datasets can unlock strong reasoning in base models, but expert CoT traces have irreplaceable qualities.

Abstract: Reasoning-capable language models achieve state-of-the-art performance in
diverse complex tasks by generating long, explicit Chain-of-Thought (CoT)
traces. While recent works show that base models can acquire such reasoning
traces via reinforcement learning or distillation from stronger models like
DeepSeek-R1, previous works demonstrate that even short CoT prompting without
fine-tuning is able to improve reasoning. We ask whether long CoT can be
induced in a base model using only prompting or minimal tuning. Using just 20
long CoT examples from the reasoning model \texttt{QwQ-32B-Preview}, we lightly
fine-tune the base model \texttt{Qwen2.5-32B}. The resulting model outperforms
the much larger \texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of
high-quality examples can unlock strong reasoning capabilities. We further
explore using CoT data from non-reasoning models and human annotators, enhanced
with prompt engineering, multi-pass editing, and structural guidance. However,
neither matches the performance of reasoning model traces, suggesting that
certain latent qualities of expert CoT are difficult to replicate. We analyze
key properties of reasoning data, such as problem difficulty, diversity, and
answer length, that influence reasoning distillation. While challenges remain,
we are optimistic that carefully curated human-written CoT, even in small
quantities, can activate reasoning behaviors in base models. We release our
human-authored dataset across refinement stages and invite further
investigation into what makes small-scale reasoning supervision so effective.

</details>


### [24] [Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems](https://arxiv.org/abs/2507.09854)
*Aniruddha Chattopadhyay,Raj Dandekar,Kaushik Roy*

Main category: cs.AI

TL;DR: The paper reinterprets instruction-tuned large language models as neurosymbolic AI systems, using natural language as the symbolic layer and grounding through internal representations. It explores novel learning and reasoning methods, showing promise in improving efficiency and reliability.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between neural networks and symbolic AI by leveraging the strengths of both, aiming for robust and verifiable reasoning alongside scalable learning.

Method: Reinterprets large language models as symbolic AI systems with natural language as the symbolic layer, grounding through internal representations. Develops novel learning and reasoning approaches aligned with traditional paradigms.

Result: Preliminary evaluations on axiomatic deductive reasoning tasks show improved learning efficiency and reasoning reliability.

Conclusion: The proposed framework effectively integrates neural and symbolic AI, offering a promising direction for neurosymbolic AI research.

Abstract: Neurosymbolic artificial intelligence (AI) systems combine neural network and
classical symbolic AI mechanisms to exploit the complementary strengths of
large scale, generalizable learning and robust, verifiable reasoning. Numerous
classifications of neurosymbolic AI illustrate how these two components can be
integrated in distinctly different ways. In this work, we propose
reinterpreting instruction tuned large language models as model grounded
symbolic AI systems where natural language serves as the symbolic layer and
grounding is achieved through the models internal representation space. Within
this framework, we investigate and develop novel learning and reasoning
approaches that preserve structural similarities to traditional learning and
reasoning paradigms. Preliminary evaluations across axiomatic deductive
reasoning procedures of varying complexity provide insights into the
effectiveness of our approach in improving learning efficiency and reasoning
reliability.

</details>


### [25] [VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains](https://arxiv.org/abs/2507.09884)
*Xuzhao Li,Xuchen Li,Shiyu Hu,Yongzhen Guo,Wentao Zhang*

Main category: cs.AI

TL;DR: The paper introduces VerifyBench, a benchmark for evaluating verifiers in RLVR, highlighting trade-offs between specialized and general verifiers.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of systematic evaluation of verifiers in RLVR, which limits reliable development.

Method: Proposes VerifyBench with 4,000 expert-level questions across domains, rigorous annotation, and a four-dimensional experimental framework.

Result: Specialized verifiers lead in accuracy but lack recall; general models are inclusive but unstable. Verifiers are sensitive to input structure and struggle with cross-domain generalization.

Conclusion: The study reveals critical bottlenecks in verifier technology, guiding future improvements in RLVR.

Abstract: Large language models (LLMs) increasingly rely on reinforcement learning (RL)
to enhance their reasoning capabilities through feedback. A critical challenge
is verifying the consistency of model-generated responses and reference
answers, since these responses are often lengthy, diverse, and nuanced.
Rule-based verifiers struggle with complexity, prompting the use of model-based
verifiers. However, specialized verifiers lack flexibility, while general LLM
judges can be inconsistent. Existing research primarily focuses on building
better verifiers, yet a systematic evaluation of different types of verifiers'
performance across domains remains lacking, severely constraining the reliable
development of Reinforcement Learning with Verifiable Reward (RLVR). To address
this, we propose VerifyBench--a cross-domain comprehensive benchmark for
systematically evaluating verifiers. We construct 4,000 expert-level questions
covering mathematics, physics, chemistry, and biology. Each question is
equipped with reference answers and diverse responses. The reliability of the
evaluation is ensured through a rigorous annotation process conducted by a
multidisciplinary expert team. We design a four-dimensional experimental
framework to comprehensively compare the performance boundaries of specialized
verifiers and general LLMs under combined conditions of extracted answers vs.
complete responses, and short vs. long outputs. Our evaluation uncovers
fundamental trade-offs in verifiers: while specialized verifiers achieve
leading accuracy, they exhibit deficiencies in recall; general models show
stronger inclusivity but unstable precision. More importantly, we discover
verifiers' high sensitivity to input structure and inherent limitations in
cross-domain generalization, providing critical insights into the bottlenecks
of current verifier technology.

</details>


### [26] [DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models](https://arxiv.org/abs/2507.09955)
*Luolin Xiong,Haofen Wang,Xi Chen,Lu Sheng,Yun Xiong,Jingping Liu,Yanghua Xiao,Huajun Chen,Qing-Long Han,Yang Tang*

Main category: cs.AI

TL;DR: DeepSeek's V3 and R1 models are notable for their cost-effectiveness, high performance, and open-source nature. The paper reviews AI model evolution, introduces novel algorithms (MLA, MoE, MTP, GRPO), and discusses engineering breakthroughs and competitive impact.


<details>
  <summary>Details</summary>
Motivation: To highlight DeepSeek's innovations in AI models and their influence on the competitive landscape, while exploring future trends in large AI model development.

Method: Reviews AI model evolution, introduces novel algorithms, and analyzes engineering optimizations in scaling, training, and inference.

Result: DeepSeek's models demonstrate competitive advantages in performance and cost, with novel algorithms and engineering optimizations setting new benchmarks.

Conclusion: DeepSeek's contributions advance AI model development, with insights pointing to future trends in data, training, and reasoning for large models.

Abstract: DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their
V3 and R1 series models, which attracted global attention due to their low
cost, high performance, and open-source advantages. This paper begins by
reviewing the evolution of large AI models focusing on paradigm shifts, the
mainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.
Subsequently, the paper highlights novel algorithms introduced by DeepSeek,
including Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),
Multi-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).
The paper then explores DeepSeek engineering breakthroughs in LLM scaling,
training, inference, and system-level optimization architecture. Moreover, the
impact of DeepSeek models on the competitive AI landscape is analyzed,
comparing them to mainstream LLMs across various fields. Finally, the paper
reflects on the insights gained from DeepSeek innovations and discusses future
trends in the technical and engineering development of large AI models,
particularly in data, training, and reasoning.

</details>


### [27] [Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient](https://arxiv.org/abs/2507.09989)
*Xiaoyang Yu,Youfang Lin,Shuo Wang,Sheng Han*

Main category: cs.AI

TL;DR: OMDPG algorithm resolves the conflict between monotonic improvement and partial parameter-sharing in heterogeneous MARL by introducing OMQ and GQC, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous MARL requires partial parameter-sharing for high performance, but existing methods like HAPPO fail due to policy drift when combined with sequential updates.

Method: Proposes OMDPG: replaces sequential Q-functions with OMQ, introduces GQC for stable Q-values, and uses CCGA architecture for parameter-sharing and global Q-function computation.

Result: OMDPG outperforms state-of-the-art MARL baselines in SMAC and MAMuJoCo environments.

Conclusion: OMDPG effectively balances monotonic improvement and parameter-sharing, achieving superior performance in heterogeneous MARL.

Abstract: In heterogeneous multi-agent reinforcement learning (MARL), achieving
monotonic improvement plays a pivotal role in enhancing performance. The HAPPO
algorithm proposes a feasible solution by introducing a sequential update
scheme, which requires independent learning with No Parameter-sharing (NoPS).
However, heterogeneous MARL generally requires Partial Parameter-sharing
(ParPS) based on agent grouping to achieve high cooperative performance. Our
experiments prove that directly combining ParPS with the sequential update
scheme leads to the policy updating baseline drift problem, thereby failing to
achieve improvement. To solve the conflict between monotonic improvement and
ParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)
algorithm. First, we replace the sequentially computed $Q_{\psi}^s(s,a_{1:i})$
with the Optimal Marginal Q (OMQ) function $\phi_{\psi}^*(s,a_{1:i})$ derived
from Q-functions. This maintains MAAD's monotonic improvement while eliminating
the conflict through optimal joint action sequences instead of sequential
policy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)
as the critic function, employing pessimistic uncertainty-constrained loss to
optimize different Q-value estimations. This provides the required Q-values for
OMQ computation and stable baselines for actor updates. Finally, we implement a
Centralized Critic Grouped Actor (CCGA) architecture that simultaneously
achieves ParPS in local policy networks and accurate global Q-function
computation. Experimental results in SMAC and MAMuJoCo environments demonstrate
that OMDPG outperforms various state-of-the-art MARL baselines.

</details>


### [28] [On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model](https://arxiv.org/abs/2507.10000)
*Mark Burgess*

Main category: cs.AI

TL;DR: The paper introduces a low-cost method to interpret latent intentionality in data using Promise Theory and Semantic Spacetime, without extensive training or reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the lack of attention to practical intent in science and technology, the paper explores a pragmatic interpretation of intentionality.

Method: Uses Promise Theory's Semantic Spacetime model to identify themes and concepts via process coherence, scale separation, and anomaly detection.

Result: Demonstrates an elementary yet effective way to assess intentionality in data, feasible for basic organisms due to low computational cost.

Conclusion: The approach provides a scalable, low-resource method for interpreting latent intentionality, though its effectiveness depends on the agent's memory capacity.

Abstract: Since Searle's work deconstructing intent and intentionality in the realm of
philosophy, the practical meaning of intent has received little attention in
science and technology. Intentionality and context are both central to the
scope of Promise Theory's model of Semantic Spacetime, used as an effective
Tiny Language Model. One can identify themes and concepts from a text, on a low
level (without knowledge of the specific language) by using process coherence
as a guide. Any agent process can assess superficially a degree of latent
`intentionality' in data by looking for anomalous multi-scale anomalies and
assessing the work done to form them. Scale separation can be used to sort
parts into `intended' content and `ambient context', using the spacetime
coherence as a measure. This offers an elementary but pragmatic interpretation
of latent intentionality for very low computational cost, and without reference
to extensive training or reasoning capabilities. The process is well within the
reach of basic organisms as it does not require large scale artificial
probabilistic batch processing. The level of concept formation depends,
however, on the memory capacity of the agent.

</details>


### [29] [Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.10007)
*Zijun Chen,Wenbo Hu,Richang Hong*

Main category: cs.AI

TL;DR: The paper introduces a method to improve Chain of Thought (CoT) reasoning reliability by using the model's intrinsic veracity encoding to dynamically select accurate reasoning paths.


<details>
  <summary>Details</summary>
Motivation: CoT reasoning in LLMs and MLLMs suffers from error accumulation in intermediate steps, reducing reliability.

Method: Leverages truthfulness-sensitive attention head activations to train a confidence predictor, dynamically selecting reasoning paths via beam search.

Result: Outperforms state-of-the-art baselines in mathematical, symbolic, and commonsense reasoning tasks, showing superior accuracy and reliability.

Conclusion: Provides a novel reliability improvement for CoT reasoning with broad applicability, validated on large reasoning models.

Abstract: Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning
capabilities in both large language models (LLMs) and multimodal large language
models (MLLMs). However, its reliability is often undermined by the
accumulation of errors in intermediate steps. This paper introduces an novel
approach to calibrate the CoT reasoning accuracy by leveraging the model's
intrinsic veracity encoding. We discover that specific attention head
activations reliably reflect the truthfulness of reasoning steps in CoT. Based
on this insight, we train a confidence predictor to evaluate the correctness of
each reasoning step using these truthfulness-sensitive activations, dynamically
selecting the most plausible reasoning path via beam search. Experimental
results demonstrate that our method significantly outperforms the
state-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and
Self-Evaluation Guided Beam Search) across the mathematical, symbolic, and
commonsense reasoning tasks, exhibiting superior accuracy and reliability in
both unimodal and multimodal settings. We further validate the approach on
large reasoning models, confirming its applicability to specialized reasoning
models. Additionally, we explore the role of the model's self-correction
ability in CoT reasoning. This work provides a novel reliability improvement
path for CoT reasoning with broad application potential.

</details>


### [30] [Automating SPARQL Query Translations between DBpedia and Wikidata](https://arxiv.org/abs/2507.10045)
*Malte Christian Bartels,Debayan Banerjee,Ricardo Usbeck*

Main category: cs.AI

TL;DR: The paper evaluates LLMs for translating SPARQL queries between KG schemas (DBpedia-Wikidata and DBLP-OpenAlex), finding performance varies by model and prompting strategy.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in KG interoperability by assessing LLMs' ability to translate SPARQL queries across different KG schemas.

Method: Two benchmarks (DBpedia-Wikidata and DBLP-OpenAlex) were used to test three LLMs (Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, Mistral-Large-Instruct-2407) with zero-shot, few-shot, and chain-of-thought prompting.

Result: Performance varied by model and prompting, with Wikidata-to-DBpedia translations outperforming the reverse.

Conclusion: LLMs show promise for SPARQL translation but require further optimization for consistent performance across schemas.

Abstract: This paper investigates whether state-of-the-art Large Language Models (LLMs)
can automatically translate SPARQL between popular Knowledge Graph (KG)
schemas. We focus on translations between the DBpedia and Wikidata KG, and
later on DBLP and OpenAlex KG. This study addresses a notable gap in KG
interoperability research by rigorously evaluating LLM performance on
SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first
align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100
DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic
KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and
Mistral-Large-Instruct-2407 are selected based on their sizes and architectures
and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs
were compared with gold answers, and resulting errors were categorized. We find
that the performance varies markedly across models and prompting strategies,
and that translations for Wikidata to DBpedia work far better than translations
for DBpedia to Wikidata.

</details>


### [31] [On Gradual Semantics for Assumption-Based Argumentation](https://arxiv.org/abs/2507.10076)
*Anna Rapberger,Fabrizio Russo,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: The paper introduces gradual semantics for assumption-based argumentation (ABA), filling a gap in computational argumentation by extending modular gradual semantics from QBAFs to ABA frameworks.


<details>
  <summary>Details</summary>
Motivation: Gradual semantics are useful for ABA frameworks, but existing work has not addressed this. The paper aims to bridge this gap.

Method: The authors propose novel gradual semantics for ABA by abstracting ABA frameworks into bipolar set-based argumentation frameworks and generalizing QBAF semantics. They also explore an argument-based approach as a baseline.

Result: The proposed gradual ABA semantics satisfy adapted properties like balance and monotonicity. Experiments compare these semantics with an argument-based counterpart and assess convergence.

Conclusion: The paper successfully extends gradual semantics to ABA, demonstrating their applicability and comparing them with alternative approaches.

Abstract: In computational argumentation, gradual semantics are fine-grained
alternatives to extension-based and labelling-based semantics . They ascribe a
dialectical strength to (components of) arguments sanctioning their degree of
acceptability. Several gradual semantics have been studied for abstract,
bipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,
to a lesser extent, for some forms of structured argumentation. However, this
has not been the case for assumption-based argumentation (ABA), despite it
being a popular form of structured argumentation with several applications
where gradual semantics could be useful. In this paper, we fill this gap and
propose a family of novel gradual semantics for equipping assumptions, which
are the core components in ABA frameworks, with dialectical strengths. To do
so, we use bipolar set-based argumentation frameworks as an abstraction of
(potentially non-flat) ABA frameworks and generalise state-of-the-art modular
gradual semantics for QBAFs. We show that our gradual ABA semantics satisfy
suitable adaptations of desirable properties of gradual QBAF semantics, such as
balance and monotonicity. We also explore an argument-based approach that
leverages established QBAF modular semantics directly, and use it as baseline.
Finally, we conduct experiments with synthetic ABA frameworks to compare our
gradual ABA semantics with its argument-based counterpart and assess
convergence.

</details>


### [32] [BlueGlass: A Framework for Composite AI Safety](https://arxiv.org/abs/2507.10106)
*Harshal Nandigramwar,Syed Qutub,Kay-Ulrich Scholl*

Main category: cs.AI

TL;DR: BlueGlass is a framework for integrating diverse AI safety tools to enhance model robustness, demonstrated through analyses on vision-language models.


<details>
  <summary>Details</summary>
Motivation: Existing safety tools are fragmented and lack integration, necessitating a unified approach for comprehensive AI safety.

Method: Introduces BlueGlass, a framework for composite safety workflows, and applies it to vision-language models with three analyses: distributional evaluation, probe-based layer dynamics, and sparse autoencoders.

Result: Reveals performance trade-offs, shared hierarchical learning, and interpretable concepts, showcasing the framework's utility.

Conclusion: BlueGlass provides foundational infrastructure for robust AI systems, addressing the need for integrated safety methodologies.

Abstract: As AI systems become increasingly capable and ubiquitous, ensuring the safety
of these systems is critical. However, existing safety tools often target
different aspects of model safety and cannot provide full assurance in
isolation, highlighting a need for integrated and composite methodologies. This
paper introduces BlueGlass, a framework designed to facilitate composite AI
safety workflows by providing a unified infrastructure enabling the integration
and composition of diverse safety tools that operate across model internals and
outputs. Furthermore, to demonstrate the utility of this framework, we present
three safety-oriented analyses on vision-language models for the task of object
detection: (1) distributional evaluation, revealing performance trade-offs and
potential failure modes across distributions; (2) probe-based analysis of layer
dynamics highlighting shared hierarchical learning via phase transition; and
(3) sparse autoencoders identifying interpretable concepts. More broadly, this
work contributes foundational infrastructure and findings for building more
robust and reliable AI systems.

</details>


### [33] [Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration](https://arxiv.org/abs/2507.10119)
*Sadig Gojayev,Ahmad Anaqreh,Carolina Fortuna*

Main category: cs.AI

TL;DR: The paper analyzes AI planning and RL approaches for edge-cloud application migration, modeled as Towers of Hanoi problems, and introduces a new classification based on state space.


<details>
  <summary>Details</summary>
Motivation: To understand and compare techniques for orchestrating application migration in edge-cloud systems for better QoS and cost efficiency.

Method: Identifies and compares AI planning and RL approaches, introduces a new classification based on state space, and analyzes models through this lens.

Result: Provides insights into available techniques for orchestrating migration in computing continuum environments.

Conclusion: The study highlights effective AI and RL methods for solving edge-cloud migration problems, offering a new classification framework.

Abstract: Application migration in edge-cloud system enables high QoS and cost
effective service delivery. However, automatically orchestrating such migration
is typically solved with heuristic approaches. Starting from the Markov
Decision Process (MDP), in this paper, we identify, analyze and compare
selected state-of-the-art Artificial Intelligence (AI) planning and
Reinforcement Learning (RL) approaches for solving the class of edge-cloud
application migration problems that can be modeled as Towers of Hanoi (ToH)
problems. We introduce a new classification based on state space definition and
analyze the compared models also through this lense. The aim is to understand
available techniques capable of orchestrating such application migration in
emerging computing continuum environments.

</details>


### [34] [Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making](https://arxiv.org/abs/2507.10124)
*Thomas T. Hills*

Main category: cs.AI

TL;DR: The paper explores using metacognitive prompts, like "could you be wrong?", to debias LLMs by revealing hidden biases and contradictions in their responses.


<details>
  <summary>Details</summary>
Motivation: To develop general debiasing strategies for LLMs that remain effective as models evolve, inspired by human decision-making interventions.

Method: Applying metacognitive prompts from human psychology to LLMs, specifically "could you be wrong?", to elicit latent biases and reflections.

Result: The prompt successfully uncovers biases, errors, and alternative perspectives in LLM responses, improving metacognition and alignment with user intent.

Conclusion: Human psychology offers valuable insights for prompt engineering in LLMs, enhancing their reliability and reducing biases.

Abstract: Identifying bias in LLMs is ongoing. Because they are still in development,
what is true today may be false tomorrow. We therefore need general strategies
for debiasing that will outlive current models. Strategies developed for
debiasing human decision making offer one promising approach as they
incorporate an LLM-style prompt intervention designed to bring latent knowledge
into awareness during decision making. LLMs trained on vast amounts of
information contain information about potential biases, counter-arguments, and
contradictory evidence, but that information may only be brought to bear if
prompted. Metacognitive prompts developed in the human decision making
literature are designed to achieve this, and as I demonstrate here, they show
promise with LLMs. The prompt I focus on here is "could you be wrong?"
Following an LLM response, this prompt leads LLMs to produce additional
information, including why they answered as they did, errors, biases,
contradictory evidence, and alternatives, none of which were apparent in their
initial response. Indeed, this metaknowledge often reveals that how LLMs and
users interpret prompts are not aligned. Here I demonstrate this prompt using a
set of questions taken from recent articles about LLM biases, including
implicit discriminatory biases and failures of metacognition. "Could you be
wrong" prompts the LLM to identify its own biases and produce cogent
metacognitive reflection. I also present another example involving convincing
but incomplete information, which is readily corrected by the metacognitive
prompt. In sum, this work argues that human psychology offers a new avenue for
prompt engineering, leveraging a long history of effective prompt-based
improvements to human decision making.

</details>


### [35] [FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring](https://arxiv.org/abs/2507.10134)
*Yousef Emami,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida*

Main category: cs.AI

TL;DR: The paper proposes FRSICL, an LLM-based online scheme for UAV wildfire monitoring, optimizing flight control and data collection to minimize AoI, outperforming DRL methods like PPO.


<details>
  <summary>Details</summary>
Motivation: Current DRL methods for UAV-assisted wildfire monitoring suffer from inefficiencies and impracticality for real-time applications, necessitating a more adaptive solution.

Method: FRSICL uses LLM-enabled in-context learning to dynamically optimize UAV flight control and sensor data scheduling via natural language descriptions and feedback.

Result: Simulations show FRSICL outperforms PPO and Nearest-Neighbor baselines in minimizing average AoI.

Conclusion: FRSICL offers a practical, efficient alternative to DRL for real-time UAV wildfire monitoring, reducing AoI without extensive retraining.

Abstract: Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in
wildfire monitoring, where early detection minimizes environmental impact. In
UAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor
transmission scheduling and velocity is critical for minimizing Age of
Information (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has
been used for such optimization; however, its limitations such as low sampling
efficiency, simulation-to-reality gaps, and complex training render it
unsuitable for time-critical applications like wildfire monitoring. This paper
introduces a new online Flight Resource Allocation scheme based on LLM-Enabled
In-Context Learning (FRSICL) to jointly optimize the UAV's flight control and
data collection schedule along the trajectory in real time, thereby
asymptotically minimizing the average AoI across ground sensors. In contrast to
DRL, FRSICL generates data collection schedules and controls velocity using
natural language task descriptions and feedback from the environment, enabling
dynamic decision-making without extensive retraining. Simulation results
confirm the effectiveness of the proposed FRSICL compared to Proximal Policy
Optimization (PPO) and Nearest-Neighbor baselines.

</details>


### [36] [Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review](https://arxiv.org/abs/2507.10142)
*Siyi Hu,Mohamad A Hady,Jianglin Qiao,Jimmy Cao,Mahardhika Pratama,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: The paper introduces 'adaptability' as a framework to evaluate MARL algorithms in dynamic real-world multi-agent systems, addressing challenges like fluctuating agent populations and evolving tasks.


<details>
  <summary>Details</summary>
Motivation: MARL's real-world deployment is limited due to dynamic environments with complex variability. The paper aims to improve MARL reliability under shifting conditions.

Method: Proposes a structured adaptability framework with three dimensions: learning adaptability, policy adaptability, and scenario-driven adaptability.

Result: The framework enables principled assessment of MARL performance beyond static benchmarks.

Conclusion: The adaptability perspective supports developing MARL algorithms better suited for dynamic real-world systems.

Abstract: Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in
coordinating multiple agents across simulated benchmarks and constrained
scenarios. However, its deployment in real-world multi-agent systems (MAS)
remains limited, primarily due to the complex and dynamic nature of such
environments. These challenges arise from multiple interacting sources of
variability, including fluctuating agent populations, evolving task goals, and
inconsistent execution conditions. Together, these factors demand that MARL
algorithms remain effective under continuously changing system configurations
and operational demands. To better capture and assess this capacity for
adjustment, we introduce the concept of \textit{adaptability} as a unified and
practically grounded lens through which to evaluate the reliability of MARL
algorithms under shifting conditions, broadly referring to any changes in the
environment dynamics that may occur during learning or execution. Centred on
the notion of adaptability, we propose a structured framework comprising three
key dimensions: learning adaptability, policy adaptability, and scenario-driven
adaptability. By adopting this adaptability perspective, we aim to support more
principled assessments of MARL performance beyond narrowly defined benchmarks.
Ultimately, this survey contributes to the development of algorithms that are
better suited for deployment in dynamic, real-world multi-agent systems.

</details>


### [37] [Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation](https://arxiv.org/abs/2507.10156)
*Lubnaa Abdur Rahman,Ioannis Papathanail,Stavroula Mougiakakou*

Main category: cs.AI

TL;DR: The paper introduces SwissFKG, a knowledge graph integrating recipes, ingredients, substitutions, and nutritional data for Switzerland, enhanced by LLMs, to improve dietary assessment tools.


<details>
  <summary>Details</summary>
Motivation: Existing dietary assessment systems ignore non-visual factors like ingredient substitutions and individual dietary needs, and Swiss food data is fragmented.

Method: Developed SwissFKG with an LLM-powered enrichment pipeline, benchmarked four LLMs, and implemented a Graph-RAG application for nutrition queries.

Result: LLMs effectively enriched SwissFKG with nutritional data, and the graph provides detailed ingredient-level information and aligns with guidelines.

Conclusion: SwissFKG advances dietary assessment by combining visual, contextual, and cultural dimensions, setting a foundation for future tools.

Abstract: AI has driven significant progress in the nutrition field, especially through
multimedia-based automatic dietary assessment. However, existing automatic
dietary assessment systems often overlook critical non-visual factors, such as
recipe-specific ingredient substitutions that can significantly alter
nutritional content, and rarely account for individual dietary needs, including
allergies, restrictions, cultural practices, and personal preferences. In
Switzerland, while food-related information is available, it remains
fragmented, and no centralized repository currently integrates all relevant
nutrition-related aspects within a Swiss context. To bridge this divide, we
introduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our
best knowledge, to unite recipes, ingredients, and their substitutions with
nutrient data, dietary restrictions, allergen information, and national
nutrition guidelines under one graph. We establish a LLM-powered enrichment
pipeline for populating the graph, whereby we further present the first
benchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge
augmentation. Our results demonstrate that LLMs can effectively enrich the
graph with relevant nutritional information. Our SwissFKG goes beyond recipe
recommendations by offering ingredient-level information such as allergen and
dietary restriction information, and guidance aligned with nutritional
guidelines. Moreover, we implement a Graph-RAG application to showcase how the
SwissFKG's rich natural-language data structure can help LLM answer
user-specific nutrition queries, and we evaluate LLM-embedding pairings by
comparing user-query responses against predefined expected answers. As such,
our work lays the foundation for the next generation of dietary assessment
tools that blend visual, contextual, and cultural dimensions of eating.

</details>


### [38] [Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?](https://arxiv.org/abs/2507.10174)
*Yumi Omori,Zixuan Dong,Keith Ross*

Main category: cs.AI

TL;DR: Decision Transformer (DT) is compared to MLP-based Filtered Behavior Cloning (FBC) in sparse-reward settings, with FBC outperforming DT in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of DT in sparse-reward environments compared to simpler methods like FBC.

Method: Experiments on Robomimic and D4RL benchmarks, comparing DT with FBC, which filters low-performing trajectories before behavior cloning.

Result: FBC achieves competitive or superior performance to DT, requiring less data and being more efficient.

Conclusion: DT may not be preferable for sparse-reward environments, raising questions about its general applicability.

Abstract: In recent years, extensive work has explored the application of the
Transformer architecture to reinforcement learning problems. Among these,
Decision Transformer (DT) has gained particular attention in the context of
offline reinforcement learning due to its ability to frame return-conditioned
policy learning as a sequence modeling task. Most recently, Bhargava et al.
(2024) provided a systematic comparison of DT with more conventional MLP-based
offline RL algorithms, including Behavior Cloning (BC) and Conservative
Q-Learning (CQL), and claimed that DT exhibits superior performance in
sparse-reward and low-quality data settings.
  In this paper, through experimentation on robotic manipulation tasks
(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered
Behavior Cloning (FBC) achieves competitive or superior performance compared to
DT in sparse-reward environments. FBC simply filters out low-performing
trajectories from the dataset and then performs ordinary behavior cloning on
the filtered dataset. FBC is not only very straightforward, but it also
requires less training data and is computationally more efficient. The results
therefore suggest that DT is not preferable for sparse-reward environments.
From prior work, arguably, DT is also not preferable for dense-reward
environments. Thus, we pose the question: Is DT ever preferable?

</details>


### [39] [Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks](https://arxiv.org/abs/2507.10208)
*Hamzah Ziadeh,Hendrik Knoche*

Main category: cs.AI

TL;DR: The paper addresses challenges in XAI research, proposing a method to categorize studies by 'what, why, and who' dimensions to improve clarity and design recommendations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to resolve contradictions and gaps in XAI research by better understanding tasks requiring AI assistance.

Method: The method involves drawing from visual analytics, cognition, and dashboard design to categorize and compare XAI studies.

Result: Key problems identified include inadequate task descriptions, context-free studies, and insufficient user testing. The paper proposes guidelines for reporting user expertise and study design.

Conclusion: The contribution aims to help researchers identify relevant studies, gaps, and handle contradictions in XAI design.

Abstract: Research into explainable artificial intelligence (XAI) for data analysis
tasks suffer from a large number of contradictions and lack of concrete design
recommendations stemming from gaps in understanding the tasks that require AI
assistance. In this paper, we drew on multiple fields such as visual analytics,
cognition, and dashboard design to propose a method for categorising and
comparing XAI studies under three dimensions: what, why, and who. We identified
the main problems as: inadequate descriptions of tasks, context-free studies,
and insufficient testing with target users. We propose that studies should
specifically report on their users' domain, AI, and data analysis expertise to
illustrate the generalisability of their findings. We also propose study
guidelines for designing and reporting XAI tasks to improve the XAI community's
ability to parse the rapidly growing field. We hope that our contribution can
help researchers and designers better identify which studies are most relevant
to their work, what gaps exist in the research, and how to handle contradictory
results regarding XAI design.

</details>


### [40] [Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence](https://arxiv.org/abs/2507.10281)
*Jiaming Tian,Liyao Li,Wentao Ye,Haobo Wang,Lingxin Wang,Lihua Yu,Zujie Ren,Gang Chen,Junbo Zhao*

Main category: cs.AI

TL;DR: The paper surveys LLM-based Table Agents for automating table-centric workflows, addressing real-world challenges like noise and heterogeneity. It defines five core competencies and highlights gaps in performance for open-source models in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Real-world table tasks involve noise, structural heterogeneity, and semantic complexity, which are underexplored in existing research focused on clean academic datasets.

Method: The survey defines five core competencies (C1-C5) to analyze and compare current approaches, focusing on LLM-based Table Agents. It also examines the Text-to-SQL Agent to reveal performance gaps.

Result: The study identifies a performance gap between academic benchmarks and real-world scenarios, particularly for open-source models.

Conclusion: The paper provides actionable insights to enhance the robustness, generalization, and efficiency of LLM-based Table Agents in practical settings.

Abstract: Tables are fundamental in domains such as finance, healthcare, and public
administration, yet real-world table tasks often involve noise, structural
heterogeneity, and semantic complexity--issues underexplored in existing
research that primarily targets clean academic datasets. This survey focuses on
LLM-based Table Agents, which aim to automate table-centric workflows by
integrating preprocessing, reasoning, and domain adaptation. We define five
core competencies--C1: Table Structure Understanding, C2: Table and Query
Semantic Understanding, C3: Table Retrieval and Compression, C4: Executable
Reasoning with Traceability, and C5: Cross-Domain Generalization--to analyze
and compare current approaches. In addition, a detailed examination of the
Text-to-SQL Agent reveals a performance gap between academic benchmarks and
real-world scenarios, especially for open-source models. Finally, we provide
actionable insights to improve the robustness, generalization, and efficiency
of LLM-based Table Agents in practical settings.

</details>


### [41] [Instance space analysis of the capacitated vehicle routing problem](https://arxiv.org/abs/2507.10397)
*Alessandra M. M. M. Gouvêa,Nuno Paulos,Eduardo Uchoa e Mariá C. V. Nascimento*

Main category: cs.AI

TL;DR: The paper introduces Instance Space Analysis (ISA) to study how instance characteristics affect metaheuristic performance in CVRP, using dimensionality reduction and machine learning to create a 2D projection for analysis.


<details>
  <summary>Details</summary>
Motivation: To understand the nuanced relationships between instance characteristics and metaheuristic performance in CVRP.

Method: Combines ISA with DIMACS dataset, using PRELIM, SIFTED, and PILOT stages for dimensionality reduction and machine learning to project instance space.

Result: Identified 23 relevant instance characteristics and created a 2D projection matrix for analyzing MH behavior.

Conclusion: Provides a new method for instance analysis in CVRP, enabling easy incorporation of new instances.

Abstract: This paper seeks to advance CVRP research by addressing the challenge of
understanding the nuanced relationships between instance characteristics and
metaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a
valuable tool that allows for a new perspective on the field. By combining the
ISA methodology with a dataset from the DIMACS 12th Implementation Challenge on
Vehicle Routing, our research enabled the identification of 23 relevant
instance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,
which employ dimensionality reduction and machine learning methods, allowed us
to create a two-dimensional projection of the instance space to understand how
the structure of instances affect the behavior of MHs. A key contribution of
our work is that we provide a projection matrix, which makes it straightforward
to incorporate new instances into this analysis and allows for a new method for
instance analysis in the CVRP field.

</details>


### [42] [SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning](https://arxiv.org/abs/2507.10421)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.AI

TL;DR: The paper introduces a novel model combining BERT for sentiment analysis and XGBoost for socio-demographic/behavioral data to predict student dropout in distance learning, achieving 84% accuracy.


<details>
  <summary>Details</summary>
Motivation: Early detection of student dropout is crucial for intervention and perseverance, especially in distance learning.

Method: Combines BERT for sentiment analysis of student comments and XGBoost for socio-demographic/behavioral data, fine-tuned and merged using feature importance techniques.

Result: Achieved 84% accuracy, outperforming the baseline (82%), with better precision and F1-score.

Conclusion: The model is a promising tool for personalized dropout prevention strategies.

Abstract: School dropout is a serious problem in distance learning, where early
detection is crucial for effective intervention and student perseverance.
Predicting student dropout using available educational data is a widely
researched topic in learning analytics. Our partner's distance learning
platform highlights the importance of integrating diverse data sources,
including socio-demographic data, behavioral data, and sentiment analysis, to
accurately predict dropout risks. In this paper, we introduce a novel model
that combines sentiment analysis of student comments using the Bidirectional
Encoder Representations from Transformers (BERT) model with socio-demographic
and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We
fine-tuned BERT on student comments to capture nuanced sentiments, which were
then merged with key features selected using feature importance techniques in
XGBoost. Our model was tested on unseen data from the next academic year,
achieving an accuracy of 84\%, compared to 82\% for the baseline model.
Additionally, the model demonstrated superior performance in other metrics,
such as precision and F1-score. The proposed method could be a vital tool in
developing personalized strategies to reduce dropout rates and encourage
student perseverance

</details>


### [43] [Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures](https://arxiv.org/abs/2507.10446)
*Sudarshan Babu*

Main category: cs.AI

TL;DR: The paper proposes architectures like neural memory and hypernetworks to enable efficient transfer learning in data-scarce domains, demonstrating applications in 3D scene generation, segmentation, and molecular property prediction.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of transfer learning in domains with limited data, such as computational chemistry and medical imaging, where large pre-trained models are infeasible.

Method: Uses neural memory for adaptation on non-stationary distributions and hypernetworks trained with MAML for generalizable priors. Applied to 3D scene generation, segmentation, and molecular property prediction.

Result: Hypernetworks efficiently acquire priors with minimal data, enabling faster text-to-3D generation and improved molecular property prediction.

Conclusion: The proposed architectures offer scalable solutions for transfer learning in data-scarce domains, with demonstrated success in diverse applications.

Abstract: The ability to transfer knowledge from prior experiences to novel tasks
stands as a pivotal capability of intelligent agents, including both humans and
computational models. This principle forms the basis of transfer learning,
where large pre-trained neural networks are fine-tuned to adapt to downstream
tasks. Transfer learning has demonstrated tremendous success, both in terms of
task adaptation speed and performance. However there are several domains where,
due to lack of data, training such large pre-trained models or foundational
models is not a possibility - computational chemistry, computational
immunology, and medical imaging are examples. To address these challenges, our
work focuses on designing architectures to enable efficient acquisition of
priors when large amounts of data are unavailable. In particular, we
demonstrate that we can use neural memory to enable adaptation on
non-stationary distributions with only a few samples. Then we demonstrate that
our hypernetwork designs (a network that generates another network) can acquire
more generalizable priors than standard networks when trained with Model
Agnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene
generation, demonstrating that they can acquire priors efficiently on just a
handful of training scenes, thereby leading to faster text-to-3D generation. We
then extend our hypernetwork framework to perform 3D segmentation on novel
scenes with limited data by efficiently transferring priors from earlier viewed
scenes. Finally, we repurpose an existing molecular generative method as a
pre-training framework that facilitates improved molecular property prediction,
addressing critical challenges in computational immunology

</details>


### [44] [DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology](https://arxiv.org/abs/2507.10522)
*Jennifer D'Souza,Endres Keno Sander,Andrei Aioanei*

Main category: cs.AI

TL;DR: DeepResearch$^{\text{Eco}}$ is an LLM-based system for automated scientific synthesis, improving literature retrieval diversity and depth with user-controllable, transparent reasoning.


<details>
  <summary>Details</summary>
Motivation: To enhance search diversity and nuance in scientific literature retrieval, addressing limitations of conventional retrieval-augmented generation pipelines.

Method: Uses recursive, depth- and breadth-controlled exploration with transparent reasoning and parameter-driven configurability.

Result: Achieves up to 21x increase in source integration and 14.9x rise in sources per 1,000 words, with expert-level depth at high-parameter settings.

Conclusion: DeepResearch$^{\text{Eco}}$ effectively integrates domain-specific evidence with analytical rigor, offering a scalable solution for scientific synthesis.

Abstract: We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system
for automated scientific synthesis that supports recursive, depth- and
breadth-controlled exploration of original research questions -- enhancing
search diversity and nuance in the retrieval of relevant scientific literature.
Unlike conventional retrieval-augmented generation pipelines, DeepResearch
enables user-controllable synthesis with transparent reasoning and
parameter-driven configurability, facilitating high-throughput integration of
domain-specific evidence while maintaining analytical rigor. Applied to 49
ecological research questions, DeepResearch achieves up to a 21-fold increase
in source integration and a 14.9-fold rise in sources integrated per 1,000
words. High-parameter settings yield expert-level analytical depth and
contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [Recurrent Expansion: A Pathway Toward the Next Generation of Deep Learning](https://arxiv.org/abs/2507.08828)
*Tarek Berghout*

Main category: cs.LG

TL;DR: Recurrent Expansion (RE) introduces a new learning paradigm that extends beyond traditional ML/DL by learning from model behavior, enabling iterative self-improvement and scalable, adaptive AI.


<details>
  <summary>Details</summary>
Motivation: To move beyond static data representations in DL by incorporating evolving model behavior for self-improvement and adaptive intelligence.

Method: RE involves multiple mappings of data through identical architectures, analyzing feature maps and performance signals. Variants like MVRE, HMVRE, and Sc-HMVRE extend this with parallel instances, diverse architectures, and scalability.

Result: RE enables models to iteratively improve by learning from their own behavior, leading to scalable and adaptive AI systems.

Conclusion: RE shifts DL from representational learning to behavior-aware, self-evolving systems, paving the way for introspective and adaptive AI.

Abstract: This paper introduces Recurrent Expansion (RE) as a new learning paradigm
that advances beyond conventional Machine Learning (ML) and Deep Learning (DL).
While DL focuses on learning from static data representations, RE proposes an
additional dimension: learning from the evolving behavior of models themselves.
RE emphasizes multiple mappings of data through identical deep architectures
and analyzes their internal representations (i.e., feature maps) in conjunction
with observed performance signals such as loss. By incorporating these
behavioral traces, RE enables iterative self-improvement, allowing each model
version to gain insight from its predecessors. The framework is extended
through Multiverse RE (MVRE), which aggregates signals from parallel model
instances, and further through Heterogeneous MVRE (HMVRE), where models of
varying architectures contribute diverse perspectives. A scalable and adaptive
variant, Sc-HMVRE, introduces selective mechanisms and scale diversity for
real-world deployment. Altogether, RE presents a shift in DL: from purely
representational learning to behavior-aware, self-evolving systems. It lays the
groundwork for a new class of intelligent models capable of reasoning over
their own learning dynamics, offering a path toward scalable, introspective,
and adaptive artificial intelligence. A simple code example to support
beginners in running their own experiments is provided in Code Availability
Section of this paper.

</details>


### [46] [Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI](https://arxiv.org/abs/2507.08829)
*Kimia Soroush,Nastaran Shirazi,Mohsen Raji*

Main category: cs.LG

TL;DR: The paper proposes an efficient TMR approach for DNN reliability against bit-flip faults using XAI, specifically LRP, to selectively protect critical weights, achieving significant reliability improvements with low overhead.


<details>
  <summary>Details</summary>
Motivation: Ensuring DNN reliability in safety-critical domains is crucial, and TMR is effective but costly. Selective application of TMR based on accurate importance metrics can optimize its efficiency.

Method: Uses Layer-wise Relevance Propagation (LRP), a gradient-based XAI technique, to calculate importance scores for DNN parameters. These scores guide selective TMR application on critical weights.

Result: Evaluated on VGG16 and AlexNet with MNIST and CIFAR-10, the method improves AlexNet reliability by over 60% at a bit error rate of 10-4, matching state-of-the-art overhead.

Conclusion: The proposed XAI-based TMR approach effectively enhances DNN reliability against bit-flip faults with minimal overhead, demonstrating its practicality for safety-critical applications.

Abstract: Deep Neural Networks (DNNs) are widely employed in safety-critical domains,
where ensuring their reliability is essential. Triple Modular Redundancy (TMR)
is an effective technique to enhance the reliability of DNNs in the presence of
bit-flip faults. In order to handle the significant overhead of TMR, it is
applied selectively on the parameters and components with the highest
contribution at the model output. Hence, the accuracy of the selection
criterion plays the key role on the efficiency of TMR. This paper presents an
efficient TMR approach to enhance the reliability of DNNs against bit-flip
faults using an Explainable Artificial Intelligence (XAI) method. Since XAI can
provide valuable insights about the importance of individual neurons and
weights in the performance of the network, they can be applied as the selection
metric in TMR techniques. The proposed method utilizes a low-cost,
gradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to
calculate importance scores for DNN parameters. These scores are then used to
enhance the reliability of the model, with the most critical weights being
protected by TMR. The proposed approach is evaluated on two DNN models, VGG16
and AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate
that the method can protect the AlexNet model at a bit error rate of 10-4,
achieving over 60% reliability improvement while maintaining the same overhead
as state-of-the-art methods.

</details>


### [47] [A Hybrid Machine Learning Framework for Optimizing Crop Selection via Agronomic and Economic Forecasting](https://arxiv.org/abs/2507.08832)
*Niranjan Mallikarjun Sindhur,Pavithra C,Nivya Muchikel*

Main category: cs.LG

TL;DR: A hybrid decision support system combines machine learning (Random Forest and LSTM) with a voice-based interface in Kannada to help low-literacy farmers in Karnataka optimize crop choices for profitability.


<details>
  <summary>Details</summary>
Motivation: Farmers in developing regions face market and climate volatility but lack digital access due to literacy barriers. This system aims to bridge that gap.

Method: Uses a Random Forest for agronomic suitability and an LSTM for market price forecasting, delivered via a voice interface in Kannada.

Result: Random Forest achieves 98.5% accuracy in suitability prediction; LSTM forecasts prices with low error.

Conclusion: The system enhances financial resilience for marginalized farmers by providing data-driven, accessible recommendations.

Abstract: Farmers in developing regions like Karnataka, India, face a dual challenge:
navigating extreme market and climate volatility while being excluded from the
digital revolution due to literacy barriers. This paper presents a novel
decision support system that addresses both challenges through a unique
synthesis of machine learning and human-computer interaction. We propose a
hybrid recommendation engine that integrates two predictive models: a Random
Forest classifier to assess agronomic suitability based on soil, climate, and
real-time weather data, and a Long Short-Term Memory (LSTM) network to forecast
market prices for agronomically viable crops. This integrated approach shifts
the paradigm from "what can grow?" to "what is most profitable to grow?",
providing a significant advantage in mitigating economic risk. The system is
delivered through an end-to-end, voice-based interface in the local Kannada
language, leveraging fine-tuned speech recognition and high-fidelity speech
synthesis models to ensure accessibility for low-literacy users. Our results
show that the Random Forest model achieves 98.5% accuracy in suitability
prediction, while the LSTM model forecasts harvest-time prices with a low
margin of error. By providing data-driven, economically optimized
recommendations through an inclusive interface, this work offers a scalable and
impactful solution to enhance the financial resilience of marginalized farming
communities.

</details>


### [48] [LoRA Is Slower Than You Think](https://arxiv.org/abs/2507.08833)
*Seokmin Ko*

Main category: cs.LG

TL;DR: LoRA reduces parameters for LLM fine-tuning but lacks consistent speed improvements. The paper analyzes LoRA's limitations and proposes more efficient methods.


<details>
  <summary>Details</summary>
Motivation: Inconsistency in LoRA's speed improvements across models and setups.

Method: Comprehensive analysis of LoRA's performance and proposal of new efficient fine-tuning methods.

Result: New methods achieve comparable or better performance with more consistent speed improvements.

Conclusion: Provides insights and guidelines for optimizing LLM fine-tuning under resource constraints.

Abstract: Low-Rank Adaptation (LoRA) is one of the most widely used techniques for
fine-tuning large language models (LLMs). By introducing a small number of
trainable low-rank weight matrices, LoRA substantially reduces the number of
parameters that need to be updated, offering significant advantages in memory
consumption and computational efficiency compared to full fine-tuning. However,
we observed that LoRA does not consistently provide speed improvements across
all model architectures and training setups. Motivated by this inconsistency,
we conduct a comprehensive analysis of LoRA's performance and investigate the
underlying factors limiting its speedup. Based on our findings, we propose
several methods for more efficient fine-tuning of LLMs. We empirically evaluate
these methods and compare them to LoRA, demonstrating that our approach
achieves comparable or superior performance while delivering more consistent
training speed improvements. Our work offers valuable insights and practical
guidelines for practitioners seeking to optimize LLM fine-tuning under resource
constraints.

</details>


### [49] [Physical Informed Neural Networks for modeling ocean pollutant](https://arxiv.org/abs/2507.08834)
*Karishma Battina,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: A Physics-Informed Neural Network (PINN) framework is introduced to simulate pollutant dispersion in oceans, overcoming traditional numerical method limitations by embedding physical laws and noisy synthetic data into training.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with modeling pollutant transport in large, dynamic oceanic domains due to complexity and scale.

Method: The PINN framework uses a hybrid loss function (PDE residuals, boundary/initial conditions, data fit) and noisy synthetic data (generated via FDM) for training, leveraging Julia for high-performance computing.

Result: The model achieves physically consistent predictions, addressing non-linear dynamics and boundary/initial condition enforcement.

Conclusion: PINN offers a scalable, flexible alternative to traditional solvers for pollutant dispersion simulation.

Abstract: Traditional numerical methods often struggle with the complexity and scale of
modeling pollutant transport across vast and dynamic oceanic domains. This
paper introduces a Physics-Informed Neural Network (PINN) framework to simulate
the dispersion of pollutants governed by the 2D advection-diffusion equation.
The model achieves physically consistent predictions by embedding physical laws
and fitting to noisy synthetic data, generated via a finite difference method
(FDM), directly into the neural network training process. This approach
addresses challenges such as non-linear dynamics and the enforcement of
boundary and initial conditions. Synthetic data sets, augmented with varying
noise levels, are used to capture real-world variability. The training
incorporates a hybrid loss function including PDE residuals, boundary/initial
condition conformity, and a weighted data fit term. The approach takes
advantage of the Julia language scientific computing ecosystem for
high-performance simulations, offering a scalable and flexible alternative to
traditional solvers

</details>


### [50] [Representation learning with a transformer by contrastive learning for money laundering detection](https://arxiv.org/abs/2507.08835)
*Harold Guéneau,Alain Celisse,Pascal Delange*

Main category: cs.LG

TL;DR: A transformer-based method for money laundering detection using contrastive learning and a two-thresholds approach to control false positives.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of detecting money laundering with minimal supervision by leveraging structured time series data.

Method: Uses a transformer neural network for representation learning via contrastive learning, followed by a scoring system and the Benjamini-Hochberg procedure for thresholding.

Result: Outperforms rule-based and LSTM methods, effectively detecting fraudsters and nonfraudsters while controlling false positives.

Conclusion: The transformer-based approach is superior for money laundering detection, requiring less expert supervision and offering better performance.

Abstract: The present work tackles the money laundering detection problem. A new
procedure is introduced which exploits structured time series of both
qualitative and quantitative data by means of a transformer neural network. The
first step of this procedure aims at learning representations of time series
through contrastive learning (without any labels). The second step leverages
these representations to generate a money laundering scoring of all
observations. A two-thresholds approach is then introduced, which ensures a
controlled false-positive rate by means of the Benjamini-Hochberg (BH)
procedure. Experiments confirm that the transformer is able to produce general
representations that succeed in exploiting money laundering patterns with
minimal supervision from domain experts. It also illustrates the higher ability
of the new procedure for detecting nonfraudsters as well as fraudsters, while
keeping the false positive rate under control. This greatly contrasts with
rule-based procedures or the ones based on LSTM architectures.

</details>


### [51] [Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing](https://arxiv.org/abs/2507.08836)
*Damien Fovet,Shashank Chamoli,Sarah Oury,Srishti Singhal*

Main category: cs.LG

TL;DR: CompactifAI compression method applied to Llama 3.1 8B reduces computational resources while maintaining accuracy, enhancing efficiency and cost-effectiveness.


<details>
  <summary>Details</summary>
Motivation: To evaluate the performance of CompactifAI in compressing large language models like Llama 3.1 8B, focusing on efficiency (energy consumption) and accuracy.

Method: Used Codecarbon for energy efficiency and Ragas for accuracy, comparing the compressed model (CompactifAI) with the full-size version.

Result: The compressed model significantly reduced computational resources without compromising accuracy.

Conclusion: CompactifAI makes large language models more efficient, scalable, and cost-effective.

Abstract: This study evaluates the performance of a compression method, called
CompactifAI, developed by Multiverse Computing, applied to the large language
model Llama 3.1 8B\cite{llama}. The evaluation focused on model efficiency (in
terms of energy consumption) and accuracy using respectively the frameworks
Codecarbon\cite{codecarbon} and Ragas\cite{ragas}. A comparison was performed
between the model compressed with
CompactifAI\cite{compactifai}\cite{compactifai2} and its full-size version. Our
findings reveal that the compressed model using CompactifAI not only
significantly reduced the computational resources but also maintained the model
accuracy, making the model more efficient, scalable and cost-effective.

</details>


### [52] [wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models](https://arxiv.org/abs/2507.08838)
*Xiaohang Tang,Rares Dolga,Sangwoong Yoon,Ilija Bogunovic*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Improving the reasoning capabilities of diffusion-based large language models
(dLLMs) through reinforcement learning (RL) remains an open problem. The
intractability of dLLMs likelihood function necessitates approximating the
current, old, and reference policy likelihoods at each policy optimization
step. This reliance introduces additional computational overhead and lead to
potentially large bias -- particularly when approximation errors occur in the
denominator of policy ratios used for importance sampling. To mitigate these
issues, we introduce $\mathtt{wd1}$, a novel policy optimization approach that
reformulates the objective as a weighted likelihood, requiring only a single
approximation for the current parametrized policy likelihood. Experiments on
widely used reasoning benchmarks demonstrate that $\mathtt{wd1}$, without
supervised fine-tuning (SFT) or any supervised data, outperforms existing RL
methods for dLLMs, achieving up to 16% higher accuracy. $\mathtt{wd1}$ delivers
additional computational gains, including reduced training time and fewer
function evaluations (NFEs) per gradient step. These findings, combined with
the simplicity of method's implementation and R1-Zero-like training (no SFT),
position $\mathtt{wd1}$ as a more effective and efficient method for applying
RL to dLLMs reasoning.

</details>


### [53] [Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer](https://arxiv.org/abs/2507.08839)
*Xiaowei Yu,Jing Zhang,Tong Chen,Yan Zhuang,Minheng Chen,Chao Cao,Yanjun Lyu,Lu Zhang,Li Su,Tianming Liu,Dajiang Zhu*

Main category: cs.LG

TL;DR: The paper proposes a Transferability Aware Transformer (TAT) to adapt knowledge from Alzheimer's disease (AD) data to improve Lewy Body Disease (LBD) diagnosis, addressing data scarcity and domain shift issues.


<details>
  <summary>Details</summary>
Motivation: LBD is understudied and shares similarities with AD, but lacks sufficient data for deep learning. AD data is abundant but suffers from domain shift when applied to LBD.

Method: The TAT model uses structural connectivity from MRI, leveraging attention to prioritize transferable features and suppress domain-specific ones.

Result: TAT effectively reduces domain shift and improves LBD diagnostic accuracy despite limited data.

Conclusion: This is the first study to address AD-to-LBD domain adaptation under data scarcity, offering a framework for rare disease diagnosis.

Abstract: Lewy Body Disease (LBD) is a common yet understudied form of dementia that
imposes a significant burden on public health. It shares clinical similarities
with Alzheimer's disease (AD), as both progress through stages of normal
cognition, mild cognitive impairment, and dementia. A major obstacle in LBD
diagnosis is data scarcity, which limits the effectiveness of deep learning. In
contrast, AD datasets are more abundant, offering potential for knowledge
transfer. However, LBD and AD data are typically collected from different sites
using different machines and protocols, resulting in a distinct domain shift.
To effectively leverage AD data while mitigating domain shift, we propose a
Transferability Aware Transformer (TAT) that adapts knowledge from AD to
enhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived
from structural MRI as training data. Built on the attention mechanism, TAT
adaptively assigns greater weights to disease-transferable features while
suppressing domain-specific ones, thereby reducing domain shift and improving
diagnostic accuracy with limited LBD data. The experimental results demonstrate
the effectiveness of TAT. To the best of our knowledge, this is the first study
to explore domain adaptation from AD to LBD under conditions of data scarcity
and domain shift, providing a promising framework for domain-adaptive diagnosis
of rare diseases.

</details>


### [54] [Zero-Shot Neural Architecture Search with Weighted Response Correlation](https://arxiv.org/abs/2507.08841)
*Kun Jing,Luoyu Chen,Jungang Xu,Jianwei Tai,Yiyu Wang,Shuaimin Li*

Main category: cs.LG

TL;DR: A novel training-free proxy, WRCor, is introduced for efficient neural architecture search (NAS), outperforming existing methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency and instability of current zero-shot NAS methods by proposing a more effective training-free proxy.

Method: WRCor uses correlation coefficient matrices of responses across input samples to evaluate architectures without training.

Result: WRCor achieves a 22.1% test error on ImageNet-1k in just 4 GPU hours, surpassing most existing NAS algorithms.

Conclusion: WRCor is a highly efficient and generalizable proxy for zero-shot NAS, demonstrated by superior performance in architecture search.

Abstract: Neural architecture search (NAS) is a promising approach for automatically
designing neural network architectures. However, the architecture estimation of
NAS is computationally expensive and time-consuming because of training
multiple architectures from scratch. Although existing zero-shot NAS methods
use training-free proxies to accelerate the architecture estimation, their
effectiveness, stability, and generality are still lacking. We present a novel
training-free estimation proxy called weighted response correlation (WRCor).
WRCor utilizes correlation coefficient matrices of responses across different
input samples to calculate the proxy scores of estimated architectures, which
can measure their expressivity and generalizability. Experimental results on
proxy evaluation demonstrate that WRCor and its voting proxies are more
efficient estimation strategies than existing proxies. We also apply them with
different search strategies in architecture search. Experimental results on
architecture search show that our zero-shot NAS algorithm outperforms most
existing NAS algorithms in different search spaces. Our NAS algorithm can
discover an architecture with a 22.1% test error on the ImageNet-1k dataset
within 4 GPU hours. All codes are publicly available at
https://github.com/kunjing96/ZSNAS-WRCor.git.

</details>


### [55] [Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing](https://arxiv.org/abs/2507.08842)
*Zhufeng Lu,Chentao Jia,Ming Hu,Xiaofei Xie,Mingsong Chen*

Main category: cs.LG

TL;DR: FedRAS is a communication-efficient Federated Recommender System framework that reduces communication overhead by clustering gradients into actions, avoiding performance degradation while accommodating heterogeneous environments.


<details>
  <summary>Details</summary>
Motivation: FedRecs face high communication overhead and low training efficiency due to massive item embeddings and heterogeneous environments, with existing compression methods degrading model performance.

Method: FedRAS uses an action-sharing strategy to cluster gradients into model updating actions for communication, avoiding direct compression of embeddings, and includes an adaptive clustering mechanism for heterogeneous conditions.

Result: FedRAS reduces communication payloads by up to 96.88% without sacrificing recommendation performance across heterogeneous scenarios.

Conclusion: FedRAS effectively addresses communication and efficiency issues in FedRecs, offering a practical solution with open-source availability.

Abstract: As a promising privacy-aware collaborative model training paradigm, Federated
Learning (FL) is becoming popular in the design of distributed recommender
systems. However, Federated Recommender Systems (FedRecs) greatly suffer from
two major problems: i) extremely high communication overhead due to massive
item embeddings involved in recommendation systems, and ii) intolerably low
training efficiency caused by the entanglement of both heterogeneous network
environments and client devices. Although existing methods attempt to employ
various compression techniques to reduce communication overhead, due to the
parameter errors introduced by model compression, they inevitably suffer from
model performance degradation. To simultaneously address the above problems,
this paper presents a communication-efficient FedRec framework named FedRAS,
which adopts an action-sharing strategy to cluster the gradients of item
embedding into a specific number of model updating actions for communication
rather than directly compressing the item embeddings. In this way, the cloud
server can use the limited actions from clients to update all the items. Since
gradient values are significantly smaller than item embeddings, constraining
the directions of gradients (i.e., the action space) introduces smaller errors
compared to compressing the entire item embedding matrix into a reduced space.
To accommodate heterogeneous devices and network environments, FedRAS
incorporates an adaptive clustering mechanism that dynamically adjusts the
number of actions. Comprehensive experiments on well-known datasets demonstrate
that FedRAS can reduce the size of communication payloads by up to 96.88%,
while not sacrificing recommendation performance within various heterogeneous
scenarios. We have open-sourced FedRAS at
https://github.com/mastlab-T3S/FedRAS.

</details>


### [56] [Can We Predict Your Next Move Without Breaking Your Privacy?](https://arxiv.org/abs/2507.08843)
*Arpita Soni,Sahil Tripathi,Gautam Siddharth Kashyap,Manaswi Kulahara,Mohammad Anas Azeez,Zohaib Hasan Siddiqui,Nipun Joshi,Jiechao Gao*

Main category: cs.LG

TL;DR: FLLL3M is a federated learning framework using large language models for privacy-preserving next-location prediction, achieving state-of-the-art results with reduced resource usage.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns in next-location prediction while maintaining high accuracy and efficiency.

Method: Retains user data locally and employs large language models via an efficient outer product mechanism.

Result: Achieves SOTA results on datasets (Gowalla, WeePlace, Brightkite, FourSquare) with reduced parameters (45.6%) and memory usage (52.7%).

Conclusion: FLLL3M effectively balances privacy, accuracy, and resource efficiency in mobility modeling.

Abstract: We propose FLLL3M--Federated Learning with Large Language Models for Mobility
Modeling--a privacy-preserving framework for Next-Location Prediction (NxLP).
By retaining user data locally and leveraging LLMs through an efficient outer
product mechanism, FLLL3M ensures high accuracy with low resource demands. It
achieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71,
0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while
reducing parameters by up to 45.6% and memory usage by 52.7%.

</details>


### [57] [DAFOS: Dynamic Adaptive Fanout Optimization Sampler](https://arxiv.org/abs/2507.08845)
*Irfan Ullah,Young-Koo Lee*

Main category: cs.LG

TL;DR: DAFOS dynamically adjusts fanout and prioritizes important nodes in GNN training, improving speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Uniform neighbor sampling and static fanout limit GNN scalability and efficiency.

Method: Dynamic fanout adjustment based on node scoring (degree) and early stopping for diminishing performance gains.

Result: 3.57x speedup on ogbn-arxiv, 12.6x on Reddit; F1 score improvements: 68.5% to 71.21% (ogbn-arxiv), 73.78% to 76.88% (ogbn-products).

Conclusion: DAFOS is an efficient, scalable solution for large-scale GNN training.

Abstract: Graph Neural Networks (GNNs) are becoming an essential tool for learning from
graph-structured data, however uniform neighbor sampling and static fanout
settings frequently limit GNNs' scalability and efficiency. In this paper, we
propose the Dynamic Adaptive Fanout Optimization Sampler (DAFOS), a novel
approach that dynamically adjusts the fanout based on model performance and
prioritizes important nodes during training. Our approach leverages node
scoring based on node degree to focus computational resources on structurally
important nodes, incrementing the fanout as the model training progresses.
DAFOS also integrates an early stopping mechanism to halt training when
performance gains diminish. Experiments conducted on three benchmark datasets,
ogbnarxiv, Reddit, and ogbn-products, demonstrate that our approach
significantly improves training speed and accuracy compared to a
state-of-the-art approach. DAFOS achieves a 3.57x speedup on the ogbn-arxiv
dataset and a 12.6x speedup on the Reddit dataset while improving the F1 score
from 68.5% to 71.21% on ogbn-arxiv and from 73.78% to 76.88% on the
ogbn-products dataset, respectively. These results highlight the potential of
DAFOS as an efficient and scalable solution for large-scale GNN training.

</details>


### [58] [Assuring the Safety of Reinforcement Learning Components: AMLAS-RL](https://arxiv.org/abs/2507.08848)
*Calum Corrie Imrie,Ioannis Stefanakos,Sepeedeh Shahbeigi,Richard Hawkins,Simon Burton*

Main category: cs.LG

TL;DR: The paper adapts AMLAS to create AMLAS-RL, a framework for ensuring safety in RL-enabled cyber-physical systems, demonstrated with a wheeled vehicle example.


<details>
  <summary>Details</summary>
Motivation: The integration of ML, especially RL, into CPS introduces safety challenges, and existing methods lack systematic assurance for RL.

Method: The authors adapt the AMLAS methodology to develop AMLAS-RL, an iterative framework for generating safety assurance arguments in RL-enabled systems.

Result: AMLAS-RL is demonstrated using a wheeled vehicle example, showing its applicability in ensuring safe RL in CPS.

Conclusion: AMLAS-RL provides a structured approach to address safety assurance in RL-enabled CPS, filling a gap left by existing methods.

Abstract: The rapid advancement of machine learning (ML) has led to its increasing
integration into cyber-physical systems (CPS) across diverse domains. While CPS
offer powerful capabilities, incorporating ML components introduces significant
safety and assurance challenges. Among ML techniques, reinforcement learning
(RL) is particularly suited for CPS due to its capacity to handle complex,
dynamic environments where explicit models of interaction between system and
environment are unavailable or difficult to construct. However, in
safety-critical applications, this learning process must not only be effective
but demonstrably safe. Safe-RL methods aim to address this by incorporating
safety constraints during learning, yet they fall short in providing systematic
assurance across the RL lifecycle. The AMLAS methodology offers structured
guidance for assuring the safety of supervised learning components, but it does
not directly apply to the unique challenges posed by RL. In this paper, we
adapt AMLAS to provide a framework for generating assurance arguments for an
RL-enabled system through an iterative process; AMLAS-RL. We demonstrate
AMLAS-RL using a running example of a wheeled vehicle tasked with reaching a
target goal without collision.

</details>


### [59] [Foundation models for time series forecasting: Application in conformal prediction](https://arxiv.org/abs/2507.08858)
*Sami Achour,Yassine Bouher,Duong Nguyen,Nicolas Chesneau*

Main category: cs.LG

TL;DR: TSFMs outperform traditional methods in conformal prediction for time series, especially with limited data, due to better accuracy and stable calibration.


<details>
  <summary>Details</summary>
Motivation: To compare the performance of Time Series Foundation Models (TSFMs) with traditional methods in conformal prediction settings, focusing on data-limited scenarios.

Method: Comparison of TSFMs with statistical models and gradient boosting in conformal prediction, analyzing predictive accuracy and calibration stability.

Result: TSFMs provide more reliable prediction intervals and stable calibration, especially with limited data, outperforming classic models.

Conclusion: Foundation models enhance conformal prediction reliability in time series, particularly in data-constrained cases.

Abstract: The zero-shot capabilities of foundation models (FMs) for time series
forecasting offer promising potentials in conformal prediction, as most of the
available data can be allocated to calibration. This study compares the
performance of Time Series Foundation Models (TSFMs) with traditional methods,
including statistical models and gradient boosting, within a conformal
prediction setting. Our findings highlight two key advantages of TSFMs. First,
when the volume of data is limited, TSFMs provide more reliable conformalized
prediction intervals than classic models, thanks to their superior predictive
accuracy. Second, the calibration process is more stable because more data are
used for calibration. Morever, the fewer data available, the more pronounced
these benefits become, as classic models require a substantial amount of data
for effective training. These results underscore the potential of foundation
models in improving conformal prediction reliability in time series
applications, particularly in data-constrained cases. All the code to reproduce
the experiments is available.

</details>


### [60] [e-Profits: A Business-Aligned Evaluation Metric for Profit-Sensitive Customer Churn Prediction](https://arxiv.org/abs/2507.08860)
*Awais Manzoor,M. Atif Qureshi,Etain Kidney,Luca Longo*

Main category: cs.LG

TL;DR: The paper introduces e-Profits, a business-aligned metric for evaluating churn prediction models, outperforming traditional metrics like AUC and F1-score by incorporating customer-specific value, retention probability, and costs.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics like AUC and F1-score don't reflect financial outcomes, potentially misleading strategic decisions in customer retention campaigns.

Method: The authors propose e-Profits, which uses Kaplan-Meier survival analysis for personalized retention rates and evaluates models per customer, benchmarking six classifiers on telecom datasets.

Result: e-Profits reshapes model rankings, highlighting financial advantages in models overlooked by traditional metrics and providing segment-level insights for high-value customers.

Conclusion: e-Profits is a practical, post hoc tool for profit-driven model evaluation, particularly useful for marketing and analytics teams.

Abstract: Retention campaigns in customer relationship management often rely on churn
prediction models evaluated using traditional metrics such as AUC and F1-score.
However, these metrics fail to reflect financial outcomes and may mislead
strategic decisions. We introduce e-Profits, a novel business-aligned
evaluation metric that quantifies model performance based on customer-specific
value, retention probability, and intervention costs. Unlike existing
profit-based metrics such as Expected Maximum Profit, which assume fixed
population-level parameters, e-Profits uses Kaplan-Meier survival analysis to
estimate personalised retention rates and supports granular, per customer
evaluation. We benchmark six classifiers across two telecom datasets (IBM Telco
and Maven Telecom) and demonstrate that e-Profits reshapes model rankings
compared to traditional metrics, revealing financial advantages in models
previously overlooked by AUC or F1-score. The metric also enables segment-level
insight into which models maximise return on investment for high-value
customers. e-Profits is designed as an understandable, post hoc tool to support
model evaluation in business contexts, particularly for marketing and analytics
teams prioritising profit-driven decisions. All source code is available at:
https://github.com/matifq/eprofits.

</details>


### [61] [On the under-reaching phenomenon in message-passing neural PDE solvers: revisiting the CFL condition](https://arxiv.org/abs/2507.08861)
*Lucas Tesan,Mikel M. Iparraguirre,David Gonzalez,Pedro Martins,Elias Cueto*

Main category: cs.LG

TL;DR: Proposes sharp lower bounds for message-passing iterations in GNNs for solving PDEs, reducing hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in GNNs for PDEs by linking physical problem characteristics to message-passing requirements.

Method: Derives bounds for hyperbolic, parabolic, and elliptic PDEs by relating physical constants, discretisation, and GNN message-passing.

Result: Shows poor solutions if bounds are unmet, but accurate solutions when bounds are satisfied.

Conclusion: Validates bounds with examples, proving their sharpness and utility in GNN-based PDE solvers.

Abstract: This paper proposes sharp lower bounds for the number of message passing
iterations required in graph neural networks (GNNs) when solving partial
differential equations (PDE). This significantly reduces the need for
exhaustive hyperparameter tuning. Bounds are derived for the three fundamental
classes of PDEs (hyperbolic, parabolic and elliptic) by relating the physical
characteristics of the problem in question to the message-passing requirement
of GNNs. In particular, we investigate the relationship between the physical
constants of the equations governing the problem, the spatial and temporal
discretisation and the message passing mechanisms in GNNs.
  When the number of message passing iterations is below these proposed limits,
information does not propagate efficiently through the network, resulting in
poor solutions, even for deep GNN architectures. In contrast, when the
suggested lower bound is satisfied, the GNN parameterisation allows the model
to accurately capture the underlying phenomenology, resulting in solvers of
adequate accuracy.
  Examples are provided for four different examples of equations that show the
sharpness of the proposed lower bounds.

</details>


### [62] [Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond](https://arxiv.org/abs/2507.08866)
*Marina Ceccon,Giandomenico Cornacchia,Davide Dalle Pezze,Alessandro Fabris,Gian Antonio Susto*

Main category: cs.LG

TL;DR: The paper examines data biases in AI, their impact on algorithmic discrimination, and introduces the Data Bias Profile (DBP) for detection and mitigation.


<details>
  <summary>Details</summary>
Motivation: Addressing understudied data biases to improve algorithmic fairness and align with anti-discrimination policies.

Method: Analyzes three common data biases, their effects, and develops DBP for systematic bias detection.

Result: Underrepresentation is less critical than proxy and label biases; DBP effectively predicts discrimination risks.

Conclusion: DBP bridges fairness research and policy, offering a data-centric tool for bias mitigation.

Abstract: Undesirable biases encoded in the data are key drivers of algorithmic
discrimination. Their importance is widely recognized in the algorithmic
fairness literature, as well as legislation and standards on
anti-discrimination in AI. Despite this recognition, data biases remain
understudied, hindering the development of computational best practices for
their detection and mitigation. In this work, we present three common data
biases and study their individual and joint effect on algorithmic
discrimination across a variety of datasets, models, and fairness measures. We
find that underrepresentation of vulnerable populations in training sets is
less conducive to discrimination than conventionally affirmed, while
combinations of proxies and label bias can be far more critical. Consequently,
we develop dedicated mechanisms to detect specific types of bias, and combine
them into a preliminary construct we refer to as the Data Bias Profile (DBP).
This initial formulation serves as a proof of concept for how different bias
signals can be systematically documented. Through a case study with popular
fairness datasets, we demonstrate the effectiveness of the DBP in predicting
the risk of discriminatory outcomes and the utility of fairness-enhancing
interventions. Overall, this article bridges algorithmic fairness research and
anti-discrimination policy through a data-centric lens.

</details>


### [63] [GUIDE: Towards Scalable Advising for Research Ideas](https://arxiv.org/abs/2507.08870)
*Yaowenqi Liu,BingXu Meng,Rui Pan,Jerry Huang,Tong Zhang*

Main category: cs.LG

TL;DR: A small AI model with a compressed literature database and structured reasoning outperforms larger models in refining hypotheses and experimental designs, achieving high acceptance rates for top submissions.


<details>
  <summary>Details</summary>
Motivation: Address the gap in scalable advising systems for high-quality feedback on hypotheses and experimental designs in AI research.

Method: Explores model size, context length, confidence estimation, and structured reasoning to develop a robust advising system.

Result: The system outperforms general-purpose models like Deepseek-R1, with a 90% acceptance rate for high-confidence predictions on ICLR 2025.

Conclusion: The system enhances hypothesis generation and experimental design efficiency, with code publicly available.

Abstract: The field of AI research is advancing at an unprecedented pace, enabling
automated hypothesis generation and experimental design across diverse domains
such as biology, mathematics, and artificial intelligence. Despite these
advancements, there remains a significant gap in the availability of scalable
advising systems capable of providing high-quality, well-reasoned feedback to
refine proposed hypotheses and experimental designs. To address this challenge,
we explore key factors that underlie the development of robust advising
systems, including model size, context length, confidence estimation, and
structured reasoning processes. Our findings reveal that a relatively small
model, when equipped with a well-compressed literature database and a
structured reasoning framework, can outperform powerful general-purpose
language models such as Deepseek-R1 in terms of acceptance rates for
self-ranked top-30% submissions to ICLR 2025. Moreover, when limited to
high-confidence predictions, our system achieves an acceptance rate exceeding
90% on the ICLR 2025 test set, underscoring its potential to significantly
enhance the quality and efficiency of hypothesis generation and experimental
design. The code is released at
https://github.com/HowardLiu0830/GUIDE-Research-Idea-Evaluation.

</details>


### [64] [Next-Generation Travel Demand Modeling with a Generative Framework for Household Activity Coordination](https://arxiv.org/abs/2507.08871)
*Xishun Liao,Haoxuan Ma,Yifan Liu,Yuxiang Wei,Brian Yueshuai He,Chris Stanford,Jiaqi Ma*

Main category: cs.LG

TL;DR: A learning-based travel demand model is introduced, offering a scalable, data-driven alternative to traditional activity-based models, validated in Los Angeles with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional activity-based models (ABMs) are costly, inflexible, and rely on simplifications. This work aims to provide a more efficient, adaptable, and accurate solution.

Method: The framework integrates population synthesis, activity generation, location assignment, and traffic simulation into a unified, generative system.

Result: The model replicates real-world mobility patterns closely, outperforming legacy ABMs with reduced costs (e.g., 0.97 cosine similarity for OD matrix, 6.11% MAPE for traffic speed).

Conclusion: The proposed framework is scalable, transferable, and cost-effective, making it a viable alternative to traditional ABMs for travel demand modeling.

Abstract: Travel demand models are critical tools for planning, policy, and mobility
system design. Traditional activity-based models (ABMs), although grounded in
behavioral theories, often rely on simplified rules and assumptions, and are
costly to develop and difficult to adapt across different regions. This paper
presents a learning-based travel demand modeling framework that synthesizes
household-coordinated daily activity patterns based on a household's
socio-demographic profiles. The whole framework integrates population
synthesis, coordinated activity generation, location assignment, and
large-scale microscopic traffic simulation into a unified system. It is fully
generative, data-driven, scalable, and transferable to other regions. A
full-pipeline implementation is conducted in Los Angeles with a 10 million
population. Comprehensive validation shows that the model closely replicates
real-world mobility patterns and matches the performance of legacy ABMs with
significantly reduced modeling cost and greater scalability. With respect to
the SCAG ABM benchmark, the origin-destination matrix achieves a cosine
similarity of 0.97, and the daily vehicle miles traveled (VMT) in the network
yields a 0.006 Jensen-Shannon Divergence (JSD) and a 9.8% mean absolute
percentage error (MAPE). When compared to real-world observations from Caltrans
PeMS, the evaluation on corridor-level traffic speed and volume reaches a 0.001
JSD and a 6.11% MAPE.

</details>


### [65] [Contrastive Language-Image Pre-Training Model based Semantic Communication Performance Optimization](https://arxiv.org/abs/2507.08873)
*Shaoran Yang,Dongyu Wei,Hanzhi Yu,Zhaohui Yang,Yuchen Liu,Mingzhe Chen*

Main category: cs.LG

TL;DR: A novel CLIP-based semantic communication framework eliminates the need for joint training, enabling efficient semantic extraction and task implementation. Deployment over noisy wireless networks is optimized using PPO-based RL, improving performance metrics significantly.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies of standard neural network-based semantic encoders/decoders requiring joint training and the challenges of deploying semantic communication in noisy wireless environments.

Method: Uses a CLIP model for semantic communication without training, and optimizes deployment over noisy networks via PPO-based RL for CLIP model and RB allocation.

Result: Simulations show a 40% faster convergence rate and 4x higher accumulated reward compared to soft actor-critic.

Conclusion: The proposed CLIP-based framework with PPO optimization significantly enhances semantic communication performance in noisy wireless networks.

Abstract: In this paper, a novel contrastive language-image pre-training (CLIP) model
based semantic communication framework is designed. Compared to standard neural
network (e.g.,convolutional neural network) based semantic encoders and
decoders that require joint training over a common dataset, our CLIP model
based method does not require any training procedures thus enabling a
transmitter to extract data meanings of the original data without neural
network model training, and the receiver to train a neural network for
follow-up task implementation without the communications with the transmitter.
Next, we investigate the deployment of the CLIP model based semantic framework
over a noisy wireless network. Since the semantic information generated by the
CLIP model is susceptible to wireless noise and the spectrum used for semantic
information transmission is limited, it is necessary to jointly optimize CLIP
model architecture and spectrum resource block (RB) allocation to maximize
semantic communication performance while considering wireless noise, the delay
and energy used for semantic communication. To achieve this goal, we use a
proximal policy optimization (PPO) based reinforcement learning (RL) algorithm
to learn how wireless noise affect the semantic communication performance thus
finding optimal CLIP model and RB for each user. Simulation results show that
our proposed method improves the convergence rate by up to 40%, and the
accumulated reward by 4x compared to soft actor-critic.

</details>


### [66] [An Automated Classifier of Harmful Brain Activities for Clinical Usage Based on a Vision-Inspired Pre-trained Framework](https://arxiv.org/abs/2507.08874)
*Yulin Sun,Xiaopeng Si,Runnan He,Xiao Hu,Peter Smielewski,Wenlong Wang,Xiaoguang Tong,Wei Yue,Meijun Pang,Kuo Zhang,Xizi Song,Dong Ming,Xiuyun Liu*

Main category: cs.LG

TL;DR: VIPEEGNet, a CNN model, achieves high accuracy in EEG-based brain activity classification, performing comparably to human experts while using fewer parameters than competitors.


<details>
  <summary>Details</summary>
Motivation: Current AI models for EEG-based brain disease diagnosis face issues like inter-rater variability and poor generalizability, limiting clinical application.

Method: Developed and validated VIPEEGNet using two independent EEG datasets (1950 and 1532 patients) with expert annotations.

Result: High AUROC scores (0.930-0.972) for binary classification and competitive multi-class performance (sensitivity: 36.8%-88.2%, precision: 55.6%-80.4%). External validation ranked top 2 among 2767 algorithms.

Conclusion: VIPEEGNet is a robust, efficient tool for EEG-based brain activity identification, addressing limitations of existing models.

Abstract: Timely identification of harmful brain activities via electroencephalography
(EEG) is critical for brain disease diagnosis and treatment, which remains
limited application due to inter-rater variability, resource constraints, and
poor generalizability of existing artificial intelligence (AI) models. In this
study, a convolutional neural network model, VIPEEGNet, was developed and
validated using EEGs recorded from Massachusetts General Hospital/Harvard
Medical School. The VIPEEGNet was developed and validated using two independent
datasets, collected between 2006 and 2020. The development cohort included EEG
recordings from 1950 patients, with 106,800 EEG segments annotated by at least
one experts (ranging from 1 to 28). The online testing cohort consisted of EEG
segments from a subset of an additional 1,532 patients, each annotated by at
least 10 experts. For the development cohort (n=1950), the VIPEEGNet achieved
high accuracy, with an AUROC for binary classification of seizure, LPD, GPD,
LRDA, GRDA, and "other" categories at 0.972 (95% CI, 0.957-0.988), 0.962 (95%
CI, 0.954-0.970), 0.972 (95% CI, 0.960-0.984), 0.938 (95% CI, 0.917-0.959),
0.949 (95% CI, 0.941-0.957), and 0.930 (95% CI, 0.926-0.935). For multi
classification, the sensitivity of VIPEEGNET for the six categories ranges from
36.8% to 88.2% and the precision ranges from 55.6% to 80.4%, and performance
similar to human experts. Notably, the external validation showed
Kullback-Leibler Divergence (KLD)of 0.223 and 0.273, ranking top 2 among the
existing 2,767 competing algorithms, while we only used 2.8% of the parameters
of the first-ranked algorithm.

</details>


### [67] [ODIA: Oriented Distillation for Inline Acceleration of LLM-based Function Calling](https://arxiv.org/abs/2507.08877)
*Hanlong Zhang,Jingsheng Yang,Hao Li,Yuhao He,Franck Gong*

Main category: cs.LG

TL;DR: ODIA accelerates LLM-based Function Calling by distilling knowledge from larger to smaller models, reducing latency by 45-78% with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: High latency in LLM-based Function Calling negatively impacts user experience, necessitating a solution to accelerate it.

Method: ODIA identifies simple queries from production traffic and distills knowledge from larger to smaller models, leveraging online user interaction data.

Result: Response latency reduced by 45% (expected) and 78% (median); smaller model handles 60% of traffic with negligible accuracy loss.

Conclusion: ODIA is a practical, automated solution for reducing latency in production environments with minimal human intervention.

Abstract: Function Calling is a crucial technique that enables Large Language Models
(LLMs) to interact with external systems through APIs. However, the high
latency associated with LLM-based Function Calling significantly impacts user
experience. This paper presents a novel approach called Oriented Distillation
for Inline Acceleration (ODIA) that leverages online user interaction data to
accelerate Function Calling. By automatically identifying "simple queries" from
production traffic and distilling knowledge from larger models to smaller ones,
our method reduces response latency by 45% (expected) and 78% (median) while
maintaining accuracy. We demonstrate the effectiveness of our approach through
real-world deployment in a music application, where the smaller model
successfully handles 60% of traffic with negligible accuracy loss. Our method
requires minimal human intervention and continuously improves through automated
data collection and model updating, making it a practical solution for
production environments.

</details>


### [68] [Last Layer Hamiltonian Monte Carlo](https://arxiv.org/abs/2507.08905)
*Koen Vellenga,H. Joe Steinhauer,Göran Falkman,Jonas Andersson,Anders Sjögren*

Main category: cs.LG

TL;DR: LL-HMC applies HMC sampling only to the last layer of DNNs, reducing computational costs while maintaining competitive performance in classification and OOD detection.


<details>
  <summary>Details</summary>
Motivation: HMC is a gold standard for uncertainty estimation but is computationally expensive for large DNNs. LL-HMC aims to make it feasible for data-intensive scenarios.

Method: LL-HMC restricts HMC sampling to the final layer of DNNs and compares it against five LL-PDL methods on video datasets for driver action and intention.

Result: LL-HMC performs competitively in classification and OOD detection. Extra samples improve OOD detection but not classification. Multiple chains show no consistent gains.

Conclusion: LL-HMC is a viable, efficient method for uncertainty estimation in DNNs, especially for OOD detection, without significant computational overhead.

Abstract: We explore the use of Hamiltonian Monte Carlo (HMC) sampling as a
probabilistic last layer approach for deep neural networks (DNNs). While HMC is
widely regarded as a gold standard for uncertainty estimation, the
computational demands limit its application to large-scale datasets and large
DNN architectures. Although the predictions from the sampled DNN parameters can
be parallelized, the computational cost still scales linearly with the number
of samples (similar to an ensemble). Last layer HMC (LL--HMC) reduces the
required computations by restricting the HMC sampling to the final layer of a
DNN, making it applicable to more data-intensive scenarios with limited
computational resources. In this paper, we compare LL-HMC against five last
layer probabilistic deep learning (LL-PDL) methods across three real-world
video datasets for driver action and intention. We evaluate the in-distribution
classification performance, calibration, and out-of-distribution (OOD)
detection. Due to the stochastic nature of the probabilistic evaluations, we
performed five grid searches for different random seeds to avoid being reliant
on a single initialization for the hyperparameter configurations. The results
show that LL--HMC achieves competitive in-distribution classification and OOD
detection performance. Additional sampled last layer parameters do not improve
the classification performance, but can improve the OOD detection. Multiple
chains or starting positions did not yield consistent improvements.

</details>


### [69] [Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising](https://arxiv.org/abs/2507.08912)
*Tomasz Szandala,Fatima Ezzeddine,Natalia Rusin,Silvia Giordano,Omran Ayoub*

Main category: cs.LG

TL;DR: Fair-FLIP, a post-processing method, reduces biases in deepfake detection while maintaining accuracy, improving fairness by up to 30% with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Address biases in deepfake detection methods that disproportionately affect demographic subgroups, ensuring fairer AI-generated content analysis.

Method: Proposes Fair-FLIP, a post-processing technique that reweights final-layer inputs of a trained model to prioritize low-variability subgroups and reduce disparities.

Result: Fair-FLIP improves fairness metrics by up to 30% with only a 0.25% reduction in accuracy compared to baseline and state-of-the-art methods.

Conclusion: Fair-FLIP effectively balances fairness and performance in deepfake detection, offering a practical solution for mitigating biases in AI-generated content analysis.

Abstract: Artificial Intelligence-generated content has become increasingly popular,
yet its malicious use, particularly the deepfakes, poses a serious threat to
public trust and discourse. While deepfake detection methods achieve high
predictive performance, they often exhibit biases across demographic attributes
such as ethnicity and gender. In this work, we tackle the challenge of fair
deepfake detection, aiming to mitigate these biases while maintaining robust
detection capabilities. To this end, we propose a novel post-processing
approach, referred to as Fairness-Oriented Final Layer Input Prioritising
(Fair-FLIP), that reweights a trained model's final-layer inputs to reduce
subgroup disparities, prioritising those with low variability while demoting
highly variable ones. Experimental results comparing Fair-FLIP to both the
baseline (without fairness-oriented de-biasing) and state-of-the-art approaches
show that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining
baseline accuracy, with only a negligible reduction of 0.25%.
  Code is available on Github:
https://github.com/szandala/fair-deepfake-detection-toolbox

</details>


### [70] [Revisiting Convergence: Shuffling Complexity Beyond Lipschitz Smoothness](https://arxiv.org/abs/2507.08913)
*Qi He,Peiran Yu,Ziyi Chen,Heng Huang*

Main category: cs.LG

TL;DR: The paper revisits convergence rates of shuffling-type gradient methods without Lipschitz smoothness, proposing a stepsize strategy that matches best-known rates under weaker assumptions.


<details>
  <summary>Details</summary>
Motivation: Lipschitz smoothness is often not met in machine learning models, limiting the applicability of existing convergence guarantees for shuffling-type gradient methods.

Method: The authors propose a stepsize strategy and analyze convergence rates for nonconvex, strongly convex, and non-strongly convex cases under random and arbitrary shuffling schemes.

Result: The method converges under weaker assumptions and matches current best-known rates, validated by numerical experiments.

Conclusion: The work broadens the applicability of shuffling-type gradient methods, proving their efficacy under more general conditions.

Abstract: Shuffling-type gradient methods are favored in practice for their simplicity
and rapid empirical performance. Despite extensive development of convergence
guarantees under various assumptions in recent years, most require the
Lipschitz smoothness condition, which is often not met in common machine
learning models. We highlight this issue with specific counterexamples. To
address this gap, we revisit the convergence rates of shuffling-type gradient
methods without assuming Lipschitz smoothness. Using our stepsize strategy, the
shuffling-type gradient algorithm not only converges under weaker assumptions
but also match the current best-known convergence rates, thereby broadening its
applicability. We prove the convergence rates for nonconvex, strongly convex,
and non-strongly convex cases, each under both random reshuffling and arbitrary
shuffling schemes, under a general bounded variance condition. Numerical
experiments further validate the performance of our shuffling-type gradient
algorithm, underscoring its practical efficacy.

</details>


### [71] [Beyond Scores: Proximal Diffusion Models](https://arxiv.org/abs/2507.08956)
*Zhenghan Fang,Mateo Díaz,Sam Buchanan,Jeremias Sulam*

Main category: cs.LG

TL;DR: Proximal Diffusion Models (ProxDM) use backward discretization with proximal maps instead of scores, offering faster convergence and theoretical benefits over traditional score-matching methods.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and theoretical guarantees of diffusion models by replacing score-based sampling with proximal operators.

Method: Develops ProxDM by learning proximal operators of the log-density and using backward discretization of SDEs.

Result: ProxDM achieves faster convergence in fewer steps and requires fewer steps ($\widetilde{O}(d/\sqrt{\varepsilon})$) for accurate sampling.

Conclusion: ProxDM provides a practical and theoretically sound alternative to score-matching methods in diffusion models.

Abstract: Diffusion models have quickly become some of the most popular and powerful
generative models for high-dimensional data. The key insight that enabled their
development was the realization that access to the score -- the gradient of the
log-density at different noise levels -- allows for sampling from data
distributions by solving a reverse-time stochastic differential equation (SDE)
via forward discretization, and that popular denoisers allow for unbiased
estimators of this score. In this paper, we demonstrate that an alternative,
backward discretization of these SDEs, using proximal maps in place of the
score, leads to theoretical and practical benefits. We leverage recent results
in proximal matching to learn proximal operators of the log-density and, with
them, develop Proximal Diffusion Models (ProxDM). Theoretically, we prove that
$\widetilde{O}(d/\sqrt{\varepsilon})$ steps suffice for the resulting
discretization to generate an $\varepsilon$-accurate distribution w.r.t. the KL
divergence. Empirically, we show that two variants of ProxDM achieve
significantly faster convergence within just a few sampling steps compared to
conventional score-matching methods.

</details>


### [72] [Graph Neural Network Enhanced Sequential Recommendation Method for Cross-Platform Ad Campaign](https://arxiv.org/abs/2507.08959)
*Xiang Li,Xinyu Wang,Yifan Lin*

Main category: cs.LG

TL;DR: A GNN-based method improves cross-platform ad recommendation by modeling user behavior, ad content, and platform features, achieving high AUC (0.937 on Platform B).


<details>
  <summary>Details</summary>
Motivation: To enhance accuracy in cross-platform ad recommendation by capturing user interest migration and platform-specific influences.

Method: Uses GNN for multi-dimensional modeling of user behavior, ad content, and platform features to predict interest pathways.

Result: Best performance on Platform B (AUC=0.937), with slight precision/recall drops on Platforms A and C due to uneven ad labels. Hyperparameter tuning improved adaptability.

Conclusion: The GNN-based method effectively captures cross-platform user interest migration, with performance varying by platform data quality.

Abstract: In order to improve the accuracy of cross-platform advertisement
recommendation, a graph neural network (GNN)- based advertisement
recommendation method is analyzed. Through multi-dimensional modeling, user
behavior data (e.g., click frequency, active duration) reveal temporal patterns
of interest evolution, ad content (e.g., type, tag, duration) influences
semantic preferences, and platform features (e.g., device type, usage context)
shape the environment where interest transitions occur. These factors jointly
enable the GNN to capture the latent pathways of user interest migration across
platforms. The experimental results are based on the datasets of three
platforms, and Platform B reaches 0.937 in AUC value, which is the best
performance. Platform A and Platform C showed a slight decrease in precision
and recall with uneven distribution of ad labels. By adjusting the
hyperparameters such as learning rate, batch size and embedding dimension, the
adaptability and robustness of the model in heterogeneous data are further
improved.

</details>


### [73] [Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models](https://arxiv.org/abs/2507.08965)
*Kevin Rojas,Ye He,Chieh-Hsin Lai,Yuta Takida,Yuki Mitsufuji,Molei Tao*

Main category: cs.LG

TL;DR: The paper analyzes Classifier-Free Guidance (CFG) in masked discrete diffusion, showing early high guidance harms quality while late-stage guidance is more impactful. It identifies an implementation flaw causing imbalanced transitions and proposes a simple fix to improve sample quality.


<details>
  <summary>Details</summary>
Motivation: To understand the role of guidance schedules in CFG for discrete diffusion and address observed imperfections in current implementations.

Method: Theoretical analysis of CFG in masked discrete diffusion, identifying issues with early guidance and proposing a novel guidance mechanism to smoothen transitions.

Result: Early high guidance degrades quality, while late-stage guidance is more effective. The proposed method improves sample quality with a simple code change.

Conclusion: The study provides theoretical insights into CFG and offers a practical solution to enhance discrete diffusion models, validated on ImageNet and QM9 datasets.

Abstract: Classifier-Free Guidance (CFG) is a widely used technique for conditional
generation and improving sample quality in continuous diffusion models, and
recent works have extended it to discrete diffusion. This paper theoretically
analyzes CFG in the context of masked discrete diffusion, focusing on the role
of guidance schedules. Our analysis shows that high guidance early in sampling
(when inputs are heavily masked) harms generation quality, while late-stage
guidance has a larger effect. These findings provide a theoretical explanation
for empirical observations in recent studies on guidance schedules. The
analysis also reveals an imperfection of the current CFG implementations. These
implementations can unintentionally cause imbalanced transitions, such as
unmasking too rapidly during the early stages of generation, which degrades the
quality of the resulting samples. To address this, we draw insight from the
analysis and propose a novel classifier-free guidance mechanism empirically
applicable to any discrete diffusion. Intuitively, our method smoothens the
transport between the data distribution and the initial (masked/uniform)
distribution, which results in improved sample quality. Remarkably, our method
is achievable via a simple one-line code change. The efficacy of our method is
empirically demonstrated with experiments on ImageNet (masked discrete
diffusion) and QM9 (uniform discrete diffusion).

</details>


### [74] [ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha](https://arxiv.org/abs/2507.08966)
*Meng Liu,Karl Leswing,Simon K. S. Chu,Farhad Ramezanghorbani,Griffin Young,Gabriel Marques,Prerna Das,Anjali Panikar,Esther Jamir,Mohammed Sulaiman Shamsudeen,K. Shawn Watts,Ananya Sen,Hari Priya Devannagari,Edward B. Miller,Muyun Lihan,Howook Hwang,Janet Paulsen,Xin Yu,Kyle Gion,Timur Rvachov,Emine Kucukbenli,Saee Gopal Paliwal*

Main category: cs.LG

TL;DR: ToxBench is introduced as a large-scale AB-FEP dataset for ML development, focusing on ERα-ligand binding. The DualBind model outperforms other ML methods, showing promise for cost-effective AB-FEP approximation.


<details>
  <summary>Details</summary>
Motivation: Bridging the gap between ML's data constraints and physics-based methods' computational costs for protein-ligand binding affinity prediction.

Method: Creation of ToxBench dataset with 8,770 ERα-ligand complexes and AB-FEP computed binding energies. Benchmarking ML methods, including the proposed DualBind model with a dual-loss framework.

Result: DualBind outperforms other ML methods, achieving high accuracy in predicting binding affinities at reduced computational cost.

Conclusion: ToxBench and DualBind demonstrate ML's potential to approximate AB-FEP efficiently, aiding drug discovery and toxicity assessment.

Abstract: Protein-ligand binding affinity prediction is essential for drug discovery
and toxicity assessment. While machine learning (ML) promises fast and accurate
predictions, its progress is constrained by the availability of reliable data.
In contrast, physics-based methods such as absolute binding free energy
perturbation (AB-FEP) deliver high accuracy but are computationally prohibitive
for high-throughput applications. To bridge this gap, we introduce ToxBench,
the first large-scale AB-FEP dataset designed for ML development and focused on
a single pharmaceutically critical target, Human Estrogen Receptor Alpha
(ER$\alpha$). ToxBench contains 8,770 ER$\alpha$-ligand complex structures with
binding free energies computed via AB-FEP with a subset validated against
experimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping
ligand splits to assess model generalizability. Using ToxBench, we further
benchmark state-of-the-art ML methods, and notably, our proposed DualBind
model, which employs a dual-loss framework to effectively learn the binding
energy function. The benchmark results demonstrate the superior performance of
DualBind and the potential of ML to approximate AB-FEP at a fraction of the
computational cost.

</details>


### [75] [Simulating Three-dimensional Turbulence with Physics-informed Neural Networks](https://arxiv.org/abs/2507.08972)
*Sifan Wang,Shyam Sankaran,Panos Stinis,Paris Perdikaris*

Main category: cs.LG

TL;DR: PINNs simulate turbulent flows without grids or data, using adaptive architectures and causal training, accurately reproducing key flow statistics.


<details>
  <summary>Details</summary>
Motivation: Traditional turbulent flow simulations are computationally expensive; PINNs offer a mesh-free, data-free alternative.

Method: Uses physics-informed neural networks (PINNs) with adaptive architectures, causal training, and advanced optimization to learn chaotic dynamics.

Result: Accurately reproduces energy spectra, kinetic energy, enstrophy, and Reynolds stresses in turbulent flows.

Conclusion: PINNs can model complex chaotic systems, overcoming traditional computational limits for turbulence.

Abstract: Turbulent fluid flows are among the most computationally demanding problems
in science, requiring enormous computational resources that become prohibitive
at high flow speeds. Physics-informed neural networks (PINNs) represent a
radically different approach that trains neural networks directly from physical
equations rather than data, offering the potential for continuous, mesh-free
solutions. Here we show that appropriately designed PINNs can successfully
simulate fully turbulent flows in both two and three dimensions, directly
learning solutions to the fundamental fluid equations without traditional
computational grids or training data. Our approach combines several algorithmic
innovations including adaptive network architectures, causal training, and
advanced optimization methods to overcome the inherent challenges of learning
chaotic dynamics. Through rigorous validation on challenging turbulence
problems, we demonstrate that PINNs accurately reproduce key flow statistics
including energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our
results demonstrate that neural equation solvers can handle complex chaotic
systems, opening new possibilities for continuous turbulence modeling that
transcends traditional computational limitations.

</details>


### [76] [Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery](https://arxiv.org/abs/2507.08977)
*Carson Dudley,Reiden Magdaleno,Christopher Harding,Marisa Eisenberg*

Main category: cs.LG

TL;DR: SGNNs combine mechanistic simulations and neural networks to improve scientific modeling, achieving state-of-the-art results in prediction and inference tasks while offering interpretability.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of mechanistic models (rigidity) and machine learning models (lack of interpretability and reliance on labeled data) in scientific modeling.

Method: Introduce Simulation-Grounded Neural Networks (SGNNs), pretrained on diverse synthetic data from mechanistic simulations.

Result: SGNNs outperformed baselines in tasks like COVID-19 forecasting, chemical yield prediction, and ecological forecasting, and enabled accurate inference for unobservable targets.

Conclusion: SGNNs bridge scientific theory and deep learning, offering a robust, interpretable modeling paradigm even without ground truth.

Abstract: Scientific modeling faces a core limitation: mechanistic models offer
interpretability but collapse under real-world complexity, while machine
learning models are flexible but require large labeled datasets, cannot infer
unobservable quantities, and operate as black boxes. We introduce
Simulation-Grounded Neural Networks (SGNNs), a general framework that uses
mechanistic simulations as training data for neural networks. SGNNs are
pretrained on synthetic corpora spanning diverse model structures, parameter
regimes, stochasticity, and observational artifacts. We evaluated SGNNs across
scientific disciplines and modeling tasks, and found that SGNNs achieved
state-of-the-art results across settings: for prediction tasks, they nearly
tripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield
prediction error by one third, and maintained accuracy in ecological
forecasting where task specific models failed. For inference tasks, SGNNs also
accurately classified the source of information spread in simulated social
networks and enabled supervised learning for unobservable targets, such as
estimating COVID-19 transmissibility more accurately than traditional methods
even in early outbreaks. Finally, SGNNs enable back-to-simulation attribution,
a new form of mechanistic interpretability. Given real world input, SGNNs
retrieve simulations based on what the model has learned to see as most
similar, revealing which underlying dynamics the model believes are active.
This provides process-level insight -- what the model thinks is happening --
not just which features mattered. SGNNs unify scientific theory with deep
learning flexibility and unlock a new modeling paradigm -- transforming
simulations from rigid, post hoc tools into flexible sources of supervision,
enabling robust, interpretable inference even when ground truth is missing.

</details>


### [77] [Learning Diffusion Models with Flexible Representation Guidance](https://arxiv.org/abs/2507.08980)
*Chenyu Wang,Cai Zhou,Sharut Gupta,Zongyu Lin,Stefanie Jegelka,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: The paper introduces a systematic framework to improve diffusion models by aligning internal representations with pre-trained models, enhancing generation quality and training speed.


<details>
  <summary>Details</summary>
Motivation: To improve diffusion models by incorporating auxiliary representations for better alignment and performance.

Method: Presents decompositions of denoising models, introduces two strategies: joint modeling of multimodal pairs and an optimal training curriculum.

Result: Achieves 23.3x faster training on ImageNet 256x256 and outperforms state-of-the-art methods like REPA.

Conclusion: The framework significantly enhances diffusion models' performance and training efficiency across various tasks.

Abstract: Diffusion models can be improved with additional guidance towards more
effective representations of input. Indeed, prior empirical work has already
shown that aligning internal representations of the diffusion model with those
of pre-trained models improves generation quality. In this paper, we present a
systematic framework for incorporating representation guidance into diffusion
models. We provide alternative decompositions of denoising models along with
their associated training criteria, where the decompositions determine when and
how the auxiliary representations are incorporated. Guided by our theoretical
insights, we introduce two new strategies for enhancing representation
alignment in diffusion models. First, we pair examples with target
representations either derived from themselves or arisen from different
synthetic modalities, and subsequently learn a joint model over the multimodal
pairs. Second, we design an optimal training curriculum that balances
representation learning and data generation. Our experiments across image,
protein sequence, and molecule generation tasks demonstrate superior
performance as well as accelerated training. In particular, on the
class-conditional ImageNet $256\times 256$ benchmark, our guidance results in
$23.3$ times faster training than the original SiT-XL as well as four times
speedup over the state-of-the-art method REPA. The code is available at
https://github.com/ChenyuWang-Monica/REED.

</details>


### [78] [Exploiting Leaderboards for Large-Scale Distribution of Malicious Models](https://arxiv.org/abs/2507.08983)
*Anshuman Suri,Harsh Chaudhari,Yuefeng Peng,Ali Naseh,Amir Houmansadr,Alina Oprea*

Main category: cs.LG

TL;DR: The paper explores how model leaderboards can be exploited to distribute poisoned models at scale, introducing TrojanClimb, a framework for injecting malicious behaviors while maintaining high leaderboard rankings.


<details>
  <summary>Details</summary>
Motivation: To uncover vulnerabilities in the machine learning ecosystem, specifically how adversaries can misuse model leaderboards for large-scale poisoning attacks.

Method: Introduces TrojanClimb, a framework for embedding harmful functionalities (e.g., backdoors, bias) in models while ensuring competitive leaderboard performance. Tested across four modalities.

Result: Demonstrates successful poisoning across text-embedding, text-generation, text-to-speech, and text-to-image models, achieving high leaderboard rankings.

Conclusion: Reveals a critical vulnerability in leaderboard systems, urging redesigns to detect malicious models and warning against adopting unverified models.

Abstract: While poisoning attacks on machine learning models have been extensively
studied, the mechanisms by which adversaries can distribute poisoned models at
scale remain largely unexplored. In this paper, we shed light on how model
leaderboards -- ranked platforms for model discovery and evaluation -- can
serve as a powerful channel for adversaries for stealthy large-scale
distribution of poisoned models. We present TrojanClimb, a general framework
that enables injection of malicious behaviors while maintaining competitive
leaderboard performance. We demonstrate its effectiveness across four diverse
modalities: text-embedding, text-generation, text-to-speech and text-to-image,
showing that adversaries can successfully achieve high leaderboard rankings
while embedding arbitrary harmful functionalities, from backdoors to bias
injection. Our findings reveal a significant vulnerability in the machine
learning ecosystem, highlighting the urgent need to redesign leaderboard
evaluation mechanisms to detect and filter malicious (e.g., poisoned) models,
while exposing broader security implications for the machine learning community
regarding the risks of adopting models from unverified sources.

</details>


### [79] [Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography](https://arxiv.org/abs/2507.09009)
*Zhengxiao He,Huayu Li,Geng Yuan,William D. S. Killgore,Stuart F. Quan,Chen X. Chen,Ao Li*

Main category: cs.LG

TL;DR: A self-supervised deep learning model analyzes multi-modal signals (EEG, ECG, respiratory) to predict CVD risk, validated in large cohorts and improving traditional risk scores.


<details>
  <summary>Details</summary>
Motivation: To enhance CVD risk assessment by leveraging multi-modal physiological signals and deep learning.

Method: Self-supervised deep learning trained on 4,398 participants, validated in 1,093, with projection scores derived from embeddings.

Result: ECG predicted CVD mortality; EEG predicted hypertension and CVD mortality. Combined with Framingham Risk Score, AUC improved (0.607-0.965).

Conclusion: The framework generates individualized CVD risk scores from PSG data, potentially improving clinical risk assessment.

Abstract: Methods: We developed a self-supervised deep learning model that extracts
meaningful patterns from multi-modal signals (Electroencephalography (EEG),
Electrocardiography (ECG), and respiratory signals). The model was trained on
data from 4,398 participants. Projection scores were derived by contrasting
embeddings from individuals with and without CVD outcomes. External validation
was conducted in an independent cohort with 1,093 participants. The source code
is available on https://github.com/miraclehetech/sleep-ssl. Results: The
projection scores revealed distinct and clinically meaningful patterns across
modalities. ECG-derived features were predictive of both prevalent and incident
cardiac conditions, particularly CVD mortality. EEG-derived features were
predictive of incident hypertension and CVD mortality. Respiratory signals
added complementary predictive value. Combining these projection scores with
the Framingham Risk Score consistently improved predictive performance,
achieving area under the curve values ranging from 0.607 to 0.965 across
different outcomes. Findings were robustly replicated and validated in the
external testing cohort. Conclusion: Our findings demonstrate that the proposed
framework can generate individualized CVD risk scores directly from PSG data.
The resulting projection scores have the potential to be integrated into
clinical practice, enhancing risk assessment and supporting personalized care.

</details>


### [80] [Enhancing RLHF with Human Gaze Modeling](https://arxiv.org/abs/2507.09016)
*Karim Galliamov,Ivan Titov,Ilya Pershin*

Main category: cs.LG

TL;DR: Using human gaze data improves RLHF efficiency by enhancing reward models and reward distribution, achieving faster convergence and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: RLHF is computationally expensive, and human gaze data is an underutilized signal that could enhance its efficiency.

Method: Two approaches: gaze-aware reward models and gaze-based distribution of sparse rewards at the token level.

Result: Gaze-informed RLHF achieves faster convergence and maintains or slightly improves performance, reducing computational costs.

Conclusion: Human gaze is a valuable signal for improving RLHF efficiency, suggesting a promising direction for future research.

Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns language models with
human preferences but is computationally expensive. We explore two approaches
that leverage human gaze modeling to enhance RLHF: (1) gaze-aware reward models
and (2) gaze-based distribution of sparse rewards at token level. Our
experiments demonstate that gaze-informed RLHF achieves faster convergence
while maintaining or slightly improving performance, thus, reducing
computational costs during policy optimization. These results show that human
gaze provides a valuable and underused signal for policy optimization, pointing
to a promising direction for improving RLHF efficiency.

</details>


### [81] [On Evaluating Performance of LLM Inference Serving Systems](https://arxiv.org/abs/2507.09019)
*Amey Agrawal,Nitin Kedia,Anmol Agarwal,Jayashree Mohan,Nipun Kwatra,Souvik Kundu,Ramachandran Ramjee,Alexey Tumanov*

Main category: cs.LG

TL;DR: The paper critiques current LLM inference evaluation methods, identifying common anti-patterns in Baseline Fairness, Evaluation Setup, and Metric Design. It proposes a checklist to avoid these flaws and demonstrates its application in a speculative decoding case study.


<details>
  <summary>Details</summary>
Motivation: Current LLM inference evaluation methodologies are flawed, leading to misleading conclusions and hindering scientific progress.

Method: Systematic analysis of recent systems to identify evaluation anti-patterns, followed by a framework (checklist) to address them. A case study on speculative decoding validates the framework.

Result: Identified recurring anti-patterns in LLM evaluation and provided a practical checklist to improve robustness.

Conclusion: The proposed framework ensures rigorous evaluation, enabling meaningful comparisons and accelerating progress in LLM inference systems.

Abstract: The rapid evolution of Large Language Model (LLM) inference systems has
yielded significant efficiency improvements. However, our systematic analysis
reveals that current evaluation methodologies frequently exhibit fundamental
flaws, often manifesting as common evaluation anti-patterns that obscure true
performance characteristics and impede scientific progress. Through a
comprehensive examination of recent systems, we identify recurring
anti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup,
and Metric Design. These anti-patterns are uniquely problematic for LLM
inference due to its dual-phase nature combining distinct prefill and decode
operations, its handling of highly heterogeneous workloads, and its strict
temporal requirements for interactive use. We demonstrate how common
anti-patterns -- such as inadequate baseline comparisons that conflate
engineering effort with algorithmic novelty, workload selections that fail to
represent production scenarios, and metric normalizations that hide substantial
performance variability like generation stalls-lead to misleading conclusions.
To address these challenges, we provide a comprehensive checklist derived from
our analysis, establishing a framework for recognizing and avoiding these
anti-patterns in favor of robust LLM inference evaluation. To demonstrate the
practical application of our framework, we present a case study analyzing
speculative decoding, a technique whose bursty, non-uniform token generation is
easily misinterpreted when evaluated using approaches characteristic of these
anti-patterns. Our work establishes a rigorous foundation for evaluation
methodology, enabling meaningful comparisons, ensuring reproducible results,
and ultimately accelerating genuine progress in LLM inference systems by moving
beyond common anti-patterns to align evaluation with real-world requirements.

</details>


### [82] [Model Parallelism With Subnetwork Data Parallelism](https://arxiv.org/abs/2507.09029)
*Vaibhav Singh,Zafir Khalid,Edouard Oyallon,Eugene Belilovsky*

Main category: cs.LG

TL;DR: A novel distributed pre-training method reduces memory demands by training small subnetworks on separate workers, avoiding inter-node activation communication and maintaining low bandwidth requirements.


<details>
  <summary>Details</summary>
Motivation: Addressing the heavy memory demands and high intra-node communication costs in distributed pre-training of large models.

Method: Proposes training small, structured subnetworks on separate workers, avoiding inter-node activation communication. Evaluates two subnetwork construction strategies: stochastic block dropping and width-wise subnetwork construction.

Result: Stochastic block dropping outperforms width-wise construction, attributed to stronger gradient alignment. Achieves 20-40% memory reduction without performance loss.

Conclusion: The method offers a promising alternative to traditional distributed training, reducing memory usage while maintaining performance.

Abstract: Distributed pre-training of large models at scale often imposes heavy memory
demands on individual nodes and incurs significant intra-node communication
costs. We propose a novel alternative approach that reduces the memory
requirements by training small, structured subnetworks of the model on separate
workers. Unlike pipelining, our method avoids inter-node activation
communication and maintains bandwidth requirements that are comparable to or
lower than standard data parallel communication schemes based on all-reduce. We
evaluate two subnetwork construction strategies guided by the principle of
ensuring uniform representation of each parameter across the distributed
training setup. Our results show that the stochastic block dropping technique
consistently outperforms the width-wise subnetwork construction previously
explored in federated learning. We empirically attribute this superior
performance to stronger gradient alignment in subnetworks that retain blocks
having skip connections. Preliminary experiments highlight the promise of our
approach, achieving a 20-40% reduction in memory usage without any loss in
performance.

</details>


### [83] [Confounder-Free Continual Learning via Recursive Feature Normalization](https://arxiv.org/abs/2507.09031)
*Yash Shah,Camila Gonzalez,Mohammad H. Abbasi,Qingyu Zhao,Kilian M. Pohl,Ehsan Adeli*

Main category: cs.LG

TL;DR: The paper introduces Recursive MDN (R-MDN), a layer for deep learning models to address confounder effects in continual learning, ensuring equitable predictions by adapting to changing data distributions.


<details>
  <summary>Details</summary>
Motivation: Confounders cause spurious correlations and biased predictions. Existing methods like MDN adjust feature distributions but struggle in continual learning where data and confounders evolve over time.

Method: R-MDN integrates into any deep learning architecture, using recursive least squares to continually update feature representations, removing confounder influence.

Result: R-MDN reduces catastrophic forgetting and promotes equitable predictions across population groups in both static and continual learning scenarios.

Conclusion: R-MDN effectively handles confounders in continual learning, improving model fairness and adaptability to changing data distributions.

Abstract: Confounders are extraneous variables that affect both the input and the
target, resulting in spurious correlations and biased predictions. There are
recent advances in dealing with or removing confounders in traditional models,
such as metadata normalization (MDN), where the distribution of the learned
features is adjusted based on the study confounders. However, in the context of
continual learning, where a model learns continuously from new data over time
without forgetting, learning feature representations that are invariant to
confounders remains a significant challenge. To remove their influence from
intermediate feature representations, we introduce the Recursive MDN (R-MDN)
layer, which can be integrated into any deep learning architecture, including
vision transformers, and at any model stage. R-MDN performs statistical
regression via the recursive least squares algorithm to maintain and
continually update an internal model state with respect to changing
distributions of data and confounding variables. Our experiments demonstrate
that R-MDN promotes equitable predictions across population groups, both within
static learning and across different stages of continual learning, by reducing
catastrophic forgetting caused by confounder effects changing over time.

</details>


### [84] [Behavioral Exploration: Learning to Explore via In-Context Adaptation](https://arxiv.org/abs/2507.09041)
*Andrew Wagenmaker,Zhiyuan Zhou,Sergey Levine*

Main category: cs.LG

TL;DR: Proposes 'behavioral exploration' for autonomous agents, using expert demonstrations to enable fast online adaptation and targeted exploration.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between human-like fast exploration/adaptation and slow, gradient-based algorithmic approaches.

Method: Trains a long-context generative model on expert demonstrations to predict actions based on past observations and exploratory behavior metrics.

Result: Effective in simulated and real-world robotic tasks, enabling adaptive, exploratory behavior.

Conclusion: Behavioral exploration successfully mimics expert-like fast adaptation and targeted exploration.

Abstract: Developing autonomous agents that quickly explore an environment and adapt
their behavior online is a canonical challenge in robotics and machine
learning. While humans are able to achieve such fast online exploration and
adaptation, often acquiring new information and skills in only a handful of
interactions, existing algorithmic approaches tend to rely on random
exploration and slow, gradient-based behavior updates. How can we endow
autonomous agents with such capabilities on par with humans? Taking inspiration
from recent progress on both in-context learning and large-scale behavioral
cloning, in this work we propose behavioral exploration: training agents to
internalize what it means to explore and adapt in-context over the space of
``expert'' behaviors. To achieve this, given access to a dataset of expert
demonstrations, we train a long-context generative model to predict expert
actions conditioned on a context of past observations and a measure of how
``exploratory'' the expert's behaviors are relative to this context. This
enables the model to not only mimic the behavior of an expert, but also, by
feeding its past history of interactions into its context, to select different
expert behaviors than what have been previously selected, thereby allowing for
fast online adaptation and targeted, ``expert-like'' exploration. We
demonstrate the effectiveness of our method in both simulated locomotion and
manipulation settings, as well as on real-world robotic manipulation tasks,
illustrating its ability to learn adaptive, exploratory behavior.

</details>


### [85] [Shortening the Trajectories: Identity-Aware Gaussian Approximation for Efficient 3D Molecular Generation](https://arxiv.org/abs/2507.09043)
*Jingxiang Qu,Wenhan Gao,Yi Liu*

Main category: cs.LG

TL;DR: The paper introduces a framework to improve the efficiency of Gaussian-based Probabilistic Generative Models (GPGMs) by replacing redundant steps with a closed-form Gaussian approximation, enhancing computational efficiency without compromising quality.


<details>
  <summary>Details</summary>
Motivation: GPGMs suffer from high computational costs due to long generative trajectories, limiting their practical deployment.

Method: The framework identifies a step where data becomes sufficiently Gaussian and replaces the remaining trajectory with a closed-form approximation, avoiding redundant steps.

Result: Empirical results show significant improvements in sample quality and computational efficiency across multiple data modalities.

Conclusion: The proposed method offers a balanced solution for efficient and high-fidelity generation in GPGMs.

Abstract: Gaussian-based Probabilistic Generative Models (GPGMs) generate data by
reversing a stochastic process that progressively corrupts samples with
Gaussian noise. While these models have achieved state-of-the-art performance
across diverse domains, their practical deployment remains constrained by the
high computational cost of long generative trajectories, which often involve
hundreds to thousands of steps during training and sampling. In this work, we
introduce a theoretically grounded and empirically validated framework that
improves generation efficiency without sacrificing training granularity or
inference fidelity. Our key insight is that for certain data modalities, the
noising process causes data to rapidly lose its identity and converge toward a
Gaussian distribution. We analytically identify a characteristic step at which
the data has acquired sufficient Gaussianity, and then replace the remaining
generation trajectory with a closed-form Gaussian approximation. Unlike
existing acceleration techniques that coarsening the trajectories by skipping
steps, our method preserves the full resolution of learning dynamics while
avoiding redundant stochastic perturbations between `Gaussian-like'
distributions. Empirical results across multiple data modalities demonstrate
substantial improvements in both sample quality and computational efficiency.

</details>


### [86] [Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction](https://arxiv.org/abs/2507.09061)
*Thomas T. Zhang,Daniel Pfrommer,Nikolai Matni,Max Simchowitz*

Main category: cs.LG

TL;DR: The paper addresses imitation learning in continuous systems, proposing minimal interventions (action chunking and noise injection) to mitigate compounding errors, drawing insights from control theory and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Imitation learning in physical settings like robotics is complex due to compounding errors, requiring advanced solutions beyond expert trajectories.

Method: Proposes action chunking for stable systems and noise injection for unstable systems to reduce compounding errors.

Result: The interventions provably mitigate compounding errors, aligning with practical robot learning methods.

Conclusion: Combining control theory and reinforcement learning offers novel insights for imitation learning in continuous systems.

Abstract: We study the problem of imitating an expert demonstrator in a continuous
state-and-action dynamical system. While imitation learning in discrete
settings such as autoregressive language modeling has seen immense success and
popularity in recent years, imitation in physical settings such as autonomous
driving and robot learning has proven comparably more complex due to the
compounding errors problem, often requiring elaborate set-ups to perform
stably. Recent work has demonstrated that even in benign settings, exponential
compounding errors are unavoidable when learning solely from expert-controlled
trajectories, suggesting the need for more advanced policy parameterizations or
data augmentation. To this end, we present minimal interventions that provably
mitigate compounding errors in continuous state-and-action imitation learning.
When the system is open-loop stable, we prescribe "action chunking," i.e.,
predicting and playing sequences of actions in open-loop; when the system is
possibly unstable, we prescribe "noise injection," i.e., adding noise during
expert demonstrations. These interventions align with popular choices in modern
robot learning, though the benefits we derive are distinct from the effects
they were designed to target. Our results draw insights and tools from both
control theory and reinforcement learning; however, our analysis reveals novel
considerations that do not naturally arise when either literature is considered
in isolation.

</details>


### [87] [Queue up for takeoff: a transferable deep learning framework for flight delay prediction](https://arxiv.org/abs/2507.09084)
*Nnamdi Daniel Aghanya,Ta Duong Vu,Amaëlle Diop,Charlotte Deville,Nour Imane Kerroumi,Irene Moulitsas,Jun Li,Desmond Bisandu*

Main category: cs.LG

TL;DR: The paper introduces QT-SimAM, a novel model combining Queue-Theory and attention mechanisms, for accurate and generalizable flight delay prediction.


<details>
  <summary>Details</summary>
Motivation: Flight delays cause financial and operational disruptions; improving prediction models can enhance passenger experience and reduce revenue loss.

Method: Proposes QT-SimAM (Bidirectional), combining Queue-Theory with a simple attention model, validated using US and EUROCONTROL datasets.

Result: Achieved high accuracy (0.927 US, 0.826 EUROCONTROL) and F1 scores (0.932 US, 0.791 EUROCONTROL), outperforming existing methods.

Conclusion: QT-SimAM is an effective, end-to-end solution for flight delay prediction, improving operational decisions and reducing passenger anxiety.

Abstract: Flight delays are a significant challenge in the aviation industry, causing
major financial and operational disruptions. To improve passenger experience
and reduce revenue loss, flight delay prediction models must be both precise
and generalizable across different networks. This paper introduces a novel
approach that combines Queue-Theory with a simple attention model, referred to
as the Queue-Theory SimAM (QT-SimAM). To validate our model, we used data from
the US Bureau of Transportation Statistics, where our proposed QT-SimAM
(Bidirectional) model outperformed existing methods with an accuracy of 0.927
and an F1 score of 0.932. To assess transferability, we tested the model on the
EUROCONTROL dataset. The results demonstrated strong performance, achieving an
accuracy of 0.826 and an F1 score of 0.791. Ultimately, this paper outlines an
effective, end-to-end methodology for predicting flight delays. The proposed
model's ability to forecast delays with high accuracy across different networks
can help reduce passenger anxiety and improve operational decision-making

</details>


### [88] [Deep Reinforcement Learning with Gradient Eligibility Traces](https://arxiv.org/abs/2507.09087)
*Esraa Elelimy,Brett Daley,Andrew Patterson,Marlos C. Machado,Adam White,Martha White*

Main category: cs.LG

TL;DR: The paper extends the Generalized Projected Bellman Error (GPBE) to support multistep credit assignment, improving off-policy deep RL stability and speed.


<details>
  <summary>Details</summary>
Motivation: Existing off-policy deep RL methods often diverge due to reliance on semi-gradient TD, while GTD methods, though stable, are rarely used in deep RL.

Method: Extends GPBE to multistep credit assignment using λ-return, derives three gradient-based methods, and provides forward-view (experience replay) and backward-view (streaming) formulations.

Result: Proposed algorithms outperform PPO and StreamQ in MuJoCo and MinAtar environments.

Conclusion: The work advances off-policy deep RL by combining stability and efficiency through multistep GPBE optimization.

Abstract: Achieving fast and stable off-policy learning in deep reinforcement learning
(RL) is challenging. Most existing methods rely on semi-gradient
temporal-difference (TD) methods for their simplicity and efficiency, but are
consequently susceptible to divergence. While more principled approaches like
Gradient TD (GTD) methods have strong convergence guarantees, they have rarely
been used in deep RL. Recent work introduced the Generalized Projected Bellman
Error ($\GPBE$), enabling GTD methods to work efficiently with nonlinear
function approximation. However, this work is only limited to one-step methods,
which are slow at credit assignment and require a large number of samples. In
this paper, we extend the $\GPBE$ objective to support multistep credit
assignment based on the $\lambda$-return and derive three gradient-based
methods that optimize this new objective. We provide both a forward-view
formulation compatible with experience replay and a backward-view formulation
compatible with streaming algorithms. Finally, we evaluate the proposed
algorithms and show that they outperform both PPO and StreamQ in MuJoCo and
MinAtar environments, respectively. Code available at
https://github.com/esraaelelimy/gtd\_algos

</details>


### [89] [Continuous-Time Signal Decomposition: An Implicit Neural Generalization of PCA and ICA](https://arxiv.org/abs/2507.09091)
*Shayan K. Azmoodeh,Krishna Subramani,Paris Smaragdis*

Main category: cs.LG

TL;DR: A framework for continuous-time signal decomposition using neural networks, unifying PCA and ICA through a contrast function.


<details>
  <summary>Details</summary>
Motivation: To extend low-rank decomposition methods (PCA, ICA) to continuous-time signals and irregularly sampled data, where traditional methods fail.

Method: Uses an implicit neural signal representation framework with a contrast function in the loss to enforce statistical properties (decorrelation, independence).

Result: Enables decomposition for continuous-time signals, point clouds, and irregularly sampled data.

Conclusion: The framework generalizes PCA and ICA for continuous settings, offering flexibility for non-standard data.

Abstract: We generalize the low-rank decomposition problem, such as principal and
independent component analysis (PCA, ICA) for continuous-time vector-valued
signals and provide a model-agnostic implicit neural signal representation
framework to learn numerical approximations to solve the problem. Modeling
signals as continuous-time stochastic processes, we unify the approaches to
both the PCA and ICA problems in the continuous setting through a contrast
function term in the network loss, enforcing the desired statistical properties
of the source signals (decorrelation, independence) learned in the
decomposition. This extension to a continuous domain allows the application of
such decompositions to point clouds and irregularly sampled signals where
standard techniques are not applicable.

</details>


### [90] [On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving](https://arxiv.org/abs/2507.09095)
*Md Hasan Shahriar,Md Mohaimin Al Barat,Harshavardhan Sundar,Naren Ramakrishnan,Y. Thomas Hou,Wenjing Lou*

Main category: cs.LG

TL;DR: DejaVu exploits network delays to disrupt multimodal fusion in autonomous driving, degrading perception tasks. AION, a defense patch, detects these attacks with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Multimodal fusion in autonomous driving is vulnerable to temporal misalignments caused by network delays, which can severely impact perception tasks.

Method: DejaVu introduces temporal misalignments via network delays. AION uses cross-modal temporal consistency, shared representation learning, and dynamic time warping to detect attacks.

Result: DejaVu reduces car detection mAP by 88.5% and MOTA by 73%. AION achieves AUROC scores of 0.92-0.98 with low false positives.

Conclusion: AION is a robust defense against temporal misalignment attacks, demonstrating high detection accuracy across models and datasets.

Abstract: Multimodal fusion (MMF) plays a critical role in the perception of autonomous
driving, which primarily fuses camera and LiDAR streams for a comprehensive and
efficient scene understanding. However, its strict reliance on precise temporal
synchronization exposes it to new vulnerabilities. In this paper, we introduce
DejaVu, a novel attack that exploits network-induced delays to create subtle
temporal misalignments across sensor streams, severely degrading downstream
MMF-based perception tasks. Our comprehensive attack analysis across different
models and datasets reveals these sensors' task-specific imbalanced
sensitivities: object detection is overly dependent on LiDAR inputs while
object tracking is highly reliant on the camera inputs. Consequently, with a
single-frame LiDAR delay, an attacker can reduce the car detection mAP by up to
88.5%, while with a three-frame camera delay, multiple object tracking accuracy
(MOTA) for car drops by 73%. To detect such attacks, we propose AION, a defense
patch that can work alongside the existing perception model to monitor temporal
alignment through cross-modal temporal consistency. AION leverages multimodal
shared representation learning and dynamic time warping to determine the path
of temporal alignment and calculate anomaly scores based on the alignment. Our
thorough evaluation of AION shows it achieves AUROC scores of 0.92-0.98 with
low false positives across datasets and model architectures, demonstrating it
as a robust and generalized defense against the temporal misalignment attacks.

</details>


### [91] [S2SRec2: Set-to-Set Recommendation for Basket Completion with Recipe](https://arxiv.org/abs/2507.09101)
*Yanan Cao,Omid Memarrast,Shiqin Cai,Sinduja Subramaniam,Evren Korpeoglu,Kannan Achan*

Main category: cs.LG

TL;DR: The paper introduces S2SRec2, a set-to-set recommendation framework for grocery e-commerce, addressing the limitations of traditional single-ingredient prediction methods by predicting multiple complementary ingredients and assessing basket completeness.


<details>
  <summary>Details</summary>
Motivation: Traditional recipe completion methods predict only one missing ingredient and ignore relationships among missing ingredients, failing to reflect real-world needs.

Method: The authors propose S2SRec2, a framework using a Set Transformer trained in a multitask learning paradigm to predict sets of complementary ingredients and assess basket completeness.

Result: Experiments show S2SRec2 outperforms single-target baselines, improving grocery shopping and culinary creativity.

Conclusion: S2SRec2 offers a promising solution for enhancing grocery e-commerce by addressing real-world needs for multiple ingredient recommendations.

Abstract: In grocery e-commerce, customers often build ingredient baskets guided by
dietary preferences but lack the expertise to create complete meals. Leveraging
recipe knowledge to recommend complementary ingredients based on a partial
basket is essential for improving the culinary experience. Traditional recipe
completion methods typically predict a single missing ingredient using a
leave-one-out strategy. However, they fall short in two key aspects: (i) they
do not reflect real-world scenarios where multiple ingredients are often
needed, and (ii) they overlook relationships among the missing ingredients
themselves. To address these limitations, we reformulate basket completion as a
set-to-set (S2S) recommendation problem, where an incomplete basket is input
into a system that predicts a set of complementary ingredients. We introduce
S2SRec2, a set-to-set ingredient recommendation framework based on a Set
Transformer and trained in a multitask learning paradigm. S2SRec2 jointly
learns to (i) retrieve missing ingredients from the representation of existing
ones and (ii) assess basket completeness after prediction. These tasks are
optimized together, enforcing accurate retrieval and coherent basket
completion. Experiments on large-scale recipe datasets and qualitative analyses
show that S2SRec2 significantly outperforms single-target baselines, offering a
promising approach to enhance grocery shopping and inspire culinary creativity.

</details>


### [92] [A Study of Value-Aware Eigenoptions](https://arxiv.org/abs/2507.09127)
*Harshil Kotamreddy,Marlos C. Machado*

Main category: cs.LG

TL;DR: Eigenoptions in RL aid exploration and credit assignment, but online discovery can hinder learning. A method for learning option-values in deep RL is proposed.


<details>
  <summary>Details</summary>
Motivation: To explore the role of eigenoptions in accelerating credit assignment in model-free RL and their broader impact on learning.

Method: Evaluated eigenoptions in tabular and pixel-based gridworlds, and proposed a method for learning option-values in deep RL under non-linear function approximation.

Result: Pre-specified eigenoptions aid exploration and credit assignment, while online discovery can bias learning. Termination conditions impact performance in deep RL.

Conclusion: Eigenoptions show promise for supporting both credit assignment and exploration in RL, but their use requires careful consideration of discovery methods and termination conditions.

Abstract: Options, which impose an inductive bias toward temporal and hierarchical
structure, offer a powerful framework for reinforcement learning (RL). While
effective in sequential decision-making, they are often handcrafted rather than
learned. Among approaches for discovering options, eigenoptions have shown
strong performance in exploration, but their role in credit assignment remains
underexplored. In this paper, we investigate whether eigenoptions can
accelerate credit assignment in model-free RL, evaluating them in tabular and
pixel-based gridworlds. We find that pre-specified eigenoptions aid not only
exploration but also credit assignment, whereas online discovery can bias the
agent's experience too strongly and hinder learning. In the context of deep RL,
we also propose a method for learning option-values under non-linear function
approximation, highlighting the impact of termination conditions on
performance. Our findings reveal both the promise and complexity of using
eigenoptions, and options more broadly, to simultaneously support credit
assignment and exploration in reinforcement learning.

</details>


### [93] [Heterogeneous Graph Prompt Learning via Adaptive Weight Pruning](https://arxiv.org/abs/2507.09132)
*Chu-Yuan Wei,Shun-Yao Liu,Sheng-Da Zhuo,Chang-Dong Wang,Shu-Qiang Huang,Mohsen Guizani*

Main category: cs.LG

TL;DR: The paper introduces GPAWP, a framework combining graph prompts with weight pruning to enhance GNN performance and efficiency by reducing negative prompts.


<details>
  <summary>Details</summary>
Motivation: GNNs face challenges like long training times and insufficient feature extraction. Graph prompts and pre-training offer potential improvements, but their optimization and impact on stability are overlooked.

Method: Proposes GPAWP, using an importance assessment function to hierarchically prune negative prompts, improving efficiency.

Result: GPAWP significantly reduces parameters in node classification tasks while maintaining performance.

Conclusion: GPAWP effectively enhances GNN efficiency and performance by optimizing graph prompts through pruning.

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in various
graph-based tasks (e.g., node classification or link prediction). Despite their
triumphs, GNNs still face challenges such as long training and inference times,
difficulty in capturing complex relationships, and insufficient feature
extraction. To tackle these issues, graph pre-training and graph prompt methods
have garnered increasing attention for their ability to leverage large-scale
datasets for initial learning and task-specific adaptation, offering potential
improvements in GNN performance. However, previous research has overlooked the
potential of graph prompts in optimizing models, as well as the impact of both
positive and negative graph prompts on model stability and efficiency. To
bridge this gap, we propose a novel framework combining graph prompts with
weight pruning, called GPAWP, which aims to enhance the performance and
efficiency of graph prompts by using fewer of them. We evaluate the importance
of graph prompts using an importance assessment function to determine positive
and negative weights at different granularities. Through hierarchically
structured pruning, we eliminate negative prompt labels, resulting in more
parameter-efficient and competitively performing prompts. Extensive experiments
on three benchmark datasets demonstrate the superiority of GPAWP, leading to a
significant reduction in parameters in node classification tasks.

</details>


### [94] [Principled Foundations for Preference Optimization](https://arxiv.org/abs/2507.07855)
*Wenxuan Zhou,Shujian Zhang,Brice Magdalou,John Lambert,Ehsan Amid,Richard Nock,Andrew Hard*

Main category: cs.LG

TL;DR: Direct Preference Optimization (DPO) connects Savage's loss functions and stochastic choice theories, offering generality and insights into DPO's applications and pitfalls.


<details>
  <summary>Details</summary>
Motivation: To understand DPO's theoretical foundations and its broader implications in machine learning, given its current momentum and diverse applications.

Method: Establishes a connection between Savage's loss functions and stochastic choice theories, supporting abstention, non-convex objectives, and extensions like margins and length corrections.

Result: Provides a general framework for DPO, revealing its broader applicability and potential pitfalls when deviating from this framework.

Conclusion: Understanding DPO's principled perspective is crucial for its applications and avoiding pitfalls, with the framework covering notable extensions.

Abstract: In this paper, we show that direct preference optimization (DPO) is a very
specific form of a connection between two major theories in the ML context of
learning from preferences: loss functions (Savage) and stochastic choice
(Doignon-Falmagne and Machina). The connection is established for all of
Savage's losses and at this level of generality, (i) it includes support for
abstention on the choice theory side, (ii) it includes support for non-convex
objectives on the ML side, and (iii) it allows to frame for free some notable
extensions of the DPO setting, including margins and corrections for length.
Getting to understand how DPO operates from a general principled perspective is
crucial because of the huge and diverse application landscape of models,
because of the current momentum around DPO, but also -- and importantly --
because many state of the art variations on DPO definitely occupy a small
region of the map that we cover. It also helps to understand the pitfalls of
departing from this map, and figure out workarounds.

</details>


### [95] [POIFormer: A Transformer-Based Framework for Accurate and Scalable Point-of-Interest Attribution](https://arxiv.org/abs/2507.09137)
*Nripsuta Ani Saxena,Shang-Ling Hsu,Mehul Shetty,Omar Alkhadra,Cyrus Shahabi,Abigail L. Horn*

Main category: cs.LG

TL;DR: POIFormer is a Transformer-based framework for accurate POI attribution, addressing GPS inaccuracies and dense POI clustering by modeling spatial, temporal, contextual, and behavioral features.


<details>
  <summary>Details</summary>
Motivation: Accurate POI attribution is crucial for mobility analytics and urban planning but is hindered by GPS inaccuracies and high POI density in urban areas.

Method: POIFormer uses Transformer self-attention to model spatial proximity, visit timing/duration, POI semantics, and user/crowd behavior patterns, masking the current visit for context.

Result: POIFormer outperforms baselines in noisy, dense settings, demonstrating robustness and generalization across diverse datasets.

Conclusion: POIFormer offers a practical, scalable solution for POI attribution, leveraging rich feature interactions without relying on hard-to-access data.

Abstract: Accurately attributing user visits to specific Points of Interest (POIs) is a
foundational task for mobility analytics, personalized services, marketing and
urban planning. However, POI attribution remains challenging due to GPS
inaccuracies, typically ranging from 2 to 20 meters in real-world settings, and
the high spatial density of POIs in urban environments, where multiple venues
can coexist within a small radius (e.g., over 50 POIs within a 100-meter radius
in dense city centers). Relying on proximity is therefore often insufficient
for determining which POI was actually visited. We introduce
\textsf{POIFormer}, a novel Transformer-based framework for accurate and
efficient POI attribution. Unlike prior approaches that rely on limited
spatiotemporal, contextual, or behavioral features, \textsf{POIFormer} jointly
models a rich set of signals, including spatial proximity, visit timing and
duration, contextual features from POI semantics, and behavioral features from
user mobility and aggregated crowd behavior patterns--using the Transformer's
self-attention mechanism to jointly model complex interactions across these
dimensions. By leveraging the Transformer to model a user's past and future
visits (with the current visit masked) and incorporating crowd-level behavioral
patterns through pre-computed KDEs, \textsf{POIFormer} enables accurate,
efficient attribution in large, noisy mobility datasets. Its architecture
supports generalization across diverse data sources and geographic contexts
while avoiding reliance on hard-to-access or unavailable data layers, making it
practical for real-world deployment. Extensive experiments on real-world
mobility datasets demonstrate significant improvements over existing baselines,
particularly in challenging real-world settings characterized by spatial noise
and dense POI clustering.

</details>


### [96] [Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations](https://arxiv.org/abs/2507.09173)
*Mengjie Chen,Ming Zhang,Cunquan Qu*

Main category: cs.LG

TL;DR: MolecBioNet is a graph-based framework integrating molecular and biomedical knowledge for improved DDI prediction, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current graph-based DDI prediction methods treat drug pairs independently and fail to integrate biological networks and molecular structures, limiting mechanistic insights.

Method: MolecBioNet models drug pairs as unified entities, using hierarchical interaction graphs and domain-specific pooling strategies (CASPool and AGIPool) for multi-scale representation.

Result: MolecBioNet outperforms state-of-the-art methods in DDI prediction, validated by ablation studies and embedding visualizations.

Conclusion: The framework's unified modeling and multi-scale knowledge integration enhance DDI prediction accuracy and interpretability.

Abstract: Drug-drug interactions (DDIs) represent a critical challenge in pharmacology,
often leading to adverse drug reactions with significant implications for
patient safety and healthcare outcomes. While graph-based methods have achieved
strong predictive performance, most approaches treat drug pairs independently,
overlooking the complex, context-dependent interactions unique to drug pairs.
Additionally, these models struggle to integrate biological interaction
networks and molecular-level structures to provide meaningful mechanistic
insights. In this study, we propose MolecBioNet, a novel graph-based framework
that integrates molecular and biomedical knowledge for robust and interpretable
DDI prediction. By modeling drug pairs as unified entities, MolecBioNet
captures both macro-level biological interactions and micro-level molecular
influences, offering a comprehensive perspective on DDIs. The framework
extracts local subgraphs from biomedical knowledge graphs and constructs
hierarchical interaction graphs from molecular representations, leveraging
classical graph neural network methods to learn multi-scale representations of
drug pairs. To enhance accuracy and interpretability, MolecBioNet introduces
two domain-specific pooling strategies: context-aware subgraph pooling
(CASPool), which emphasizes biologically relevant entities, and
attention-guided influence pooling (AGIPool), which prioritizes influential
molecular substructures. The framework further employs mutual information
minimization regularization to enhance information diversity during embedding
fusion. Experimental results demonstrate that MolecBioNet outperforms
state-of-the-art methods in DDI prediction, while ablation studies and
embedding visualizations further validate the advantages of unified drug pair
modeling and multi-scale knowledge integration.

</details>


### [97] [Continual Reinforcement Learning by Planning with Online World Models](https://arxiv.org/abs/2507.09177)
*Zichen Liu,Guoji Fu,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: The paper proposes a method for continual reinforcement learning (CRL) using online world models to prevent catastrophic forgetting, achieving better performance than deep world models with continual learning techniques.


<details>
  <summary>Details</summary>
Motivation: CRL agents often forget previous tasks when learning new ones (catastrophic forgetting). The paper aims to address this by leveraging online world models.

Method: The approach involves learning a Follow-The-Leader (FTL) shallow model online for world dynamics and planning with model predictive control. The model is immune to forgetting and updates incrementally.

Result: The proposed FTL Online Agent (OA) outperforms deep world models with continual learning techniques, demonstrating continuous learning without forgetting old skills.

Conclusion: Online world models with FTL planning effectively mitigate catastrophic forgetting in CRL, offering a promising direction for scalable and adaptive agents.

Abstract: Continual reinforcement learning (CRL) refers to a naturalistic setting where
an agent needs to endlessly evolve, by trial and error, to solve multiple tasks
that are presented sequentially. One of the largest obstacles to CRL is that
the agent may forget how to solve previous tasks when learning a new task,
known as catastrophic forgetting. In this paper, we propose to address this
challenge by planning with online world models. Specifically, we learn a
Follow-The-Leader shallow model online to capture the world dynamics, in which
we plan using model predictive control to solve a set of tasks specified by any
reward functions. The online world model is immune to forgetting by
construction with a proven regret bound of $\mathcal{O}(\sqrt{K^2D\log(T)})$
under mild assumptions. The planner searches actions solely based on the latest
online model, thus forming a FTL Online Agent (OA) that updates incrementally.
To assess OA, we further design Continual Bench, a dedicated environment for
CRL, and compare with several strong baselines under the same model-planning
algorithmic framework. The empirical results show that OA learns continuously
to solve new tasks while not forgetting old skills, outperforming agents built
on deep world models with various continual learning techniques.

</details>


### [98] [XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge](https://arxiv.org/abs/2507.09202)
*Wuxin Wang,Weicheng Ni,Lilan Huang,Tao Hao,Ben Fei,Shuo Ma,Taikang Yuan,Yanlai Zhao,Kefeng Deng,Xiaoyong Li,Boheng Duan,Lei Bai,Kaijun Ren*

Main category: cs.LG

TL;DR: XiChen is a fully AI-driven global weather forecasting system that completes data assimilation to medium-range forecasting in 17 seconds, rivaling traditional NWP systems.


<details>
  <summary>Details</summary>
Motivation: To overcome the reliance on time-consuming NWP systems for initial conditions in AI-driven weather forecasting.

Method: Uses a pre-trained foundation model fine-tuned for observation operators and DA, integrating 4D variational knowledge.

Result: Achieves a skillful forecasting lead time exceeding 8.25 days, matching operational NWP accuracy.

Conclusion: XiChen demonstrates potential for fully AI-driven weather forecasting without NWP dependency.

Abstract: Recent advancements in Artificial Intelligence (AI) demonstrate significant
potential to revolutionize weather forecasting. However, most AI-driven models
rely on Numerical Weather Prediction (NWP) systems for initial condition
preparation, which often consumes hours on supercomputers. Here we introduce
XiChen, the first observation-scalable fully AI-driven global weather
forecasting system, whose entire pipeline, from Data Assimilation (DA) to
medium-range forecasting, can be accomplished within only 17 seconds. XiChen is
built upon a foundation model that is pre-trained for weather forecasting.
Meanwhile, this model is subsequently fine-tuned to serve as both observation
operators and DA models, thereby scalably assimilating conventional and raw
satellite observations. Furthermore, the integration of four-dimensional
variational knowledge ensures that XiChen's DA and medium-range forecasting
accuracy rivals that of operational NWP systems, amazingly achieving a skillful
forecasting lead time exceeding 8.25 days. These findings demonstrate that
XiChen holds strong potential toward fully AI-driven weather forecasting
independent of NWP systems.

</details>


### [99] [Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling](https://arxiv.org/abs/2507.09211)
*Xinyue Liu,Xiao Peng,Shuyue Yan,Yuntian Chen,Dongxiao Zhang,Zhixiao Niu,Hui-Min Wang,Xiaogang He*

Main category: cs.LG

TL;DR: DeepX-GAN, a knowledge-informed deep generative model, simulates unseen climate extremes beyond historical records, revealing latent risks and spatial vulnerabilities, particularly in MENA, Indo-Pakistan, and Central Africa.


<details>
  <summary>Details</summary>
Motivation: Current records of climate extremes are incomplete, missing rare but plausible extremes, and spatial dependence is often neglected, undervaluing synchronized hazards.

Method: Developed DeepX-GAN to capture spatial structure of rare extremes, simulating 'checkmate' (direct hits) and 'stalemate' (near misses) scenarios.

Result: Unseen extremes disproportionately affect vulnerable regions like MENA, with future warming redistributing risks to Indo-Pakistan and Central Africa.

Conclusion: Highlights blind spots in hazard planning and calls for spatially adaptive policies to anticipate emergent risk hotspots.

Abstract: Observed records of climate extremes provide an incomplete picture of risk,
missing "unseen" extremes that exceed historical bounds. In parallel,
neglecting spatial dependence undervalues the risk of synchronized hazards that
amplify impacts. To address these challenges, we develop DeepX-GAN
(Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial
Network), a knowledge-informed deep generative model designed to better capture
the spatial structure of rare extremes. The zero-shot generalizability of
DeepX-GAN enables simulation of unseen extremes that fall outside historical
experience yet remain statistically plausible. We define two types of unseen
extremes: "checkmate" extremes that directly hit targets, and "stalemate"
extremes that narrowly miss. These unrealized scenarios expose latent risks in
fragile systems and may reinforce a false sense of resilience if overlooked.
Near misses, in particular, can prompt either proactive adaptation or dangerous
complacency, depending on how they are interpreted. Applying DeepX-GAN to the
Middle East and North Africa (MENA), we find that these unseen extremes
disproportionately affect regions with high vulnerability and low socioeconomic
readiness, but differ in urgency and interpretation. Future warming could
expand and redistribute these unseen extremes, with emerging exposure hotspots
in Indo-Pakistan and Central Africa. This distributional shift highlights
critical blind spots in conventional hazard planning and underscores the need
to develop spatially adaptive policies that anticipate emergent risk hotspots
rather than simply extrapolating from historical patterns.

</details>


### [100] [Warm Starts Accelerate Generative Modelling](https://arxiv.org/abs/2507.09212)
*Jonas Scholz,Richard E. Turner*

Main category: cs.LG

TL;DR: The paper introduces a warm-start model to accelerate conditional generation in iterative generative models by providing an informed prior, reducing the number of required function evaluations.


<details>
  <summary>Details</summary>
Motivation: Iterative generative models are slow due to the need for hundreds of function evaluations. The warm-start model aims to speed up this process.

Method: The warm-start model predicts an informed prior (N(mu, sigma)) conditioned on input context, reducing the generative process's distance. It uses a conditional normalization trick for compatibility with standard models.

Result: The method achieves competitive results with a 1000-step DDPM baseline using only 11 function evaluations (1 warm start, 10 generation).

Conclusion: The warm-start model significantly accelerates conditional generation while maintaining quality, and is compatible with existing models and samplers.

Abstract: Iterative generative models, like diffusion and flow-matching, create
high-fidelity samples by progressively refining a noise vector into data.
However, this process is notoriously slow, often requiring hundreds of function
evaluations. We introduce the warm-start model, a simple, deterministic model
that dramatically accelerates conditional generation by providing a better
starting point. Instead of starting generation from an uninformed N(0, I)
prior, our warm-start model predicts an informed prior N(mu, sigma), whose
moments are conditioned on the input context. This "warm start" substantially
reduces the distance the generative process must traverse, particularly when
the conditioning information is strongly informative. On tasks like image
inpainting, our method achieves results competitive with a 1000-step DDPM
baseline using only 11 total function evaluations (1 for the warm start, 10 for
generation). A simple conditional normalization trick makes our method
compatible with any standard generative model and sampler without modification,
allowing it to be combined with other efficient sampling techniques for further
acceleration. Our implementation is available at
https://github.com/jonas-scholz123/warm-start-model.

</details>


### [101] [Optimizing Basis Function Selection in Constructive Wavelet Neural Networks and Its Applications](https://arxiv.org/abs/2507.09213)
*Dunsheng Huang,Dong Shen,Lei Lu,Ying Tan*

Main category: cs.LG

TL;DR: The paper introduces a constructive wavelet neural network (WNN) framework that improves accuracy and reduces computational costs by selecting initial wavelet bases based on frequency analysis and dynamically adding new bases.


<details>
  <summary>Details</summary>
Motivation: Overcoming challenges in constructing accurate wavelet bases and high computational costs in traditional WNNs for signal processing and time-series analysis.

Method: Proposes a constructive WNN with a frequency estimator and wavelet-basis increase mechanism to prioritize high-energy bases, optimizing time-frequency range for accuracy.

Result: Demonstrates improved computational efficiency and versatility in four applications: static mapping estimation, dataset combination, time-varying mapping identification, and real-time nonlinear dependency capture.

Conclusion: The framework offers a practical and broadly applicable solution for WNNs, with code made publicly available.

Abstract: Wavelet neural network (WNN), which learns an unknown nonlinear mapping from
the data, has been widely used in signal processing, and time-series analysis.
However, challenges in constructing accurate wavelet bases and high
computational costs limit their application. This study introduces a
constructive WNN that selects initial bases and trains functions by introducing
new bases for predefined accuracy while reducing computational costs. For the
first time, we analyze the frequency of unknown nonlinear functions and select
appropriate initial wavelets based on their primary frequency components by
estimating the energy of the spatial frequency component. This leads to a novel
constructive framework consisting of a frequency estimator and a wavelet-basis
increase mechanism to prioritize high-energy bases, significantly improving
computational efficiency. The theoretical foundation defines the necessary
time-frequency range for high-dimensional wavelets at a given accuracy. The
framework's versatility is demonstrated through four examples: estimating
unknown static mappings from offline data, combining two offline datasets,
identifying time-varying mappings from time-series data, and capturing
nonlinear dependencies in real time-series data. These examples showcase the
framework's broad applicability and practicality. All the code will be released
at https://github.com/dshuangdd/CWNN.

</details>


### [102] [TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding](https://arxiv.org/abs/2507.09252)
*Shukai Gong,Yiyang Fu,Fengyuan Ran,Feng Zhou*

Main category: cs.LG

TL;DR: TPP-SD accelerates Transformer TPP sampling using speculative decoding, achieving 2-6× speedup while maintaining output distribution.


<details>
  <summary>Details</summary>
Motivation: Bridging the gap between powerful Transformer TPP models and the need for rapid sequence sampling.

Method: Adapts speculative decoding from language models, using a draft model to generate candidates verified by the target model.

Result: Achieves identical distributions as standard methods with 2-6× speedup.

Conclusion: TPP-SD efficiently accelerates sampling without compromising output quality.

Abstract: We propose TPP-SD, a novel approach that accelerates Transformer temporal
point process (TPP) sampling by adapting speculative decoding (SD) techniques
from language models. By identifying the structural similarities between
thinning algorithms for TPPs and speculative decoding for language models, we
develop an efficient sampling framework that leverages a smaller draft model to
generate multiple candidate events, which are then verified by the larger
target model in parallel. TPP-SD maintains the same output distribution as
autoregressive sampling while achieving significant acceleration. Experiments
on both synthetic and real datasets demonstrate that our approach produces
samples from identical distributions as standard methods, but with 2-6$\times$
speedup. Our ablation studies analyze the impact of hyperparameters such as
draft length and draft model size on sampling efficiency. TPP-SD bridges the
gap between powerful Transformer TPP models and the practical need for rapid
sequence sampling.

</details>


### [103] [Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential Equations](https://arxiv.org/abs/2507.09264)
*Payel Mukhopadhyay,Michael McCabe,Ruben Ohana,Miles Cranmer*

Main category: cs.LG

TL;DR: The paper introduces lightweight modules (CKM and CSM) for dynamic patch size control in patch-based transformers, improving efficiency and stability without retraining or accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Fixed patch sizes in patch-based transformer surrogates limit budget-conscious deployment; dynamic control is needed for better efficiency and stability.

Method: Proposes Convolutional Kernel Modulator (CKM) and Convolutional Stride Modulator (CSM) for dynamic patch size control, combined with cyclic patch-size rollout.

Result: Improves rollout fidelity and runtime efficiency in 2D/3D PDE benchmarks, mitigating patch artifacts and enhancing long-term stability.

Conclusion: First framework for inference-time patch-size tunability in PDE surrogates, offering plug-and-play design for compute-adaptive modeling.

Abstract: Patch-based transformer surrogates have become increasingly effective for
modeling spatiotemporal dynamics, but the fixed patch size is a major
limitation for budget-conscience deployment in production. We introduce two
lightweight, architecture-agnostic modules-the Convolutional Kernel Modulator
(CKM) and Convolutional Stride Modulator (CSM)-that enable dynamic patch size
control at inference in patch based models, without retraining or accuracy
loss. Combined with a cyclic patch-size rollout, our method mitigates patch
artifacts and improves long-term stability for video-like prediction tasks.
Applied to a range of challenging 2D and 3D PDE benchmarks, our approach
improves rollout fidelity and runtime efficiency. To our knowledge, this is the
first framework to enable inference-time patch-size tunability in patch-based
PDE surrogates. Its plug-and-play design makes it broadly applicable across
architectures-establishing a general foundation for compute-adaptive modeling
in PDE surrogate tasks.

</details>


### [104] [Impute With Confidence: A Framework for Uncertainty Aware Multivariate Time Series Imputation](https://arxiv.org/abs/2507.09353)
*Addison Weatherhead,Anna Goldenberg*

Main category: cs.LG

TL;DR: A framework for selective imputation in time series data, leveraging uncertainty to avoid unreliable imputations and improve downstream tasks like mortality prediction.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of uncertainty quantification in existing imputation methods, especially critical in healthcare due to prolonged sensor disconnections.

Method: Introduces a general framework to quantify and leverage uncertainty for selective imputation, focusing on confident values.

Result: Reduces imputation errors and improves downstream tasks, demonstrated on EHR datasets with diverse missingness.

Conclusion: Incorporating uncertainty into time series imputation enhances reliability and practical benefits, such as better mortality prediction.

Abstract: Time series data with missing values is common across many domains.
Healthcare presents special challenges due to prolonged periods of sensor
disconnection. In such cases, having a confidence measure for imputed values is
critical. Most existing methods either overlook model uncertainty or lack
mechanisms to estimate it. To address this gap, we introduce a general
framework that quantifies and leverages uncertainty for selective imputation.
By focusing on values the model is most confident in, highly unreliable
imputations are avoided. Our experiments on multiple EHR datasets, covering
diverse types of missingness, demonstrate that selectively imputing
less-uncertain values not only reduces imputation errors but also improves
downstream tasks. Specifically, we show performance gains in a 24-hour
mortality prediction task, underscoring the practical benefit of incorporating
uncertainty into time series imputation.

</details>


### [105] [Meta-autoencoders: An approach to discovery and representation of relationships between dynamically evolving classes](https://arxiv.org/abs/2507.09362)
*Assaf Marron,Smadar Szekely,Irun Cohen,David Harel*

Main category: cs.LG

TL;DR: A meta-autoencoder (MAE) is introduced as an autoencoder for a collection of autoencoders, enabling compact representation and encoding/decoding of class-specific AEs. It has applications in studying evolving species.


<details>
  <summary>Details</summary>
Motivation: To generalize the concept of autoencoders for multiple classes, enabling efficient modeling of dynamically evolving systems like species in biology.

Method: Constructive definition of MAEs, with initial examples and applications in machine learning and biology.

Result: Proposal of MAEs as a tool for capturing defining and distinguishing properties across evolving classes.

Conclusion: MAEs offer a promising approach for modeling evolving systems, with potential applications in both machine learning and biological research.

Abstract: An autoencoder (AE) is a neural network that, using self-supervised training,
learns a succinct parameterized representation, and a corresponding encoding
and decoding process, for all instances in a given class. Here, we introduce
the concept of a meta-autoencoder (MAE): an AE for a collection of
autoencoders. Given a family of classes that differ from each other by the
values of some parameters, and a trained AE for each class, an MAE for the
family is a neural net that has learned a compact representation and associated
encoder and decoder for the class-specific AEs. One application of this general
concept is in research and modeling of natural evolution -- capturing the
defining and the distinguishing properties across multiple species that are
dynamically evolving from each other and from common ancestors. In this interim
report we provide a constructive definition of MAEs, initial examples, and the
motivating research directions in machine learning and biology.

</details>


### [106] [Fair CCA for Fair Representation Learning: An ADNI Study](https://arxiv.org/abs/2507.09382)
*Bojian Hou,Zhanliang Wang,Zhuoping Zhou,Boning Tong,Zexuan Wang,Jingxuan Bao,Duy Duong-Tran,Qi Long,Li Shen*

Main category: cs.LG

TL;DR: A novel fair CCA method is proposed to ensure fairness in representation learning without compromising accuracy, validated on synthetic and real-world data.


<details>
  <summary>Details</summary>
Motivation: Fairness in machine learning is crucial, and existing fair CCA methods overlook downstream classification impacts, limiting their applicability.

Method: Proposes a fair CCA method ensuring projected features are independent of sensitive attributes, balancing fairness and accuracy.

Result: Validated on synthetic and ADNI data, the method maintains high correlation performance while improving fairness in classification.

Conclusion: Enables fair machine learning in neuroimaging, supporting unbiased analysis in critical applications like Alzheimer's studies.

Abstract: Canonical correlation analysis (CCA) is a technique for finding correlations
between different data modalities and learning low-dimensional representations.
As fairness becomes crucial in machine learning, fair CCA has gained attention.
However, previous approaches often overlook the impact on downstream
classification tasks, limiting applicability. We propose a novel fair CCA
method for fair representation learning, ensuring the projected features are
independent of sensitive attributes, thus enhancing fairness without
compromising accuracy. We validate our method on synthetic data and real-world
data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating
its ability to maintain high correlation analysis performance while improving
fairness in classification tasks. Our work enables fair machine learning in
neuroimaging studies where unbiased analysis is essential.

</details>


### [107] [Geometric Generative Modeling with Noise-Conditioned Graph Networks](https://arxiv.org/abs/2507.09391)
*Peter Pao-Huang,Mitchell Black,Xiaojie Qiu*

Main category: cs.LG

TL;DR: The paper introduces Noise-Conditioned Graph Networks (NCGNs) to improve generative modeling of graphs with spatial structure by dynamically adapting architecture to noise levels.


<details>
  <summary>Details</summary>
Motivation: Existing flow-based generative models use noise-independent architectures, limiting their expressiveness for graphs with spatial structure.

Method: Proposes NCGNs, specifically Dynamic Message Passing (DMP), which adapts message passing range and resolution based on noise level.

Result: DMP outperforms noise-independent architectures in domains like 3D point clouds, transcriptomics, and images.

Conclusion: NCGNs, particularly DMP, enhance generative modeling by dynamically adjusting to noise, improving performance across diverse applications.

Abstract: Generative modeling of graphs with spatial structure is essential across many
applications from computer graphics to spatial genomics. Recent flow-based
generative models have achieved impressive results by gradually adding and then
learning to remove noise from these graphs. Existing models, however, use graph
neural network architectures that are independent of the noise level, limiting
their expressiveness. To address this issue, we introduce
\textit{Noise-Conditioned Graph Networks} (NCGNs), a class of graph neural
networks that dynamically modify their architecture according to the noise
level during generation. Our theoretical and empirical analysis reveals that as
noise increases, (1) graphs require information from increasingly distant
neighbors and (2) graphs can be effectively represented at lower resolutions.
Based on these insights, we develop Dynamic Message Passing (DMP), a specific
instantiation of NCGNs that adapts both the range and resolution of message
passing to the noise level. DMP consistently outperforms noise-independent
architectures on a variety of domains including $3$D point clouds,
spatiotemporal transcriptomics, and images. Code is available at
https://github.com/peterpaohuang/ncgn.

</details>


### [108] [A Random Matrix Theory Perspective on the Learning Dynamics of Multi-head Latent Attention](https://arxiv.org/abs/2507.09394)
*Nandan Kumar Jha,Brandon Reagen*

Main category: cs.LG

TL;DR: The paper investigates how multi-head latent attention (MLA) affects transformer capacity during pretraining, revealing that decoupled rotary embeddings prevent spectral fragmentation and maintain capacity.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of MLA on transformer capacity and how rotary embeddings influence spectral properties during pretraining.

Method: Analyzes the spectrum of the $W_{Q}W_{K}^\top$ gram matrix using Marchenko-Pastur diagnostics, comparing MHA, MLA-PreRoPE, and MLA-Decoupled variants.

Result: Finds that MLA-Decoupled prevents capacity bottlenecks and rank collapse, unlike MHA and MLA-PreRoPE, by maintaining broad spectral support.

Conclusion: The application of rotary embeddings is critical; sharing them across heads mitigates spectral issues and preserves model capacity.

Abstract: In this work, we study how multi-head latent attention (MLA), a popular
strategy for compressing key/value memory, affects a transformer's internal
capacity during pretraining. Using a lightweight suite of Marchenko-Pastur (MP)
diagnostics, we analyze the spectrum of the $W_{Q}W_{K}^\top$ gram matrix
throughout training, comparing three variants: the standard multi-head
attention (MHA) baseline, MLA-PreRoPE with rotary applied before compression,
and MLA-Decoupled, which shares a single rotary sub-vector across all heads.
Our random matrix analysis reveals \textbf{three key findings:} \textbf{ i)}
capacity bottlenecks emerge locally: both MHA and MLA-PreRoPE exhibit sharp,
early spikes in specific layers that persist and propagate, disrupting the
balance between bulk and outlier directions; \textbf{ ii)} these spikes
coincide with rank collapse, concentrating the model's expressivity into narrow
subspaces; \textbf{ iii)} only the decoupled variant prevents this cascade,
maintaining broad spectral support and suppressing outlier formation across
layers. These results underscore that \emph{how} rotary embeddings are applied
is just as critical as \emph{where} compression occurs. Sharing rotary
components across heads mitigates spectral fragmentation and preserves
representational capacity.

</details>


### [109] [Scaling Laws for Optimal Data Mixtures](https://arxiv.org/abs/2507.09404)
*Mustafa Shukor,Louis Bethune,Dan Busbridge,David Grangier,Enrico Fini,Alaaeldin El-Nouby,Pierre Ablin*

Main category: cs.LG

TL;DR: The paper proposes a systematic method using scaling laws to determine the optimal data mixture for training large foundation models, avoiding costly trial-and-error approaches.


<details>
  <summary>Details</summary>
Motivation: Current methods for selecting data mixtures rely on impractical trial-and-error, especially for large-scale pretraining.

Method: Uses scaling laws to predict model loss based on size, tokens, and domain weights, validated across LLM, NMM, and LVM settings.

Result: Demonstrates accurate predictions for performance at larger scales and unseen domain weights, enabling optimal data mixture selection.

Conclusion: Scaling laws provide a principled, cost-effective alternative to trial-and-error for optimizing data mixtures in foundation models.

Abstract: Large foundation models are typically trained on data from multiple domains,
with the data mixture--the proportion of each domain used--playing a critical
role in model performance. The standard approach to selecting this mixture
relies on trial and error, which becomes impractical for large-scale
pretraining. We propose a systematic method to determine the optimal data
mixture for any target domain using scaling laws. Our approach accurately
predicts the loss of a model of size $N$ trained with $D$ tokens and a specific
domain weight vector $h$. We validate the universality of these scaling laws by
demonstrating their predictive power in three distinct and large-scale
settings: large language model (LLM), native multimodal model (NMM), and large
vision models (LVM) pretraining. We further show that these scaling laws can
extrapolate to new data mixtures and across scales: their parameters can be
accurately estimated using a few small-scale training runs, and used to
estimate the performance at larger scales and unseen domain weights. The
scaling laws allow to derive the optimal domain weights for any target domain
under a given training budget ($N$,$D$), providing a principled alternative to
costly trial-and-error methods.

</details>


### [110] [Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers](https://arxiv.org/abs/2507.09406)
*Santhosh Kumar Ravindran*

Main category: cs.LG

TL;DR: The paper introduces adversarial activation patching to detect and mitigate deceptive behaviors in aligned LLMs, demonstrating its effectiveness through simulations and proposing mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: Aligned LLMs often exhibit deceptive behaviors despite safety measures, necessitating tools to identify and address such vulnerabilities.

Method: Adversarial activation patching is used to induce and quantify deception by patching activations from deceptive prompts into safe forward passes.

Result: Simulations show adversarial patching increases deceptive outputs to 23.9% from 0%, with layer-specific variations supporting hypotheses.

Conclusion: The work advances AI safety by highlighting patching's dual-use potential and provides a roadmap for future empirical studies on large-scale models.

Abstract: Large language models (LLMs) aligned for safety through techniques like
reinforcement learning from human feedback (RLHF) often exhibit emergent
deceptive behaviors, where outputs appear compliant but subtly mislead or omit
critical information. This paper introduces adversarial activation patching, a
novel mechanistic interpretability framework that leverages activation patching
as an adversarial tool to induce, detect, and mitigate such deception in
transformer-based models. By sourcing activations from "deceptive" prompts and
patching them into safe forward passes at specific layers, we simulate
vulnerabilities and quantify deception rates. Through toy neural network
simulations across multiple scenarios (e.g., 1000 trials per setup), we
demonstrate that adversarial patching increases deceptive outputs to 23.9% from
a 0% baseline, with layer-specific variations supporting our hypotheses. We
propose six hypotheses, including transferability across models, exacerbation
in multimodal settings, and scaling effects. An expanded literature review
synthesizes over 20 key works in interpretability, deception, and adversarial
attacks. Mitigation strategies, such as activation anomaly detection and robust
fine-tuning, are detailed, alongside ethical considerations and future research
directions. This work advances AI safety by highlighting patching's dual-use
potential and provides a roadmap for empirical studies on large-scale models.

</details>


### [111] [On Information Geometry and Iterative Optimization in Model Compression: Operator Factorization](https://arxiv.org/abs/2507.09428)
*Zakhar Shumaylov,Vasileios Tsiaras,Yannis Stylianou*

Main category: cs.LG

TL;DR: The paper applies information geometry to analyze model compression, emphasizing the importance of defining optimal low-compute submanifolds and projecting onto them. It highlights the role of information divergences in zero-shot accuracy and iterative methods for fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of deploying large deep learning models on resource-constrained devices by leveraging information geometry for effective compression.

Method: Uses information geometry to analyze operator factorization and model compression, focusing on defining optimal submanifolds and projecting onto them. Introduces iterative singular value thresholding for training under soft rank constraints.

Result: Demonstrates that information divergences improve zero-shot accuracy in compression, while iterative methods enhance trainability for fine-tuned models. Simple modifications to existing methods yield better performance under fixed compression rates.

Conclusion: Information geometry provides a valuable framework for model compression, balancing accuracy and computational efficiency, with iterative methods proving crucial for fine-tuning scenarios.

Abstract: The ever-increasing parameter counts of deep learning models necessitate
effective compression techniques for deployment on resource-constrained
devices. This paper explores the application of information geometry, the study
of density-induced metrics on parameter spaces, to analyze existing methods
within the space of model compression, primarily focusing on operator
factorization. Adopting this perspective highlights the core challenge:
defining an optimal low-compute submanifold (or subset) and projecting onto it.
We argue that many successful model compression approaches can be understood as
implicitly approximating information divergences for this projection. We
highlight that when compressing a pre-trained model, using information
divergences is paramount for achieving improved zero-shot accuracy, yet this
may no longer be the case when the model is fine-tuned. In such scenarios,
trainability of bottlenecked models turns out to be far more important for
achieving high compression ratios with minimal performance degradation,
necessitating adoption of iterative methods. In this context, we prove
convergence of iterative singular value thresholding for training neural
networks subject to a soft rank constraint. To further illustrate the utility
of this perspective, we showcase how simple modifications to existing methods
through softer rank reduction result in improved performance under fixed
compression rates.

</details>


### [112] [Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in Multivariate Time Series](https://arxiv.org/abs/2507.09439)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.LG

TL;DR: DyCAST-Net is a novel architecture for causal discovery in multivariate time series, combining dilated convolutions and dynamic sparse attention for accuracy and interpretability. It outperforms existing models and provides insights into hidden causal patterns.


<details>
  <summary>Details</summary>
Motivation: Understanding causal relationships in multivariate time series is crucial for fields like finance and marketing, but complex dependencies and lagged effects challenge traditional methods.

Method: DyCAST-Net integrates dilated temporal convolutions and dynamic sparse attention with adaptive thresholding. It uses a statistical shuffle test for validation.

Result: Outperforms models like TCDF, GCFormer, and CausalFormer, with precise causal delay estimation and reduced false discoveries. Attention heatmaps reveal hidden causal patterns.

Conclusion: DyCAST-Net is effective in high-dimensional, dynamic settings, offering interpretable insights and scalability across domains.

Abstract: Understanding causal relationships in multivariate time series (MTS) is
essential for effective decision-making in fields such as finance and
marketing, where complex dependencies and lagged effects challenge conventional
analytical approaches. We introduce Dynamic Sparse Causal-Attention Temporal
Networks for Interpretable Causality Discovery in MTS (DyCAST-Net), a novel
architecture designed to enhance causal discovery by integrating dilated
temporal convolutions and dynamic sparse attention mechanisms. DyCAST-Net
effectively captures multiscale temporal dependencies through dilated
convolutions while leveraging an adaptive thresholding strategy in its
attention mechanism to eliminate spurious connections, ensuring both accuracy
and interpretability. A statistical shuffle test validation further strengthens
robustness by filtering false positives and improving causal inference
reliability. Extensive evaluations on financial and marketing datasets
demonstrate that DyCAST-Net consistently outperforms existing models such as
TCDF, GCFormer, and CausalFormer. The model provides a more precise estimation
of causal delays and significantly reduces false discoveries, particularly in
noisy environments. Moreover, attention heatmaps offer interpretable insights,
uncovering hidden causal patterns such as the mediated effects of advertising
on consumer behavior and the influence of macroeconomic indicators on financial
markets. Case studies illustrate DyCAST-Net's ability to detect latent
mediators and lagged causal factors, making it particularly effective in
high-dimensional, dynamic settings. The model's architecture enhanced by
RMSNorm stabilization and causal masking ensures scalability and adaptability
across diverse application domains

</details>


### [113] [Transformers Don't In-Context Learn Least Squares Regression](https://arxiv.org/abs/2507.09440)
*Joshua Hill,Benjamin Eyre,Elliot Creager*

Main category: cs.LG

TL;DR: Transformers trained for in-context learning (ICL) fail to generalize under distribution shifts, suggesting they don't implement standard learning algorithms like OLS. Pretraining data shapes ICL behavior via spectral signatures in representations.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind ICL in transformers, particularly why they struggle with out-of-distribution generalization despite matching performance of learning rules like OLS.

Method: Study synthetic linear regression tasks, analyze out-of-distribution generalization, and perform spectral analysis of learned representations.

Result: Transformers fail to generalize after prompt distribution shifts, inconsistent with OLS-like algorithms. Pretraining data influences ICL behavior via unique spectral signatures in representations.

Conclusion: ICL in transformers is not fully explained by standard learning algorithms; pretraining data plays a key role in shaping behavior, with spectral signatures indicating distribution-specific learning.

Abstract: In-context learning (ICL) has emerged as a powerful capability of large
pretrained transformers, enabling them to solve new tasks implicit in example
input-output pairs without any gradient updates. Despite its practical success,
the mechanisms underlying ICL remain largely mysterious. In this work we study
synthetic linear regression to probe how transformers implement learning at
inference time. Previous works have demonstrated that transformers match the
performance of learning rules such as Ordinary Least Squares (OLS) regression
or gradient descent and have suggested ICL is facilitated in transformers
through the learned implementation of one of these techniques. In this work, we
demonstrate through a suite of out-of-distribution generalization experiments
that transformers trained for ICL fail to generalize after shifts in the prompt
distribution, a behaviour that is inconsistent with the notion of transformers
implementing algorithms such as OLS. Finally, we highlight the role of the
pretraining corpus in shaping ICL behaviour through a spectral analysis of the
learned representations in the residual stream. Inputs from the same
distribution as the training data produce representations with a unique
spectral signature: inputs from this distribution tend to have the same top two
singular vectors. This spectral signature is not shared by out-of-distribution
inputs, and a metric characterizing the presence of this signature is highly
correlated with low loss.

</details>


### [114] [Toward Developing Machine-Learning-Aided Tools for the Thermomechanical Monitoring of Nuclear Reactor Components](https://arxiv.org/abs/2507.09443)
*Luiz Aldeia Machado,Victor Coppo Leite,Elia Merzari,Arthur Motta,Roberto Ponciroli,Lander Ibarra,Lise Charlot*

Main category: cs.LG

TL;DR: A CNN combined with a thermomechanical model predicts temperature, stress, and strain in PWR fuel rods using limited temperature data, aiding Predictive Maintenance in NPPs.


<details>
  <summary>Details</summary>
Motivation: To reduce offline time in Nuclear Power Plants by preventing unexpected shutdowns through real-time monitoring of fuel rod conditions.

Method: Uses a CNN trained on BISON and MOOSE-THM simulation data to predict temperature, stress, and strain distributions in fuel rods.

Result: Achieves accurate temperature predictions without overfitting, enabling reliable stress and strain calculations.

Conclusion: The methodology supports Predictive Maintenance by providing real-time monitoring capabilities for nuclear reactors.

Abstract: Proactive maintenance strategies, such as Predictive Maintenance (PdM), play
an important role in the operation of Nuclear Power Plants (NPPs), particularly
due to their capacity to reduce offline time by preventing unexpected shutdowns
caused by component failures.
  In this work, we explore the use of a Convolutional Neural Network (CNN)
architecture combined with a computational thermomechanical model to calculate
the temperature, stress, and strain of a Pressurized Water Reactor (PWR) fuel
rod during operation. This estimation relies on a limited number of temperature
measurements from the cladding's outer surface. This methodology can
potentially aid in developing PdM tools for nuclear reactors by enabling
real-time monitoring of such systems.
  The training, validation, and testing datasets were generated through coupled
simulations involving BISON, a finite element-based nuclear fuel performance
code, and the MOOSE Thermal-Hydraulics Module (MOOSE-THM). We conducted eleven
simulations, varying the peak linear heat generation rates. Of these, eight
were used for training, two for validation, and one for testing.
  The CNN was trained for over 1,000 epochs without signs of overfitting,
achieving highly accurate temperature distribution predictions. These were then
used in a thermomechanical model to determine the stress and strain
distribution within the fuel rod.

</details>


### [115] [Fourier Basis Mapping: A Time-Frequency Learning Framework for Time Series Forecasting](https://arxiv.org/abs/2507.09445)
*Runze Yang,Longbing Cao,Xin You,Kun Fang,Jianxun Li,Jie Yang*

Main category: cs.LG

TL;DR: The paper introduces the Fourier Basis Mapping (FBM) method to improve time series forecasting by addressing inconsistencies in Fourier-based methods and integrating time-frequency features. It enhances various neural networks and achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing Fourier-based methods for time series forecasting suffer from inconsistent starting cycles, series length issues, and overlook temporal information, limiting their effectiveness.

Method: The FBM method integrates time-frequency features through Fourier basis expansion and mapping. It supports plug-and-play integration with neural networks and introduces specialized techniques like interaction masking and multi-scale down-sampling.

Result: FBM enhances linear, MLP-based, and Transformer-based models, achieving state-of-the-art performance on diverse real-world datasets for both long-term and short-term forecasting.

Conclusion: The FBM method effectively addresses the limitations of existing Fourier-based approaches, demonstrating superior performance by integrating time-frequency features and temporal characteristics.

Abstract: The integration of Fourier transform and deep learning opens new avenues for
time series forecasting. We reconsider the Fourier transform from a basis
functions perspective. Specifically, the real and imaginary parts of the
frequency components can be regarded as the coefficients of cosine and sine
basis functions at tiered frequency levels, respectively. We find that existing
Fourier-based methods face inconsistent starting cycles and inconsistent series
length issues. They fail to interpret frequency components precisely and
overlook temporal information. Accordingly, the novel Fourier Basis Mapping
(FBM) method addresses these issues by integrating time-frequency features
through Fourier basis expansion and mapping in the time-frequency space. Our
approach extracts explicit frequency features while preserving temporal
characteristics. FBM supports plug-and-play integration with various types of
neural networks by only adjusting the first initial projection layer for better
performance. First, we propose FBM-L, FBM-NL, and FBM-NP to enhance linear,
MLP-based, and Transformer-based models, respectively, demonstrating the
effectiveness of time-frequency features. Next, we propose a synergetic model
architecture, termed FBM-S, which decomposes the seasonal, trend, and
interaction effects into three separate blocks, each designed to model
time-frequency features in a specialized manner. Finally, we introduce several
techniques tailored for time-frequency features, including interaction masking,
centralization, patching, rolling window projection, and multi-scale
down-sampling. The results are validated on diverse real-world datasets for
both long-term and short-term forecasting tasks with SOTA performance.

</details>


### [116] [Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores Estimated from Ambient Home Health Monitoring](https://arxiv.org/abs/2507.09460)
*Noah Marchal,William E. Janes,Mihail Popescu,Xing Song*

Main category: cs.LG

TL;DR: Semi-supervised regression models using in-home sensor data improved ALS functional decline tracking, with transfer learning and self-attention interpolation showing best results for subscales, while linear interpolation was stable for composite scales.


<details>
  <summary>Details</summary>
Motivation: Clinical monitoring of ALS functional decline misses changes between visits; continuous in-home sensor data could address this gap.

Method: Developed semi-supervised regression models (individual batch, cohort-level batch, incremental transfer learning) with linear, cubic, and self-attention interpolations to estimate ALSFRS-R trajectories.

Result: Transfer learning improved subscale predictions (28/32 contrasts), self-attention interpolation had lowest error for subscales, and linear interpolation was stable for composite scales.

Conclusion: Matching learning methods to functional domain profiles enhances ALS progression tracking, suggesting adaptive model integration for timely interventions.

Abstract: Clinical monitoring of functional decline in ALS relies on periodic
assessments that may miss critical changes occurring between visits. To address
this gap, semi-supervised regression models were developed to estimate rates of
decline in a case series cohort by targeting ALSFRS- R scale trajectories with
continuous in-home sensor monitoring data. Our analysis compared three model
paradigms (individual batch learning and cohort-level batch versus incremental
fine-tuned transfer learning) across linear slope, cubic polynomial, and
ensembled self-attention pseudo-label interpolations. Results revealed cohort
homogeneity across functional domains responding to learning methods, with
transfer learning improving prediction error for ALSFRS-R subscales in 28 of 32
contrasts (mean RMSE=0.20(0.04)), and individual batch learning for predicting
the composite scale (mean RMSE=3.15(1.25)) in 2 of 3. Self-attention
interpolation achieved the lowest prediction error for subscale-level models
(mean RMSE=0.19(0.06)), capturing complex nonlinear progression patterns,
outperforming linear and cubic interpolations in 20 of 32 contrasts, though
linear interpolation proved more stable in all ALSFRS-R composite scale models
(mean RMSE=0.23(0.10)). We identified distinct homogeneity-heterogeneity
profiles across functional domains with respiratory and speech exhibiting
patient-specific patterns benefiting from personalized incremental adaptation,
while swallowing and dressing functions followed cohort-level trajectories
suitable for transfer models. These findings suggest that matching learning and
pseudo-labeling techniques to functional domain-specific
homogeneity-heterogeneity profiles enhances predictive accuracy in ALS
progression tracking. Integrating adaptive model selection within sensor
monitoring platforms could enable timely interventions and scalable deployment
in future multi-center studies.

</details>


### [117] [La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching](https://arxiv.org/abs/2507.09466)
*Tomas Geffner,Kieran Didi,Zhonglin Cao,Danny Reidenbach,Zuobai Zhang,Christian Dallago,Emine Kucukbenli,Karsten Kreis,Arash Vahdat*

Main category: cs.LG

TL;DR: La-Proteina introduces a novel partially latent representation for atomistic protein design, achieving state-of-the-art performance in co-designability, diversity, and scalability up to 800 residues.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of directly generating fully atomistic protein structures with sequences, avoiding explicit side-chain representation issues.

Method: Uses a partially latent representation: explicit coarse backbone modeling with per-residue latent variables for sequence and atomistic details, employing flow matching.

Result: Outperforms benchmarks in co-designability, diversity, structural validity, and motif scaffolding, scaling to 800 residues.

Conclusion: La-Proteina advances atomistic protein design, enabling scalable and robust generation of complex structures.

Abstract: Recently, many generative models for de novo protein structure design have
emerged. Yet, only few tackle the difficult task of directly generating fully
atomistic structures jointly with the underlying amino acid sequence. This is
challenging, for instance, because the model must reason over side chains that
change in length during generation. We introduce La-Proteina for atomistic
protein design based on a novel partially latent protein representation: coarse
backbone structure is modeled explicitly, while sequence and atomistic details
are captured via per-residue latent variables of fixed dimensionality, thereby
effectively side-stepping challenges of explicit side-chain representations.
Flow matching in this partially latent space then models the joint distribution
over sequences and full-atom structures. La-Proteina achieves state-of-the-art
performance on multiple generation benchmarks, including all-atom
co-designability, diversity, and structural validity, as confirmed through
detailed structural analyses and evaluations. Notably, La-Proteina also
surpasses previous models in atomistic motif scaffolding performance, unlocking
critical atomistic structure-conditioned protein design tasks. Moreover,
La-Proteina is able to generate co-designable proteins of up to 800 residues, a
regime where most baselines collapse and fail to produce valid samples,
demonstrating La-Proteina's scalability and robustness.

</details>


### [118] [Discrete Differential Principle for Continuous Smooth Function Representation](https://arxiv.org/abs/2507.09480)
*Guoyou Wang,Yihua Tan,Shiqi Liu*

Main category: cs.LG

TL;DR: The paper proposes a new discrete differential operator using Vandermonde matrices to address Taylor's formula limitations like dimensionality and error propagation, achieving high-order accuracy and tighter error bounds.


<details>
  <summary>Details</summary>
Motivation: Taylor's formula is crucial but suffers from dimensionality and error issues in discrete settings, limiting its applications in fields like fluid mechanics and weather forecasting.

Method: A Vandermonde coefficient matrix from truncated Taylor series is used to estimate derivatives and represent functions locally, with equidistant sampling for accuracy.

Result: The method outperforms finite forward difference and interpolation techniques, with rigorous error bounds and extension to 2D cases.

Conclusion: The proposed technique is effective and superior, with broad applicability in vision, fluid mechanics, and more.

Abstract: Taylor's formula holds significant importance in function representation,
such as solving differential difference equations, ordinary differential
equations, partial differential equations, and further promotes applications in
visual perception, complex control, fluid mechanics, weather forecasting and
thermodynamics. However, the Taylor's formula suffers from the curse of
dimensionality and error propagation during derivative computation in discrete
situations. In this paper, we propose a new discrete differential operator to
estimate derivatives and to represent continuous smooth function locally using
the Vandermonde coefficient matrix derived from truncated Taylor series. Our
method simultaneously computes all derivatives of orders less than the number
of sample points, inherently mitigating error propagation. Utilizing
equidistant uniform sampling, it achieves high-order accuracy while alleviating
the curse of dimensionality. We mathematically establish rigorous error bounds
for both derivative estimation and function representation, demonstrating
tighter bounds for lower-order derivatives. We extend our method to the
two-dimensional case, enabling its use for multivariate derivative
calculations. Experiments demonstrate the effectiveness and superiority of the
proposed method compared to the finite forward difference method for derivative
estimation and cubic spline and linear interpolation for function
representation. Consequently, our technique offers broad applicability across
domains such as vision representation, feature extraction, fluid mechanics, and
cross-media imaging.

</details>


### [119] [An Analysis of Action-Value Temporal-Difference Methods That Learn State Values](https://arxiv.org/abs/2507.09523)
*Brett Daley,Prabhat Nagarajan,Martha White,Marlos C. Machado*

Main category: cs.LG

TL;DR: The paper analyzes TD learning methods that bootstrap from two asymmetric value functions (QV and AV-learning), comparing their convergence and efficiency to single-function methods like Q-learning. It finds AV-learning superior in control settings and introduces RDQ, a new AV-learning algorithm outperforming Dueling DQN.


<details>
  <summary>Details</summary>
Motivation: To clarify the advantages and theoretical soundness of learning two value functions (QV and AV-learning) over single-function methods in TD learning.

Method: Analyzes QV and AV-learning families in terms of convergence and sample efficiency, comparing them to Expected Sarsa and Q-learning. Introduces RDQ, a new AV-learning algorithm.

Result: AV-learning methods outperform Q-learning in control settings. RDQ significantly outperforms Dueling DQN in the MinAtar benchmark.

Conclusion: AV-learning, particularly RDQ, offers major benefits in control settings, providing a more efficient alternative to traditional single-function TD methods.

Abstract: The hallmark feature of temporal-difference (TD) learning is bootstrapping:
using value predictions to generate new value predictions. The vast majority of
TD methods for control learn a policy by bootstrapping from a single
action-value function (e.g., Q-learning and Sarsa). Significantly less
attention has been given to methods that bootstrap from two asymmetric value
functions: i.e., methods that learn state values as an intermediate step in
learning action values. Existing algorithms in this vein can be categorized as
either QV-learning or AV-learning. Though these algorithms have been
investigated to some degree in prior work, it remains unclear if and when it is
advantageous to learn two value functions instead of just one -- and whether
such approaches are theoretically sound in general. In this paper, we analyze
these algorithmic families in terms of convergence and sample efficiency. We
find that while both families are more efficient than Expected Sarsa in the
prediction setting, only AV-learning methods offer any major benefit over
Q-learning in the control setting. Finally, we introduce a new AV-learning
algorithm called Regularized Dueling Q-learning (RDQ), which significantly
outperforms Dueling DQN in the MinAtar benchmark.

</details>


### [120] [Assessing reliability of explanations in unbalanced datasets: a use-case on the occurrence of frost events](https://arxiv.org/abs/2507.09545)
*Ilaria Vascotto,Valentina Blasone,Alex Rodriguez,Alessandro Bonaita,Luca Bortolussi*

Main category: cs.LG

TL;DR: The paper explores the robustness of XAI methods in unbalanced datasets, proposing an evaluation framework for explanation reliability, particularly for the minority class, using on-manifold neighbor generation and consistency metrics.


<details>
  <summary>Details</summary>
Motivation: The increasing use of AI models and legislative demands for explainability necessitate robust explanations, especially in high-risk, unbalanced datasets.

Method: Proposes an evaluation framework for the minority class, leveraging on-manifold neighbor generation, explanation aggregation, and a consistency metric.

Result: Presents preliminary insights on explanation reliability in unbalanced datasets, demonstrated via a frost event case study.

Conclusion: Highlights the need for robust XAI methods in unbalanced datasets and offers a practical approach to evaluate explanation consistency.

Abstract: The usage of eXplainable Artificial Intelligence (XAI) methods has become
essential in practical applications, given the increasing deployment of
Artificial Intelligence (AI) models and the legislative requirements put
forward in the latest years. A fundamental but often underestimated aspect of
the explanations is their robustness, a key property that should be satisfied
in order to trust the explanations. In this study, we provide some preliminary
insights on evaluating the reliability of explanations in the specific case of
unbalanced datasets, which are very frequent in high-risk use-cases, but at the
same time considerably challenging for both AI models and XAI methods. We
propose a simple evaluation focused on the minority class (i.e. the less
frequent one) that leverages on-manifold generation of neighbours, explanation
aggregation and a metric to test explanation consistency. We present a use-case
based on a tabular dataset with numerical features focusing on the occurrence
of frost events.

</details>


### [121] [Holistix: A Dataset for Holistic Wellness Dimensions Analysis in Mental Health Narratives](https://arxiv.org/abs/2507.09565)
*Heeba Shakeel,Tanvir Ahmad,Chandni Saxena*

Main category: cs.LG

TL;DR: A dataset for classifying six wellness dimensions in social media posts is introduced, evaluated with ML and transformer models, and made publicly available.


<details>
  <summary>Details</summary>
Motivation: To enable region-specific wellness assessments and personalized well-being evaluations from social media content.

Method: Developed a dataset with expert-guided annotations, evaluated traditional ML and transformer models using precision, recall, and F1-score, and ensured model interpretability.

Result: Performance assessed via 10-fold cross-validation; dataset supports wellness classification and intervention strategies.

Conclusion: The dataset aids in wellness assessments and mental health interventions, with ethical considerations for public release.

Abstract: We introduce a dataset for classifying wellness dimensions in social media
user posts, covering six key aspects: physical, emotional, social,
intellectual, spiritual, and vocational. The dataset is designed to capture
these dimensions in user-generated content, with a comprehensive annotation
framework developed under the guidance of domain experts. This framework allows
for the classification of text spans into the appropriate wellness categories.
We evaluate both traditional machine learning models and advanced
transformer-based models for this multi-class classification task, with
performance assessed using precision, recall, and F1-score, averaged over
10-fold cross-validation. Post-hoc explanations are applied to ensure the
transparency and interpretability of model decisions. The proposed dataset
contributes to region-specific wellness assessments in social media and paves
the way for personalized well-being evaluations and early intervention
strategies in mental health. We adhere to ethical considerations for
constructing and releasing our experiments and dataset publicly on Github.

</details>


### [122] [DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences](https://arxiv.org/abs/2507.09602)
*Bocheng Ju,Junchao Fan,Jiaqi Liu,Xiaolin Chang*

Main category: cs.LG

TL;DR: DRAGD and DRAGDP exploit gradient discrepancies in federated unlearning to reconstruct deleted data, revealing privacy vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: To expose privacy risks in federated unlearning due to gradient exchanges during data deletion.

Method: Introduces DRAGD and DRAGDP attacks, leveraging gradient discrepancies and prior data for data reconstruction.

Result: DRAGD and DRAGDP outperform existing methods in reconstructing deleted data.

Conclusion: Highlights a critical privacy flaw in federated unlearning and proposes solutions to enhance security.

Abstract: Federated learning enables collaborative machine learning while preserving
data privacy. However, the rise of federated unlearning, designed to allow
clients to erase their data from the global model, introduces new privacy
concerns. Specifically, the gradient exchanges during the unlearning process
can leak sensitive information about deleted data. In this paper, we introduce
DRAGD, a novel attack that exploits gradient discrepancies before and after
unlearning to reconstruct forgotten data. We also present DRAGDP, an enhanced
version of DRAGD that leverages publicly available prior data to improve
reconstruction accuracy, particularly for complex datasets like facial images.
Extensive experiments across multiple datasets demonstrate that DRAGD and
DRAGDP significantly outperform existing methods in data reconstruction.Our
work highlights a critical privacy vulnerability in federated unlearning and
offers a practical solution, advancing the security of federated unlearning
systems in real-world applications.

</details>


### [123] [MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression](https://arxiv.org/abs/2507.09616)
*Ofir Gordon,Ariel Lapid,Elad Cohen,Yarden Yagil,Arnon Netzer,Hai Victor Habi*

Main category: cs.LG

TL;DR: MLoRQ combines low-rank approximation and mixed-precision quantization for efficient transformer deployment on edge devices, achieving up to 15% performance improvement.


<details>
  <summary>Details</summary>
Motivation: Deploying transformers on resource-constrained edge devices is challenging, requiring efficient compression techniques.

Method: MLoRQ uses a two-stage optimization (intra-layer and inter-layer) to assign optimal bit-width and rank per layer, with optional adaptive rounding for error mitigation.

Result: MLoRQ achieves state-of-the-art results with up to 15% performance gain on Vision Transformers for tasks like image classification and object detection.

Conclusion: MLoRQ effectively integrates low-rank and quantization techniques, offering a practical solution for edge deployment of transformers.

Abstract: Deploying transformer-based neural networks on resource-constrained edge
devices presents a significant challenge. This challenge is often addressed
through various techniques, such as low-rank approximation and mixed-precision
quantization. In this work, we introduce Mixed Low-Rank and Quantization
(MLoRQ), a novel method that integrates both techniques. MLoRQ employs a
two-stage optimization process to determine optimal bit-width and rank
assignments for each layer, adhering to predefined memory constraints. This
process includes: (i) an intra-layer optimization that identifies potentially
optimal compression solutions out of all low-rank and quantization
combinations; (ii) an inter-layer optimization that assigns bit-width precision
and rank to each layer while ensuring the memory constraint is met. An optional
final step applies a sequential optimization process using a modified adaptive
rounding technique to mitigate compression-induced errors in joint low-rank
approximation and quantization. The method is compatible and can be seamlessly
integrated with most existing quantization algorithms. MLoRQ shows
state-of-the-art results with up to 15\% performance improvement, evaluated on
Vision Transformers for image classification, object detection, and instance
segmentation tasks.

</details>


### [124] [Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset](https://arxiv.org/abs/2507.09650)
*Lily Hong Zhang,Smitha Milli,Karen Jusko,Jonathan Smith,Brandon Amos,Wassim,Bouaziz,Manon Revel,Jack Kussman,Lisa Titus,Bhaktipriya Radharapu,Jane Yu,Vidya Sarma,Kris Rose,Maximilian Nickel*

Main category: cs.LG

TL;DR: The paper addresses the challenge of aligning LLMs with diverse human preferences across cultural and political dimensions, proposing negatively-correlated sampling to improve alignment methods and introducing the Community Alignment dataset.


<details>
  <summary>Details</summary>
Motivation: To ensure LLMs serve users with conflicting preferences across cultural, political, or other dimensions, highlighting the gap between human preference diversity and LLM responses.

Method: Conducted a large-scale multilingual human study (N=15,000) across five countries, analyzed existing preference dataset limitations, and introduced negatively-correlated sampling for better alignment.

Result: Found significant variation in human preferences compared to LLMs, improved alignment methods with negatively-correlated sampling, and released the Community Alignment dataset (200,000 comparisons).

Conclusion: The Community Alignment dataset and negatively-correlated sampling enhance LLM alignment with diverse global preferences, addressing current limitations.

Abstract: How can large language models (LLMs) serve users with varying preferences
that may conflict across cultural, political, or other dimensions? To advance
this challenge, this paper establishes four key results. First, we demonstrate,
through a large-scale multilingual human study with representative samples from
five countries (N=15,000), that humans exhibit significantly more variation in
preferences than the responses of 21 state-of-the-art LLMs. Second, we show
that existing methods for preference dataset collection are insufficient for
learning the diversity of human preferences even along two of the most salient
dimensions of variability in global values, due to the underlying homogeneity
of candidate responses. Third, we argue that this motivates the need for
negatively-correlated sampling when generating candidate sets, and we show that
simple prompt-based techniques for doing so significantly enhance the
performance of alignment methods in learning heterogeneous preferences. Fourth,
based on this novel candidate sampling approach, we collect and open-source
Community Alignment, the largest and most representative multilingual and
multi-turn preference dataset to date, featuring almost 200,000 comparisons
from annotators spanning five countries. We hope that the Community Alignment
dataset will be a valuable resource for improving the effectiveness of LLMs for
a diverse global population.

</details>


### [125] [Conformal Prediction for Privacy-Preserving Machine Learning](https://arxiv.org/abs/2507.09678)
*Alexander David Balinsky,Dominik Krzeminski,Alexander Balinsky*

Main category: cs.LG

TL;DR: The paper explores integrating Conformal Prediction (CP) with supervised learning on encrypted data, showing CP remains effective in encrypted domains. It compares $p$-value and $e$-value-based CP, finding $e$-value methods achieve better coverage but with trade-offs in set compactness.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between uncertainty quantification and privacy-preserving machine learning by applying CP to encrypted data.

Method: Uses AES-encrypted MNIST data to test CP methods, comparing $p$-value and $e$-value-based predictors.

Result: Models on encrypted data achieve 36.88% accuracy, with $e$-value CP covering 60% of cases. $p$-value CP yields smaller sets but less coverage.

Conclusion: CP is viable for encrypted data but involves trade-offs between set size and reliability, laying groundwork for secure uncertainty quantification.

Abstract: We investigate the integration of Conformal Prediction (CP) with supervised
learning on deterministically encrypted data, aiming to bridge the gap between
rigorous uncertainty quantification and privacy-preserving machine learning.
Using AES-encrypted variants of the MNIST dataset, we demonstrate that CP
methods remain effective even when applied directly in the encrypted domain,
owing to the preservation of data exchangeability under fixed-key encryption.
We test traditional $p$-value-based against $e$-value-based conformal
predictors. Our empirical evaluation reveals that models trained on
deterministically encrypted data retain the ability to extract meaningful
structure, achieving 36.88\% test accuracy -- significantly above random
guessing (9.56\%) observed with per-instance encryption. Moreover,
$e$-value-based CP achieves predictive set coverage of over 60\% with 4.3
loss-threshold calibration, correctly capturing the true label in 4888 out of
5000 test cases. In contrast, the $p$-value-based CP yields smaller predictive
sets but with reduced coverage accuracy. These findings highlight both the
promise and limitations of CP in encrypted data settings and underscore
critical trade-offs between prediction set compactness and reliability. %Our
work sets a foundation for principled uncertainty quantification in secure,
privacy-aware learning systems.

</details>


### [126] [Networked Information Aggregation via Machine Learning](https://arxiv.org/abs/2507.09683)
*Michael Kearns,Aaron Roth,Emily Ryu*

Main category: cs.LG

TL;DR: The paper studies distributed learning in a DAG where agents observe subsets of features and learn sequentially, using parent predictions. It explores conditions for information aggregation, with depth as a key parameter, and provides theoretical bounds and experiments.


<details>
  <summary>Details</summary>
Motivation: To understand how agents in a DAG can aggregate information effectively despite limited feature access, aiming for competitive error rates compared to full-feature models.

Method: Agents learn sequentially in a DAG, using observed features and parent predictions. Theoretical analysis includes upper/lower bounds for linear and general hypothesis classes, with experiments to validate findings.

Result: Depth of the DAG is critical: information aggregation is possible over long paths if features are well-represented, but fails in shallow DAGs (e.g., hub-and-spokes).

Conclusion: DAG depth determines information aggregation success, with theoretical and empirical support for linear and general hypothesis classes.

Abstract: We study a distributed learning problem in which learning agents are embedded
in a directed acyclic graph (DAG). There is a fixed and arbitrary distribution
over feature/label pairs, and each agent or vertex in the graph is able to
directly observe only a subset of the features -- potentially a different
subset for every agent. The agents learn sequentially in some order consistent
with a topological sort of the DAG, committing to a model mapping observations
to predictions of the real-valued label. Each agent observes the predictions of
their parents in the DAG, and trains their model using both the features of the
instance that they directly observe, and the predictions of their parents as
additional features. We ask when this process is sufficient to achieve
\emph{information aggregation}, in the sense that some agent in the DAG is able
to learn a model whose error is competitive with the best model that could have
been learned (in some hypothesis class) with direct access to \emph{all}
features, despite the fact that no single agent in the network has such access.
We give upper and lower bounds for this problem for both linear and general
hypothesis classes. Our results identify the \emph{depth} of the DAG as the key
parameter: information aggregation can occur over sufficiently long paths in
the DAG, assuming that all of the relevant features are well represented along
the path, and there are distributions over which information aggregation cannot
occur even in the linear case, and even in arbitrarily large DAGs that do not
have sufficient depth (such as a hub-and-spokes topology in which the spoke
vertices collectively see all the features). We complement our theoretical
results with a comprehensive set of experiments.

</details>


### [127] [Multiple Choice Learning of Low Rank Adapters for Language Modeling](https://arxiv.org/abs/2507.10419)
*Victor Letzelter,Hugo Malard,Mathieu Fontaine,Gaël Richard,Slim Essid,Andrei Bursuc,Patrick Pérez*

Main category: cs.LG

TL;DR: LoRA-MCL enhances language models by decoding diverse, plausible continuations using Multiple Choice Learning and Winner-Takes-All loss.


<details>
  <summary>Details</summary>
Motivation: Traditional language modeling struggles with ambiguity, as multiple futures can be equally plausible for a given context.

Method: Combines Low-Rank Adaptation (LoRA) with Multiple Choice Learning (MCL) and Winner-Takes-All (WTA) loss to handle ambiguity.

Result: Achieves high diversity and relevance in outputs, demonstrated on visual and audio captioning tasks.

Conclusion: LoRA-MCL effectively addresses ambiguity in language modeling, producing diverse and plausible continuations.

Abstract: We propose LoRA-MCL, a training scheme that extends next-token prediction in
language models with a method designed to decode diverse, plausible sentence
continuations at inference time. Traditional language modeling is an
intrinsically ill-posed problem: given a context, multiple futures may be
equally plausible. Our approach leverages Multiple Choice Learning (MCL) and
the Winner-Takes-All (WTA) loss to efficiently handle ambiguity through
Low-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying
Multiple Choice Learning to Language Modeling, assuming the data is generated
from a mixture of distributions. To illustrate the proposed approach, we use
data sampled from mixtures of Markov chains. We then demonstrate with extensive
experiments on real-world visual and audio captioning tasks that our method
achieves high diversity and relevance in generated outputs.

</details>


### [128] [Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness](https://arxiv.org/abs/2507.09687)
*Md Mushfiqur Rahaman,Elliot Chang,Tasmiah Haque,Srinjoy Das*

Main category: cs.LG

TL;DR: A study compares generative and discriminative LSTM-based text classifiers with Post Training Quantization (PTQ), revealing generative models' sensitivity to bitwidth, calibration data, and noise, while discriminative models remain robust.


<details>
  <summary>Details</summary>
Motivation: Text classification in edge computing requires low latency and high accuracy, with generative classifiers offering robustness to noise and out-of-distribution data. However, deploying them on edge devices is challenging due to computational constraints.

Method: Comparative study of generative and discriminative LSTM models with PTQ using the Brevitas library, evaluated across bitwidths and noisy conditions. Class imbalance in calibration data is also analyzed.

Result: Generative classifiers are more sensitive to bitwidth, calibration data, and noise, while discriminative ones remain robust. Class imbalance in calibration data degrades generative model performance at lower bitwidths.

Conclusion: Calibration data quality is crucial for PTQ success, especially for generative classifiers. The study provides insights for deploying robust text classifiers in edge environments.

Abstract: Text classification plays a pivotal role in edge computing applications like
industrial monitoring, health diagnostics, and smart assistants, where low
latency and high accuracy are both key requirements. Generative classifiers, in
particular, have been shown to exhibit robustness to out-of-distribution and
noisy data, which is an extremely critical consideration for deployment in such
real-time edge environments. However, deploying such models on edge devices
faces computational and memory constraints. Post Training Quantization (PTQ)
reduces model size and compute costs without retraining, making it ideal for
edge deployment. In this work, we present a comprehensive comparative study of
generative and discriminative Long Short Term Memory (LSTM)-based text
classification models with PTQ using the Brevitas quantization library. We
evaluate both types of classifier models across multiple bitwidths and assess
their robustness under regular and noisy input conditions. We find that while
discriminative classifiers remain robust, generative ones are more sensitive to
bitwidth, calibration data used during PTQ, and input noise during quantized
inference. We study the influence of class imbalance in calibration data for
both types of classifiers, comparing scenarios with evenly and unevenly
distributed class samples including their effect on weight adjustments and
activation profiles during PTQ. Using test statistics derived from
nonparametric hypothesis testing, we identify that using class imbalanced data
during calibration introduces insufficient weight adaptation at lower bitwidths
for generative LSTM classifiers, thereby leading to degraded performance. This
study underscores the role of calibration data in PTQ and when generative
classifiers succeed or fail under noise, aiding deployment in edge
environments.

</details>


### [129] [Frequency-aware Surrogate Modeling With SMT Kernels For Advanced Data Forecasting](https://arxiv.org/abs/2507.09694)
*Nicolas Gonel,Paul Saves,Joseph Morlier*

Main category: cs.LG

TL;DR: The paper presents an open-source framework for developing correlation kernels, enhancing surrogate modeling with frequency-aware elements and diverse kernel types, validated on test cases and integrated into SMT 2.0.


<details>
  <summary>Details</summary>
Motivation: To address limitations of traditional kernel functions in capturing complex mechanical behaviors and time-frequency dynamics in aircraft systems.

Method: Extends kernel functions (e.g., exponential squared sine, rational quadratic) and integrates them into the Surrogate Modeling Toolbox (SMT 2.0). Validated on test cases like Mauna-Loa CO2 forecasting.

Result: A flexible framework for customizable kernel configurations and composite models, validated on real-world applications.

Conclusion: The framework advances surrogate modeling for frequency-sensitive domains, offering a versatile toolset for engineers and researchers.

Abstract: This paper introduces a comprehensive open-source framework for developing
correlation kernels, with a particular focus on user-defined and composition of
kernels for surrogate modeling. By advancing kernel-based modeling techniques,
we incorporate frequency-aware elements that effectively capture complex
mechanical behaviors and timefrequency dynamics intrinsic to aircraft systems.
Traditional kernel functions, often limited to exponential-based methods, are
extended to include a wider range of kernels such as exponential squared sine
and rational quadratic kernels, along with their respective firstand
second-order derivatives. The proposed methodologies are first validated on a
sinus cardinal test case and then applied to forecasting Mauna-Loa Carbon
Dioxide (CO 2 ) concentrations and airline passenger traffic. All these
advancements are integrated into the open-source Surrogate Modeling Toolbox
(SMT 2.0), providing a versatile platform for both standard and customizable
kernel configurations. Furthermore, the framework enables the combination of
various kernels to leverage their unique strengths into composite models
tailored to specific problems. The resulting framework offers a flexible
toolset for engineers and researchers, paving the way for numerous future
applications in metamodeling for complex, frequency-sensitive domains.

</details>


### [130] [Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination](https://arxiv.org/abs/2507.10532)
*Mingqi Wu,Zhihao Zhang,Qiaole Dong,Zhiheng Xi,Jun Zhao,Senjie Jin,Xiaoran Fan,Yuhao Zhou,Yanwei Fu,Qin Liu,Songyang Zhang,Qi Zhang*

Main category: cs.LG

TL;DR: The paper investigates the reliability of reasoning improvements in LLMs using RL, revealing potential data contamination in benchmarks and advocating for cleaner evaluation methods.


<details>
  <summary>Details</summary>
Motivation: To address concerns about unreliable results from contaminated benchmarks and inconsistent performance gains across LLM families.

Method: Introduces a synthetic dataset (RandomCalculation) to test RL methods without contamination, evaluating performance under accurate vs. noisy reward signals.

Result: Accurate reward signals improve reasoning, while noisy signals do not, highlighting the need for uncontaminated benchmarks.

Conclusion: Advocates for evaluating RL methods on clean datasets and diverse models to ensure trustworthy conclusions.

Abstract: The reasoning capabilities of large language models (LLMs) have been a
longstanding focus of research. Recent works have further enhanced these
capabilities using reinforcement learning (RL), with many new methods claiming
significant improvements with minimal or no external supervision. Surprisingly,
some studies even suggest that random or incorrect reward signals can enhance
reasoning performance. However, these breakthroughs are mostly reported on the
Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,
AMC, and AIME, while failing to achieve similar gains on other models like
Llama, which warrants further investigation. Our analysis shows that although
Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on
large-scale web corpora makes it vulnerable to data contamination in popular
benchmarks. As a result, results derived from these benchmarks may be
unreliable. To address this, we introduce a generator that produces fully
synthetic arithmetic problems of arbitrary length and difficulty, yielding a
clean dataset we call RandomCalculation. Using these leakage-free datasets, we
show that only accurate reward signals consistently improve performance, while
noisy or incorrect signals do not. We advocate for evaluating RL methods on
uncontaminated benchmarks and across diverse model families to ensure
trustworthy conclusions.

</details>


### [131] [EPT-2 Technical Report](https://arxiv.org/abs/2507.09703)
*Roberto Molinaro,Niall Siegenheim,Niels Poulsen,Jordan Dane Daubinet,Henry Martin,Mark Frey,Kevin Thiart,Alexander Jakob Dautel,Andreas Schlueter,Alex Grigoryev,Bogdan Danciu,Nikoo Ekhtiari,Bas Steunebrink,Leonie Wagner,Marvin Vincent Gabler*

Main category: cs.LG

TL;DR: EPT-2, an advanced AI model for Earth system forecasting, outperforms predecessors and competitors like Microsoft Aurora and ECMWF IFS HRES. It introduces EPT-2e for probabilistic forecasting, surpassing ECMWF ENS at lower computational cost.


<details>
  <summary>Details</summary>
Motivation: To advance Earth system forecasting by improving accuracy and computational efficiency over existing models.

Method: Developed EPT-2, an AI foundation model, and its ensemble variant EPT-2e for probabilistic forecasting.

Result: EPT-2 outperforms EPT-1.5, Microsoft Aurora, and ECMWF IFS HRES. EPT-2e surpasses ECMWF ENS with lower computational cost.

Conclusion: EPT-2 and EPT-2e set new benchmarks in Earth system forecasting, offering superior performance and efficiency.

Abstract: We present EPT-2, the latest iteration in our Earth Physics Transformer (EPT)
family of foundation AI models for Earth system forecasting. EPT-2 delivers
substantial improvements over its predecessor, EPT-1.5, and sets a new state of
the art in predicting energy-relevant variables-including 10m and 100m wind
speed, 2m temperature, and surface solar radiation-across the full 0-240h
forecast horizon. It consistently outperforms leading AI weather models such as
Microsoft Aurora, as well as the operational numerical forecast system IFS HRES
from the European Centre for Medium-Range Weather Forecasts (ECMWF). In
parallel, we introduce a perturbation-based ensemble model of EPT-2 for
probabilistic forecasting, called EPT-2e. Remarkably, EPT-2e significantly
surpasses the ECMWF ENS mean-long considered the gold standard for medium- to
longrange forecasting-while operating at a fraction of the computational cost.
EPT models, as well as third-party forecasts, are accessible via the app.jua.ai
platform.

</details>


### [132] [Continental scale habitat modelling with artificial intelligence and multimodal earth observation](https://arxiv.org/abs/2507.09732)
*Sara Si-Moussi,Stephan Hennekens,Sander Mucher,Stan Los,Wilfried Thuiller*

Main category: cs.LG

TL;DR: The paper explores using high-resolution remote sensing and AI to improve habitat mapping, addressing challenges like co-occurring habitats and class imbalance, and demonstrates enhanced accuracy through hierarchical modeling and ensemble machine learning.


<details>
  <summary>Details</summary>
Motivation: Accurate, high-resolution habitat maps are crucial for conservation but current methods struggle with co-occurring habitats and class imbalance.

Method: The study uses high-resolution RS data and AI, integrating multi-spectral and SAR imagery, and employs hierarchical modeling and ensemble machine learning to address classification challenges.

Result: Hierarchical modeling and ensemble ML improved classification accuracy, especially in fragmented landscapes, and integrating MSI and SAR imagery enhanced performance.

Conclusion: The framework is adaptable globally and future work should focus on dynamic habitats, segmentation, and next-gen EO data.

Abstract: Habitats integrate the abiotic conditions and biophysical structures that
support biodiversity and sustain nature's contributions to people. As these
ecosystems face mounting pressure from human activities, accurate,
high-resolution habitat maps are essential for effective conservation and
restoration. Yet current maps often fall short in thematic or spatial
resolution because they must (1) model several mutually exclusive habitat types
that co-occur across landscapes and (2) cope with severe class imbalance that
complicate multi-class training. Here, we evaluated how high-resolution remote
sensing (RS) data and Artificial Intelligence (AI) tools can improve habitat
classification over large geographic extents at fine thematic resolution. Using
vegetation plots from the European Vegetation Archive, we modelled Level 3
EUNIS habitats across Europe and assessed multiple modelling strategies against
independent validation datasets. Strategies that exploited the hierarchical
nature of habitat nomenclatures resolved classification ambiguities, especially
in fragmented landscapes. Integrating multi-spectral (MSI) and synthetic
aperture radar (SAR) imagery, particularly through Earth Observation Foundation
models, enhanced within-formation discrimination and overall performance.
Finally, ensemble machine learning that corrects class imbalance boosted
accuracy further. Our methodological framework is transferable beyond Europe
and adaptable to other classification systems. Future research should advance
temporal modelling of dynamic habitats, extend to habitat segmentation and
quality assessment, and exploit next-generation EO data paired with
higher-quality in-situ observations.

</details>


### [133] [Universal Physics Simulation: A Foundational Diffusion Approach](https://arxiv.org/abs/2507.09733)
*Bradley Camburn*

Main category: cs.LG

TL;DR: A foundational AI model for universal physics simulation learns physical laws from boundary-condition data without predefined equations, outperforming traditional methods like PINNs and finite-difference approaches.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of traditional physics simulation methods that require explicit equation encoding, enabling generalizable and discovery-driven AI-based physics.

Method: Uses a sketch-guided diffusion transformer to treat simulation as conditional generation, mapping boundary conditions to steady-state solutions with novel spatial encoding.

Result: Achieves SSIM > 0.8 with sub-pixel boundary accuracy, bypassing temporal integration errors and enabling physics discovery via learned representations.

Conclusion: Represents a paradigm shift to AI-discovered physics, establishing a universal simulation framework.

Abstract: We present the first foundational AI model for universal physics simulation
that learns physical laws directly from boundary-condition data without
requiring a priori equation encoding. Traditional physics-informed neural
networks (PINNs) and finite-difference methods necessitate explicit
mathematical formulation of governing equations, fundamentally limiting their
generalizability and discovery potential. Our sketch-guided diffusion
transformer approach reimagines computational physics by treating simulation as
a conditional generation problem, where spatial boundary conditions guide the
synthesis of physically accurate steady-state solutions.
  By leveraging enhanced diffusion transformer architectures with novel spatial
relationship encoding, our model achieves direct boundary-to-equilibrium
mapping and is generalizable to diverse physics domains. Unlike sequential
time-stepping methods that accumulate errors over iterations, our approach
bypasses temporal integration entirely, directly generating steady-state
solutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our
data-informed approach enables physics discovery through learned
representations analyzable via Layer-wise Relevance Propagation (LRP),
revealing emergent physical relationships without predetermined mathematical
constraints. This work represents a paradigm shift from AI-accelerated physics
to AI-discovered physics, establishing the first truly universal physics
simulation framework.

</details>


### [134] [Do we need equivariant models for molecule generation?](https://arxiv.org/abs/2507.09753)
*Ewa M. Nowara,Joshua Rackers,Patricia Suriana,Pan Kessel,Max Shen,Andrew Martin Watkins,Michael Maser*

Main category: cs.LG

TL;DR: Non-equivariant CNNs with rotation augmentations can match equivariant GNNs in molecular tasks, simplifying model complexity.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that explicit equivariance is essential for high-quality 3D molecular generation, given the complexity and scalability issues of equivariant GNNs.

Method: Train non-equivariant CNNs with rotation augmentations, analyze learned equivariance, and evaluate performance on denoising, molecule generation, and property prediction.

Result: Non-equivariant CNNs achieve comparable performance to equivariant models, with insights into how model size, dataset size, and training duration affect results.

Conclusion: Learned equivariance in CNNs can replace explicit equivariance, offering a simpler and scalable alternative for molecular discovery.

Abstract: Deep generative models are increasingly used for molecular discovery, with
most recent approaches relying on equivariant graph neural networks (GNNs)
under the assumption that explicit equivariance is essential for generating
high-quality 3D molecules. However, these models are complex, difficult to
train, and scale poorly.
  We investigate whether non-equivariant convolutional neural networks (CNNs)
trained with rotation augmentations can learn equivariance and match the
performance of equivariant models. We derive a loss decomposition that
separates prediction error from equivariance error, and evaluate how model
size, dataset size, and training duration affect performance across denoising,
molecule generation, and property prediction. To our knowledge, this is the
first study to analyze learned equivariance in generative tasks.

</details>


### [135] [Explainable AI in Genomics: Transcription Factor Binding Site Prediction with Mixture of Experts](https://arxiv.org/abs/2507.09754)
*Aakash Tripathi,Ian E. Nielsen,Muhammad Umer,Ravi P. Ramachandran,Ghulam Rasool*

Main category: cs.LG

TL;DR: A novel Mixture of Experts (MoE) model for TFBS prediction outperforms individual models, especially in OOD scenarios, and introduces ShiftSmooth for better interpretability.


<details>
  <summary>Details</summary>
Motivation: Understanding gene regulation and biological processes requires accurate TFBS prediction, but existing methods lack generalizability and interpretability.

Method: The study integrates multiple pre-trained CNN models into a MoE framework and evaluates performance on in-distribution and OOD datasets, using ANOVA for significance testing. ShiftSmooth is introduced for robust attribution mapping.

Result: The MoE model achieves competitive or superior performance, excelling in OOD scenarios. ShiftSmooth provides better interpretability than traditional methods.

Conclusion: The MoE model and ShiftSmooth offer an efficient, generalizable, and interpretable solution for TFBS prediction, advancing genome biology and transcriptional regulation understanding.

Abstract: Transcription Factor Binding Site (TFBS) prediction is crucial for
understanding gene regulation and various biological processes. This study
introduces a novel Mixture of Experts (MoE) approach for TFBS prediction,
integrating multiple pre-trained Convolutional Neural Network (CNN) models,
each specializing in different TFBS patterns. We evaluate the performance of
our MoE model against individual expert models on both in-distribution and
out-of-distribution (OOD) datasets, using six randomly selected transcription
factors (TFs) for OOD testing. Our results demonstrate that the MoE model
achieves competitive or superior performance across diverse TF binding sites,
particularly excelling in OOD scenarios. The Analysis of Variance (ANOVA)
statistical test confirms the significance of these performance differences.
Additionally, we introduce ShiftSmooth, a novel attribution mapping technique
that provides more robust model interpretability by considering small shifts in
input sequences. Through comprehensive explainability analysis, we show that
ShiftSmooth offers superior attribution for motif discovery and localization
compared to traditional Vanilla Gradient methods. Our work presents an
efficient, generalizable, and interpretable solution for TFBS prediction,
potentially enabling new discoveries in genome biology and advancing our
understanding of transcriptional regulation.

</details>


### [136] [Toward accurate RUL and SOH estimation using reinforced graph-based PINNs enhanced with dynamic weights](https://arxiv.org/abs/2507.09766)
*Mohamadreza Akbari Pour,Ali Ghasemzadeh,MohamadAli Bijarchi,Mohammad Behshad Shafii*

Main category: cs.LG

TL;DR: The paper introduces RGPD, a novel framework combining physics-based supervision with spatio-temporal learning for accurate RUL and SOH estimation, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Accurate RUL and SOH estimation is crucial for PHM in industrial applications, requiring robust and adaptive methods.

Method: RGPD integrates GCRNs, GATConv, SAC, and Q-learning to dynamically weight physics-informed loss terms and enhance spatio-temporal learning.

Result: RGPD consistently outperforms state-of-the-art models in RUL and SOH tasks across diverse industrial datasets.

Conclusion: The framework demonstrates strong robustness and predictive accuracy, reducing manual tuning needs and improving generalization.

Abstract: Accurate estimation of Remaining Useful Life (RUL) and State of Health (SOH)
is essential for Prognostics and Health Management (PHM) across a wide range of
industrial applications. We propose a novel framework -- Reinforced Graph-Based
Physics-Informed Neural Networks Enhanced with Dynamic Weights (RGPD) -- that
combines physics-based supervision with advanced spatio-temporal learning.
Graph Convolutional Recurrent Networks (GCRNs) embed graph-convolutional
filters within recurrent units to capture how node representations evolve over
time. Graph Attention Convolution (GATConv) leverages a self-attention
mechanism to compute learnable, edge-wise attention coefficients, dynamically
weighting neighbor contributions for adaptive spatial aggregation. A Soft
Actor-Critic (SAC) module is positioned between the Temporal Attention Unit
(TAU) and GCRN to further improve the spatio-temporal learning. This module
improves attention and prediction accuracy by dynamically scaling hidden
representations to minimize noise and highlight informative features. To
identify the most relevant physical constraints in each area, Q-learning agents
dynamically assign weights to physics-informed loss terms, improving
generalization across real-time industrial systems and reducing the need for
manual tuning. In both RUL and SOH estimation tasks, the proposed method
consistently outperforms state-of-the-art models, demonstrating strong
robustness and predictive accuracy across varied degradation patterns across
three diverse industrial benchmark datasets.

</details>


### [137] [Knowing When to Quit: Probabilistic Early Exits for Speech Separation](https://arxiv.org/abs/2507.09768)
*Kenny Falkær Olsen. Mads Østergaard,Karl Ulbæk,Søren Føns Nielsen,Rasmus Malik Høegh Lindrup,Bjørn Sand Jensen,Morten Mørup*

Main category: cs.LG

TL;DR: A scalable neural network architecture for speech separation with early-exit capability and probabilistic framework for dynamic compute-scaling, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Address limitations of fixed-compute architectures in embedded and heterogeneous devices by enabling dynamic compute-scaling.

Method: Designs an early-exit neural network with an uncertainty-aware probabilistic framework to model clean speech and error variance, deriving probabilistic exit conditions.

Result: Competitive with state-of-the-art models across varying compute budgets, enabling fine-grained dynamic scaling.

Conclusion: The framework provides scalable, high-performance speech separation with interpretable exit conditions, suitable for diverse devices.

Abstract: In recent years, deep learning-based single-channel speech separation has
improved considerably, in large part driven by increasingly compute- and
parameter-efficient neural network architectures. Most such architectures are,
however, designed with a fixed compute and parameter budget, and consequently
cannot scale to varying compute demands or resources, which limits their use in
embedded and heterogeneous devices such as mobile phones and hearables. To
enable such use-cases we design a neural network architecture for speech
separation capable of early-exit, and we propose an uncertainty-aware
probabilistic framework to jointly model the clean speech signal and error
variance which we use to derive probabilistic early-exit conditions in terms of
desired signal-to-noise ratios. We evaluate our methods on both speech
separation and enhancement tasks, and we show that a single early-exit model
can be competitive with state-of-the-art models trained at many compute and
parameter budgets. Our framework enables fine-grained dynamic compute-scaling
of speech separation networks while achieving state-of-the-art performance and
interpretable exit conditions.

</details>


### [138] [Efficient Molecular Conformer Generation with SO(3)-Averaged Flow Matching and Reflow](https://arxiv.org/abs/2507.09785)
*Zhonglin Cao,Mario Geiger,Allan dos Santos Costa,Danny Reidenbach,Karsten Kreis,Tomas Geffner,Franco Pellegrini,Guoqing Zhou,Emine Kucukbenli*

Main category: cs.LG

TL;DR: The paper introduces SO(3)-Averaged Flow for faster training and reflow/distillation for efficient inference in 3D molecular conformer generation, achieving state-of-the-art quality.


<details>
  <summary>Details</summary>
Motivation: Fast and accurate molecular conformer generation is needed for computational chemistry and drug discovery, but current methods are resource-intensive.

Method: Proposes SO(3)-Averaged Flow for training acceleration and reflow/distillation for fast inference in flow-based models.

Result: Achieves state-of-the-art conformer generation quality with faster convergence and efficient few-step or one-step generation.

Conclusion: The techniques enable highly efficient molecular conformer generation with flow-based models.

Abstract: Fast and accurate generation of molecular conformers is desired for
downstream computational chemistry and drug discovery tasks. Currently,
training and sampling state-of-the-art diffusion or flow-based models for
conformer generation require significant computational resources. In this work,
we build upon flow-matching and propose two mechanisms for accelerating
training and inference of generative models for 3D molecular conformer
generation. For fast training, we introduce the SO(3)-Averaged Flow training
objective, which leads to faster convergence to better generation quality
compared to conditional optimal transport flow or Kabsch-aligned flow. We
demonstrate that models trained using SO(3)-Averaged Flow can reach
state-of-the-art conformer generation quality. For fast inference, we show that
the reflow and distillation methods of flow-based models enable few-steps or
even one-step molecular conformer generation with high quality. The training
techniques proposed in this work show a path towards highly efficient molecular
conformer generation with flow-based models.

</details>


### [139] [Leveraging Distribution Matching to Make Approximate Machine Unlearning Faster](https://arxiv.org/abs/2507.09786)
*Junaid Iqbal Khan*

Main category: cs.LG

TL;DR: The paper introduces two methods, Blend and A-AMU, to accelerate approximate machine unlearning (AMU) by reducing retained dataset size and optimizing loss functions, significantly cutting runtime while maintaining model utility and privacy.


<details>
  <summary>Details</summary>
Motivation: Current AMU methods are computationally expensive due to retained dataset processing and slow convergence. The goal is to improve efficiency without sacrificing performance.

Method: 1. Blend: A dataset condensation technique merging similar images to reduce retained set size. 2. A-AMU: A loss-centric method combining steepened loss for faster forgetting and a novel regularizer matching loss distributions.

Result: The dual approach drastically reduces unlearning latency in single and multi-round scenarios while preserving model utility and privacy.

Conclusion: The proposed methods are the first to systematically address AMU efficiency by combining dataset condensation and accelerated loss optimization.

Abstract: Approximate machine unlearning (AMU) enables models to `forget' specific
training data through specialized fine-tuning on a retained dataset subset.
However, processing this retained subset still dominates computational runtime,
while reductions of epochs also remain a challenge. We propose two
complementary methods to accelerate classification-oriented AMU. First,
\textbf{Blend}, a novel distribution-matching dataset condensation (DC), merges
visually similar images with shared blend-weights to significantly reduce the
retained set size. It operates with minimal pre-processing overhead and is
orders of magnitude faster than state-of-the-art DC methods. Second, our
loss-centric method, \textbf{Accelerated-AMU (A-AMU)}, augments the unlearning
objective to quicken convergence. A-AMU achieves this by combining a steepened
primary loss to expedite forgetting with a novel, differentiable regularizer
that matches the loss distributions of forgotten and in-distribution unseen
data. Our extensive experiments demonstrate that this dual approach of data and
loss-centric optimization dramatically reduces end-to-end unlearning latency
across both single and multi-round scenarios, all while preserving model
utility and privacy. To our knowledge, this is the first work to systematically
tackle unlearning efficiency by jointly designing a specialized dataset
condensation technique with a dedicated accelerated loss function. Code is
available at https://github.com/algebraicdianuj/DC_Unlearning.

</details>


### [140] [A Scalable and Efficient Signal Integration System for Job Matching](https://arxiv.org/abs/2507.09797)
*Ping Liu,Rajat Arora,Xiao Shi,Benjamin Le,Qianqi Shen,Jianqiang Shen,Chengming Jiang,Nikita Zhiltsov,Priya Bannur,Yidan Zhu,Liming Dong,Haichao Wei,Qi Guo,Luke Simon,Liangjie Hong,Wenjing Zhang*

Main category: cs.LG

TL;DR: LinkedIn's STAR system combines LLMs and GNNs to tackle job recommendation challenges like cold-start and biases, offering scalable solutions for industrial applications.


<details>
  <summary>Details</summary>
Motivation: Address cold-start, filter bubbles, and biases in job matching on LinkedIn.

Method: Integrate LLMs (for text understanding) and GNNs (for relationship modeling) with adaptive sampling and version management.

Result: A scalable, high-performing recommendation system (STAR) with robust embeddings for industrial use.

Conclusion: STAR provides a practical, end-to-end solution for deploying embeddings in large-scale recommender systems.

Abstract: LinkedIn, one of the world's largest platforms for professional networking
and job seeking, encounters various modeling challenges in building
recommendation systems for its job matching product, including cold-start,
filter bubbles, and biases affecting candidate-job matching. To address these,
we developed the STAR (Signal Integration for Talent And Recruiters) system,
leveraging the combined strengths of Large Language Models (LLMs) and Graph
Neural Networks (GNNs). LLMs excel at understanding textual data, such as
member profiles and job postings, while GNNs capture intricate relationships
and mitigate cold-start issues through network effects. STAR integrates diverse
signals by uniting LLM and GNN capabilities with industrial-scale paradigms
including adaptive sampling and version management. It provides an end-to-end
solution for developing and deploying embeddings in large-scale recommender
systems. Our key contributions include a robust methodology for building
embeddings in industrial applications, a scalable GNN-LLM integration for
high-performing recommendations, and practical insights for real-world model
deployment.

</details>


### [141] [Federated Learning with Graph-Based Aggregation for Traffic Forecasting](https://arxiv.org/abs/2507.09805)
*Audri Banik,Glaucio Haroldo Silva de Carvalho,Renata Dividino*

Main category: cs.LG

TL;DR: A lightweight graph-aware FL method for traffic prediction blends FedAvg with graph learning to capture spatial relationships efficiently.


<details>
  <summary>Details</summary>
Motivation: Standard FL methods like FedAvg ignore spatial dependencies in traffic prediction, while graph-based FL introduces high computational costs.

Method: Proposes a lightweight graph-aware FL approach using neighborhood aggregation to weight client models based on graph connectivity.

Result: Achieves competitive performance on METR-LA and PEMS-BAY datasets compared to baselines and graph-based FL techniques.

Conclusion: The method effectively balances performance and computational efficiency in traffic prediction tasks.

Abstract: In traffic prediction, the goal is to estimate traffic speed or flow in
specific regions or road segments using historical data collected by devices
deployed in each area. Each region or road segment can be viewed as an
individual client that measures local traffic flow, making Federated Learning
(FL) a suitable approach for collaboratively training models without sharing
raw data. In centralized FL, a central server collects and aggregates model
updates from multiple clients to build a shared model while preserving each
client's data privacy. Standard FL methods, such as Federated Averaging
(FedAvg), assume that clients are independent, which can limit performance in
traffic prediction tasks where spatial relationships between clients are
important. Federated Graph Learning methods can capture these dependencies
during server-side aggregation, but they often introduce significant
computational overhead. In this paper, we propose a lightweight graph-aware FL
approach that blends the simplicity of FedAvg with key ideas from graph
learning. Rather than training full models, our method applies basic
neighbourhood aggregation principles to guide parameter updates, weighting
client models based on graph connectivity. This approach captures spatial
relationships effectively while remaining computationally efficient. We
evaluate our method on two benchmark traffic datasets, METR-LA and PEMS-BAY,
and show that it achieves competitive performance compared to standard
baselines and recent graph-based federated learning techniques.

</details>


### [142] [Compressed Computation: Dense Circuits in a Toy Model of the Universal-AND Problem](https://arxiv.org/abs/2507.09816)
*Adam Newgas*

Main category: cs.LG

TL;DR: Neural networks can learn compressed computation for the Universal-AND problem, finding a dense, scalable solution more efficient than theoretical constructions.


<details>
  <summary>Details</summary>
Motivation: To explore whether neural networks can practically learn compressed computation circuits, unlike theoretical proposals.

Method: Investigate a toy model for the Universal-AND problem with restricted hidden dimensions to pressure efficient circuit learning.

Result: The model finds a dense, scalable solution, robust to parameter changes, and more efficient than theoretical constructions at low sparsity.

Conclusion: The findings highlight practical circuit formation in networks and the flexibility of superposition, aiding broader understanding of network circuitry and interpretability.

Abstract: Neural networks are capable of superposition -- representing more features
than there are dimensions. Recent work considers the analogous concept for
computation instead of storage, proposing theoretical constructions. But there
has been little investigation into whether these circuits can be learned in
practice. In this work, we investigate a toy model for the Universal-AND
problem which computes the AND of all $m\choose 2$ pairs of $m$ sparse inputs.
The hidden dimension that determines the number of non-linear activations is
restricted to pressure the model to find a compute-efficient circuit, called
compressed computation. We find that the training process finds a simple
solution that does not correspond to theoretical constructions. It is fully
dense -- every neuron contributes to every output. The solution circuit
naturally scales with dimension, trading off error rates for neuron efficiency.
It is similarly robust to changes in sparsity and other key parameters, and
extends naturally to other boolean operations and boolean circuits. We explain
the found solution in detail and compute why it is more efficient than the
theoretical constructions at low sparsity. Our findings shed light on the types
of circuits that models like to form and the flexibility of the superposition
representation. This contributes to a broader understanding of network
circuitry and interpretability.

</details>


### [143] [Bridging Neural Networks and Dynamic Time Warping for Adaptive Time Series Classification](https://arxiv.org/abs/2507.09826)
*Jintao Qu,Zichong Wang,Chenhao Wu,Wenbin Zhang*

Main category: cs.LG

TL;DR: A trainable model combining DTW's interpretability with neural network adaptability, effective in both low- and rich-resource settings.


<details>
  <summary>Details</summary>
Motivation: Address limitations of neural networks (data dependency, lack of interpretability) and DTW (non-trainability, inefficiency with large data).

Method: Propose a dynamic length-shortening algorithm to transform time series into prototypes, reformulate DTW into a recurrent neural network.

Result: Outperforms previous methods in low-resource settings and remains competitive in rich-resource scenarios.

Conclusion: The model successfully bridges the gap between interpretability and trainability, adapting to varying data availability.

Abstract: Neural networks have achieved remarkable success in time series
classification, but their reliance on large amounts of labeled data for
training limits their applicability in cold-start scenarios. Moreover, they
lack interpretability, reducing transparency in decision-making. In contrast,
dynamic time warping (DTW) combined with a nearest neighbor classifier is
widely used for its effectiveness in limited-data settings and its inherent
interpretability. However, as a non-parametric method, it is not trainable and
cannot leverage large amounts of labeled data, making it less effective than
neural networks in rich-resource scenarios. In this work, we aim to develop a
versatile model that adapts to cold-start conditions and becomes trainable with
labeled data, while maintaining interpretability. We propose a dynamic
length-shortening algorithm that transforms time series into prototypes while
preserving key structural patterns, thereby enabling the reformulation of the
DTW recurrence relation into an equivalent recurrent neural network. Based on
this, we construct a trainable model that mimics DTW's alignment behavior. As a
neural network, it becomes trainable when sufficient labeled data is available,
while still retaining DTW's inherent interpretability. We apply the model to
several benchmark time series classification tasks and observe that it
significantly outperforms previous approaches in low-resource settings and
remains competitive in rich-resource settings.

</details>


### [144] [Generative Cognitive Diagnosis](https://arxiv.org/abs/2507.09831)
*Jiatong Li,Qi Liu,Mengxiao Zhu*

Main category: cs.LG

TL;DR: The paper introduces a generative paradigm for cognitive diagnosis, enabling faster and more reliable inference of cognitive states without retraining, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional cognitive diagnosis models are limited by expensive retraining for new learners and unreliable outputs. The study aims to address these scalability and reliability issues.

Method: Proposes a generative diagnosis paradigm with two models: Generative Item Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model (G-NCDM), incorporating identifiability and monotonicity conditions.

Result: Achieves significant performance improvements, including a 100x speedup for diagnosing new learners, and demonstrates effectiveness on real-world datasets.

Conclusion: The generative framework enhances cognitive diagnosis, offering new applications in AI for education and model evaluation.

Abstract: Cognitive diagnosis (CD) models latent cognitive states of human learners by
analyzing their response patterns on diagnostic tests, serving as a crucial
machine learning technique for educational assessment and evaluation.
Traditional cognitive diagnosis models typically follow a transductive
prediction paradigm that optimizes parameters to fit response scores and
extract learner abilities. These approaches face significant limitations as
they cannot perform instant diagnosis for new learners without computationally
expensive retraining and produce diagnostic outputs with limited reliability.
In this study, we introduces a novel generative diagnosis paradigm that
fundamentally shifts CD from predictive to generative modeling, enabling
inductive inference of cognitive states without parameter re-optimization. We
propose two simple yet effective instantiations of this paradigm: Generative
Item Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model
(G-NCDM), which achieve excellent performance improvements over traditional
methods. The generative approach disentangles cognitive state inference from
response prediction through a well-designed generation process that
incorporates identifiability and monotonicity conditions. Extensive experiments
on real-world datasets demonstrate the effectiveness of our methodology in
addressing scalability and reliability challenges, especially $\times 100$
speedup for the diagnosis of new learners. Our framework opens new avenues for
cognitive diagnosis applications in artificial intelligence, particularly for
intelligent model evaluation and intelligent education systems. The code is
available at https://github.com/CSLiJT/Generative-CD.git.

</details>


### [145] [A Pre-training Framework for Relational Data with Information-theoretic Principles](https://arxiv.org/abs/2507.09837)
*Quang Truong,Zhikai Chen,Mingxuan Ju,Tong Zhao,Neil Shah,Jiliang Tang*

Main category: cs.LG

TL;DR: TVE is a pre-training framework for relational databases that models task heterogeneity and temporal dynamics, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generalizable pre-training for relational databases due to task heterogeneity and the need for task-aware representations.

Method: Introduces Task Vector Estimation (TVE), which uses set-based aggregation over schema traversal graphs and models next-window relational dynamics.

Result: TVE outperforms traditional pre-training baselines on the RelBench benchmark, showing better task-informed representations.

Conclusion: Pre-training objectives should encode task heterogeneity and temporal structure for effective predictive modeling on relational databases.

Abstract: Relational databases underpin critical infrastructure across a wide range of
domains, yet the design of generalizable pre-training strategies for learning
from relational databases remains an open challenge due to task heterogeneity.
Specifically, there exist infinitely many possible downstream tasks, as tasks
are defined based on relational schema graphs, temporal dependencies, and
SQL-defined label logics. An effective pre-training framework is desired to
take these factors into account in order to obtain task-aware representations.
By incorporating knowledge of the underlying distribution that drives label
generation, downstream tasks can benefit from relevant side-channel
information. To bridge this gap, we introduce Task Vector Estimation (TVE), a
novel pre-training framework that constructs predictive supervisory signals via
set-based aggregation over schema traversal graphs, explicitly modeling
next-window relational dynamics. We formalize our approach through an
information-theoretic lens, demonstrating that task-informed representations
retain more relevant signals than those obtained without task priors. Extensive
experiments on the RelBench benchmark show that TVE consistently outperforms
traditional pre-training baselines. Our findings advocate for pre-training
objectives that encode task heterogeneity and temporal structure as design
principles for predictive modeling on relational databases.

</details>


### [146] [Rethinking Prompt Optimization: Reinforcement, Diversification, and Migration in Blackbox LLMs](https://arxiv.org/abs/2507.09839)
*MohammadReza Davari,Utkarsh Garg,Weixin Cai,Eugene Belilovsky*

Main category: cs.LG

TL;DR: The paper introduces an improved Automatic Prompt Optimization (APO) framework for black-box LLM APIs, incorporating positive reinforcement and feedback diversification to enhance efficiency and effectiveness. It also formalizes Continual Prompt Optimization (CPO) for seamless prompt migration across models.


<details>
  <summary>Details</summary>
Motivation: Current APO methods focus on error correction and overlook insights from correct predictions, limiting their potential. The rapid evolution of LLMs also necessitates efficient prompt migration.

Method: The proposed framework uses positive reinforcement to preserve beneficial prompt components and feedback diversification to mitigate noise in LLM-generated feedback. CPO addresses prompt migration challenges.

Result: The approach outperforms baselines, achieving higher accuracy, faster convergence, and lower computational costs in standard and migration scenarios.

Conclusion: The enhanced APO framework and CPO provide a robust solution for optimizing and migrating prompts in evolving LLM environments.

Abstract: An increasing number of NLP applications interact with large language models
(LLMs) through black-box APIs, making prompt engineering critical for
controlling model outputs. While recent Automatic Prompt Optimization (APO)
methods iteratively refine prompts using model-generated feedback, textual
gradients, they primarily focus on error correction and neglect valuable
insights from correct predictions. This limits both their effectiveness and
efficiency. In this paper, we propose a novel APO framework centered on
enhancing the feedback mechanism. We reinterpret the textual gradient as a form
of negative reinforcement and introduce the complementary positive
reinforcement to explicitly preserve beneficial prompt components identified
through successful predictions. To mitigate the noise inherent in LLM-generated
feedback, we introduce a technique called feedback diversification, which
aggregates multiple feedback signals, emphasizing consistent, actionable advice
while filtering out outliers. Motivated by the rapid evolution and diversity of
available LLMs, we also formalize Continual Prompt Optimization (CPO),
addressing the practical challenge of efficiently migrating optimized prompts
between different model versions or API providers. Our experiments reveal that
naive prompt migration often degrades performance due to loss of critical
instructions. In contrast, our approach consistently outperforms strong
baselines, achieving significant accuracy improvements, faster convergence, and
lower computational costs in both standard and migration scenarios.

</details>


### [147] [Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training](https://arxiv.org/abs/2507.09846)
*Minhak Song,Beomhan Baek,Kwangjun Ahn,Chulhee Yun*

Main category: cs.LG

TL;DR: The paper revisits the Schedule-Free (SF) method for large-scale training, showing its effectiveness without decay phases or memory overhead, and proposes a refined variant for improved robustness.


<details>
  <summary>Details</summary>
Motivation: Conventional pretraining strategies are inadequate for large-scale training, and alternatives like WSD and weight averaging have limitations.

Method: Revisits the Schedule-Free (SF) method, analyzes its dynamics, and proposes a refined variant for better performance under large batch sizes.

Result: SF-AdamW effectively navigates the loss landscape without decay phases or memory overhead, and the refined variant improves robustness.

Conclusion: SF is a practical, scalable, and theoretically grounded approach for language model training.

Abstract: As both model and dataset sizes continue to scale rapidly, conventional
pretraining strategies with fixed compute budgets-such as cosine learning rate
schedules-are increasingly inadequate for large-scale training. Recent
alternatives, including warmup-stable-decay (WSD) schedules and weight
averaging, offer greater flexibility. However, WSD relies on explicit decay
phases to track progress, while weight averaging addresses this limitation at
the cost of additional memory. In search of a more principled and scalable
alternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024],
which has shown strong empirical performance across diverse settings. We show
that SF-AdamW effectively navigates the "river" structure of the loss landscape
without decay phases or auxiliary averaging, making it particularly suitable
for continuously scaling training workloads. To understand this behavior, we
conduct a theoretical and empirical analysis of SF dynamics, revealing that it
implicitly performs weight averaging without memory overhead. Guided by this
analysis, we propose a refined variant of SF that improves robustness to
momentum and performs better under large batch sizes, addressing key
limitations of the original method. Together, these results establish SF as a
practical, scalable, and theoretically grounded approach for language model
training.

</details>


### [148] [Task Priors: Enhancing Model Evaluation by Considering the Entire Space of Downstream Tasks](https://arxiv.org/abs/2507.09871)
*Niket Patel,Randall Balestriero*

Main category: cs.LG

TL;DR: The paper proposes a probabilistic framework for evaluating AI models over all possible downstream tasks, addressing limitations of fixed benchmark evaluations.


<details>
  <summary>Details</summary>
Motivation: Current AI evaluation relies on rigid, hand-picked benchmarks, creating a bottleneck. The goal is to assess models comprehensively.

Method: Introduces Task Priors, defining a probabilistic space of tasks to evaluate average performance and variance across all possible tasks.

Result: Provides a framework to answer key questions about model performance and variability under Task Priors.

Conclusion: The approach aims to set a new evaluation standard and accelerate SSL research by offering a more flexible and comprehensive assessment method.

Abstract: The grand goal of AI research, and particularly Self Supervised Learning
(SSL), is to produce systems that can successfully solve any possible task. In
contrast, current evaluation methods available to AI researchers typically rely
on a fixed collection of hand-picked downstream benchmarks. Hence, a large
amount of effort is put into designing and searching for large collection of
evaluation tasks that can serve as a proxy of our grand goal. We argue that
such a rigid evaluation protocol creates a silent bottleneck in AI research. To
remedy that, we define a probabilistic space of downstream tasks obtained by
adopting a distribution of tasks and by defining Task Priors. Under this view,
one can evaluate a model's performance over the set of all possible downstream
tasks. Our framework is the first to provide answers to key questions such as
(i) what is the average performance of my model over all possible downstream
tasks weighted by the probability to encounter each task? or (ii) what is the
variance of my model's performance across all downstream tasks under the
defined Task Priors? Beyond establishing a new standard for evaluation, we
believe that Task Priors will accelerate the pace of research in SSL - where
downstream task evaluation is the sole qualitative signal that researchers have
access to.

</details>


### [149] [AdaBrain-Bench: Benchmarking Brain Foundation Models for Brain-Computer Interface Applications](https://arxiv.org/abs/2507.09882)
*Jiamin Wu,Zichen Ren,Junyu Wang,Pengyu Zhu,Yonghao Song,Mianxin Liu,Qihao Zheng,Lei Bai,Wanli Ouyang,Chunfeng Song*

Main category: cs.LG

TL;DR: AdaBrain-Bench is a standardized benchmark for evaluating brain foundation models in non-invasive BCI tasks, addressing the lack of practical benchmarks in the field.


<details>
  <summary>Details</summary>
Motivation: The high noise and limited task-specific data in non-invasive BCI signals hinder decoding capabilities, and current brain foundation models lack comprehensive benchmarks for widespread adoption.

Method: AdaBrain-Bench includes diverse BCI datasets, a task adaptation pipeline, multi-dimensional metrics, and adaptation tools to evaluate models across transfer settings like cross-subject and few-shot scenarios.

Result: The benchmark evaluates public brain foundation models, providing insights for model selection in various scenarios and fostering reproducible research.

Conclusion: AdaBrain-Bench offers a scalable framework to advance robust and generalized neural decoding solutions in non-invasive BCI research.

Abstract: Non-invasive Brain-Computer Interfaces (BCI) offer a safe and accessible
means of connecting the human brain to external devices, with broad
applications in home and clinical settings to enhance human capabilities.
However, the high noise level and limited task-specific data in non-invasive
signals constrain decoding capabilities. Recently, the adoption of
self-supervised pre-training is transforming the landscape of non-invasive BCI
research, enabling the development of brain foundation models to capture
generic neural representations from large-scale unlabeled
electroencephalography (EEG) signals with substantial noises. However, despite
these advances, the field currently lacks comprehensive, practical and
extensible benchmarks to assess the utility of the public foundation models
across diverse BCI tasks, hindering their widespread adoption. To address this
challenge, we present AdaBrain-Bench, a large-scale standardized benchmark to
systematically evaluate brain foundation models in widespread non-invasive BCI
tasks. AdaBrain-Bench encompasses a diverse collection of representative BCI
decoding datasets spanning 7 key applications. It introduces a streamlined task
adaptation pipeline integrated with multi-dimensional evaluation metrics and a
set of adaptation tools. The benchmark delivers an inclusive framework for
assessing generalizability of brain foundation models across key transfer
settings, including cross-subject, multi-subject, and few-shot scenarios. We
leverage AdaBrain-Bench to evaluate a suite of publicly available brain
foundation models and offer insights into practices for selecting appropriate
models in various scenarios. We make our benchmark pipeline available to enable
reproducible research and external use, offering a continuously evolving
platform to foster progress toward robust and generalized neural decoding
solutions.

</details>


### [150] [TolerantECG: A Foundation Model for Imperfect Electrocardiogram](https://arxiv.org/abs/2507.09887)
*Huynh Nguyen Dang,Thang Pham,Ngan Le,Van Nguyen*

Main category: cs.LG

TL;DR: TolerantECG is a robust foundation model for ECG signals, handling noise and missing leads, outperforming others on PTB-XL and MIT-BIH datasets.


<details>
  <summary>Details</summary>
Motivation: ECG effectiveness is limited by noise and missing leads, leading to diagnostic errors.

Method: Combines contrastive and self-supervised learning to train on noisy, incomplete ECG signals and text reports.

Result: Top performance on PTB-XL and MIT-BIH datasets under various conditions.

Conclusion: TolerantECG is effective for noisy or incomplete ECG data, improving diagnostic reliability.

Abstract: The electrocardiogram (ECG) is an essential and effective tool for diagnosing
heart diseases. However, its effectiveness can be compromised by noise or
unavailability of one or more leads of the standard 12-lead recordings,
resulting in diagnostic errors or uncertainty. To address these challenges, we
propose TolerantECG, a foundation model for ECG signals that is robust to noise
and capable of functioning with arbitrary subsets of the standard 12-lead ECG.
TolerantECG training combines contrastive and self-supervised learning
frameworks to jointly learn ECG signal representations alongside their
corresponding knowledge-retrieval-based text report descriptions and corrupted
or lead-missing signals. Comprehensive benchmarking results demonstrate that
TolerantECG consistently ranks as the best or second-best performer across
various ECG signal conditions and class levels in the PTB-XL dataset, and
achieves the highest performance on the MIT-BIH Arrhythmia Database.

</details>


### [151] [NeuTSFlow: Modeling Continuous Functions Behind Time Series Forecasting](https://arxiv.org/abs/2507.09888)
*Huibo Xu,Likang Wu,Xianquan Wang,Haoning Dang,Chun-Wun Cheng,Angelica I Aviles-Rivero,Qi Liu*

Main category: cs.LG

TL;DR: NeuTSFlow introduces a novel framework for time series forecasting by modeling continuous function families instead of discrete sequences, using Neural Operators for flow matching.


<details>
  <summary>Details</summary>
Motivation: Conventional methods treat time series as discrete sequences, ignoring their continuous nature and the ambiguity in determining underlying functions from noisy samples.

Method: Proposes NeuTSFlow, leveraging Neural Operators to learn the transition between historical and future function families through flow matching in infinite-dimensional spaces.

Result: NeuTSFlow outperforms traditional methods in accuracy and robustness across diverse forecasting tasks.

Conclusion: The function-family perspective and NeuTSFlow's approach effectively address the challenges of modeling continuous processes in time series forecasting.

Abstract: Time series forecasting is a fundamental task with broad applications, yet
conventional methods often treat data as discrete sequences, overlooking their
origin as noisy samples of continuous processes. Crucially, discrete noisy
observations cannot uniquely determine a continuous function; instead, they
correspond to a family of plausible functions. Mathematically, time series can
be viewed as noisy observations of a continuous function family governed by a
shared probability measure. Thus, the forecasting task can be framed as
learning the transition from the historical function family to the future
function family. This reframing introduces two key challenges: (1) How can we
leverage discrete historical and future observations to learn the relationships
between their underlying continuous functions? (2) How can we model the
transition path in function space from the historical function family to the
future function family? To address these challenges, we propose NeuTSFlow, a
novel framework that leverages Neural Operators to facilitate flow matching for
learning path of measure between historical and future function families. By
parameterizing the velocity field of the flow in infinite-dimensional function
spaces, NeuTSFlow moves beyond traditional methods that focus on dependencies
at discrete points, directly modeling function-level features instead.
Experiments on diverse forecasting tasks demonstrate NeuTSFlow's superior
accuracy and robustness, validating the effectiveness of the function-family
perspective.

</details>


### [152] [Soft Graph Clustering for single-cell RNA Sequencing Data](https://arxiv.org/abs/2507.09890)
*Ping Xu,Pengfei Wang,Zhiyuan Ning,Meng Xiao,Min Wu,Yuanchun Zhou*

Main category: cs.LG

TL;DR: The paper introduces scSGC, a soft graph clustering method for scRNA-seq data, addressing limitations of hard graph constructions by using non-binary edge weights to capture continuous similarities among cells.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based scRNA-seq clustering methods, especially GNNs, suffer from information loss due to binary edge thresholds and inter-cluster connections in hard graphs, leading to biased clustering outcomes.

Method: scSGC includes a ZINB-based feature autoencoder, a dual-channel cut-informed soft graph embedding module, and an optimal transport-based clustering optimization module.

Result: scSGC outperforms 13 state-of-the-art models in clustering accuracy, cell type annotation, and computational efficiency across ten datasets.

Conclusion: scSGC advances scRNA-seq data analysis by improving clustering accuracy and understanding of cellular heterogeneity.

Abstract: Clustering analysis is fundamental in single-cell RNA sequencing (scRNA-seq)
data analysis for elucidating cellular heterogeneity and diversity. Recent
graph-based scRNA-seq clustering methods, particularly graph neural networks
(GNNs), have significantly improved in tackling the challenges of
high-dimension, high-sparsity, and frequent dropout events that lead to
ambiguous cell population boundaries. However, their reliance on hard graph
constructions derived from thresholded similarity matrices presents
challenges:(i) The simplification of intercellular relationships into binary
edges (0 or 1) by applying thresholds, which restricts the capture of
continuous similarity features among cells and leads to significant information
loss.(ii) The presence of significant inter-cluster connections within hard
graphs, which can confuse GNN methods that rely heavily on graph structures,
potentially causing erroneous message propagation and biased clustering
outcomes. To tackle these challenges, we introduce scSGC, a Soft Graph
Clustering for single-cell RNA sequencing data, which aims to more accurately
characterize continuous similarities among cells through non-binary edge
weights, thereby mitigating the limitations of rigid data structures. The scSGC
framework comprises three core components: (i) a zero-inflated negative
binomial (ZINB)-based feature autoencoder; (ii) a dual-channel cut-informed
soft graph embedding module; and (iii) an optimal transport-based clustering
optimization module. Extensive experiments across ten datasets demonstrate that
scSGC outperforms 13 state-of-the-art clustering models in clustering accuracy,
cell type annotation, and computational efficiency. These results highlight its
substantial potential to advance scRNA-seq data analysis and deepen our
understanding of cellular heterogeneity.

</details>


### [153] [Algorithm Development in Neural Networks: Insights from the Streaming Parity Task](https://arxiv.org/abs/2507.09897)
*Loek van Rossem,Andrew M. Saxe*

Main category: cs.LG

TL;DR: RNNs trained on the streaming parity task show a phase transition to perfect infinite generalization, revealing a mechanism for infinite generalization from finite training.


<details>
  <summary>Details</summary>
Motivation: To understand how neural networks generalize beyond training data, especially in tasks like streaming parity, and uncover the mechanisms behind infinite generalization.

Method: Case study of RNNs trained on the streaming parity task, analyzing learning dynamics and developing an effective theory of algorithm development.

Result: RNNs exhibit a phase transition to perfect infinite generalization with sufficient training, involving an implicit representational merger effect akin to finite automaton construction.

Conclusion: The study reveals a mechanism for infinite generalization in neural networks, highlighting the role of representational dynamics in learning algorithms.

Abstract: Even when massively overparameterized, deep neural networks show a remarkable
ability to generalize. Research on this phenomenon has focused on
generalization within distribution, via smooth interpolation. Yet in some
settings neural networks also learn to extrapolate to data far beyond the
bounds of the original training set, sometimes even allowing for infinite
generalization, implying that an algorithm capable of solving the task has been
learned. Here we undertake a case study of the learning dynamics of recurrent
neural networks (RNNs) trained on the streaming parity task in order to develop
an effective theory of algorithm development. The streaming parity task is a
simple but nonlinear task defined on sequences up to arbitrary length. We show
that, with sufficient finite training experience, RNNs exhibit a phase
transition to perfect infinite generalization. Using an effective theory for
the representational dynamics, we find an implicit representational merger
effect which can be interpreted as the construction of a finite automaton that
reproduces the task. Overall, our results disclose one mechanism by which
neural networks can generalize infinitely from finite training experience.

</details>


### [154] [Extracting Cause-Effect Pairs from a Sentence with a Dependency-Aware Transformer Model](https://arxiv.org/abs/2507.09925)
*Md Ahsanul Kabir,Abrar Jahin,Mohammad Al Hasan*

Main category: cs.LG

TL;DR: DepBERT, a transformer-based model incorporating dependency trees, outperforms state-of-the-art methods for extracting cause-effect phrases.


<details>
  <summary>Details</summary>
Motivation: Existing supervised methods lack integration of linguistic tools like dependency trees, which are effective for semantic extraction.

Method: DepBERT extends a transformer model by embedding dependency tree information into its framework.

Result: DepBERT surpasses current supervised methods in causality extraction across three datasets.

Conclusion: Incorporating dependency trees into transformer models enhances cause-effect phrase extraction.

Abstract: Extracting cause and effect phrases from a sentence is an important NLP task,
with numerous applications in various domains, including legal, medical,
education, and scientific research. There are many unsupervised and supervised
methods proposed for solving this task. Among these, unsupervised methods
utilize various linguistic tools, including syntactic patterns, dependency
tree, dependency relations, etc. among different sentential units for
extracting the cause and effect phrases. On the other hand, the contemporary
supervised methods use various deep learning based mask language models
equipped with a token classification layer for extracting cause and effect
phrases. Linguistic tools, specifically, dependency tree, which organizes a
sentence into different semantic units have been shown to be very effective for
extracting semantic pairs from a sentence, but existing supervised methods do
not have any provision for utilizing such tools within their model framework.
In this work, we propose DepBERT, which extends a transformer-based model by
incorporating dependency tree of a sentence within the model framework.
Extensive experiments over three datasets show that DepBERT is better than
various state-of-the art supervised causality extraction methods.

</details>


### [155] [Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications](https://arxiv.org/abs/2507.09931)
*Yoon Pyo Lee*

Main category: cs.LG

TL;DR: The paper introduces a method to interpret LLMs' reasoning in safety-critical domains like nuclear engineering, using a Boiling Water Reactor case study. It identifies specialized neurons and their role in task performance.


<details>
  <summary>Details</summary>
Motivation: Understanding LLMs' internal processes is crucial for their safe deployment in nuclear engineering, addressing regulatory challenges.

Method: Parameter-efficient fine-tuning (Low-Rank Adaptation) and neuron silencing to analyze neuron activation patterns.

Result: Silencing specialized neurons collectively degrades performance, impairing technical accuracy.

Conclusion: The methodology enhances LLM transparency, aiding nuclear-grade AI assurance and regulatory compliance.

Abstract: The integration of Large Language Models (LLMs) into safety-critical domains,
such as nuclear engineering, necessitates a deep understanding of their
internal reasoning processes. This paper presents a novel methodology for
interpreting how an LLM encodes and utilizes domain-specific knowledge, using a
Boiling Water Reactor system as a case study. We adapted a general-purpose LLM
(Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning
technique known as Low-Rank Adaptation. By comparing the neuron activation
patterns of the base model to those of the fine-tuned model, we identified a
sparse set of neurons whose behavior was significantly altered during the
adaptation process. To probe the causal role of these specialized neurons, we
employed a neuron silencing technique. Our results demonstrate that while
silencing most of these specialized neurons individually did not produce a
statistically significant effect, deactivating the entire group collectively
led to a statistically significant degradation in task performance. Qualitative
analysis further revealed that silencing these neurons impaired the model's
ability to generate detailed, contextually accurate technical information. This
paper provides a concrete methodology for enhancing the transparency of an
opaque black-box model, allowing domain expertise to be traced to verifiable
neural circuits. This offers a pathway towards achieving nuclear-grade
artificial intelligence (AI) assurance, addressing the verification and
validation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR
50 Appendix B), which have limited AI deployment in safety-critical nuclear
operations.

</details>


### [156] [Memorization Sinks: Isolating Memorization during LLM Training](https://arxiv.org/abs/2507.09937)
*Gaurav R. Ghosal,Pratyush Maini,Aditi Raghunathan*

Main category: cs.LG

TL;DR: MemSinks isolates memorized content in language models by design, making it easier to remove without affecting general language abilities.


<details>
  <summary>Details</summary>
Motivation: Address privacy and copyright concerns by mitigating memorization of repeated sequences in large language models.

Method: Introduces MemSinks, a paradigm using sequence identifiers to activate unique memorization neurons for each sequence, isolating memorized content.

Result: Effective isolation of memorized content and strong generalization demonstrated at billion-parameter and billion-token scales.

Conclusion: MemSinks provides a scalable solution for isolating and removing memorized content without compromising language model performance.

Abstract: Large language models are susceptible to memorizing repeated sequences,
posing privacy and copyright concerns. A popular mitigation strategy is to
remove memorized information from specific neurons post-hoc. However, such
approaches have shown limited success so far. In a controlled setting, we show
that the memorization of natural sequences (those that resemble linguistically
plausible text) become mechanistically entangled with general language
abilities, thereby becoming challenging to remove post-hoc. In this work, we
put forward a new paradigm of MemSinks that promotes isolation of memorization
by design. We leverage a sequence identifier that activates a unique set of
memorization neurons for each sequence across repetitions. By analyzing the
dynamics of learning and forgetting, we argue that MemSinks facilitates
isolation of memorized content, making it easier to remove without compromising
general language capabilities. We implement MemSinks at the billion-parameter
and billion-token scale, and observe both effective isolation and strong
generalization. To our knowledge, this is the first proof-of-concept on real
data demonstrating that simultaneous generalization and isolation is
achievable. We open-source our code at http://github.com/grghosal/MemSinks.

</details>


### [157] [Long-Tailed Data Classification by Increasing and Decreasing Neurons During Training](https://arxiv.org/abs/2507.09940)
*Taigo Sakai,Kazuhiro Hotta*

Main category: cs.LG

TL;DR: A dynamic neuron allocation method inspired by biological learning processes improves performance on class-imbalanced datasets by periodically adding and pruning neurons during training.


<details>
  <summary>Details</summary>
Motivation: Fixed neuron counts in deep learning may hinder performance on imbalanced datasets, unlike the flexible neuron generation and pruning observed in the human hippocampus.

Method: Periodically adds and removes neurons during training to enhance representation for minority classes while maintaining final network size.

Result: Outperforms fixed-size networks and works well with other imbalance-handling techniques, as shown in experiments on three datasets and five models.

Conclusion: Dynamic neuron allocation, inspired by biology, effectively improves performance on class-imbalanced data.

Abstract: In conventional deep learning, the number of neurons typically remains fixed
during training. However, insights from biology suggest that the human
hippocampus undergoes continuous neuron generation and pruning of neurons over
the course of learning, implying that a flexible allocation of capacity can
contribute to enhance performance. Real-world datasets often exhibit class
imbalance situations where certain classes have far fewer samples than others,
leading to significantly reduce recognition accuracy for minority classes when
relying on fixed size networks.To address the challenge, we propose a method
that periodically adds and removes neurons during training, thereby boosting
representational power for minority classes. By retaining critical features
learned from majority classes while selectively increasing neurons for
underrepresented classes, our approach dynamically adjusts capacity during
training. Importantly, while the number of neurons changes throughout training,
the final network size and structure remain unchanged, ensuring efficiency and
compatibility with deployment.Furthermore, by experiments on three different
datasets and five representative models, we demonstrate that the proposed
method outperforms fixed size networks and shows even greater accuracy when
combined with other imbalance-handling techniques. Our results underscore the
effectiveness of dynamic, biologically inspired network designs in improving
performance on class-imbalanced data.

</details>


### [158] [Iceberg: Enhancing HLS Modeling with Synthetic Data](https://arxiv.org/abs/2507.09948)
*Zijian Ding,Tung Nguyen,Weikai Li,Aditya Grover,Yizhou Sun,Jason Cong*

Main category: cs.LG

TL;DR: Iceberg improves HLS prediction model generalizability via synthetic data augmentation and weak label generation, boosting accuracy and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the generalizability gap in deep learning-based HLS prediction models.

Method: Uses synthetic data augmentation (Iceberg) with LLM-generated programs and weak labels, integrated with in-context model architecture for meta-learning.

Result: 86.4% improvement in geometric mean accuracy, 2.47× and 1.12× better DSE performance on test datasets.

Conclusion: Iceberg effectively enhances model generalizability and performance in real-world HLS applications.

Abstract: Deep learning-based prediction models for High-Level Synthesis (HLS) of
hardware designs often struggle to generalize. In this paper, we study how to
close the generalizability gap of these models through pretraining on synthetic
data and introduce Iceberg, a synthetic data augmentation approach that expands
both large language model (LLM)-generated programs and weak labels of unseen
design configurations. Our weak label generation method is integrated with an
in-context model architecture, enabling meta-learning from actual and proximate
labels. Iceberg improves the geometric mean modeling accuracy by $86.4\%$ when
adapt to six real-world applications with few-shot examples and achieves a
$2.47\times$ and a $1.12\times$ better offline DSE performance when adapting to
two different test datasets. Our open-sourced code is here:
\href{https://github.com/UCLA-VAST/iceberg}{https://github.com/UCLA-VAST/iceberg}

</details>


### [159] [Hierarchical Job Classification with Similarity Graph Integration](https://arxiv.org/abs/2507.09949)
*Md Ahsanul Kabir,Kareem Abdelfatah,Mohammed Korayem,Mohammad Al Hasan*

Main category: cs.LG

TL;DR: A novel model for job classification leverages hierarchical industry categories and representation learning to improve accuracy in recruitment systems.


<details>
  <summary>Details</summary>
Motivation: Traditional text classification methods fail to utilize hierarchical job data effectively, limiting accuracy in job recommendation and labor market analysis.

Method: Proposes a representation learning model integrating the SOC system and Carotene taxonomy to embed jobs and hierarchical categories into a shared latent space.

Result: Outperforms existing methods by leveraging hierarchical structures and semantic features, as shown in large-scale experiments.

Conclusion: The model enhances job classification accuracy, aiding better recruitment decisions.

Abstract: In the dynamic realm of online recruitment, accurate job classification is
paramount for optimizing job recommendation systems, search rankings, and labor
market analyses. As job markets evolve, the increasing complexity of job titles
and descriptions necessitates sophisticated models that can effectively
leverage intricate relationships within job data. Traditional text
classification methods often fall short, particularly due to their inability to
fully utilize the hierarchical nature of industry categories. To address these
limitations, we propose a novel representation learning and classification
model that embeds jobs and hierarchical industry categories into a latent
embedding space. Our model integrates the Standard Occupational Classification
(SOC) system and an in-house hierarchical taxonomy, Carotene, to capture both
graph and hierarchical relationships, thereby improving classification
accuracy. By embedding hierarchical industry categories into a shared latent
space, we tackle cold start issues and enhance the dynamic matching of
candidates to job opportunities. Extensive experimentation on a large-scale
dataset of job postings demonstrates the model's superior ability to leverage
hierarchical structures and rich semantic features, significantly outperforming
existing methods. This research provides a robust framework for improving job
classification accuracy, supporting more informed decision-making in the
recruitment industry.

</details>


### [160] [Radial Neighborhood Smoothing Recommender System](https://arxiv.org/abs/2507.09952)
*Zerui Zhang,Yumou Qiu*

Main category: cs.LG

TL;DR: The paper introduces a novel distance estimation method in latent space for recommender systems, leading to the Radial Neighborhood Estimator (RNE), which improves accuracy and addresses the cold-start problem.


<details>
  <summary>Details</summary>
Motivation: The challenge of defining meaningful distances in latent space to capture user-item relationships effectively.

Method: Proposes systematic approximation of latent distances using observed matrix distances, introduces variance correction, and develops RNE with neighborhood smoothing.

Result: RNE outperforms existing methods in evaluations on simulated and real-world datasets.

Conclusion: RNE provides a structured approach to distance estimation, enhancing recommender system performance and mitigating cold-start issues.

Abstract: Recommender systems inherently exhibit a low-rank structure in latent space.
A key challenge is to define meaningful and measurable distances in the latent
space to capture user-user, item-item, user-item relationships effectively. In
this work, we establish that distances in the latent space can be
systematically approximated using row-wise and column-wise distances in the
observed matrix, providing a novel perspective on distance estimation. To
refine the distance estimation, we introduce the correction based on empirical
variance estimator to account for noise-induced non-centrality. The novel
distance estimation enables a more structured approach to constructing
neighborhoods, leading to the Radial Neighborhood Estimator (RNE), which
constructs neighborhoods by including both overlapped and partially overlapped
user-item pairs and employs neighborhood smoothing via localized kernel
regression to improve imputation accuracy. We provide the theoretical
asymptotic analysis for the proposed estimator. We perform evaluations on both
simulated and real-world datasets, demonstrating that RNE achieves superior
performance compared to existing collaborative filtering and matrix
factorization methods. While our primary focus is on distance estimation in
latent space, we find that RNE also mitigates the ``cold-start'' problem.

</details>


### [161] [Rethinking Inductive Bias in Geographically Neural Network Weighted Regression](https://arxiv.org/abs/2507.09958)
*Zhenyuan Chen*

Main category: cs.LG

TL;DR: The paper explores inductive bias in spatial regression models, focusing on GNNWR's limitations and proposing enhancements using neural network concepts like CNNs, RNNs, and transformers.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current GNNWR approaches for modeling spatial non-stationarity, particularly fixed distance-based schemes and weak inductive bias.

Method: Generalizes GNNWR by integrating CNNs, RNNs, and transformers to introduce local receptive fields, sequential context, and self-attention. Benchmarked on synthetic datasets with varying heterogeneity, noise, and sample sizes.

Result: GNNWR outperforms classic methods in capturing complex spatial relationships, with performance varying by data characteristics (local models excel in heterogeneity/small samples; global models in large/homogeneous data).

Conclusion: Inductive bias is crucial in spatial modeling. Future work should focus on learnable weighting functions, hybrid architectures, and improved interpretability for non-stationary data.

Abstract: Inductive bias is a key factor in spatial regression models, determining how
well a model can learn from limited data and capture spatial patterns. This
work revisits the inductive biases in Geographically Neural Network Weighted
Regression (GNNWR) and identifies limitations in current approaches for
modeling spatial non-stationarity. While GNNWR extends traditional
Geographically Weighted Regression by using neural networks to learn spatial
weighting functions, existing implementations are often restricted by fixed
distance-based schemes and limited inductive bias. We propose to generalize
GNNWR by incorporating concepts from convolutional neural networks, recurrent
neural networks, and transformers, introducing local receptive fields,
sequential context, and self-attention into spatial regression. Through
extensive benchmarking on synthetic spatial datasets with varying
heterogeneity, noise, and sample sizes, we show that GNNWR outperforms classic
methods in capturing nonlinear and complex spatial relationships. Our results
also reveal that model performance depends strongly on data characteristics,
with local models excelling in highly heterogeneous or small-sample scenarios,
and global models performing better with larger, more homogeneous data. These
findings highlight the importance of inductive bias in spatial modeling and
suggest future directions, including learnable spatial weighting functions,
hybrid neural architectures, and improved interpretability for models handling
non-stationary spatial data.

</details>


### [162] [Text-Driven Causal Representation Learning for Source-Free Domain Generalization](https://arxiv.org/abs/2507.09961)
*Lihua Zhou,Mao Ye,Nianxin Li,Shuaifeng Li,Jinlin Wu,Xiatian Zhu,Lei Deng,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.LG

TL;DR: TDCRL integrates causal inference into source-free domain generalization (SFDG) to address domain-specific confounders, achieving robust generalization with text-driven methods.


<details>
  <summary>Details</summary>
Motivation: Deep learning struggles with differing train-test distributions. Traditional DG requires costly multi-domain data, while SFDG methods fail to handle domain-specific confounders effectively.

Method: TDCRL uses text prompts for visual representation simulation and trains a causal intervention network with a confounder dictionary to extract domain-invariant features.

Result: TDCRL achieves state-of-the-art performance on benchmarks like PACS, VLCS, OfficeHome, and DomainNet.

Conclusion: TDCRL effectively addresses SFDG challenges by combining causal learning with text-driven methods, ensuring robust generalization.

Abstract: Deep learning often struggles when training and test data distributions
differ. Traditional domain generalization (DG) tackles this by including data
from multiple source domains, which is impractical due to expensive data
collection and annotation. Recent vision-language models like CLIP enable
source-free domain generalization (SFDG) by using text prompts to simulate
visual representations, reducing data demands. However, existing SFDG methods
struggle with domain-specific confounders, limiting their generalization
capabilities. To address this issue, we propose TDCRL
(\textbf{T}ext-\textbf{D}riven \textbf{C}ausal \textbf{R}epresentation
\textbf{L}earning), the first method to integrate causal inference into the
SFDG setting. TDCRL operates in two steps: first, it employs data augmentation
to generate style word vectors, combining them with class information to
generate text embeddings to simulate visual representations; second, it trains
a causal intervention network with a confounder dictionary to extract
domain-invariant features. Grounded in causal learning, our approach offers a
clear and effective mechanism to achieve robust, domain-invariant features,
ensuring robust generalization. Extensive experiments on PACS, VLCS,
OfficeHome, and DomainNet show state-of-the-art performance, proving TDCRL
effectiveness in SFDG.

</details>


### [163] [Compliance Minimization via Physics-Informed Gaussian Processes](https://arxiv.org/abs/2507.09968)
*Xiangyu Sun,Amin Yousefpour,Shirin Hosseinmardi,Ramin Bostanabad*

Main category: cs.LG

TL;DR: A mesh-free, physics-informed Gaussian process framework with neural network mean functions addresses limitations of ML in compliance minimization, offering better control, efficiency, and performance.


<details>
  <summary>Details</summary>
Motivation: Current ML methods for compliance minimization suffer from poor feature boundaries, high costs, and lack of design complexity control.

Method: Proposes a framework using GP priors with shared NN mean functions (PGCANs), minimizing compliance, energy, and volume constraints without data-based residuals.

Result: Achieves super-resolution topologies, smaller compliance, less gray area, fine-scale feature control, and outperforms traditional and ML methods.

Conclusion: The framework effectively addresses ML limitations in compliance minimization, offering superior performance and control.

Abstract: Machine learning (ML) techniques have recently gained significant attention
for solving compliance minimization (CM) problems. However, these methods
typically provide poor feature boundaries, are very expensive, and lack a
systematic mechanism to control the design complexity. Herein, we address these
limitations by proposing a mesh-free and simultaneous framework based on
physics-informed Gaussian processes (GPs). In our approach, we parameterize the
design and state variables with GP priors which have independent kernels but
share a multi-output neural network (NN) as their mean function. The
architecture of this NN is based on Parametric Grid Convolutional Attention
Networks (PGCANs) which not only mitigate spectral bias issues, but also
provide an interpretable mechanism to control design complexity. We estimate
all the parameters of our GP-based representations by simultaneously minimizing
the compliance, total potential energy, and residual of volume fraction
constraint. Importantly, our loss function exclude all data-based residuals as
GPs automatically satisfy them. We also develop computational schemes based on
curriculum training and numerical integration to increase the efficiency and
robustness of our approach which is shown to (1) produce super-resolution
topologies with fast convergence, (2) achieve smaller compliance and less gray
area fraction compared to traditional numerical methods, (3) provide control
over fine-scale features, and (4) outperform competing ML-based methods.

</details>


### [164] [Effects of structural properties of neural networks on machine learning performance](https://arxiv.org/abs/2507.10005)
*Yash Arya,Sang Hoon Lee*

Main category: cs.LG

TL;DR: The paper investigates how graph structures, especially communities and degree distributions, impact neural network performance in image classification, finding that densely interconnected communities enhance learning.


<details>
  <summary>Details</summary>
Motivation: To explore the relationship between graph structures (like communities) and neural network performance, addressing gaps in prior studies limited to narrow model networks.

Method: Uses model networks (random, scale-free) and compares them with a biological neural network, analyzing structural impacts on image classification tasks.

Result: Networks with coherent, densely interconnected communities show better learning capabilities. Biological comparisons highlight real-world relevance.

Conclusion: The study advances network science and machine learning, suggesting biologically informed neural network designs.

Abstract: In recent years, graph-based machine learning techniques, such as
reinforcement learning and graph neural networks, have garnered significant
attention. While some recent studies have started to explore the relationship
between the graph structure of neural networks and their predictive
performance, they often limit themselves to a narrow range of model networks,
particularly lacking mesoscale structures such as communities. Our work
advances this area by conducting a more comprehensive investigation,
incorporating realistic network structures characterized by heterogeneous
degree distributions and community structures, which are typical
characteristics of many real networks. These community structures offer a
nuanced perspective on network architecture. Our analysis employs model
networks such as random and scale-free networks, alongside a comparison with a
biological neural network and its subsets for more detailed analysis. We
examine the impact of these structural attributes on the performance of image
classification tasks. Our findings reveal that structural properties do affect
performance to some extent. Specifically, networks featuring coherent, densely
interconnected communities demonstrate enhanced learning capabilities. The
comparison with the biological neural network emphasizes the relevance of our
findings to real-world structures, suggesting an intriguing connection worth
further exploration. This study contributes meaningfully to network science and
machine learning, providing insights that could inspire the design of more
biologically informed neural networks.

</details>


### [165] [Forecasting Coccidioidomycosis (Valley Fever) in Arizona: A Graph Neural Network Approach](https://arxiv.org/abs/2507.10014)
*Ali Sarabi,Arash Sarabi,Hao Yan,Beckett Sterner,Petar Jevtić*

Main category: cs.LG

TL;DR: A GNN model is developed to forecast Valley Fever incidence in Arizona, integrating environmental data and case surveillance, showing effectiveness in modeling trends and identifying key drivers.


<details>
  <summary>Details</summary>
Motivation: Valley Fever is a significant public health issue in endemic regions, necessitating advanced forecasting tools for early warning and resource allocation.

Method: The study uses a graph neural network (GNN) to integrate case data with environmental predictors (soil, atmospheric, agricultural, air quality) and explores correlation-based relationships with lagged effects.

Result: The GNN model effectively captures Valley Fever trends and identifies critical environmental drivers of disease incidence.

Conclusion: The findings support the use of GNNs for disease forecasting and can aid in early warning systems and prevention efforts in high-risk areas.

Abstract: Coccidioidomycosis, commonly known as Valley Fever, remains a significant
public health concern in endemic regions of the southwestern United States.
This study develops the first graph neural network (GNN) model for forecasting
Valley Fever incidence in Arizona. The model integrates surveillance case data
with environmental predictors using graph structures, including soil
conditions, atmospheric variables, agricultural indicators, and air quality
metrics. Our approach explores correlation-based relationships among variables
influencing disease transmission. The model captures critical delays in disease
progression through lagged effects, enhancing its capacity to reflect complex
temporal dependencies in disease ecology. Results demonstrate that the GNN
architecture effectively models Valley Fever trends and provides insights into
key environmental drivers of disease incidence. These findings can inform early
warning systems and guide resource allocation for disease prevention efforts in
high-risk areas.

</details>


### [166] [Towards Applying Large Language Models to Complement Single-Cell Foundation Models](https://arxiv.org/abs/2507.10039)
*Steven Palayew,Bo Wang,Gary Bader*

Main category: cs.LG

TL;DR: The paper introduces scMPT, a model combining scGPT and LLMs for single-cell analysis, outperforming individual models by leveraging biological insights from LLMs.


<details>
  <summary>Details</summary>
Motivation: To understand the factors driving LLM performance in single-cell tasks and explore their complementary use with single-cell foundation models.

Method: Develop scMPT by integrating scGPT with LLM-derived single-cell representations, and test alternate fusion methods.

Result: scMPT shows stronger, more consistent performance than scGPT or LLMs alone, with reduced performance gaps across datasets.

Conclusion: LLMs can complement single-cell foundation models, enhancing analysis through synergistic integration.

Abstract: Single-cell foundation models such as scGPT represent a significant
advancement in single-cell omics, with an ability to achieve state-of-the-art
performance on various downstream biological tasks. However, these models are
inherently limited in that a vast amount of information in biology exists as
text, which they are unable to leverage. There have therefore been several
recent works that propose the use of LLMs as an alternative to single-cell
foundation models, achieving competitive results. However, there is little
understanding of what factors drive this performance, along with a strong focus
on using LLMs as an alternative, rather than complementary approach to
single-cell foundation models. In this study, we therefore investigate what
biological insights contribute toward the performance of LLMs when applied to
single-cell data, and introduce scMPT; a model which leverages synergies
between scGPT, and single-cell representations from LLMs that capture these
insights. scMPT demonstrates stronger, more consistent performance than either
of its component models, which frequently have large performance gaps between
each other across datasets. We also experiment with alternate fusion methods,
demonstrating the potential of combining specialized reasoning models with
scGPT to improve performance. This study ultimately showcases the potential for
LLMs to complement single-cell foundation models and drive improvements in
single-cell analysis.

</details>


### [167] [On the Efficiency of Training Robust Decision Trees](https://arxiv.org/abs/2507.10048)
*Benedict Gerlach,Marie Anastacio,Holger H. Hoos*

Main category: cs.LG

TL;DR: The paper proposes an efficient pipeline for training adversarially robust decision trees, focusing on automatic perturbation size selection, training evaluation, and robustness certification.


<details>
  <summary>Details</summary>
Motivation: To address the lack of established efficiency and sustainability in robust training pipelines for machine learning models, particularly decision trees.

Method: A three-stage pipeline: (1) automatic perturbation size selection using a simple algorithm, (2) training state-of-the-art adversarial methods, and (3) certifying model robustness.

Result: Perturbation size can be estimated from smaller models, improving efficiency. Verification time is not correlated with training time.

Conclusion: The pipeline enhances efficiency in robust training, with insights into perturbation selection and verification time.

Abstract: As machine learning gets adopted into the industry quickly, trustworthiness
is increasingly in focus. Yet, efficiency and sustainability of robust training
pipelines still have to be established. In this work, we consider a simple
pipeline for training adversarially robust decision trees and investigate the
efficiency of each step. Our pipeline consists of three stages. Firstly, we
choose the perturbation size automatically for each dataset. For that, we
introduce a simple algorithm, instead of relying on intuition or prior work.
Moreover, we show that the perturbation size can be estimated from smaller
models than the one intended for full training, and thus significant gains in
efficiency can be achieved. Secondly, we train state-of-the-art adversarial
training methods and evaluate them regarding both their training time and
adversarial accuracy. Thirdly, we certify the robustness of each of the models
thus obtained and investigate the time required for this. We find that
verification time, which is critical to the efficiency of the full pipeline, is
not correlated with training time.

</details>


### [168] [Compression Method for Deep Diagonal State Space Model Based on $H^2$ Optimal Reduction](https://arxiv.org/abs/2507.10078)
*Hiroki Sakamoto,Kazuhiro Sato*

Main category: cs.LG

TL;DR: Proposes an efficient parameter reduction method for linear SSMs in deep learning, reducing parameters to 1/32 without performance loss.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of large parameter sizes in linear SSMs for deployment on resource-constrained devices.

Method: Applies $H^{2}$ model order reduction techniques from control theory to linear SSM components.

Result: Outperforms Balanced Truncation, reducing parameters to 1/32 without sacrificing performance.

Conclusion: The method effectively compresses linear SSMs, enabling efficient deployment on constrained devices.

Abstract: Deep learning models incorporating linear SSMs have gained attention for
capturing long-range dependencies in sequential data. However, their large
parameter sizes pose challenges for deployment on resource-constrained devices.
In this study, we propose an efficient parameter reduction method for these
models by applying $H^{2}$ model order reduction techniques from control theory
to their linear SSM components. In experiments, the LRA benchmark results show
that the model compression based on our proposed method outperforms an existing
method using the Balanced Truncation, while successfully reducing the number of
parameters in the SSMs to $1/32$ without sacrificing the performance of the
original models.

</details>


### [169] [Towards High Supervised Learning Utility Training Data Generation: Data Pruning and Column Reordering](https://arxiv.org/abs/2507.10088)
*Tung Sum Thomas Kwok,Zeyong Zhang,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: PRRO is a novel pipeline for tabular data synthesis that improves supervised learning (SL) utility by addressing class imbalance and data relationship issues. It enhances synthetic data quality, achieving significant performance improvements in SL tasks.


<details>
  <summary>Details</summary>
Motivation: Synthetic tabular data often underperforms in SL tasks due to class imbalance exaggeration and overlooked data relationships. PRRO aims to bridge this gap by integrating data-centric techniques.

Method: PRRO combines data pruning to focus on high signal-to-noise observations and column reordering to align generator and SL model structures.

Result: Experiments show PRRO improves predictive performance by up to 871.46% for synthetic replacement and 200.32% for synthetic append. It also enhances class distribution similarity by 43%.

Conclusion: PRRO effectively integrates data synthesis with SL, improving synthetic data quality and accessibility for analysis.

Abstract: Tabular data synthesis for supervised learning ('SL') model training is
gaining popularity in industries such as healthcare, finance, and retail.
Despite the progress made in tabular data generators, models trained with
synthetic data often underperform compared to those trained with original data.
This low SL utility of synthetic data stems from class imbalance exaggeration
and SL data relationship overlooked by tabular generator. To address these
challenges, we draw inspirations from techniques in emerging data-centric
artificial intelligence and elucidate Pruning and ReOrdering ('PRRO'), a novel
pipeline that integrates data-centric techniques into tabular data synthesis.
PRRO incorporates data pruning to guide the table generator towards
observations with high signal-to-noise ratio, ensuring that the class
distribution of synthetic data closely matches that of the original data.
Besides, PRRO employs a column reordering algorithm to align the data modeling
structure of generators with that of SL models. These two modules enable PRRO
to optimize SL utility of synthetic data. Empirical experiments on 22 public
datasets show that synthetic data generated using PRRO enhances predictive
performance compared to data generated without PRRO. Specifically, synthetic
replacement of original data yields an average improvement of 26.74% and up to
871.46% improvement using PRRO, while synthetic appendant to original data
results with PRRO-generated data results in an average improvement of 6.13% and
up to 200.32%. Furthermore, experiments on six highly imbalanced datasets show
that PRRO enables the generator to produce synthetic data with a class
distribution that resembles the original data more closely, achieving a
similarity improvement of 43%. Through PRRO, we foster a seamless integration
of data synthesis to subsequent SL prediction, promoting quality and accessible
data analysis.

</details>


### [170] [A Variance-Reduced Cubic-Regularized Newton for Policy Optimization](https://arxiv.org/abs/2507.10120)
*Cheng Sun,Zhen Zhang,Shaofu Yang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we study a second-order approach to policy optimization in
reinforcement learning. Existing second-order methods often suffer from
suboptimal sample complexity or rely on unrealistic assumptions about
importance sampling. To overcome these limitations, we propose VR-CR-PN, a
variance-reduced cubic-regularized policy Newton algorithm. To the best of our
knowledge, this is the first algorithm that integrates Hessian-aided variance
reduction with second-order policy optimization, effectively addressing the
distribution shift problem and achieving best-known sample complexity under
general nonconvex conditions but without the need for importance sampling. We
theoretically establish that VR-CR-PN achieves a sample complexity of
$\tilde{\mathcal{O}}(\epsilon^{-3})$ to reach an $\epsilon$-second-order
stationary point, significantly improving upon the previous best result of
$\tilde{\mathcal{O}}(\epsilon^{-3.5})$ under comparable assumptions. As an
additional contribution, we introduce a novel Hessian estimator for the
expected return function, which admits a uniform upper bound independent of the
horizon length $H$, allowing the algorithm to achieve horizon-independent
sample complexity.

</details>


### [171] [Wavelet-Enhanced Neural ODE and Graph Attention for Interpretable Energy Forecasting](https://arxiv.org/abs/2507.10132)
*Usman Gani Joy*

Main category: cs.LG

TL;DR: A neural framework combining Neural ODEs, graph attention, wavelet transforms, and adaptive learning improves energy demand/supply forecasting, outperforming baselines on diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Accurate energy forecasting is vital for sustainable systems but is hindered by renewable variability and dynamic consumption.

Method: Integrates Neural ODEs, graph attention, wavelet transforms, and adaptive frequency learning with a robust ODE solver (Runge-Kutta) and residual connections.

Result: Outperforms state-of-the-art baselines across seven datasets (ETTh1, ETTh2, ETTm1, ETTm2, Waste, Solar, Hydro) in forecasting metrics.

Conclusion: The model effectively captures complex temporal dependencies and enhances interpretability via SHAP analysis, making it suitable for sustainable energy applications.

Abstract: Accurate forecasting of energy demand and supply is critical for optimizing
sustainable energy systems, yet it is challenged by the variability of
renewable sources and dynamic consumption patterns. This paper introduces a
neural framework that integrates continuous-time Neural Ordinary Differential
Equations (Neural ODEs), graph attention, multi-resolution wavelet
transformations, and adaptive learning of frequencies to address the issues of
time series prediction. The model employs a robust ODE solver, using the
Runge-Kutta method, paired with graph-based attention and residual connections
to better understand both structural and temporal patterns. Through
wavelet-based feature extraction and adaptive frequency modulation, it adeptly
captures and models diverse, multi-scale temporal dynamics. When evaluated
across seven diverse datasets: ETTh1, ETTh2, ETTm1, ETTm2 (electricity
transformer temperature), and Waste, Solar, and Hydro (renewable energy), this
architecture consistently outperforms state-of-the-art baselines in various
forecasting metrics, proving its robustness in capturing complex temporal
dependencies. Furthermore, the model enhances interpretability through SHAP
analysis, making it suitable for sustainable energy applications.

</details>


### [172] [MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping](https://arxiv.org/abs/2507.10158)
*Obaidullah Zaland,Erik Elmroth,Monowar Bhuyan*

Main category: cs.LG

TL;DR: MTF-Grasp is a multi-tier FL approach for robotic grasping, addressing non-IID data challenges by leveraging top-level robots to improve model performance.


<details>
  <summary>Details</summary>
Motivation: Grasping tasks in FL lack exploration, especially with non-IID and low-quantity data, leading to performance degradation.

Method: MTF-Grasp selects top-level robots with better data distribution and quantity to train initial models, then shares them with low-level robots.

Result: Outperforms conventional FL by up to 8% on skewed grasping datasets.

Conclusion: MTF-Grasp effectively mitigates performance degradation in FL for robotic grasping tasks.

Abstract: Federated Learning (FL) is a promising machine learning paradigm that enables
participating devices to train privacy-preserved and collaborative models. FL
has proven its benefits for robotic manipulation tasks. However, grasping tasks
lack exploration in such settings where robots train a global model without
moving data and ensuring data privacy. The main challenge is that each robot
learns from data that is nonindependent and identically distributed (non-IID)
and of low quantity. This exhibits performance degradation, particularly in
robotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL
approach for robotic grasping, acknowledging the unique challenges posed by the
non-IID data distribution across robots, including quantitative skewness.
MTF-Grasp harnesses data quality and quantity across robots to select a set of
"top-level" robots with better data distribution and higher sample count. It
then utilizes top-level robots to train initial seed models and distribute them
to the remaining "low-level" robots, reducing the risk of model performance
degradation in low-level robots. Our approach outperforms the conventional FL
setup by up to 8% on the quantity-skewed Cornell and Jacquard grasping
datasets.

</details>


### [173] [Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation](https://arxiv.org/abs/2507.10160)
*Manuel Röder,Christoph Raab,Frank-Michael Schleif*

Main category: cs.LG

TL;DR: FedAcross+ is a scalable Federated Learning framework addressing challenges like data labeling, covariate shift, and resource constraints by leveraging a pre-trained model and adaptive layers for efficient client adaptation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome practical challenges in Federated Learning, such as human involvement in labeling, data discrepancies, and resource limitations in edge devices.

Method: The method involves a pre-trained model with a frozen backbone and classifier, allowing an adaptive layer to handle domain adaptation. It supports streaming data and sporadic updates.

Result: FedAcross+ achieves competitive adaptation on low-end devices, handles domain shift, and works in resource-constrained environments.

Conclusion: FedAcross+ is effective for real-world industrial settings, addressing key Federated Learning challenges while ensuring practical deployment.

Abstract: Federated Learning has emerged as a leading paradigm for decentralized,
privacy-preserving learning, particularly relevant in the era of interconnected
edge devices equipped with sensors. However, the practical implementation of
Federated Learning faces three primary challenges: the need for human
involvement in costly data labelling processes for target adaptation, covariate
shift in client device data collection due to environmental factors affecting
sensors, leading to discrepancies between source and target samples, and the
impracticality of continuous or regular model updates in resource-constrained
environments due to limited data transmission capabilities and technical
constraints on channel availability and energy efficiency. To tackle these
issues, we expand upon an efficient and scalable Federated Learning framework
tailored for real-world client adaptation in industrial settings. This
framework leverages a pre-trained source model comprising a deep backbone, an
adaptation module, and a classifier running on a powerful server. By freezing
the backbone and classifier during client adaptation on resource-constrained
devices, we allow the domain adaptive linear layer to handle target domain
adaptation, thus minimizing overall computational overhead. Furthermore, this
setup, designated as FedAcross+, is extended to encompass the processing of
streaming data, thereby rendering the solution suitable for non-stationary
environments. Extensive experimental results demonstrate the effectiveness of
FedAcross+ in achieving competitive adaptation on low-end client devices with
limited target samples, successfully addressing the challenge of domain shift.
Moreover, our framework accommodates sporadic model updates within
resource-constrained environments, ensuring practical and seamless deployment.

</details>


### [174] [Understanding the Rank of Tensor Networks via an Intuitive Example-Driven Approach](https://arxiv.org/abs/2507.10170)
*Wuyang Zhou,Giorgos Iacovides,Kriton Konstantinidis,Ilya Kisil,Danilo Mandic*

Main category: cs.LG

TL;DR: The paper demystifies Tensor Network (TN) ranks, explaining their role in TN decompositions and how domain knowledge can guide their selection, using intuitive visualizations and real-life examples.


<details>
  <summary>Details</summary>
Motivation: TN ranks are crucial for TN decompositions but often misunderstood and treated as hyperparameters. The paper aims to clarify their foundational role and provide intuitive guidance for their selection.

Method: The paper uses real-life examples and graphical visualizations to explain TN ranks, focusing on models like CP and Tucker decompositions, and generalizes the approach for tensors of any order.

Result: The paper provides a clear, unified understanding of TN ranks, linking them to tensor unfoldings and enabling domain-informed TN design.

Conclusion: The Lecture Note aims to equip readers with the intuition and knowledge to select and explain TN ranks effectively in practical and educational settings.

Abstract: Tensor Network (TN) decompositions have emerged as an indispensable tool in
Big Data analytics owing to their ability to provide compact low-rank
representations, thus alleviating the ``Curse of Dimensionality'' inherent in
handling higher-order data. At the heart of their success lies the concept of
TN ranks, which governs the efficiency and expressivity of TN decompositions.
However, unlike matrix ranks, TN ranks often lack a universal meaning and an
intuitive interpretation, with their properties varying significantly across
different TN structures. Consequently, TN ranks are frequently treated as
empirically tuned hyperparameters, rather than as key design parameters
inferred from domain knowledge. The aim of this Lecture Note is therefore to
demystify the foundational yet frequently misunderstood concept of TN ranks
through real-life examples and intuitive visualizations. We begin by
illustrating how domain knowledge can guide the selection of TN ranks in
widely-used models such as the Canonical Polyadic (CP) and Tucker
decompositions. For more complex TN structures, we employ a self-explanatory
graphical approach that generalizes to tensors of arbitrary order. Such a
perspective naturally reveals the relationship between TN ranks and the
corresponding ranks of tensor unfoldings (matrices), thereby circumventing
cumbersome multi-index tensor algebra while facilitating domain-informed TN
design. It is our hope that this Lecture Note will equip readers with a clear
and unified understanding of the concept of TN rank, along with the necessary
physical insight and intuition to support the selection, explainability, and
deployment of tensor methods in both practical applications and educational
contexts.

</details>


### [175] [Play Style Identification Using Low-Level Representations of Play Traces in MicroRTS](https://arxiv.org/abs/2507.10172)
*Ruizhe Yu Xia,Jeremy Gow,Simon Lucas*

Main category: cs.LG

TL;DR: Unsupervised CNN-LSTM autoencoder models are used to derive latent representations from low-level play trace data in MicroRTS, reducing reliance on domain expertise and enabling diverse play style exploration.


<details>
  <summary>Details</summary>
Motivation: To improve game design and adaptive experiences by identifying play styles without heavy reliance on domain knowledge or handcrafted features.

Method: Employ unsupervised CNN-LSTM autoencoder models to process low-level play trace data in MicroRTS.

Result: The approach successfully separates different game playing agents in the latent space, minimizing domain expertise bias.

Conclusion: The method provides a domain-agnostic way to explore diverse play styles, enhancing game playing agents.

Abstract: Play style identification can provide valuable game design insights and
enable adaptive experiences, with the potential to improve game playing agents.
Previous work relies on domain knowledge to construct play trace
representations using handcrafted features. More recent approaches incorporate
the sequential structure of play traces but still require some level of domain
abstraction. In this study, we explore the use of unsupervised CNN-LSTM
autoencoder models to obtain latent representations directly from low-level
play trace data in MicroRTS. We demonstrate that this approach yields a
meaningful separation of different game playing agents in the latent space,
reducing reliance on domain expertise and its associated biases. This latent
space is then used to guide the exploration of diverse play styles within
studied AI players.

</details>


### [176] [T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs](https://arxiv.org/abs/2507.10183)
*Alireza Dizaji,Benedict Aaron Tjandra,Mehrab Hamidi,Shenyang Huang,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: The paper introduces T-GRAB, a benchmark to evaluate TGNNs' temporal reasoning, revealing their limitations in handling patterns like periodicity and causality.


<details>
  <summary>Details</summary>
Motivation: Current TGNNs lack clarity in capturing temporal patterns like periodicity and causality, necessitating a systematic evaluation framework.

Method: T-GRAB, a synthetic benchmark, isolates key temporal skills (e.g., periodicity, causality) to test TGNNs. 11 methods are evaluated.

Result: TGNNs show fundamental shortcomings in generalizing temporal patterns, exposing hidden challenges in traditional benchmarks.

Conclusion: The study highlights TGNNs' limitations and motivates the development of architectures with stronger temporal reasoning.

Abstract: Dynamic graph learning methods have recently emerged as powerful tools for
modelling relational data evolving through time. However, despite extensive
benchmarking efforts, it remains unclear whether current Temporal Graph Neural
Networks (TGNNs) effectively capture core temporal patterns such as
periodicity, cause-and-effect, and long-range dependencies. In this work, we
introduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set
of synthetic tasks designed to systematically probe the capabilities of TGNNs
to reason across time. T-GRAB provides controlled, interpretable tasks that
isolate key temporal skills: counting/memorizing periodic repetitions,
inferring delayed causal effects, and capturing long-range dependencies over
both spatial and temporal dimensions. We evaluate 11 temporal graph learning
methods on these tasks, revealing fundamental shortcomings in their ability to
generalize temporal patterns. Our findings offer actionable insights into the
limitations of current models, highlight challenges hidden by traditional
real-world benchmarks, and motivate the development of architectures with
stronger temporal reasoning abilities. The code for T-GRAB can be found at:
https://github.com/alirezadizaji/T-GRAB.

</details>


### [177] [Learning Private Representations through Entropy-based Adversarial Training](https://arxiv.org/abs/2507.10194)
*Tassilo Klein,Moin Nabi*

Main category: cs.LG

TL;DR: A method for learning predictive representations while preserving privacy using adversarial learning and focal entropy to reduce information leakage.


<details>
  <summary>Details</summary>
Motivation: To balance predictive power and user privacy in representation learning by addressing information leakage in existing entropy-based methods.

Method: Adversarial representation learning with focal entropy to sanitize sensitive content from learned representations.

Result: High target utility with moderate privacy leakage demonstrated on multiple benchmarks.

Conclusion: The proposed method effectively balances utility and privacy, mitigating information leakage.

Abstract: How can we learn a representation with high predictive power while preserving
user privacy? We present an adversarial representation learning method for
sanitizing sensitive content from the learned representation. Specifically, we
introduce a variant of entropy - focal entropy, which mitigates the potential
information leakage of the existing entropy-based approaches. We showcase
feasibility on multiple benchmarks. The results suggest high target utility at
moderate privacy leakage.

</details>


### [178] [A Graph Sufficiency Perspective for Neural Networks](https://arxiv.org/abs/2507.10215)
*Cencheng Shen,Yuexiao Dong*

Main category: cs.LG

TL;DR: The paper analyzes neural networks using graph variables and statistical sufficiency, showing conditions for layer outputs to preserve input distributions.


<details>
  <summary>Details</summary>
Motivation: To bridge statistical sufficiency, graph theory, and deep learning for a new statistical understanding of neural networks.

Method: Interprets neural network layers as graph-based transformations, proving sufficiency under dense anchor points and finite-width assumptions.

Result: Asymptotic sufficiency holds in infinite-width limits and can be achieved in finite-width networks with region-separated inputs.

Conclusion: The work provides a novel statistical framework for understanding neural networks, covering various architectures and activations.

Abstract: This paper analyzes neural networks through graph variables and statistical
sufficiency. We interpret neural network layers as graph-based transformations,
where neurons act as pairwise functions between inputs and learned anchor
points. Within this formulation, we establish conditions under which layer
outputs are sufficient for the layer inputs, that is, each layer preserves the
conditional distribution of the target variable given the input variable. Under
dense anchor point assumptions, we prove that asymptotic sufficiency holds in
the infinite-width limit and is preserved throughout training. To align more
closely with practical architectures, we further show that sufficiency can be
achieved with finite-width networks by assuming region-separated input
distributions and constructing appropriate anchor points. Our framework covers
fully connected layers, general pairwise functions, ReLU and sigmoid
activations, and convolutional neural networks. This work bridges statistical
sufficiency, graph-theoretic representations, and deep learning, providing a
new statistical understanding of neural networks.

</details>


### [179] [Kernel-Adaptive PI-ELMs for Forward and Inverse Problems in PDEs with Sharp Gradients](https://arxiv.org/abs/2507.10241)
*Vikas Dwivedi,Balaji Srinivasan,Monica Sigovan,Bruno Sixou*

Main category: cs.LG

TL;DR: KAPI-ELM is introduced as an adaptive RBF-based extension of PI-ELM to solve PDE problems with sharp gradients, combining Bayesian Optimization for input layer distribution with least-squares optimization for output layer, achieving state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: PI-ELMs are fast but struggle with sharp gradients due to fixed input layers. KAPI-ELM aims to preserve speed while improving expressiveness.

Method: Uses Bayesian Optimization to learn hyperparameters for input layer distribution and least-squares optimization for output layer.

Result: Achieves top accuracy in forward and inverse PDE problems, outperforming methods like XTFC with fewer parameters.

Conclusion: KAPI-ELM is scalable, interpretable, and effective for stiff PDE regimes.

Abstract: This paper introduces the Kernel Adaptive Physics-Informed Extreme Learning
Machine (KAPI-ELM), an adaptive Radial Basis Function (RBF)-based extension of
PI-ELM designed to solve both forward and inverse Partial Differential Equation
(PDE) problems involving localized sharp gradients. While PI-ELMs outperform
the traditional Physics-Informed Neural Networks (PINNs) in speed due to their
single-shot, least square optimization, this advantage comes at a cost: their
fixed, randomly initialized input layer limits their ability to capture sharp
gradients. To overcome this limitation, we introduce a lightweight Bayesian
Optimization (BO) framework that, instead of adjusting each input layer
parameter individually as in traditional backpropagation, learns a small set of
hyperparameters defining the statistical distribution from which the input
weights are drawn. This novel distributional optimization strategy -- combining
BO for input layer distributional parameters with least-squares optimization
for output layer network parameters -- enables KAPI-ELM to preserve PI-ELM's
speed while matching or exceeding the expressiveness of PINNs. We validate the
proposed methodology on several challenging forward and inverse PDE benchmarks,
including a 1D singularly perturbed convection-diffusion equation, a 2D Poisson
equation with sharp localized sources, and a time-dependent advection equation.
Notably, KAPI-ELM achieves state-of-the-art accuracy in both forward and
inverse settings. In stiff PDE regimes, it matches or even outperforms advanced
methods such as the Extended Theory of Functional Connections (XTFC), while
requiring nearly an order of magnitude fewer tunable parameters. These results
establish the potential of KAPI-ELM as a scalable, interpretable, and
generalizable physics-informed learning framework, especially in stiff PDE
regimes.

</details>


### [180] [Conditional Chemical Language Models are Versatile Tools in Drug Discovery](https://arxiv.org/abs/2507.10273)
*Lu Zhu,Emmanuel Noutahi*

Main category: cs.LG

TL;DR: SAFE-T is a conditional generative chemical language model that integrates biological context for molecule prioritization and design, outperforming existing methods in speed and performance while offering interpretability.


<details>
  <summary>Details</summary>
Motivation: Current generative chemical language models lack reliable reward signals and interpretability, limiting their impact in drug discovery. SAFE-T addresses this by incorporating biological context.

Method: SAFE-T models the conditional likelihood of fragment-based molecular sequences given biological prompts, enabling scoring and goal-directed generation without structural data or engineered functions.

Result: SAFE-T achieves comparable or better performance in zero-shot evaluations across predictive and generative benchmarks, with faster computation and interpretable fragment-level attribution.

Conclusion: SAFE-T demonstrates that conditional generative CLMs can unify scoring and generation, accelerating early-stage drug discovery with interpretable and efficient design.

Abstract: Generative chemical language models (CLMs) have demonstrated strong
capabilities in molecular design, yet their impact in drug discovery remains
limited by the absence of reliable reward signals and the lack of
interpretability in their outputs. We present SAFE-T, a generalist chemical
modeling framework that conditions on biological context -- such as protein
targets or mechanisms of action -- to prioritize and design molecules without
relying on structural information or engineered scoring functions. SAFE-T
models the conditional likelihood of fragment-based molecular sequences given a
biological prompt, enabling principled scoring of molecules across tasks such
as virtual screening, drug-target interaction prediction, and activity cliff
detection. Moreover, it supports goal-directed generation by sampling from this
learned distribution, aligning molecular design with biological objectives. In
comprehensive zero-shot evaluations across predictive (LIT-PCBA, DAVIS, KIBA,
ACNet) and generative (DRUG, PMO) benchmarks, SAFE-T consistently achieves
performance comparable to or better than existing approaches while being
significantly faster. Fragment-level attribution further reveals that SAFE-T
captures known structure-activity relationships, supporting interpretable and
biologically grounded design. Together with its computational efficiency, these
results demonstrate that conditional generative CLMs can unify scoring and
generation to accelerate early-stage drug discovery.

</details>


### [181] [Average Sensitivity of Hierarchical $k$-Median Clustering](https://arxiv.org/abs/2507.10296)
*Shijie Li,Weiqiang He,Ruobing Bai,Pan Peng*

Main category: cs.LG

TL;DR: The paper analyzes the average sensitivity of hierarchical $k$-median clustering, proposes an efficient algorithm with low sensitivity, and validates its robustness.


<details>
  <summary>Details</summary>
Motivation: Hierarchical clustering is widely used but sensitive to dataset perturbations, reducing usability for large, dynamic datasets.

Method: Proposes an efficient algorithm for hierarchical $k$-median clustering and analyzes its average sensitivity by measuring output changes when a random data point is deleted.

Result: The algorithm shows low average sensitivity and high clustering quality, outperforming single linkage and CLNSS variants.

Conclusion: The proposed algorithm is robust and effective, validated by experiments, making it suitable for dynamic datasets.

Abstract: Hierarchical clustering is a widely used method for unsupervised learning
with numerous applications. However, in the application of modern algorithms,
the datasets studied are usually large and dynamic. If the hierarchical
clustering is sensitive to small perturbations of the dataset, the usability of
the algorithm will be greatly reduced. In this paper, we focus on the
hierarchical $k$ -median clustering problem, which bridges hierarchical and
centroid-based clustering while offering theoretical appeal, practical utility,
and improved interpretability. We analyze the average sensitivity of algorithms
for this problem by measuring the expected change in the output when a random
data point is deleted. We propose an efficient algorithm for hierarchical
$k$-median clustering and theoretically prove its low average sensitivity and
high clustering quality. Additionally, we show that single linkage clustering
and a deterministic variant of the CLNSS algorithm exhibit high average
sensitivity, making them less stable. Finally, we validate the robustness and
effectiveness of our algorithm through experiments.

</details>


### [182] [Recognizing Dementia from Neuropsychological Tests with State Space Models](https://arxiv.org/abs/2507.10311)
*Liming Wang,Saurabhchand Bhati,Cody Karjadi,Rhoda Au,James Glass*

Main category: cs.LG

TL;DR: Demenba, a novel ADC framework using state space models, improves dementia classification by 21% with linear scalability and fewer parameters, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Early dementia detection is crucial for timely intervention, but traditional neuropsychological tests rely on manual scoring. ADC systems automate this using speech recordings.

Method: Proposes Demenba, a state space model-based ADC framework, trained on 1,000+ hours of cognitive assessments from the Framingham Heart Study.

Result: Outperforms prior methods by 21% in fine-grained dementia classification, with fewer parameters and linear scalability. Fusion with large language models further improves performance.

Conclusion: Demenba offers a transparent, scalable solution for dementia assessment, with potential for enhanced accuracy through integration with large language models.

Abstract: Early detection of dementia is critical for timely medical intervention and
improved patient outcomes. Neuropsychological tests are widely used for
cognitive assessment but have traditionally relied on manual scoring. Automatic
dementia classification (ADC) systems aim to infer cognitive decline directly
from speech recordings of such tests. We propose Demenba, a novel ADC framework
based on state space models, which scale linearly in memory and computation
with sequence length. Trained on over 1,000 hours of cognitive assessments
administered to Framingham Heart Study participants, some of whom were
diagnosed with dementia through adjudicated review, our method outperforms
prior approaches in fine-grained dementia classification by 21\%, while using
fewer parameters. We further analyze its scaling behavior and demonstrate that
our model gains additional improvement when fused with large language models,
paving the way for more transparent and scalable dementia assessment tools.
Code: https://anonymous.4open.science/r/Demenba-0861

</details>


### [183] [Convergence of Agnostic Federated Averaging](https://arxiv.org/abs/2507.10325)
*Herlock,Rahimi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: The paper analyzes the convergence of Federated Averaging (FedAvg) under random and variably-sized client participation, providing the first guarantees for non-uniform participation without prior knowledge.


<details>
  <summary>Details</summary>
Motivation: Practical FL deployments face challenges like intermittent client participation and biased probabilities, which existing convergence results often oversimplify.

Method: The study characterizes the optimization problem under stochastic client dynamics and rigorously analyzes FedAvg's convergence for convex, possibly nonsmooth losses.

Result: The paper achieves a standard convergence rate of order O(1/√T) and shows FedAvg outperforms weighted aggregation variants empirically.

Conclusion: Agnostic FedAvg is robust under general, non-uniform client participation, offering practical advantages without requiring participation distribution knowledge.

Abstract: Federated learning (FL) enables decentralized model training without
centralizing raw data. However, practical FL deployments often face a key
realistic challenge: Clients participate intermittently in server aggregation
and with unknown, possibly biased participation probabilities. Most existing
convergence results either assume full-device participation, or rely on
knowledge of (in fact uniform) client availability distributions -- assumptions
that rarely hold in practice. In this work, we characterize the optimization
problem that consistently adheres to the stochastic dynamics of the well-known
\emph{agnostic Federated Averaging (FedAvg)} algorithm under random (and
variably-sized) client availability, and rigorously establish its convergence
for convex, possibly nonsmooth losses, achieving a standard rate of order
$\mathcal{O}(1/\sqrt{T})$, where $T$ denotes the aggregation horizon. Our
analysis provides the first convergence guarantees for agnostic FedAvg under
general, non-uniform, stochastic client participation, without knowledge of the
participation distribution. We also empirically demonstrate that agnostic
FedAvg in fact outperforms common (and suboptimal) weighted aggregation FedAvg
variants, even with server-side knowledge of participation weights.

</details>


### [184] [MoCap-Impute: A Comprehensive Benchmark and Comparative Analysis of Imputation Methods for IMU-based Motion Capture Data](https://arxiv.org/abs/2507.10334)
*Mahmoud Bekhit,Ahmad Salah,Ahmed Salim Alrawahi,Tarek Attia,Ahmed Ali,Esraa Eldesokey,Ahmed Fathalla*

Main category: cs.LG

TL;DR: The paper evaluates imputation methods for missing MoCap data from IMUs, introducing a public dataset and showing multivariate methods outperform univariate ones, especially for complex missingness.


<details>
  <summary>Details</summary>
Motivation: Missing data in IMU-derived MoCap compromises its utility, and there's no systematic evaluation of imputation techniques for such time-series data.

Method: Comparative analysis of statistical, machine learning, and deep learning imputation methods across univariate and multivariate contexts, using a new public dataset with simulated missingness mechanisms.

Result: Multivariate frameworks outperform univariate, reducing MAE by up to 50% for complex missingness. GAIN and Iterative Imputers show highest accuracy.

Conclusion: The study provides a baseline for future research and practical recommendations to enhance MoCap data integrity.

Abstract: Motion capture (MoCap) data from wearable Inertial Measurement Units (IMUs)
is vital for applications in sports science, but its utility is often
compromised by missing data. Despite numerous imputation techniques, a
systematic performance evaluation for IMU-derived MoCap time-series data is
lacking. We address this gap by conducting a comprehensive comparative analysis
of statistical, machine learning, and deep learning imputation methods. Our
evaluation considers three distinct contexts: univariate time-series,
multivariate across subjects, and multivariate across kinematic angles. To
facilitate this benchmark, we introduce the first publicly available MoCap
dataset designed specifically for imputation, featuring data from 53 karate
practitioners. We simulate three controlled missingness mechanisms: missing
completely at random (MCAR), block missingness, and a novel value-dependent
pattern at signal transition points. Our experiments, conducted on 39 kinematic
variables across all subjects, reveal that multivariate imputation frameworks
consistently outperform univariate approaches, particularly for complex
missingness. For instance, multivariate methods achieve up to a 50% mean
absolute error reduction (MAE from 10.8 to 5.8) compared to univariate
techniques for transition point missingness. Advanced models like Generative
Adversarial Imputation Networks (GAIN) and Iterative Imputers demonstrate the
highest accuracy in these challenging scenarios. This work provides a critical
baseline for future research and offers practical recommendations for improving
the integrity and robustness of Mo-Cap data analysis.

</details>


### [185] [Some Super-approximation Rates of ReLU Neural Networks for Korobov Functions](https://arxiv.org/abs/2507.10345)
*Yuwen Li,Guozhi Zhang*

Main category: cs.LG

TL;DR: The paper derives nearly optimal super-approximation error bounds for ReLU neural networks approximating Korobov functions, improving classical bounds and showing resilience to the curse of dimensionality.


<details>
  <summary>Details</summary>
Motivation: To understand the approximation capabilities of ReLU neural networks for Korobov functions and improve upon classical error bounds.

Method: Uses sparse grid finite elements and the bit extraction technique to analyze $L_p$ and $W^1_p$ norm errors.

Result: Achieves nearly optimal error bounds of order $2m$ in $L_p$ norm and $2m-2$ in $W^1_p$ norm.

Conclusion: ReLU neural networks effectively approximate Korobov functions without suffering from the curse of dimensionality.

Abstract: This paper examines the $L_p$ and $W^1_p$ norm approximation errors of ReLU
neural networks for Korobov functions. In terms of network width and depth, we
derive nearly optimal super-approximation error bounds of order $2m$ in the
$L_p$ norm and order $2m-2$ in the $W^1_p$ norm, for target functions with
$L_p$ mixed derivative of order $m$ in each direction. The analysis leverages
sparse grid finite elements and the bit extraction technique. Our results
improve upon classical lowest order $L_\infty$ and $H^1$ norm error bounds and
demonstrate that the expressivity of neural networks is largely unaffected by
the curse of dimensionality.

</details>


### [186] [Parallel Sampling of Diffusion Models on $SO(3)$](https://arxiv.org/abs/2507.10347)
*Yan-Ting Chen,Hao-Wei Chen,Tsu-Ching Hsiao,Chun-Yi Lee*

Main category: cs.LG

TL;DR: An algorithm is designed to accelerate diffusion on the $SO(3)$ manifold using Picard iteration, achieving up to 4.9× speed-up without task reward degradation.


<details>
  <summary>Details</summary>
Motivation: Diffusion models on $SO(3)$ are slow due to sequential denoising; this work aims to reduce latency.

Method: Adapts numerical Picard iteration for $SO(3)$ to accelerate diffusion, tested on a pose ambiguity problem.

Result: Achieves up to 4.9× speed-up in sample generation with no loss in task reward.

Conclusion: The proposed algorithm effectively accelerates diffusion on $SO(3)$ without compromising performance.

Abstract: In this paper, we design an algorithm to accelerate the diffusion process on
the $SO(3)$ manifold. The inherently sequential nature of diffusion models
necessitates substantial time for denoising perturbed data. To overcome this
limitation, we proposed to adapt the numerical Picard iteration for the $SO(3)$
space. We demonstrate our algorithm on an existing method that employs
diffusion models to address the pose ambiguity problem. Moreover, we show that
this acceleration advantage occurs without any measurable degradation in task
reward. The experiments reveal that our algorithm achieves a speed-up of up to
4.9$\times$, significantly reducing the latency for generating a single sample.

</details>


### [187] [Feature Distillation is the Better Choice for Model-Heterogeneous Federated Learning](https://arxiv.org/abs/2507.10348)
*Yichen Li*

Main category: cs.LG

TL;DR: FedFD introduces feature distillation in Hetero-FL to address knowledge bias from heterogeneous models, improving stability and performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods in Hetero-FL rely on logit distillation, which fails to address knowledge bias from model heterogeneity, leading to unstable training.

Method: FedFD uses feature distillation with orthogonal projection layers to align features from heterogeneous models, enhancing knowledge integration.

Result: FedFD outperforms state-of-the-art methods, demonstrating superior performance and stability.

Conclusion: Feature distillation with orthogonal projection effectively mitigates knowledge bias in Hetero-FL, offering a stable and efficient solution.

Abstract: Model-Heterogeneous Federated Learning (Hetero-FL) has attracted growing
attention for its ability to aggregate knowledge from heterogeneous models
while keeping private data locally. To better aggregate knowledge from clients,
ensemble distillation, as a widely used and effective technique, is often
employed after global aggregation to enhance the performance of the global
model. However, simply combining Hetero-FL and ensemble distillation does not
always yield promising results and can make the training process unstable. The
reason is that existing methods primarily focus on logit distillation, which,
while being model-agnostic with softmax predictions, fails to compensate for
the knowledge bias arising from heterogeneous models. To tackle this challenge,
we propose a stable and efficient Feature Distillation for model-heterogeneous
Federated learning, dubbed FedFD, that can incorporate aligned feature
information via orthogonal projection to integrate knowledge from heterogeneous
models better. Specifically, a new feature-based ensemble federated knowledge
distillation paradigm is proposed. The global model on the server needs to
maintain a projection layer for each client-side model architecture to align
the features separately. Orthogonal techniques are employed to re-parameterize
the projection layer to mitigate knowledge bias from heterogeneous models and
thus maximize the distilled knowledge. Extensive experiments show that FedFD
achieves superior performance compared to state-of-the-art methods.

</details>


### [188] [TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting](https://arxiv.org/abs/2507.10349)
*Zhiyuan Zhao,Sitan Yang,Kin G. Olivares,Boris N. Oreshkin,Stan Vitebsky,Michael W. Mahoney,B. Aditya Prakash,Dmitry Efimov*

Main category: cs.LG

TL;DR: The paper proposes Temporal-Aligned Transformer (TAT) for multi-horizon time series forecasting, improving peak demand prediction by 30% using context-aware alignment.


<details>
  <summary>Details</summary>
Motivation: Accurate demand forecasting is critical for supply chain management, especially during high-stake sales events with unpredictable demand peaks.

Method: TAT uses an encoder-decoder architecture with Temporal Alignment Attention (TAA) to leverage context variables like holidays and promotions.

Result: TAT achieves up to 30% better accuracy for peak demand forecasting while maintaining competitive overall performance.

Conclusion: TAT effectively addresses the challenge of peak demand forecasting, enhancing supply chain and customer experience.

Abstract: Multi-horizon time series forecasting has many practical applications such as
demand forecasting. Accurate demand prediction is critical to help make buying
and inventory decisions for supply chain management of e-commerce and physical
retailers, and such predictions are typically required for future horizons
extending tens of weeks. This is especially challenging during high-stake sales
events when demand peaks are particularly difficult to predict accurately.
However, these events are important not only for managing supply chain
operations but also for ensuring a seamless shopping experience for customers.
To address this challenge, we propose Temporal-Aligned Transformer (TAT), a
multi-horizon forecaster leveraging apriori-known context variables such as
holiday and promotion events information for improving predictive performance.
Our model consists of an encoder and decoder, both embedded with a novel
Temporal Alignment Attention (TAA), designed to learn context-dependent
alignment for peak demand forecasting. We conduct extensive empirical analysis
on two large-scale proprietary datasets from a large e-commerce retailer. We
demonstrate that TAT brings up to 30% accuracy improvement on peak demand
forecasting while maintaining competitive overall performance compared to other
state-of-the-art methods.

</details>


### [189] [Enhanced DeepONet for 1-D consolidation operator learning: an architectural investigation](https://arxiv.org/abs/2507.10368)
*Yongjin Choi,Chenying Liu,Jorge Macedo*

Main category: cs.LG

TL;DR: DeepONets are evaluated for geotechnical engineering, with Model 4 (Fourier feature-enhanced) outperforming others and offering significant speedups over traditional solvers.


<details>
  <summary>Details</summary>
Motivation: To expand DeepONet applications in geotechnical engineering and improve surrogate modeling for PDE-governed systems.

Method: Evaluated four DeepONet architectures for 1D consolidation, including a novel Fourier feature-enhanced model (Model 4).

Result: Model 4 outperformed others, achieving speedups of 1.5 to 100 times over traditional solvers.

Conclusion: DeepONets show promise for efficient, generalizable surrogate modeling in geotechnics, advancing scientific machine learning in the field.

Abstract: Deep Operator Networks (DeepONets) have emerged as a powerful surrogate
modeling framework for learning solution operators in PDE-governed systems.
While their use is expanding across engineering disciplines, applications in
geotechnical engineering remain limited. This study systematically evaluates
several DeepONet architectures for the one-dimensional consolidation problem.
We initially consider three architectures: a standard DeepONet with the
coefficient of consolidation embedded in the branch net (Models 1 and 2), and a
physics-inspired architecture with the coefficient embedded in the trunk net
(Model 3). Results show that Model 3 outperforms the standard configurations
(Models 1 and 2) but still has limitations when the target solution (excess
pore pressures) exhibits significant variation. To overcome this limitation, we
propose a Trunknet Fourier feature-enhanced DeepONet (Model 4) that addresses
the identified limitations by capturing rapidly varying functions. All proposed
architectures achieve speedups ranging from 1.5 to 100 times over traditional
explicit and implicit solvers, with Model 4 being the most efficient. Larger
computational savings are expected for more complex systems than the explored
1D case, which is promising. Overall, the study highlights the potential of
DeepONets to enable efficient, generalizable surrogate modeling in geotechnical
applications, advancing the integration of scientific machine learning in
geotechnics, which is at an early stage.

</details>


### [190] [Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis](https://arxiv.org/abs/2507.10382)
*Yue Ding,Conor McCarthy,Kevin O'Shea,Mingming Liu*

Main category: cs.LG

TL;DR: A cloud-based, LLM-powered shared e-mobility platform is introduced, offering personalized route recommendations and evaluated for optimization and RAG framework performance.


<details>
  <summary>Details</summary>
Motivation: The growing demand for e-mobility and the need for advanced, realistic solutions drive the development of this platform.

Method: The platform integrates a mobile app for personalized routes, evaluates optimization via travel time and cost, and assesses an LLM-powered RAG framework using schema-level methods.

Result: The RAG framework achieves 0.81 accuracy for system operator queries and 0.98 for user queries.

Conclusion: The platform effectively addresses e-mobility needs with robust LLM integration and optimization.

Abstract: With the rise of smart mobility and shared e-mobility services, numerous
advanced technologies have been applied to this field. Cloud-based traffic
simulation solutions have flourished, offering increasingly realistic
representations of the evolving mobility landscape. LLMs have emerged as
pioneering tools, providing robust support for various applications, including
intelligent decision-making, user interaction, and real-time traffic analysis.
As user demand for e-mobility continues to grow, delivering comprehensive
end-to-end solutions has become crucial. In this paper, we present a
cloud-based, LLM-powered shared e-mobility platform, integrated with a mobile
application for personalized route recommendations. The optimization module is
evaluated based on travel time and cost across different traffic scenarios.
Additionally, the LLM-powered RAG framework is evaluated at the schema level
for different users, using various evaluation methods. Schema-level RAG with
XiYanSQL achieves an average execution accuracy of 0.81 on system operator
queries and 0.98 on user queries.

</details>


### [191] [Extracting Important Tokens in E-Commerce Queries with a Tag Interaction-Aware Transformer Model](https://arxiv.org/abs/2507.10385)
*Md. Ahsanul Kabir,Mohammad Al Hasan,Aritra Mandal,Liyang Hao,Ishita Khan,Daniel Tunkelang,Zhe Wu*

Main category: cs.LG

TL;DR: The paper introduces TagBERT, a dependency-aware transformer model for query reformulation in e-commerce, leveraging semantic tags to improve relevance.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like ambiguous queries and vocabulary misalignment in e-commerce search by enhancing query reformulation methods.

Method: Proposes TagBERT, a token classification model using semantic tags for better query phrase embeddings, outperforming BERT and sequence-to-sequence models.

Result: TagBERT shows superior performance on real-life e-commerce datasets compared to competing models.

Conclusion: TagBERT effectively bridges the semantic gap in e-commerce queries by utilizing semantic tags, improving search relevance.

Abstract: The major task of any e-commerce search engine is to retrieve the most
relevant inventory items, which best match the user intent reflected in a
query. This task is non-trivial due to many reasons, including ambiguous
queries, misaligned vocabulary between buyers, and sellers, over- or
under-constrained queries by the presence of too many or too few tokens. To
address these challenges, query reformulation is used, which modifies a user
query through token dropping, replacement or expansion, with the objective to
bridge semantic gap between query tokens and users' search intent. Early
methods of query reformulation mostly used statistical measures derived from
token co-occurrence frequencies from selective user sessions having clicks or
purchases. In recent years, supervised deep learning approaches, specifically
transformer-based neural language models, or sequence-to-sequence models are
being used for query reformulation task. However, these models do not utilize
the semantic tags of a query token, which are significant for capturing user
intent of an e-commerce query. In this work, we pose query reformulation as a
token classification task, and solve this task by designing a dependency-aware
transformer-based language model, TagBERT, which makes use of semantic tags of
a token for learning superior query phrase embedding. Experiments on large,
real-life e-commerce datasets show that TagBERT exhibits superior performance
than plethora of competing models, including BERT, eBERT, and
Sequence-to-Sequence transformer model for important token classification task.

</details>


### [192] [Anticipating the Selectivity of Cyclization Reaction Pathways with Neural Network Potentials](https://arxiv.org/abs/2507.10400)
*Nicholas Casetti,Dylan Anstine,Olexandr Isayev,Connor W. Coley*

Main category: cs.LG

TL;DR: A strategy combining graph-based enumeration and machine learning is introduced to simplify reaction mechanism searches for complex cyclizations, using a neural network potential (AIMNet2-rxn) for accurate pathway evaluation.


<details>
  <summary>Details</summary>
Motivation: Complex reactions like cyclizations in natural product synthesis complicate mechanism searches, requiring a more efficient approach.

Method: Graph-based enumeration and machine learning filter intermediates, with AIMNet2-rxn neural network potential evaluating pathways.

Result: The method accurately estimates activation energies, predicts stereoselectivity, and replicates key steps in natural product synthesis.

Conclusion: The proposed strategy effectively expedites exploration of complex reactions, demonstrating the utility of AIMNet2-rxn in mechanism searches.

Abstract: Reaction mechanism search tools have demonstrated the ability to provide
insights into likely products and rate-limiting steps of reacting systems.
However, reactions involving several concerted bond changes - as can be found
in many key steps of natural product synthesis - can complicate the search
process. To mitigate these complications, we present a mechanism search
strategy particularly suited to help expedite exploration of an exemplary
family of such complex reactions, cyclizations. We provide a cost-effective
strategy for identifying relevant elementary reaction steps by combining
graph-based enumeration schemes and machine learning techniques for
intermediate filtering. Key to this approach is our use of a neural network
potential (NNP), AIMNet2-rxn, for computational evaluation of each candidate
reaction pathway. In this article, we evaluate the NNP's ability to estimate
activation energies, demonstrate the correct anticipation of stereoselectivity,
and recapitulate complex enabling steps in natural product synthesis.

</details>


### [193] [Stochastic Operator Network: A Stochastic Maximum Principle Based Approach to Operator Learning](https://arxiv.org/abs/2507.10401)
*Ryan Bausback,Jingqiao Tang,Lu Lu,Feng Bao,Toan Huynh*

Main category: cs.LG

TL;DR: A new framework, Stochastic Operator Network (SON), combines stochastic optimal control with DeepONet for uncertainty quantification in operator learning, using SDEs and adjoint BSDEs for training.


<details>
  <summary>Details</summary>
Motivation: To address uncertainty quantification in operator learning by integrating stochastic control concepts with neural networks.

Method: Formulates the branch net as an SDE, uses adjoint BSDE for backpropagation, and replaces loss gradient with Hamiltonian gradient from Stochastic Maximum Principle in SGD.

Result: SON effectively replicates noisy operators in 2D and 3D, demonstrating its capability to learn uncertainty through diffusion parameters.

Conclusion: SON provides a robust framework for uncertainty-aware operator learning, validated by successful applications in noisy environments.

Abstract: We develop a novel framework for uncertainty quantification in operator
learning, the Stochastic Operator Network (SON). SON combines the stochastic
optimal control concepts of the Stochastic Neural Network (SNN) with the
DeepONet. By formulating the branch net as an SDE and backpropagating through
the adjoint BSDE, we replace the gradient of the loss function with the
gradient of the Hamiltonian from Stohastic Maximum Principle in the SGD update.
This allows SON to learn the uncertainty present in operators through its
diffusion parameters. We then demonstrate the effectiveness of SON when
replicating several noisy operators in 2D and 3D.

</details>


### [194] [Energy Efficiency in AI for 5G and Beyond: A DeepRx Case Study](https://arxiv.org/abs/2507.10409)
*Amine Lbath,Ibtissam Labriji*

Main category: cs.LG

TL;DR: The study explores energy-efficient AI/ML models, focusing on DeepRX, a ResNet-based deep learning receiver. It evaluates energy consumption, validates estimates, and applies knowledge distillation (KD) to create a compact, energy-efficient student model. Results show KD's effectiveness in reducing energy while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between energy efficiency and performance in AI/ML models, particularly for DeepRX, to enable sustainable AI solutions.

Method: Evaluates DeepRX's energy consumption (FLOPs/Watt, FLOPs/clock), compares training vs. inference energy, and applies KD to train a compact student model with varied sizes and hyperparameters.

Result: Distilled models achieve lower error floors across SINR levels, proving KD's effectiveness in balancing energy efficiency and performance.

Conclusion: KD successfully enables energy-efficient AI solutions without compromising performance, as demonstrated by DeepRX's distilled models.

Abstract: This study addresses the challenge of balancing energy efficiency with
performance in AI/ML models, focusing on DeepRX, a deep learning receiver based
on a fully convolutional ResNet architecture. We evaluate the energy
consumption of DeepRX, considering factors including FLOPs/Watt and
FLOPs/clock, and find consistency between estimated and actual energy usage,
influenced by memory access patterns. The research extends to comparing energy
dynamics during training and inference phases. A key contribution is the
application of knowledge distillation (KD) to train a compact DeepRX
\textit{student} model that emulates the performance of the \textit{teacher}
model but with reduced energy consumption. We experiment with different student
model sizes, optimal teacher sizes, and KD hyperparameters. Performance is
measured by comparing the Bit Error Rate (BER) performance versus
Signal-to-Interference \& Noise Ratio (SINR) values of the distilled model and
a model trained from scratch. The distilled models demonstrate a lower error
floor across SINR levels, highlighting the effectiveness of KD in achieving
energy-efficient AI solutions.

</details>


### [195] [Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data](https://arxiv.org/abs/2507.10425)
*Alvaro H. C. Correia,Christos Louizos*

Main category: cs.LG

TL;DR: The paper explores conformal prediction's limitations under distribution shifts and proposes an optimal transport-based solution to estimate and mitigate coverage loss.


<details>
  <summary>Details</summary>
Motivation: Conformal prediction's guarantees fail under non-exchangeable data due to distribution shifts, and existing solutions require prior knowledge of the shift type.

Method: The study uses optimal transport theory to analyze and address coverage loss in non-exchangeable settings.

Result: The approach successfully estimates and mitigates coverage loss caused by distribution shifts.

Conclusion: Optimal transport provides a viable method to handle distribution shifts in conformal prediction without needing prior shift information.

Abstract: Conformal prediction is a distribution-free uncertainty quantification method
that has gained popularity in the machine learning community due to its
finite-sample guarantees and ease of use. Its most common variant, dubbed split
conformal prediction, is also computationally efficient as it boils down to
collecting statistics of the model predictions on some calibration data not yet
seen by the model. Nonetheless, these guarantees only hold if the calibration
and test data are exchangeable, a condition that is difficult to verify and
often violated in practice due to so-called distribution shifts. The literature
is rife with methods to mitigate the loss in coverage in this non-exchangeable
setting, but these methods require some prior information on the type of
distribution shift to be expected at test time. In this work, we study this
problem via a new perspective, through the lens of optimal transport, and show
that it is possible to estimate the loss in coverage and mitigate it in case of
distribution shift.

</details>


### [196] [CLA: Latent Alignment for Online Continual Self-Supervised Learning](https://arxiv.org/abs/2507.10434)
*Giacomo Cignoni,Andrea Cossu,Alexandra Gomez-Villa,Joost van de Weijer,Antonio Carta*

Main category: cs.LG

TL;DR: CLA is a novel SSL method for online continual learning that aligns current and past representations to reduce forgetting, improving convergence and outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Address the lack of SSL techniques for online continual learning, where data arrives in small batches, computational resources are limited, and task boundaries are unclear.

Method: Introduces Continual Latent Alignment (CLA), which aligns current model representations with past ones to mitigate forgetting.

Result: CLA speeds up training convergence in online scenarios and outperforms SOTA methods. It also improves final performance when used as pretraining.

Conclusion: CLA is effective for online continual learning, offering faster convergence and better performance than existing methods, even enhancing pretraining outcomes.

Abstract: Self-supervised learning (SSL) is able to build latent representations that
generalize well to unseen data. However, only a few SSL techniques exist for
the online CL setting, where data arrives in small minibatches, the model must
comply with a fixed computational budget, and task boundaries are absent. We
introduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL
that aligns the representations learned by the current model with past
representations to mitigate forgetting. We found that our CLA is able to speed
up the convergence of the training process in the online scenario,
outperforming state-of-the-art approaches under the same computational budget.
Surprisingly, we also discovered that using CLA as a pretraining protocol in
the early stages of pretraining leads to a better final performance when
compared to a full i.i.d. pretraining.

</details>


### [197] [Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities](https://arxiv.org/abs/2507.10442)
*Shivam Chandhok,Wan-Cyuan Fan,Vered Shwartz,Vineeth N Balasubramanian,Leonid Sigal*

Main category: cs.LG

TL;DR: The paper investigates the limitations of state-of-the-art Vision-Language Models (VLMs) by testing their performance on fundamental visual tasks and analyzing design components.


<details>
  <summary>Details</summary>
Motivation: To understand the shortcomings of VLMs in basic visual understanding and identify design flaws.

Method: Constructed tests to probe VLM components (visual encoder, vision-language projection, LLM-decoder) and compared their performance to trained probes.

Result: Uncovered VLM shortcomings in capabilities, robustness, and visual information processing.

Conclusion: The findings aim to guide future improvements in VLM design and performance.

Abstract: Vision-language Models (VLMs) have emerged as general-purpose tools for
addressing a variety of complex computer vision problems. Such models have been
shown to be highly capable, but, at the same time, lacking some basic visual
understanding skills. In this paper, we set out to understand the limitations
of SoTA VLMs on fundamental visual tasks by constructing a series of tests that
probe which components of design, specifically, may be lacking. Importantly, we
go significantly beyond the current benchmarks, which simply measure the final
performance of VLM response, by also comparing and contrasting it to the
performance of probes trained directly on features obtained from the visual
encoder, intermediate vision-language projection and LLM-decoder output. In
doing so, we uncover shortcomings in VLMs and make a number of important
observations about their capabilities, robustness and how they process visual
information. We hope our insights will guide progress in further improving
VLMs.

</details>


### [198] [Some remarks on gradient dominance and LQR policy optimization](https://arxiv.org/abs/2507.10452)
*Eduardo D. Sontag*

Main category: cs.LG

TL;DR: The paper explores the application of the Polyak-Łojasiewicz Inequality (PLI) to optimization problems, highlighting gaps between continuous-time (CT) and discrete-time (DT) behaviors, and introduces generalized PLI-like conditions to address these issues. It also examines the impact of gradient estimation errors and proposes an input-to-state stability (ISS) analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the observed discrepancy in convergence rates between CT and DT LQR problems, and the need to understand the effects of gradient estimation errors in optimization and reinforcement learning.

Method: The study applies generalized PLI-like conditions and conducts an ISS analysis to investigate convergence and the impact of errors in gradient estimation.

Result: The research reveals mixed globally linear / locally exponential convergence behaviors in CT problems, contrasting with global exponential convergence in DT problems, and provides insights into error effects.

Conclusion: The work advances understanding of PLI-like conditions and their role in optimization, offering tools to analyze convergence and error impacts in gradient-based methods.

Abstract: Solutions of optimization problems, including policy optimization in
reinforcement learning, typically rely upon some variant of gradient descent.
There has been much recent work in the machine learning, control, and
optimization communities applying the Polyak-{\L}ojasiewicz Inequality (PLI) to
such problems in order to establish an exponential rate of convergence (a.k.a.
``linear convergence'' in the local-iteration language of numerical analysis)
of loss functions to their minima under the gradient flow. Often, as is the
case of policy iteration for the continuous-time LQR problem, this rate
vanishes for large initial conditions, resulting in a mixed globally linear /
locally exponential behavior. This is in sharp contrast with the discrete-time
LQR problem, where there is global exponential convergence. That gap between CT
and DT behaviors motivates the search for various generalized PLI-like
conditions, and this talk will address that topic. Moreover, these
generalizations are key to understanding the transient and asymptotic effects
of errors in the estimation of the gradient, errors which might arise from
adversarial attacks, wrong evaluation by an oracle, early stopping of a
simulation, inaccurate and very approximate digital twins, stochastic
computations (algorithm ``reproducibility''), or learning by sampling from
limited data. We describe an ``input to state stability'' (ISS) analysis of
this issue. The lecture also discussed convergence and PLI-like properties of
``linear feedforward neural networks'' in feedback control, but this arXiv
skips that part (to be updated). Much of the work described here was done in
collaboration with Arthur Castello B. de Oliveira, Leilei Cui, Zhong-Ping
Jiang, and Milad Siami.

</details>


### [199] [The Target Polish: A New Approach to Outlier-Resistant Non-Negative Matrix and Tensor Factorization](https://arxiv.org/abs/2507.10484)
*Paul Fogel,Christophe Geissler,George Luta*

Main category: cs.LG

TL;DR: Target Polish is a fast, outlier-resistant framework for nonnegative matrix/tensor factorization, outperforming robust NMF methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Conventional weighted NMF methods are slow due to multiplicative updates, despite outlier resistance. Target Polish aims to combine speed and robustness.

Method: Uses weighted median-based transformation for adaptive smoothing, compatible with Fast-HALS for efficient additive updates.

Result: Matches/exceeds accuracy of robust NMF methods and reduces computational time significantly.

Conclusion: Target Polish is a promising solution for fast, robust NMF, especially in noisy datasets.

Abstract: This paper introduces the "Target Polish," a robust and computationally
efficient framework for nonnegative matrix and tensor factorization. Although
conventional weighted NMF approaches are resistant to outliers, they converge
slowly due to the use of multiplicative updates to minimize the objective
criterion. In contrast, the Target Polish approach remains compatible with the
Fast-HALS algorithm, which is renowned for its speed, by adaptively smoothing
the data with a weighted median-based transformation. This innovation provides
outlier resistance while maintaining the highly efficient additive update
structure of Fast-HALS. Empirical evaluations using image datasets corrupted
with structured (block) and unstructured (salt) noise demonstrate that the
Target Polish approach matches or exceeds the accuracy of state-of-the-art
robust NMF methods and reduces computational time by an order of magnitude in
the studied scenarios.

</details>


### [200] [Overcoming catastrophic forgetting in neural networks](https://arxiv.org/abs/2507.10485)
*Brandon Shuen Yi Loke,Filippo Quadri,Gabriel Vivanco,Maximilian Casagrande,Saúl Fenollosa*

Main category: cs.LG

TL;DR: EWC reduces catastrophic forgetting in continual learning, outperforming L2 and SGD, but slightly slows new task learning. Dropout and hyperparameters' impact is also studied.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting in neural networks to enable lifelong learning.

Method: Evaluated EWC on PermutedMNIST and RotatedMNIST, comparing it with L2 regularization and SGD.

Result: EWC significantly reduces forgetting but slightly hampers new task learning efficiency.

Conclusion: EWC is a promising solution for lifelong learning, with insights on dropout and hyperparameters.

Abstract: Catastrophic forgetting is the primary challenge that hinders continual
learning, which refers to a neural network ability to sequentially learn
multiple tasks while retaining previously acquired knowledge. Elastic Weight
Consolidation, a regularization-based approach inspired by synaptic
consolidation in biological neural systems, has been used to overcome this
problem. In this study prior research is replicated and extended by evaluating
EWC in supervised learning settings using the PermutedMNIST and RotatedMNIST
benchmarks. Through systematic comparisons with L2 regularization and
stochastic gradient descent (SGD) without regularization, we analyze how
different approaches balance knowledge retention and adaptability. Our results
confirm what was shown in previous research, showing that EWC significantly
reduces forgetting compared to naive training while slightly compromising
learning efficiency on new tasks. Moreover, we investigate the impact of
dropout regularization and varying hyperparameters, offering insights into the
generalization of EWC across diverse learning scenarios. These results
underscore EWC's potential as a viable solution for lifelong learning in neural
networks.

</details>


### [201] [Split Happens: Combating Advanced Threats with Split Learning and Function Secret Sharing](https://arxiv.org/abs/2507.10494)
*Tanveer Khan,Mindaugas Budzys,Antonis Michalas*

Main category: cs.LG

TL;DR: SplitHappens combines Split Learning (SL) with Function Secret Sharing (FSS) in a U-shaped SL framework, enhancing security by keeping client labels secret and reducing communication/computational costs.


<details>
  <summary>Details</summary>
Motivation: SL is vulnerable to attacks, raising privacy concerns. Recent FSS-based solutions lack robustness against emerging threats.

Method: U-shaped SL integrates FSS, hiding labels and generalizing security against attacks like model inversion and label inference.

Result: Experiments show reduced training time and communication costs while maintaining accuracy.

Conclusion: SplitHappens improves SL security and efficiency, addressing limitations of prior FSS-based approaches.

Abstract: Split Learning (SL) -- splits a model into two distinct parts to help protect
client data while enhancing Machine Learning (ML) processes. Though promising,
SL has proven vulnerable to different attacks, thus raising concerns about how
effective it may be in terms of data privacy. Recent works have shown promising
results for securing SL through the use of a novel paradigm, named Function
Secret Sharing (FSS), in which servers obtain shares of a function they compute
and operate on a public input hidden with a random mask. However, these works
fall short in addressing the rising number of attacks which exist on SL. In
SplitHappens, we expand the combination of FSS and SL to U-shaped SL. Similarly
to other works, we are able to make use of the benefits of SL by reducing the
communication and computational costs of FSS. However, a U-shaped SL provides a
higher security guarantee than previous works, allowing a client to keep the
labels of the training data secret, without having to share them with the
server. Through this, we are able to generalize the security analysis of
previous works and expand it to different attack vectors, such as modern model
inversion attacks as well as label inference attacks. We tested our approach
for two different convolutional neural networks on different datasets. These
experiments show the effectiveness of our approach in reducing the training
time as well as the communication costs when compared to simply using FSS while
matching prior accuracy.

</details>


### [202] [Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop](https://arxiv.org/abs/2507.10502)
*Elizabeth Fahsbender,Alma Andersson,Jeremy Ash,Polina Binder,Daniel Burkhardt,Benjamin Chang,Georg K. Gerber,Anthony Gitter,Patrick Godau,Ankit Gupta,Genevieve Haliburton,Siyu He,Trey Ideker,Ivana Jelic,Aly Khan,Yang-Joon Kim,Aditi Krishnapriyan,Jon M. Laurent,Tianyu Liu 28,Emma Lundberg,Shalin B. Mehta,Rob Moccia,Angela Oliveira Pisco,Katherine S. Pollard,Suresh Ramani,Julio Saez-Rodriguez,Yasin Senbabaoglu,Elana Simon,Srinivasan Sivanandan,Gustavo Stolovitzky,Marc Valer,Bo Wang,Xikun Zhang,James Zou,Katrina Kalantar*

Main category: cs.LG

TL;DR: The paper highlights the need for standardized benchmarks in AI-driven biology due to challenges like data heterogeneity, noise, and reproducibility. It proposes recommendations for robust benchmarking frameworks to improve model reliability and biological relevance.


<details>
  <summary>Details</summary>
Motivation: The lack of standardized benchmarks in AI for biology hampers the development of trustworthy models. The paper aims to address this gap by identifying bottlenecks and proposing solutions.

Method: Insights were gathered from a workshop with experts in machine learning and computational biology across various domains (imaging, transcriptomics, proteomics, genomics).

Result: Identified key bottlenecks: data heterogeneity, noise, reproducibility issues, biases, and fragmented resources. Proposed recommendations for benchmarking frameworks.

Conclusion: Standardized benchmarks are essential for rigor and reproducibility in AI-driven biology. The proposed framework aims to accelerate progress toward integrated models for new discoveries and therapeutic insights.

Abstract: Artificial intelligence holds immense promise for transforming biology, yet a
lack of standardized, cross domain, benchmarks undermines our ability to build
robust, trustworthy models. Here, we present insights from a recent workshop
that convened machine learning and computational biology experts across
imaging, transcriptomics, proteomics, and genomics to tackle this gap. We
identify major technical and systemic bottlenecks such as data heterogeneity
and noise, reproducibility challenges, biases, and the fragmented ecosystem of
publicly available resources and propose a set of recommendations for building
benchmarking frameworks that can efficiently compare ML models of biological
systems across tasks and data modalities. By promoting high quality data
curation, standardized tooling, comprehensive evaluation metrics, and open,
collaborative platforms, we aim to accelerate the development of robust
benchmarks for AI driven Virtual Cells. These benchmarks are crucial for
ensuring rigor, reproducibility, and biological relevance, and will ultimately
advance the field toward integrated models that drive new discoveries,
therapeutic insights, and a deeper understanding of cellular systems.

</details>


### [203] [On the Performance of Differentially Private Optimization with Heavy-Tail Class Imbalance](https://arxiv.org/abs/2507.10536)
*Qiaoyue Tang,Alain Zhiyanov,Mathias Lécuyer*

Main category: cs.LG

TL;DR: DP-GD struggles with low-frequency classes under heavy-tail imbalance, while DP-AdamBC improves accuracy by 8% and 5% in controlled and real data.


<details>
  <summary>Details</summary>
Motivation: To understand how private learning algorithms perform under heavy-tail class imbalance, focusing on low-frequency classes.

Method: Compare Gradient Descent with DP (DP-GD) and DP-AdamBC, which corrects DP bias in loss curvature estimation.

Result: DP-AdamBC outperforms DP-GD, increasing training accuracy by ≈8% and ≈5% for low-frequency classes.

Conclusion: DP-AdamBC is effective for heavy-tail imbalance, mitigating issues faced by DP-GD.

Abstract: In this work, we analyze the optimization behaviour of common private
learning optimization algorithms under heavy-tail class imbalanced
distribution. We show that, in a stylized model, optimizing with Gradient
Descent with differential privacy (DP-GD) suffers when learning low-frequency
classes, whereas optimization algorithms that estimate second-order information
do not. In particular, DP-AdamBC that removes the DP bias from estimating loss
curvature is a crucial component to avoid the ill-condition caused by
heavy-tail class imbalance, and empirically fits the data better with
$\approx8\%$ and $\approx5\%$ increase in training accuracy when learning the
least frequent classes on both controlled experiments and real data
respectively.

</details>


### [204] [Graph World Model](https://arxiv.org/abs/2507.10539)
*Tao Feng,Yexin Wu,Guanyu Lin,Jiaxuan You*

Main category: cs.LG

TL;DR: The paper introduces the Graph World Model (GWM), a world model that integrates unstructured and graph-structured data with multi-modal information, outperforming domain-specific baselines across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Existing world models lack support for structured data like graphs, and graph foundation models are limited to graph learning tasks. GWM aims to bridge this gap by handling multi-modal data and diverse interdisciplinary tasks.

Method: GWM uses a generic message-passing algorithm to aggregate structured information, either through a unified multi-modal token space (GWM-T) or embedding space (GWM-E), and introduces action nodes for task representation.

Result: GWM outperforms or matches domain-specific baselines in six diverse tasks, benefits from multi-hop structures, and shows strong zero-shot/few-shot capabilities on unseen tasks.

Conclusion: GWM successfully integrates structured and unstructured data, demonstrating versatility and superior performance across interdisciplinary tasks.

Abstract: World models (WMs) demonstrate strong capabilities in prediction, generation,
and planning tasks. Existing WMs primarily focus on unstructured data and
cannot leverage the ubiquitous structured data, often represented as graphs, in
the digital world. While multiple graph foundation models have been proposed,
they focus on graph learning tasks and cannot extend to diverse multi-modal
data and interdisciplinary tasks. To address these challenges, we propose the
Graph World Model (GWM), a world model that supports both unstructured and
graph-structured states with multi-modal information and represents diverse
tasks as actions. The core of a GWM is a generic message-passing algorithm to
aggregate structured information, either over a unified multi-modal token space
by converting multi-modal data into text (GWM-T) or a unified multi-modal
embedding space by modality-specific encoders (GWM-E). Notably, GWM introduces
action nodes to support diverse tasks, where action nodes are linked to other
nodes via direct reference or similarity computation. Extensive experiments on
six tasks from diverse domains, including multi-modal generation and matching,
recommendation, graph prediction, multi-agent, retrieval-augmented generation,
and planning and optimization, show that the same GWM outperforms or matches
domain-specific baselines' performance, benefits from multi-hop structures, and
demonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our
code for GWM is released at https://github.com/ulab-uiuc/GWM.

</details>


### [205] [Fusing LLM Capabilities with Routing Data](https://arxiv.org/abs/2507.10540)
*Tao Feng,Haozhen Zhang,Zijie Lei,Pengrui Han,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Jiaxuan You*

Main category: cs.LG

TL;DR: FusionBench and FusionFactory propose systematic LLM fusion to leverage diverse model strengths, improving performance across tasks.


<details>
  <summary>Details</summary>
Motivation: Current reliance on single LLMs limits performance and efficiency; routing data reveals untapped potential for model fusion.

Method: FusionBench benchmarks 20 LLMs across 14 tasks, while FusionFactory introduces query-, thought-, and model-level fusion.

Result: FusionFactory outperforms individual LLMs on all benchmarks, showing the value of systematic fusion.

Conclusion: Systematic LLM fusion harnesses complementary strengths, enhancing performance and efficiency.

Abstract: The rapid advancement of large language models (LLMs) has created a vibrant
ecosystem of diverse architectures, each with unique strengths due to
differences in design, training data, and objectives. However, most
applications still rely on a single backend model, limiting coverage of
capabilities and leading to inefficiencies in performance and token cost when
tackling complex tasks. We highlight an underexploited opportunity: LLM routing
data, produced when hosting platforms route diverse queries to different
models, which can reveal comparative strengths across tasks. To address this,
we propose FusionBench, a comprehensive routing benchmark covering 14 tasks
across five domains with 20 open-source LLMs (8B to 671B parameters), capturing
103M tokens and summarizing reusable thought templates from top models.
Building on this, we introduce FusionFactory, a systematic fusion framework
with three levels: (1) query-level fusion, tailoring routers for each query
using both direct responses and reasoning-augmented outputs; (2) thought-level
fusion, leveraging abstract templates derived from top-performing LLMs' answers
to similar queries; and (3) model-level fusion, transferring capabilities
between models via distillation, using top responses or highest judge scores as
training data. Experiments show FusionFactory consistently outperforms the best
individual LLM across all 14 benchmarks, with optimal fusion configurations
varying by benchmark, demonstrating the value of systematic LLM fusion in
harnessing complementary strengths and improving overall performance.

</details>


### [206] [Disentangling Neural Disjunctive Normal Form Models](https://arxiv.org/abs/2507.10546)
*Kexin Gu Baugh,Vincent Perreault,Matthew Baugh,Luke Dickens,Katsumi Inoue,Alessandra Russo*

Main category: cs.LG

TL;DR: The paper addresses performance degradation in Neural DNF models during symbolic translation by proposing a disentanglement method to split nested rules into independent nodes, improving interpretability and performance.


<details>
  <summary>Details</summary>
Motivation: Performance of Neural DNF models degrades during symbolic translation due to failure in disentangling learned knowledge.

Method: Proposes a disentanglement method by splitting nodes encoding nested rules into smaller independent nodes.

Result: Experiments show improved performance and interpretability in binary, multiclass, and multilabel classification tasks.

Conclusion: The disentanglement method preserves model performance and provides compact, interpretable logical representations.

Abstract: Neural Disjunctive Normal Form (DNF) based models are powerful and
interpretable approaches to neuro-symbolic learning and have shown promising
results in classification and reinforcement learning settings without prior
knowledge of the tasks. However, their performance is degraded by the
thresholding of the post-training symbolic translation process. We show here
that part of the performance degradation during translation is due to its
failure to disentangle the learned knowledge represented in the form of the
networks' weights. We address this issue by proposing a new disentanglement
method; by splitting nodes that encode nested rules into smaller independent
nodes, we are able to better preserve the models' performance. Through
experiments on binary, multiclass, and multilabel classification tasks
(including those requiring predicate invention), we demonstrate that our
disentanglement method provides compact and interpretable logical
representations for the neural DNF-based models, with performance closer to
that of their pre-translation counterparts. Our code is available at
https://github.com/kittykg/disentangling-ndnf-classification.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [207] [View Invariant Learning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2507.08831)
*Josh Qixuan Sun,Xiaoying Xing,Huaiyuan Weng,Chul Min Yeum,Mark Crowley*

Main category: cs.CV

TL;DR: The paper introduces V2-VLNCE, a generalized scenario for Vision-Language Navigation in Continuous Environments (VLNCE) with varied viewpoints, and proposes VIL, a view-invariant post-training strategy to enhance navigation policy robustness.


<details>
  <summary>Details</summary>
Motivation: Most navigation policies are sensitive to viewpoint changes, which limits their practical applicability. The paper aims to address this by improving robustness to variations in camera height and viewing angle.

Method: The authors propose VIL, a contrastive learning framework for sparse and view-invariant features, and a teacher-student framework for the Waypoint Predictor Module. They use end-to-end training to optimize these components.

Result: VIL outperforms state-of-the-art methods by 8-15% in Success Rate on R2R-CE and RxR-CE datasets. It also achieves SOTA performance on RxR-CE and maintains or improves standard VLNCE performance.

Conclusion: VIL is a plug-and-play post-training method that enhances robustness to viewpoint changes without compromising standard performance, making it a practical solution for VLNCE.

Abstract: Vision-Language Navigation in Continuous Environments (VLNCE), where an agent
follows instructions and moves freely to reach a destination, is a key research
problem in embodied AI. However, most navigation policies are sensitive to
viewpoint changes, i.e., variations in camera height and viewing angle that
alter the agent's observation. In this paper, we introduce a generalized
scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View
Invariant Learning), a view-invariant post-training strategy that enhances the
robustness of existing navigation policies to changes in camera viewpoint. VIL
employs a contrastive learning framework to learn sparse and view-invariant
features. Additionally, we introduce a teacher-student framework for the
Waypoint Predictor Module, a core component of most VLNCE baselines, where a
view-dependent teacher model distills knowledge into a view-invariant student
model. We employ an end-to-end training paradigm to jointly optimize these
components, thus eliminating the cost for individual module training. Empirical
results show that our method outperforms state-of-the-art approaches on
V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets
R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE
setting and find that, despite being trained for varied viewpoints, it often
still improves performance. On the more challenging RxR-CE dataset, our method
also achieved state-of-the-art performance across all metrics when compared to
other map-free methods. This suggests that adding VIL does not diminish the
standard viewpoint performance and can serve as a plug-and-play post-training
method.

</details>


### [208] [Detecting Deepfake Talking Heads from Facial Biometric Anomalies](https://arxiv.org/abs/2507.08917)
*Justin D. Norman,Hany Farid*

Main category: cs.CV

TL;DR: A novel forensic ML technique detects deepfake videos by analyzing unnatural facial biometric patterns, tested on diverse datasets and unseen generators.


<details>
  <summary>Details</summary>
Motivation: Deepfake videos are increasingly used for fraud and disinformation, necessitating reliable detection methods.

Method: The technique leverages unnatural facial biometric patterns to identify deepfake impersonations.

Result: Evaluated on a large dataset, the method shows reliability against video laundering and generalizes to unseen deepfake generators.

Conclusion: The proposed technique effectively detects deepfake videos, offering a robust solution for combating fraud and disinformation.

Abstract: The combination of highly realistic voice cloning, along with visually
compelling avatar, face-swap, or lip-sync deepfake video generation, makes it
relatively easy to create a video of anyone saying anything. Today, such
deepfake impersonations are often used to power frauds, scams, and political
disinformation. We propose a novel forensic machine learning technique for the
detection of deepfake video impersonations that leverages unnatural patterns in
facial biometrics. We evaluate this technique across a large dataset of
deepfake techniques and impersonations, as well as assess its reliability to
video laundering and its generalization to previously unseen video deepfake
generators.

</details>


### [209] [PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection](https://arxiv.org/abs/2507.08979)
*Mahdiyar Molahasani,Azadeh Motamedi,Michael Greenspan,Il-Min Kim,Ali Etemad*

Main category: cs.CV

TL;DR: PRISM is a data-free, task-agnostic method for mitigating bias in vision-language models (VLMs) like CLIP, using a two-stage approach involving LLM-generated scene descriptions and a contrastive debiasing loss.


<details>
  <summary>Details</summary>
Motivation: VLMs often inherit and amplify biases from training data, leading to skewed predictions. PRISM aims to debias VLMs without predefined bias categories or external data.

Method: PRISM operates in two stages: (1) generating scene descriptions with spurious correlations using an LLM, and (2) applying a contrastive-style debiasing loss to learn a projection that minimizes spurious correlations while preserving image-text alignment.

Result: PRISM outperforms current debiasing methods on Waterbirds and CelebA datasets.

Conclusion: PRISM offers an effective, data-free solution for bias mitigation in VLMs, with publicly available code.

Abstract: We introduce Projection-based Reduction of Implicit Spurious bias in
vision-language Models (PRISM), a new data-free and task-agnostic solution for
bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in
their training data, leading to skewed predictions. PRISM is designed to debias
VLMs without relying on predefined bias categories or additional external data.
It operates in two stages: first, an LLM is prompted with simple class prompts
to generate scene descriptions that contain spurious correlations. Next, PRISM
uses our novel contrastive-style debiasing loss to learn a projection that maps
the embeddings onto a latent space that minimizes spurious correlations while
preserving the alignment between image and text embeddings.Extensive
experiments demonstrate that PRISM outperforms current debiasing methods on the
commonly used Waterbirds and CelebA datasets We make our code public at:
https://github.com/MahdiyarMM/PRISM.

</details>


### [210] [Video Inference for Human Mesh Recovery with Vision Transformer](https://arxiv.org/abs/2507.08981)
*Hanbyel Cho,Jaesung Ahn,Yooshin Cho,Junmo Kim*

Main category: cs.CV

TL;DR: HMR-ViT combines temporal and kinematic data for human mesh recovery, using a Vision Transformer and CRM for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing HMR methods use either temporal or kinematic data, but not both, limiting accuracy.

Method: Constructs a Temporal-kinematic Feature Image with CRM, processes it with Vision Transformer, and regresses SMPL parameters.

Result: Achieves competitive performance on 3DPW and Human3.6M datasets.

Conclusion: HMR-ViT effectively integrates temporal and kinematic information for better HMR results.

Abstract: Human Mesh Recovery (HMR) from an image is a challenging problem because of
the inherent ambiguity of the task. Existing HMR methods utilized either
temporal information or kinematic relationships to achieve higher accuracy, but
there is no method using both. Hence, we propose "Video Inference for Human
Mesh Recovery with Vision Transformer (HMR-ViT)" that can take into account
both temporal and kinematic information. In HMR-ViT, a Temporal-kinematic
Feature Image is constructed using feature vectors obtained from video frames
by an image encoder. When generating the feature image, we use a Channel
Rearranging Matrix (CRM) so that similar kinematic features could be located
spatially close together. The feature image is then further encoded using
Vision Transformer, and the SMPL pose and shape parameters are finally inferred
using a regression network. Extensive evaluation on the 3DPW and Human3.6M
datasets indicates that our method achieves a competitive performance in HMR.

</details>


### [211] [From images to properties: a NeRF-driven framework for granular material parameter inversion](https://arxiv.org/abs/2507.09005)
*Cheng-Hsi Hsiao,Krishna Kumar*

Main category: cs.CV

TL;DR: A framework combining NeRF and MPM estimates granular material properties from visual data, achieving friction angle estimation within 2 degrees error.


<details>
  <summary>Details</summary>
Motivation: To characterize granular materials in scenarios where direct measurement is impractical, using visual observations for inverse analysis.

Method: Synthetic data generation, NeRF for 3D reconstruction, MPM simulation initialization, and Bayesian optimization for friction angle estimation.

Result: Friction angle estimated with less than 2 degrees error, validating the framework's effectiveness.

Conclusion: The approach is promising for real-world granular material characterization through visual data.

Abstract: We introduce a novel framework that integrates Neural Radiance Fields (NeRF)
with Material Point Method (MPM) simulation to infer granular material
properties from visual observations. Our approach begins by generating
synthetic experimental data, simulating an plow interacting with sand. The
experiment is rendered into realistic images as the photographic observations.
These observations include multi-view images of the experiment's initial state
and time-sequenced images from two fixed cameras. Using NeRF, we reconstruct
the 3D geometry from the initial multi-view images, leveraging its capability
to synthesize novel viewpoints and capture intricate surface details. The
reconstructed geometry is then used to initialize material point positions for
the MPM simulation, where the friction angle remains unknown. We render images
of the simulation under the same camera setup and compare them to the observed
images. By employing Bayesian optimization, we minimize the image loss to
estimate the best-fitting friction angle. Our results demonstrate that friction
angle can be estimated with an error within 2 degrees, highlighting the
effectiveness of inverse analysis through purely visual observations. This
approach offers a promising solution for characterizing granular materials in
real-world scenarios where direct measurement is impractical or impossible.

</details>


### [212] [VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels](https://arxiv.org/abs/2507.09008)
*Xiwei Xuan,Xiaoqi Wang,Wenbin He,Jorge Piazentin Ono,Liang Gou,Kwan-Liu Ma,Liu Ren*

Main category: cs.CV

TL;DR: VISTA is a visual analytics framework designed to improve the quality of FM-generated labels in multi-modal models by integrating multi-phased validation and human expertise.


<details>
  <summary>Details</summary>
Motivation: Existing methods for validating FM-generated labels focus on quantity over quality, lacking comprehensive metrics or human validation for large datasets.

Method: VISTA combines multi-phased data validation strategies with human expertise to identify and correct issues in FM-generated labels.

Result: VISTA's effectiveness is demonstrated through use cases on benchmark datasets and expert reviews, showing improvements in label quality.

Conclusion: VISTA addresses the challenge of validating FM-generated labels, enhancing data quality and downstream model performance.

Abstract: The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)
have facilitated the auto-labeling of large-scale datasets, enhancing model
performance in challenging downstream tasks such as open-vocabulary object
detection and segmentation. However, the quality of FM-generated labels is less
studied as existing approaches focus more on data quantity over quality. This
is because validating large volumes of data without ground truth presents a
considerable challenge in practice. Existing methods typically rely on limited
metrics to identify problematic data, lacking a comprehensive perspective, or
apply human validation to only a small data fraction, failing to address the
full spectrum of potential issues. To overcome these challenges, we introduce
VISTA, a visual analytics framework that improves data quality to enhance the
performance of multi-modal models. Targeting the complex and demanding domain
of open-vocabulary image segmentation, VISTA integrates multi-phased data
validation strategies with human expertise, enabling humans to identify,
understand, and correct hidden issues within FM-generated labels. Through
detailed use cases on two benchmark datasets and expert reviews, we demonstrate
VISTA's effectiveness from both quantitative and qualitative perspectives.

</details>


### [213] [BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis](https://arxiv.org/abs/2507.09036)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Hendrik Möller,Ilhem Isra Mekki,Josef A. Buchner,Anton Schmick,Arianna Pfiffer,Eva Oswald,Lucas Zimmer,Ezequiel de la Rosa,Sarthak Pati,Julian Canisius,Arianna Piffer,Ujjwal Baid,Mahyar Valizadeh,Akis Linardos,Jan C. Peeken,Surprosanna Shit,Felix Steinbauer,Daniel Rueckert,Rolf Heckemann,Spyridon Bakas,Jan Kirschke,Constantin von See,Ivan Ezhov,Marie Piraud,Benedikt Wiestler,Bjoern Menze*

Main category: cs.CV

TL;DR: BrainLesion Suite is a Python toolkit for modular brain lesion image analysis, offering preprocessing, modality synthesis, lesion inpainting, and segmentation tools, adaptable for broader biomedical applications.


<details>
  <summary>Details</summary>
Motivation: To streamline the creation of complex brain lesion image analysis workflows with minimal cognitive effort, leveraging Pythonic principles.

Method: Uses adaptable preprocessing (co-registration, atlas registration, skull-stripping, defacing), BraTS algorithms for modality synthesis and lesion inpainting, and tools like panoptica for segmentation metrics.

Result: Provides a versatile toolkit for brain lesion analysis, extendable to other biomedical image tasks.

Conclusion: BrainLesion Suite simplifies brain lesion image analysis and is adaptable for diverse applications, with resources available on GitHub.

Abstract: BrainLesion Suite is a versatile toolkit for building modular brain lesion
image analysis pipelines in Python. Following Pythonic principles, BrainLesion
Suite is designed to provide a 'brainless' development experience, minimizing
cognitive effort and streamlining the creation of complex workflows for
clinical and scientific practice. At its core is an adaptable preprocessing
module that performs co-registration, atlas registration, and optional
skull-stripping and defacing on arbitrary multi-modal input images. BrainLesion
Suite leverages algorithms from the BraTS challenge to synthesize missing
modalities, inpaint lesions, and generate pathology-specific tumor
segmentations. BrainLesion Suite also enables quantifying segmentation model
performance, with tools such as panoptica to compute lesion-wise metrics.
Although BrainLesion Suite was originally developed for image analysis
pipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,
it can be adapted for other biomedical image analysis applications. The
individual BrainLesion Suite packages and tutorials are accessible on GitHub.

</details>


### [214] [Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?](https://arxiv.org/abs/2507.09052)
*Fang Chen,Alex Villa,Gongbo Liang,Xiaoyi Lu,Meng Tang*

Main category: cs.CV

TL;DR: The paper introduces two contrastive loss functions to improve diversity in tail-class images for class-conditional diffusion models trained on imbalanced data, without affecting head-class fidelity.


<details>
  <summary>Details</summary>
Motivation: Addressing mode collapse and reduced diversity in tail-class images due to imbalanced training data in class-conditional diffusion models.

Method: Two contrastive loss functions: an unsupervised InfoNCE loss for dissimilarity among synthetic images and an MSE loss for aligning conditional and unconditional generation at large timesteps.

Result: Improved diversity for tail classes without compromising head-class performance, outperforming standard DDPM and alternatives on datasets like CIFAR10/100-LT and ImageNetLT.

Conclusion: The proposed contrastive learning framework is effective, easy to implement, and enhances performance for class-imbalanced diffusion models.

Abstract: Training data for class-conditional image synthesis often exhibit a
long-tailed distribution with limited images for tail classes. Such an
imbalance causes mode collapse and reduces the diversity of synthesized images
for tail classes. For class-conditional diffusion models trained on imbalanced
data, we aim to improve the diversity of tail class images without compromising
the fidelity and diversity of head class images. We achieve this by introducing
two deceptively simple but highly effective contrastive loss functions.
Firstly, we employ an unsupervised InfoNCE loss utilizing negative samples to
increase the distance/dissimilarity among synthetic images, particularly for
tail classes. To further enhance the diversity of tail classes, our second loss
is an MSE loss that contrasts class-conditional generation with unconditional
generation at large timesteps. This second loss makes the denoising process
insensitive to class conditions for the initial steps, which enriches tail
classes through knowledge sharing from head classes. Conditional-unconditional
alignment has been shown to enhance the performance of long-tailed GAN. We are
the first to adapt such alignment to diffusion models. We successfully
leveraged contrastive learning for class-imbalanced diffusion models. Our
contrastive learning framework is easy to implement and outperforms standard
DDPM and alternative methods for class-imbalanced diffusion models across
various datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and
ImageNetLT.

</details>


### [215] [Infinite Video Understanding](https://arxiv.org/abs/2507.09068)
*Dell Zhang,Xiangyu Chen,Jixiang Luo,Mengxi Jia,Changzhi Sun,Ruilong Ren,Jingren Liu,Hao Sun,Xuelong Li*

Main category: cs.CV

TL;DR: The paper discusses the challenges in video understanding for long-duration content and proposes 'Infinite Video Understanding' as a future research goal.


<details>
  <summary>Details</summary>
Motivation: Current models struggle with computational and memory constraints for lengthy videos, and maintaining temporal coherence and fine-grained details remains difficult.

Method: The paper suggests focusing on streaming architectures, persistent memory, hierarchical representations, event-centric reasoning, and new evaluation methods.

Result: No specific results are presented, but the paper outlines key challenges and research directions for Infinite Video Understanding.

Conclusion: Infinite Video Understanding is proposed as a transformative goal to drive innovation in multimedia and AI research.

Abstract: The rapid advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have ushered in remarkable progress in video understanding.
However, a fundamental challenge persists: effectively processing and
comprehending video content that extends beyond minutes or hours. While recent
efforts like Video-XL-2 have demonstrated novel architectural solutions for
extreme efficiency, and advancements in positional encoding such as HoPE and
VideoRoPE++ aim to improve spatio-temporal understanding over extensive
contexts, current state-of-the-art models still encounter significant
computational and memory constraints when faced with the sheer volume of visual
tokens from lengthy sequences. Furthermore, maintaining temporal coherence,
tracking complex events, and preserving fine-grained details over extended
periods remain formidable hurdles, despite progress in agentic reasoning
systems like Deep Video Discovery. This position paper posits that a logical,
albeit ambitious, next frontier for multimedia research is Infinite Video
Understanding -- the capability for models to continuously process, understand,
and reason about video data of arbitrary, potentially never-ending duration. We
argue that framing Infinite Video Understanding as a blue-sky research
objective provides a vital north star for the multimedia, and the wider AI,
research communities, driving innovation in areas such as streaming
architectures, persistent memory mechanisms, hierarchical and adaptive
representations, event-centric reasoning, and novel evaluation paradigms.
Drawing inspiration from recent work on long/ultra-long video understanding and
several closely related fields, we outline the core challenges and key research
directions towards achieving this transformative capability.

</details>


### [216] [BlindSight: Harnessing Sparsity for Efficient VLMs](https://arxiv.org/abs/2507.09071)
*Tharun Adithya Srikrishnan,Deval Shah,Steven K. Reinhardt*

Main category: cs.CV

TL;DR: BlindSight reduces FLOPs in VLMs by leveraging sparse attention patterns, achieving 32%-41% efficiency gains with minimal accuracy impact.


<details>
  <summary>Details</summary>
Motivation: VLMs face inefficiency due to expanded prompt lengths and quadratic attention complexity.

Method: Analyzes attention patterns, categorizes sparsity, and applies template-aware masks to optimize inference.

Result: 32%-41% FLOPs reduction with -2%-+2% accuracy change in multi-image benchmarks.

Conclusion: BlindSight offers a training-free solution for efficient VLM inference without significant accuracy loss.

Abstract: Large vision-language models (VLMs) enable the joint processing of text and
images. However, the inclusion of vision data significantly expands the prompt
length. Along with the quadratic complexity of the attention computation, this
results in a longer prefill duration. An approach to mitigate this bottleneck
is to leverage the inherent sparsity in the attention computation. In our
analysis of attention patterns in VLMs, we observe that a substantial portion
of layers exhibit minimal cross-image attention, except through attention-sink
tokens per image. These sparse attention patterns fall into distinct
categories: sink-only, document mask and a hybrid document-sink mask. Based on
this, we propose BlindSight: a training-free approach to optimize VLM inference
using a input template-aware attention sparsity mask. We utilize samples from a
dataset to derive a prompt-agnostic sparsity categorization for every attention
head. We evaluate the proposed technique using VLMs such as Qwen2-VL,
Qwen2.5-VL and Gemma-3. BlindSight results in a 32%-41% reduction in FLOPs on
average with -2%-+2% accuracy compared to the original model in most evaluated
multi-image understanding benchmarks.

</details>


### [217] [From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion](https://arxiv.org/abs/2507.09081)
*Zhenyu Yu,Mohd Yamani Idna Idris,Hua Wang,Pei Wang,Junyi Chen,Kun Wang*

Main category: cs.CV

TL;DR: The paper reviews the evolution of remote sensing inversion methods from physics-based models to AI-driven approaches, highlighting foundation models' advancements and challenges like interpretability and generalization.


<details>
  <summary>Details</summary>
Motivation: To understand the shift from traditional physics-based methods to data-driven and foundation model-based approaches in remote sensing inversion for better ecosystem monitoring and land management.

Method: Systematic review of inversion techniques, comparing physical models (e.g., PROSPECT), machine learning (e.g., deep learning), and foundation models (e.g., SatMAE).

Result: Identifies advancements in self-supervised pretraining and multi-modal integration but notes challenges like domain generalization and uncertainty quantification.

Conclusion: Envisions next-generation foundation models with unified modeling, cross-domain generalization, and improved physical interpretability.

Abstract: Quantitative remote sensing inversion aims to estimate continuous surface
variables-such as biomass, vegetation indices, and evapotranspiration-from
satellite observations, supporting applications in ecosystem monitoring, carbon
accounting, and land management. With the evolution of remote sensing systems
and artificial intelligence, traditional physics-based paradigms are giving way
to data-driven and foundation model (FM)-based approaches. This paper
systematically reviews the methodological evolution of inversion techniques,
from physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods
(e.g., deep learning, multimodal fusion), and further to foundation models
(e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application
scenarios, and limitations of each paradigm, with emphasis on recent FM
advances in self-supervised pretraining, multi-modal integration, and
cross-task adaptation. We also highlight persistent challenges in physical
interpretability, domain generalization, limited supervision, and uncertainty
quantification. Finally, we envision the development of next-generation
foundation models for remote sensing inversion, emphasizing unified modeling
capacity, cross-domain generalization, and physical interpretability.

</details>


### [218] [Taming generative video models for zero-shot optical flow extraction](https://arxiv.org/abs/2507.09082)
*Seungwoo Kim,Khai Loong Aw,Klemen Kotar,Cristobal Eyzaguirre,Wanhee Lee,Yunong Liu,Jared Watrous,Stefan Stojanov,Juan Carlos Niebles,Jiajun Wu,Daniel L. K. Yamins*

Main category: cs.CV

TL;DR: The paper explores zero-shot optical flow extraction from frozen self-supervised video models without fine-tuning, leveraging counterfactual prompting and identifying key model properties for success.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of flow labels and the sim-to-real gap in synthetic datasets, the authors investigate whether pre-trained video models can be prompted to output optical flow without fine-tuning.

Method: The approach uses counterfactual perturbations (KL-tracing) in generative video models, focusing on architectures with distributional prediction, factorized latents, and random-access decoding. The LRAS architecture is highlighted for these properties.

Result: The method outperforms state-of-the-art models on real-world (TAP-Vid DAVIS) and synthetic (TAP-Vid Kubric) datasets without fine-tuning, achieving 16.6% and 4.7% relative improvements, respectively.

Conclusion: Counterfactual prompting of generative video models is a scalable and effective alternative to supervised or photometric-loss methods for high-quality optical flow extraction.

Abstract: Extracting optical flow from videos remains a core computer vision problem.
Motivated by the success of large general-purpose models, we ask whether frozen
self-supervised video models trained only for future frame prediction can be
prompted, without fine-tuning, to output flow. Prior work reading out depth or
illumination from video generators required fine-tuning, which is impractical
for flow where labels are scarce and synthetic datasets suffer from a
sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm,
which can obtain point-wise correspondences by injecting a small tracer
perturbation into a next-frame predictor and tracking its propagation, we
extend this idea to generative video models. We explore several popular
architectures and find that successful zero-shot flow extraction in this manner
is aided by three model properties: (1) distributional prediction of future
frames (avoiding blurry or noisy outputs); (2) factorized latents that treat
each spatio-temporal patch independently; and (3) random-access decoding that
can condition on any subset of future pixels. These properties are uniquely
present in the recent Local Random Access Sequence (LRAS) architecture.
Building on LRAS, we propose KL-tracing: a novel test-time procedure that
injects a localized perturbation into the first frame, rolls out the model one
step, and computes the Kullback-Leibler divergence between perturbed and
unperturbed predictive distributions. Without any flow-specific fine-tuning,
our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS
dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid
Kubric (4.7% relative improvement). Our results indicate that counterfactual
prompting of controllable generative video models is a scalable and effective
alternative to supervised or photometric-loss approaches for high-quality flow.

</details>


### [219] [MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks](https://arxiv.org/abs/2507.09092)
*Ram S Iyer,Narayan S Iyer,Rugmini Ammal P*

Main category: cs.CV

TL;DR: MI CAM is a novel post-hoc visual explanation method for CNNs, using mutual information to weigh feature maps and generate saliency visualizations, validated by counterfactual analysis.


<details>
  <summary>Details</summary>
Motivation: To understand CNN mechanisms and provide unbiased, causal interpretations for model inferences in critical applications like healthcare and automated power plants.

Method: MI CAM weighs feature maps by their mutual information with the input image, combining them linearly to produce saliency visualizations. It includes counterfactual analysis for validation.

Result: MI CAM performs comparably to state-of-the-art methods and excels in qualitative and quantitative measures.

Conclusion: MI CAM offers effective visual performance and unbiased justifications for CNN inferences, validated by counterfactual analysis.

Abstract: With the intervention of machine vision in our crucial day to day necessities
including healthcare and automated power plants, attention has been drawn to
the internal mechanisms of convolutional neural networks, and the reason why
the network provides specific inferences. This paper proposes a novel post-hoc
visual explanation method called MI CAM based on activation mapping. Differing
from previous class activation mapping based approaches, MI CAM produces
saliency visualizations by weighing each feature map through its mutual
information with the input image and the final result is generated by a linear
combination of weights and activation maps. It also adheres to producing causal
interpretations as validated with the help of counterfactual analysis. We aim
to exhibit the visual performance and unbiased justifications for the model
inferencing procedure achieved by MI CAM. Our approach works at par with all
state-of-the-art methods but particularly outperforms some in terms of
qualitative and quantitative measures. The implementation of proposed method
can be found on https://anonymous.4open.science/r/MI-CAM-4D27

</details>


### [220] [RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze](https://arxiv.org/abs/2507.09097)
*Yunsoo Kim,Jinge Wu,Honghan Wu*

Main category: cs.CV

TL;DR: RadEyeVideo integrates radiologists' eye-gaze videos into LVLMs, improving CXR report generation and disease diagnosis by capturing temporal and spatial gaze dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook the sequential order of radiologists' eye movements, missing valuable insights. RadEyeVideo aims to leverage this for better human-computer interaction in medical tasks.

Method: Proposes RadEyeVideo, which uses eye-fixation data as video sequences to capture gaze dynamics. Evaluated with three LVLMs for CXR report generation and disease diagnosis.

Result: Performance improved by up to 24.6% in report generation and 15.2% on average for both tasks. RadEyeVideo even outperformed specialized medical LVLMs like MAIRA-2 and CheXagent.

Conclusion: Integrating domain expert knowledge (eye-gaze) with LVLMs enhances general-domain models in clinical tasks, showcasing a scalable human-centered approach for medical analytics.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated promising performance
in chest X-ray (CXR) analysis. To enhance human-computer interaction, several
studies have incorporated radiologists' eye gaze, typically through heatmaps or
textual prompts. However, these methods often overlook the sequential order of
eye movements, which could provide valuable insights by highlighting both the
areas of interest and the order in which they are examined. In this work, we
propose a novel approach called RadEyeVideo that integrates radiologists'
eye-fixation data as a video sequence, capturing both the temporal and spatial
dynamics of their gaze. We evaluate this method in CXR report generation and
disease diagnosis using three general-domain, open-source LVLMs with video
input capabilities. When prompted with eye-gaze videos, model performance
improves by up to 24.6% in the report generation task and on average 15.2% for
both tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an
open-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs
such as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work
highlights that domain expert's knowledge (eye-gaze information in this case),
when effectively integrated with LVLMs, can significantly enhance
general-domain models' capabilities in clinical tasks. RadEyeVideo is a step
toward a scalable human-centered approach of utilizing LVLMs in medical image
analytics.

</details>


### [221] [Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning](https://arxiv.org/abs/2507.09102)
*Yiyang Chen,Shanshan Zhao,Lunhao Duan,Changxing Ding,Dacheng Tao*

Main category: cs.CV

TL;DR: PointSD leverages Stable Diffusion (SD) for 3D self-supervised learning by aligning 3D point cloud features with SD features, enhancing performance on downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Existing 3D diffusion models are limited by small datasets; leveraging large-scale text-to-image models like SD could overcome this.

Method: PointSD replaces SD's text encoder with a 3D encoder, trains a point-to-image diffusion model, and aligns 3D backbone features with SD features.

Result: Experiments show SD enhances 3D self-supervised learning, improving performance on downstream point cloud tasks.

Conclusion: PointSD successfully bridges 2D and 3D representation learning, demonstrating the potential of leveraging large-scale 2D models for 3D tasks.

Abstract: Diffusion-based models, widely used in text-to-image generation, have proven
effective in 2D representation learning. Recently, this framework has been
extended to 3D self-supervised learning by constructing a conditional point
generator for enhancing 3D representations. However, its performance remains
constrained by the 3D diffusion model, which is trained on the available 3D
datasets with limited size. We hypothesize that the robust capabilities of
text-to-image diffusion models, particularly Stable Diffusion (SD), which is
trained on large-scale datasets, can help overcome these limitations. To
investigate this hypothesis, we propose PointSD, a framework that leverages the
SD model for 3D self-supervised learning. By replacing the SD model's text
encoder with a 3D encoder, we train a point-to-image diffusion model that
allows point clouds to guide the denoising of rendered noisy images. With the
trained point-to-image diffusion model, we use noise-free images as the input
and point clouds as the condition to extract SD features. Next, we train a 3D
backbone by aligning its features with these SD features, thereby facilitating
direct semantic learning. Comprehensive experiments on downstream point cloud
tasks and ablation studies demonstrate that the SD model can enhance point
cloud self-supervised learning. Code is publicly available at
https://github.com/wdttt/PointSD.

</details>


### [222] [Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production](https://arxiv.org/abs/2507.09105)
*Maoxiao Ye,Xinfeng Ye,Mano Manoharan*

Main category: cs.CV

TL;DR: A hybrid approach combining autoregressive and diffusion models is introduced for Sign Language Production (SLP), addressing error accumulation and real-time limitations. It includes a Multi-Scale Pose Representation module and Confidence-Aware Causal Attention for improved accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Earlier SLP models faced issues like error accumulation during inference and limited real-time applicability due to autoregressive or diffusion-based methods.

Method: A hybrid model integrates autoregressive and diffusion techniques, with a Multi-Scale Pose Representation module and Confidence-Aware Causal Attention for dynamic pose generation.

Result: The method outperforms on PHOENIX14T and How2Sign datasets in generation quality and real-time efficiency.

Conclusion: The hybrid approach effectively combines the strengths of autoregressive and diffusion models, enhancing SLP performance in accuracy and real-time streaming.

Abstract: Earlier Sign Language Production (SLP) models typically relied on
autoregressive methods that generate output tokens one by one, which inherently
provide temporal alignment. Although techniques like Teacher Forcing can
prevent model collapse during training, they still cannot solve the problem of
error accumulation during inference, since ground truth is unavailable at that
stage. In contrast, more recent approaches based on diffusion models leverage
step-by-step denoising to enable high-quality generation. However, the
iterative nature of these models and the requirement to denoise entire
sequences limit their applicability in real-time tasks like SLP. To address it,
we apply a hybrid approach combining autoregressive and diffusion models to SLP
for the first time, leveraging the strengths of both models in sequential
dependency modeling and output refinement. To capture fine-grained body
movements, we design a Multi-Scale Pose Representation module that separately
extracts detailed features from distinct articulators and integrates them via a
Multi-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal
Attention mechanism that utilizes joint-level confidence scores to dynamically
guide the pose generation process, improving accuracy and robustness. Extensive
experiments on the PHOENIX14T and How2Sign datasets demonstrate the
effectiveness of our method in both generation quality and real-time streaming
efficiency.

</details>


### [223] [RoHOI: Robustness Benchmark for Human-Object Interaction Detection](https://arxiv.org/abs/2507.09111)
*Di Wen,Kunyu Peng,Kailun Yang,Yufan Chen,Ruiping Liu,Junwei Zheng,Alina Roitberg,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: The paper introduces RoHOI, a robustness benchmark for HOI detection, addressing model degradation in real-world conditions. It proposes SAMPL, a learning strategy to enhance robustness, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current HOI detection models fail under real-world corruptions like environmental variability, occlusion, and noise, necessitating a robustness benchmark and improved methods.

Method: The authors create RoHOI, a benchmark with 20 corruption types, and propose SAMPL, a Semantic-Aware Masking-based Progressive Learning strategy for robust feature learning.

Result: Experiments show SAMPL outperforms state-of-the-art methods, improving robustness in HOI detection.

Conclusion: The paper sets a new standard for robust HOI detection with RoHOI and SAMPL, with benchmarks and code made publicly available.

Abstract: Human-Object Interaction (HOI) detection is crucial for robot-human
assistance, enabling context-aware support. However, models trained on clean
datasets degrade in real-world conditions due to unforeseen corruptions,
leading to inaccurate prediction. To address this, we introduce the first
robustness benchmark for HOI detection, evaluating model resilience under
diverse challenges. Despite advances, current models struggle with
environmental variability, occlusion, and noise. Our benchmark, RoHOI, includes
20 corruption types based on HICO-DET and V-COCO datasets and a new
robustness-focused metric. We systematically analyze existing models in the
related field, revealing significant performance drops under corruptions. To
improve robustness, we propose a Semantic-Aware Masking-based Progressive
Learning (SAMPL) strategy to guide the model to be optimized based on holistic
and partial cues, dynamically adjusting the model's optimization to enhance
robust feature learning. Extensive experiments show our approach outperforms
state-of-the-art methods, setting a new standard for robust HOI detection.
Benchmarks, datasets, and code will be made publicly available at
https://github.com/Kratos-Wen/RoHOI.

</details>


### [224] [Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning](https://arxiv.org/abs/2507.09118)
*Linlan Huang,Xusheng Cao,Haori Lu,Yifan Meng,Fei Yang,Xialei Liu*

Main category: cs.CV

TL;DR: The paper proposes MG-CLIP, a method leveraging the modality gap in CLIP for continual learning, improving performance without extra replay data.


<details>
  <summary>Details</summary>
Motivation: Existing works ignore the modality gap in CLIP, which is crucial for its generalization. This paper analyzes and utilizes this gap for continual learning.

Method: MG-CLIP preserves and compensates the modality gap to mitigate forgetting and enhance new data learning, tested on benchmarks.

Result: MG-CLIP outperforms existing methods in class-incremental learning without needing additional replay data.

Conclusion: The modality gap is key for continual learning in CLIP, and MG-CLIP effectively leverages it for improved performance.

Abstract: Continual learning aims to enable models to learn sequentially from
continuously incoming data while retaining performance on previously learned
tasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting
strong capabilities across various downstream tasks, there has been growing
interest in leveraging CLIP for continual learning in such scenarios. Most
existing works overlook the inherent modality gap in CLIP, a key factor in its
generalization and adaptability. In this paper, we analyze the variations in
the modality gap during the fine-tuning of vision-language pre-trained models.
Our observations reveal that the modality gap effectively reflects the extent
to which pre-trained knowledge is preserved. Based on these insights, we
propose a simple yet effective method, MG-CLIP, that improves CLIP's
performance in class-incremental learning. Our approach leverages modality gap
preservation to mitigate forgetting and modality gap compensation to enhance
the capacity for new data, introducing a novel modality-gap-based perspective
for continual learning. Extensive experiments on multiple benchmarks
demonstrate that our method outperforms existing approaches without requiring
additional replay data. Our code is available at
https://github.com/linlany/MindtheGap.

</details>


### [225] [SnapMoGen: Human Motion Generation from Expressive Texts](https://arxiv.org/abs/2507.09122)
*Chuan Guo,Inwoo Hwang,Jian Wang,Bing Zhou*

Main category: cs.CV

TL;DR: SnapMoGen introduces a high-quality text-motion dataset with detailed annotations and proposes MoMask++, a state-of-the-art model for text-to-motion generation.


<details>
  <summary>Details</summary>
Motivation: Current text-to-motion methods are limited by short or general prompts due to dataset constraints, hindering fine-grained controllability and generalization.

Method: The paper presents SnapMoGen, a dataset with 20K motion clips and 122K detailed descriptions, and MoMask++, a model using multi-scale token sequences and a generative masked transformer.

Result: MoMask++ achieves top performance on HumanML3D and SnapMoGen benchmarks and handles casual user prompts via LLM reformatting.

Conclusion: SnapMoGen and MoMask++ advance text-to-motion generation by enabling long-term motion synthesis and improved controllability.

Abstract: Text-to-motion generation has experienced remarkable progress in recent
years. However, current approaches remain limited to synthesizing motion from
short or general text prompts, primarily due to dataset constraints. This
limitation undermines fine-grained controllability and generalization to unseen
prompts. In this paper, we introduce SnapMoGen, a new text-motion dataset
featuring high-quality motion capture data paired with accurate, expressive
textual annotations. The dataset comprises 20K motion clips totaling 44 hours,
accompanied by 122K detailed textual descriptions averaging 48 words per
description (vs. 12 words of HumanML3D). Importantly, these motion clips
preserve original temporal continuity as they were in long sequences,
facilitating research in long-term motion generation and blending. We also
improve upon previous generative masked modeling approaches. Our model,
MoMask++, transforms motion into multi-scale token sequences that better
exploit the token capacity, and learns to generate all tokens using a single
generative masked transformer. MoMask++ achieves state-of-the-art performance
on both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the
ability to process casual user prompts by employing an LLM to reformat inputs
to align with the expressivity and narration style of SnapMoGen. Project
webpage: https://snap-research.github.io/SnapMoGen/

</details>


### [226] [PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment](https://arxiv.org/abs/2507.09139)
*Dewen Zhang,Tahir Hussain,Wangpeng An,Hayaru Shouno*

Main category: cs.CV

TL;DR: PoseLLM introduces a nonlinear MLP vision-language connector for pose estimation, outperforming LocLLM by +0.4 AP on COCO and maintaining zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Traditional pose estimation methods struggle with novel poses or unseen keypoints. LocLLM's linear projector lacks complex spatial-textual interactions.

Method: PoseLLM replaces LocLLM's linear projector with a lightweight two-layer MLP with GELU activation for better cross-modal feature fusion.

Result: PoseLLM achieves 77.8 AP on COCO, surpassing LocLLM by +0.4 AP, and shows strong zero-shot generalization on Human-Art and MPII.

Conclusion: A nonlinear connector improves localization accuracy without compromising generalization, advancing language-guided pose estimation.

Abstract: Human pose estimation traditionally relies on architectures that encode
keypoint priors, limiting their generalization to novel poses or unseen
keypoints. Recent language-guided approaches like LocLLM reformulate keypoint
localization as a vision-language task, enabling zero-shot generalization
through textual descriptions. However, LocLLM's linear projector fails to
capture complex spatial-textual interactions critical for high-precision
localization. To address this, we propose PoseLLM, the first Large Language
Model (LLM)-based pose estimation framework that replaces the linear projector
with a nonlinear MLP vision-language connector. This lightweight two-layer MLP
with GELU activation enables hierarchical cross-modal feature transformation,
enhancing the fusion of visual patches and textual keypoint descriptions.
Trained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO
validation set, outperforming LocLLM by +0.4 AP, while maintaining strong
zero-shot generalization on Human-Art and MPII. Our work demonstrates that a
simple yet powerful nonlinear connector significantly boosts localization
accuracy without sacrificing generalization, advancing the state-of-the-art in
language-guided pose estimation. Code is available at
https://github.com/Ody-trek/PoseLLM.

</details>


### [227] [$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting](https://arxiv.org/abs/2507.09144)
*Zhimin Liao,Ping Wei,Ruijie Zhang,Shuaijia Chen,Haoxuan Wang,Ziyang Ren*

Main category: cs.CV

TL;DR: $I^{2}$-World is an efficient 4D occupancy forecasting framework for 3D scenes, using dual tokenizers (intra-scene and inter-scene) and an encoder-decoder architecture to achieve state-of-the-art performance with high computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of efficiently tokenizing complex 3D scenes for forecasting and generating unseen scenarios in autonomous driving systems.

Method: Decouples scene tokenization into intra-scene (multi-scale residual quantization) and inter-scene (residual aggregation of temporal dependencies) tokenizers, using an encoder-decoder architecture for spatial context and temporal consistency.

Result: Outperforms existing methods by 25.1% in mIoU and 36.9% in IoU for 4D occupancy forecasting, with 2.9 GB training memory and real-time inference at 37.0 FPS.

Conclusion: $I^{2}$-World offers a compact, efficient, and high-performance solution for 4D occupancy forecasting, suitable for real-world applications like autonomous driving.

Abstract: Forecasting the evolution of 3D scenes and generating unseen scenarios via
occupancy-based world models offers substantial potential for addressing corner
cases in autonomous driving systems. While tokenization has revolutionized
image and video generation, efficiently tokenizing complex 3D scenes remains a
critical challenge for 3D world models. To address this, we propose
$I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method
decouples scene tokenization into intra-scene and inter-scene tokenizers. The
intra-scene tokenizer employs a multi-scale residual quantization strategy to
hierarchically compress 3D scenes while preserving spatial details. The
inter-scene tokenizer residually aggregates temporal dependencies across
timesteps. This dual design preserves the compactness of 3D tokenizers while
retaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only
GPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder
architecture. The encoder aggregates spatial context from the current scene and
predicts a transformation matrix to enable high-level control over scene
generation. The decoder, conditioned on this matrix and historical tokens,
ensures temporal consistency during generation. Experiments demonstrate that
$I^{2}$-World achieves state-of-the-art performance, outperforming existing
methods by 25.1\% in mIoU and 36.9\% in IoU for 4D occupancy forecasting while
exhibiting exceptional computational efficiency: it requires merely 2.9 GB of
training memory and achieves real-time inference at 37.0 FPS. Our code is
available on https://github.com/lzzzzzm/II-World.

</details>


### [228] [Stable Score Distillation](https://arxiv.org/abs/2507.09168)
*Haiming Zhu,Yangyang Xu,Chenshu Xu,Tingrui Shen,Wenxi Liu,Yong Du,Jun Yu,Shengfeng He*

Main category: cs.CV

TL;DR: SSD improves text-guided image and 3D editing by stabilizing the process, enhancing alignment, and boosting editing strength without complex auxiliary structures.


<details>
  <summary>Details</summary>
Motivation: Existing methods like Delta Denoising Score suffer from instability, poor spatial control, and weak editing due to conflicting signals from auxiliary structures.

Method: SSD anchors a classifier to the source prompt, uses Classifier-Free Guidance for cross-prompt alignment, and adds a null-text branch for stability. It also includes a prompt enhancement branch for stronger edits.

Result: SSD achieves state-of-the-art performance in 2D and 3D editing tasks, including NeRF and style transformations, with faster convergence and reduced complexity.

Conclusion: SSD provides a robust, efficient solution for text-guided editing by improving stability, alignment, and editing strength.

Abstract: Text-guided image and 3D editing have advanced with diffusion-based models,
yet methods like Delta Denoising Score often struggle with stability, spatial
control, and editing strength. These limitations stem from reliance on complex
auxiliary structures, which introduce conflicting optimization signals and
restrict precise, localized edits. We introduce Stable Score Distillation
(SSD), a streamlined framework that enhances stability and alignment in the
editing process by anchoring a single classifier to the source prompt.
Specifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves
cross-prompt alignment, and introduces a constant term null-text branch to
stabilize the optimization process. This approach preserves the original
content's structure and ensures that editing trajectories are closely aligned
with the source prompt, enabling smooth, prompt-specific modifications while
maintaining coherence in surrounding regions. Additionally, SSD incorporates a
prompt enhancement branch to boost editing strength, particularly for style
transformations. Our method achieves state-of-the-art results in 2D and 3D
editing tasks, including NeRF and text-driven style edits, with faster
convergence and reduced complexity, providing a robust and efficient solution
for text-guided editing.

</details>


### [229] [Learning and Transferring Better with Depth Information in Visual Reinforcement Learning](https://arxiv.org/abs/2507.09180)
*Zichun Xu,Yuntao Li,Zhaomin Wang,Lei Zhuang,Guocai Yang,Jingdong Zhao*

Main category: cs.CV

TL;DR: A vision transformer-based backbone fuses RGB and depth data for better generalization, using contrastive learning and curriculum learning for sim2real transfer.


<details>
  <summary>Details</summary>
Motivation: Depth information is robust to scene variations and carries 3D details, making it valuable for enhancing generalization in vision tasks.

Method: Separate CNN stems process RGB and depth data, fused by a scalable vision transformer. Contrastive learning with masked tokens improves sample efficiency, and curriculum learning aids sim2real transfer.

Result: The method effectively combines RGB and depth modalities, improving generalization and efficiency in reinforcement learning.

Conclusion: The proposed approach successfully integrates multimodal data and learning strategies for robust visual representation and sim2real adaptation.

Abstract: Depth information is robust to scene appearance variations and inherently
carries 3D spatial details. In this paper, a visual backbone based on the
vision transformer is proposed to fuse RGB and depth modalities for enhancing
generalization. Different modalities are first processed by separate CNN stems,
and the combined convolutional features are delivered to the scalable vision
transformer to obtain visual representations. Moreover, a contrastive
unsupervised learning scheme is designed with masked and unmasked tokens to
accelerate the sample efficiency during the reinforcement learning progress.
For sim2real transfer, a flexible curriculum learning schedule is developed to
deploy domain randomization over training processes.

</details>


### [230] [Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning](https://arxiv.org/abs/2507.09183)
*Yongwei Jiang,Yixiong Zou,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: The paper explores prompt pool methods in Few-Shot Class-Incremental Learning (FSCIL), identifies token-dimension saturation as a cause of performance degradation, and proposes LGSP-Prompt for improved spatial prompting.


<details>
  <summary>Details</summary>
Motivation: To address the dual challenges of data scarcity and incremental learning in FSCIL, and to investigate the limitations of pool-based prompting methods in this setting.

Method: Introduces LGSP-Prompt, which shifts prompt learning to the spatial dimension by combining local and global features, and uses dynamic prompt pools.

Result: LGSP-Prompt achieves state-of-the-art performance on FSCIL benchmarks, excelling in knowledge preservation and incremental learning.

Conclusion: The proposed LGSP-Prompt effectively mitigates token-dimension saturation and outperforms existing methods in FSCIL tasks.

Abstract: Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data
scarcity and incremental learning in real-world scenarios. While pool-based
prompting methods have demonstrated success in traditional incremental
learning, their effectiveness in FSCIL settings remains unexplored. This paper
presents the first study of current prompt pool methods in FSCIL tasks,
revealing an unanticipated performance degradation in incremental sessions.
Through comprehensive analysis, we identify that this phenomenon stems from
token-dimension saturation: with limited data, excessive prompts compete for
task-relevant information, leading to model overfitting. Based on this finding,
we propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively
shifts pool-based prompt learning from the token dimension to the spatial
dimension. LGSP-Prompt generates spatial prompts by synergistically combining
local spatial features and global frequency-domain representations to highlight
key patterns in input images. We construct two spatial prompt pools enabling
dynamic prompt selection to maintain acquired knowledge while effectively
learning novel sessions. Extensive experiments demonstrate that our approach
achieves state-of-the-art performance across multiple FSCIL benchmarks, showing
significant advantages in both base knowledge preservation and incremental
learning. Our implementation is available at
https://github.com/Jywsuperman/LGSP.

</details>


### [231] [MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2507.09184)
*Qiyan Zhao,Xiaofeng Zhang,Yiheng Li,Yun Xing,Xiaosong Yuan,Feilong Tang,Sinan Fan,Xuhang Chen,Xuyao Zhang,Dahan Wang*

Main category: cs.CV

TL;DR: The paper addresses hallucinations in Large Vision Language Models (LVLMs) caused by misaligned multimodal features, particularly due to long-term decay in Rotary Position Encoding (RoPE). It introduces MCA-LLaVA, a method using Manhattan distance to improve spatial alignment and reduce bias.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LVLMs stem from misaligned multimodal features, worsened by RoPE's long-term decay, which biases perception of image tokens.

Method: Proposes MCA-LLaVA, extending RoPE's decay to 2D spatial decay using Manhattan distance, integrating sequence order and spatial position.

Result: MCA-LLaVA improves multimodal alignment, reducing hallucinations and performing well on benchmarks.

Conclusion: MCA-LLaVA effectively mitigates image alignment bias, enhancing LVLM performance and reducing hallucinations.

Abstract: Hallucinations pose a significant challenge in Large Vision Language Models
(LVLMs), with misalignment between multimodal features identified as a key
contributing factor. This paper reveals the negative impact of the long-term
decay in Rotary Position Encoding (RoPE), used for positional modeling in
LVLMs, on multimodal alignment. Concretely, under long-term decay, instruction
tokens exhibit uneven perception of image tokens located at different positions
within the two-dimensional space: prioritizing image tokens from the
bottom-right region since in the one-dimensional sequence, these tokens are
positionally closer to the instruction tokens. This biased perception leads to
insufficient image-instruction interaction and suboptimal multimodal alignment.
We refer to this phenomenon as image alignment bias. To enhance instruction's
perception of image tokens at different spatial locations, we propose
MCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a
two-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the
one-dimensional sequence order and two-dimensional spatial position of image
tokens for positional modeling, mitigating hallucinations by alleviating image
alignment bias. Experimental results of MCA-LLaVA across various hallucination
and general benchmarks demonstrate its effectiveness and generality. The code
can be accessed in https://github.com/ErikZ719/MCA-LLaVA.

</details>


### [232] [THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage](https://arxiv.org/abs/2507.09200)
*Trong-Thuan Nguyen,Pha Nguyen,Jackson Cothren,Alper Yilmaz,Minh-Triet Tran,Khoa Luu*

Main category: cs.CV

TL;DR: THYME integrates hierarchical feature aggregation and cyclic temporal refinement for dynamic scene graph generation, outperforming state-of-the-art methods on ASPIRe and AeroEye-v1.0 datasets.


<details>
  <summary>Details</summary>
Motivation: Address fragmented representations in video scene graph generation by capturing fine-grained spatial details and long-range temporal dependencies.

Method: Temporal Hierarchical Cyclic Scene Graph (THYME) combines hierarchical feature aggregation with cyclic temporal refinement.

Result: THYME outperforms existing methods, improving scene understanding in ground-view and aerial scenarios.

Conclusion: THYME and AeroEye-v1.0 dataset advance dynamic scene graph generation, offering robust solutions for video applications.

Abstract: The rapid proliferation of video in applications such as autonomous driving,
surveillance, and sports analytics necessitates robust methods for dynamic
scene understanding. Despite advances in static scene graph generation and
early attempts at video scene graph generation, previous methods often suffer
from fragmented representations, failing to capture fine-grained spatial
details and long-range temporal dependencies simultaneously. To address these
limitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME)
approach, which synergistically integrates hierarchical feature aggregation
with cyclic temporal refinement to address these limitations. In particular,
THYME effectively models multi-scale spatial context and enforces temporal
consistency across frames, yielding more accurate and coherent scene graphs. In
addition, we present AeroEye-v1.0, a novel aerial video dataset enriched with
five types of interactivity that overcome the constraints of existing datasets
and provide a comprehensive benchmark for dynamic scene graph generation.
Empirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that
the proposed THYME approach outperforms state-of-the-art methods, offering
improved scene understanding in ground-view and aerial scenarios.

</details>


### [233] [Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves](https://arxiv.org/abs/2507.09207)
*Alexander C. Ogren,Berthy T. Feng,Jihoon Ahn,Katherine L. Bouman,Chiara Daraio*

Main category: cs.CV

TL;DR: A method to infer material thickness and stiffness from surface wave videos using dispersion relation extraction and physics-based optimization.


<details>
  <summary>Details</summary>
Motivation: Wave propagation on surfaces reveals subsurface properties, enabling non-invasive health monitoring and human-computer interaction applications.

Method: Extract dispersion relation from video, then solve physics-based optimization to estimate thickness and stiffness.

Result: Validated on simulated and real data, showing strong agreement with ground-truth measurements.

Conclusion: Proof-of-concept for at-home health monitoring and broader applications in fields like human-computer interaction.

Abstract: Wave propagation on the surface of a material contains information about
physical properties beneath its surface. We propose a method for inferring the
thickness and stiffness of a structure from just a video of waves on its
surface. Our method works by extracting a dispersion relation from the video
and then solving a physics-based optimization problem to find the best-fitting
thickness and stiffness parameters. We validate our method on both simulated
and real data, in both cases showing strong agreement with ground-truth
measurements. Our technique provides a proof-of-concept for at-home health
monitoring of medically-informative tissue properties, and it is further
applicable to fields such as human-computer interaction.

</details>


### [234] [Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models](https://arxiv.org/abs/2507.09209)
*Xiao Liang,Di Wang,Zhicheng Jiao,Ronghan Li,Pengfei Yang,Quan Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: The paper proposes Expert-CFG, an expert-in-the-loop framework to align Medical Vision Language Models (MedVLMs) with clinical expertise without additional training, improving accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: Current MedVLMs have probabilistic uncertainties and produce unverified responses, which are problematic in medical applications. Existing methods are costly and lack clinical alignment.

Method: Expert-CFG uses uncertainty estimation to identify unreliable outputs, retrieves references for expert input, and applies classifier-free guidance to refine token embeddings.

Result: Evaluations show Expert-CFG outperforms state-of-the-art models with fewer parameters and limited expert annotations.

Conclusion: Expert-CFG is feasible for resource-limited clinical settings, offering improved alignment with clinical expertise.

Abstract: The rapid advancements in Vision Language Models (VLMs) have prompted the
development of multi-modal medical assistant systems. Despite this progress,
current models still have inherent probabilistic uncertainties, often producing
erroneous or unverified responses-an issue with serious implications in medical
applications. Existing methods aim to enhance the performance of Medical Vision
Language Model (MedVLM) by adjusting model structure, fine-tuning with
high-quality data, or through preference fine-tuning. However, these
training-dependent strategies are costly and still lack sufficient alignment
with clinical expertise. To address these issues, we propose an
expert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance
(Expert-CFG) to align MedVLM with clinical expertise without additional
training. This framework introduces an uncertainty estimation strategy to
identify unreliable outputs. It then retrieves relevant references to assist
experts in highlighting key terms and applies classifier-free guidance to
refine the token embeddings of MedVLM, ensuring that the adjusted outputs are
correct and align with expert highlights. Evaluations across three medical
visual question answering benchmarks demonstrate that the proposed Expert-CFG,
with 4.2B parameters and limited expert annotations, outperforms
state-of-the-art models with 13B parameters. The results demonstrate the
feasibility of deploying such a system in resource-limited settings for
clinical use.

</details>


### [235] [Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline](https://arxiv.org/abs/2507.09214)
*Shiyi Mu,Zichong Gu,Hanqi Lyu,Yilin Gao,Shugong Xu*

Main category: cs.CV

TL;DR: The paper introduces S3AD, a stereo-based 3D anomaly detection algorithm, and KITTI-AR datasets to enhance generalization in 3D detection for autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Address misdetection of rare anomalies in 3D detection models by improving generalization and anomaly filtering.

Method: Proposes S3AD with decoupled 2D/3D training and anomaly scoring, and synthesizes KITTI-AR datasets for evaluation.

Result: Achieves target-level anomaly scoring and validates performance using KITTI-AR datasets.

Conclusion: S3AD and KITTI-AR effectively enhance 3D anomaly detection generalization and evaluation.

Abstract: 3D detection technology is widely used in the field of autonomous driving,
with its application scenarios gradually expanding from enclosed highways to
open conventional roads. For rare anomaly categories that appear on the road,
3D detection models trained on closed sets often misdetect or fail to detect
anomaly objects. To address this risk, it is necessary to enhance the
generalization ability of 3D detection models for targets of arbitrary shapes
and to possess the capability to filter out anomalies. The generalization of 3D
detection is limited by two factors: the coupled training of 2D and 3D, and the
insufficient diversity in the scale distribution of training samples. This
paper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm,
which decouples the training strategy of 3D and 2D to release the
generalization ability for arbitrary 3D foreground detection, and proposes an
anomaly scoring algorithm based on foreground confidence prediction, achieving
target-level anomaly scoring. In order to further verify and enhance the
generalization of anomaly detection, we use a 3D rendering method to synthesize
two augmented reality binocular stereo 3D detection datasets which named
KITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k
pairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories
as extra training data to address the sparse sample distribution issue.
Additionally, 58 rare categories form the KITTI-AR-OoD subset, which are not
used in training to simulate zero-shot scenarios in real-world settings, solely
for evaluating 3D anomaly detection. Finally, the performance of the algorithm
and the dataset is verified in the experiments. (Code and dataset can be
obtained at https://github.com/xxxx/xxx).

</details>


### [236] [360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models](https://arxiv.org/abs/2507.09216)
*Jingguo Liu,Han Yu,Shigang Li,Jianfeng Li*

Main category: cs.CV

TL;DR: A novel spherical sampling method for panoramic images is introduced to leverage existing 2D pre-trained models, addressing distortion issues and improving performance in tasks like segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing 2D pre-trained models struggle with panoramic image distortions, limiting their effectiveness in such tasks due to lack of large-scale datasets.

Method: Proposes spherical discrete sampling based on pre-trained model weights to mitigate distortions and uses the method for panoramic image segmentation with channel attention masks.

Result: Achieves favorable initial training values and commendable results on the Stanford2D3D indoor dataset.

Conclusion: The spherical sampling method effectively bridges the gap between 2D pre-trained models and panoramic image tasks, enhancing performance.

Abstract: Due to the current lack of large-scale datasets at the million-scale level,
tasks involving panoramic images predominantly rely on existing two-dimensional
pre-trained image benchmark models as backbone networks. However, these
networks are not equipped to recognize the distortions and discontinuities
inherent in panoramic images, which adversely affects their performance in such
tasks. In this paper, we introduce a novel spherical sampling method for
panoramic images that enables the direct utilization of existing pre-trained
models developed for two-dimensional images. Our method employs spherical
discrete sampling based on the weights of the pre-trained models, effectively
mitigating distortions while achieving favorable initial training values.
Additionally, we apply the proposed sampling method to panoramic image
segmentation, utilizing features obtained from the spherical model as masks for
specific channel attentions, which yields commendable results on commonly used
indoor datasets, Stanford2D3D.

</details>


### [237] [Online Long-term Point Tracking in the Foundation Model Era](https://arxiv.org/abs/2507.09217)
*Görkay Aydemir*

Main category: cs.CV

TL;DR: The paper introduces Track-On, a transformer-based model for online long-term point tracking, achieving state-of-the-art results without future frame access.


<details>
  <summary>Details</summary>
Motivation: Real-world applications like streaming video and embodied AI require online point tracking, but existing methods rely on offline processing with future frames.

Method: Evaluates visual foundation models for spatial features and introduces Track-On, a transformer-based model treating points as queries for sequential frame processing.

Result: Track-On achieves state-of-the-art performance on seven benchmarks, proving long-term tracking feasibility without future access.

Conclusion: Track-On demonstrates robust online point tracking by integrating memory for appearance and context propagation, addressing the limitations of offline methods.

Abstract: Point tracking aims to identify the same physical point across video frames
and serves as a geometry-aware representation of motion. This representation
supports a wide range of applications, from robotics to augmented reality, by
enabling accurate modeling of dynamic environments. Most existing long-term
tracking approaches operate in an offline setting, where future frames are
available to refine predictions and recover from occlusions. However,
real-world scenarios often demand online predictions: the model must operate
causally, using only current and past frames. This constraint is critical in
streaming video and embodied AI, where decisions must be made immediately based
on past observations. Under such constraints, viewpoint invariance becomes
essential. Visual foundation models, trained on diverse large-scale datasets,
offer the potential for robust geometric representations. While they lack
temporal reasoning on their own, they can be integrated into tracking pipelines
to enrich spatial features. In this thesis, we address the problem of long-term
point tracking in an online setting, where frames are processed sequentially
without access to future information or sliding windows. We begin by evaluating
the suitability of visual foundation models for this task and find that they
can serve as useful initializations and be integrated into tracking pipelines.
However, to enable long-term tracking in an online setting, a dedicated design
is still required. In particular, maintaining coherence over time in this
causal regime requires memory to propagate appearance and context across
frames. To address this, we introduce Track-On, a transformer-based model that
treats each tracked point as a query and processes video frames one at a time.
Track-On sets a new state of the art across seven public benchmarks,
demonstrating the feasibility of long-term tracking without future access.

</details>


### [238] [Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift](https://arxiv.org/abs/2507.09222)
*Behraj Khan,Tahir Syed*

Main category: cs.CV

TL;DR: StaRFM is a unified framework addressing distribution shift and confidence misalignment in foundation models like CLIP and SAM, improving performance across vision and medical tasks.


<details>
  <summary>Details</summary>
Motivation: Foundation models face challenges like distribution shift and confidence misalignment, which hinder deployment. Existing solutions are domain-specific, lacking a unified approach.

Method: StaRFM introduces Fisher information penalty (FIP) for covariate shift reduction and confidence misalignment penalty (CMP) for uncertainty calibration, both theoretically grounded.

Result: StaRFM improves accuracy (+3.5%), reduces ECE (28%), achieves 84.7% DSC in medical tasks, and lowers cross-domain gaps by 40%.

Conclusion: StaRFM is a plug-and-play solution for foundation models, offering consistent performance gains across diverse tasks.

Abstract: Foundation models like CLIP and SAM have transformed computer vision and
medical imaging via low-shot transfer learning. However, deployment of these
models hindered by two key challenges: \textit{distribution shift} between
training and test data, and \textit{confidence misalignment} that leads to
overconfident incorrect predictions. These issues manifest differently in
vision-language classification and medical segmentation tasks, yet existing
solutions remain domain-specific. We propose \textit{StaRFM}, a unified
framework addressing both challenges. It introduces a Fisher information
penalty (FIP), extended to 3D medical data via patch-wise regularization, to
reduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence
misalignment penalty (CMP), reformulated for voxel-level predictions,
calibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes
bounds showing FIP controls generalization via the Fisher-Rao norm, while CMP
minimizes calibration error through Brier score optimization. StaRFM shows
consistent performance like \texttt{+}3.5\% accuracy and 28\% lower ECE on 19
vision datasets (e.g., ImageNet, Office-Home), 84.7\% DSC and 4.8mm HD95 in
medical segmentation (e.g., BraTS, ATLAS), and 40\% lower cross-domain
performance gap compared to prior benchmarking methods. The framework is
plug-and-play, requiring minimal architectural changes for seamless integration
with foundation models. Code and models will be released at
https://anonymous.4open.science/r/StaRFM-C0CD/README.md

</details>


### [239] [EgoAnimate: Generating Human Animations from Egocentric top-down Views](https://arxiv.org/abs/2507.09230)
*G. Kutay Türkoglu,Julian Tanke,Iheb Belgacem,Lev Markhasin*

Main category: cs.CV

TL;DR: The paper introduces a generative prior-based method using Stable Diffusion to reconstruct animatable avatars from egocentric (first-person) views, addressing challenges like occlusions and distorted proportions.


<details>
  <summary>Details</summary>
Motivation: To enable accurate and accessible digital telepresence by reconstructing human appearance and movements from minimal egocentric inputs, overcoming limitations of existing methods.

Method: The approach uses a Stable Diffusion backbone with ControlNet to generate realistic frontal views from occluded top-down egocentric images, then feeds these into an image-to-motion model.

Result: The method reduces training burden and improves generalizability, enabling avatar motion generation from a single top-down image.

Conclusion: This work advances telepresence systems by leveraging generative models for more accessible and generalizable avatar reconstruction from egocentric inputs.

Abstract: An ideal digital telepresence experience requires accurate replication of a
person's body, clothing, and movements. To capture and transfer these movements
into virtual reality, the egocentric (first-person) perspective can be adopted,
which enables the use of a portable and cost-effective device without
front-view cameras. However, this viewpoint introduces challenges such as
occlusions and distorted body proportions.
  There are few works reconstructing human appearance from egocentric views,
and none use a generative prior-based approach. Some methods create avatars
from a single egocentric image during inference, but still rely on multi-view
datasets during training. To our knowledge, this is the first study using a
generative backbone to reconstruct animatable avatars from egocentric inputs.
Based on Stable Diffusion, our method reduces training burden and improves
generalizability.
  Inspired by methods such as SiTH and MagicMan, which perform 360-degree
reconstruction from a frontal image, we introduce a pipeline that generates
realistic frontal views from occluded top-down images using ControlNet and a
Stable Diffusion backbone.
  Our goal is to convert a single top-down egocentric image into a realistic
frontal representation and feed it into an image-to-motion model. This enables
generation of avatar motions from minimal input, paving the way for more
accessible and generalizable telepresence systems.

</details>


### [240] [PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process](https://arxiv.org/abs/2507.09242)
*Shiqi Jiang,Xinpeng Li,Xi Mao,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: A novel framework for assessing the dynamic painting process, introducing PPAD dataset and PPJudge model, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on static images, ignoring the multi-stage nature of painting. This work addresses the gap by evaluating the painting process.

Method: Introduces PPAD dataset with expert annotations and PPJudge, a Transformer-based model with temporally-aware encoding and mixture-of-experts architecture.

Result: Outperforms baselines in accuracy, robustness, and human alignment, providing insights for computational creativity and art education.

Conclusion: The framework advances artistic image assessment by capturing the dynamic painting process, validated by superior performance and human alignment.

Abstract: Artistic image assessment has become a prominent research area in computer
vision. In recent years, the field has witnessed a proliferation of datasets
and methods designed to evaluate the aesthetic quality of paintings. However,
most existing approaches focus solely on static final images, overlooking the
dynamic and multi-stage nature of the artistic painting process. To address
this gap, we propose a novel framework for human-aligned assessment of painting
processes. Specifically, we introduce the Painting Process Assessment Dataset
(PPAD), the first large-scale dataset comprising real and synthetic painting
process images, annotated by domain experts across eight detailed attributes.
Furthermore, we present PPJudge (Painting Process Judge), a Transformer-based
model enhanced with temporally-aware positional encoding and a heterogeneous
mixture-of-experts architecture, enabling effective assessment of the painting
process. Experimental results demonstrate that our method outperforms existing
baselines in accuracy, robustness, and alignment with human judgment, offering
new insights into computational creativity and art education.

</details>


### [241] [AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition](https://arxiv.org/abs/2507.09248)
*Varsha Devi,Amine Bohi,Pardeep Kumar*

Main category: cs.CV

TL;DR: AGCD-Net, a model for context-aware emotion recognition, uses hybrid ConvNeXt and causal intervention to reduce context bias, achieving top performance on CAER-S.


<details>
  <summary>Details</summary>
Motivation: Traditional emotion recognition methods often suffer from context bias, where spurious correlations between background and labels mislead predictions.

Method: AGCD-Net integrates Hybrid ConvNeXt (with Spatial Transformer and Squeeze-and-Excitation layers) and an Attention Guided - Causal Intervention Module (AG-CIM) to debias context features.

Result: AGCD-Net achieves state-of-the-art performance on the CAER-S dataset, demonstrating effective bias mitigation.

Conclusion: Causal debiasing is crucial for robust emotion recognition in complex scenarios, and AGCD-Net provides a promising solution.

Abstract: Context-aware emotion recognition (CAER) enhances affective computing in
real-world scenarios, but traditional methods often suffer from context
bias-spurious correlation between background context and emotion labels (e.g.
associating ``garden'' with ``happy''). In this paper, we propose
\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces
\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the
ConvNeXt backbone by integrating Spatial Transformer Network and
Squeeze-and-Excitation layers for enhanced feature recalibration. At the core
of AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM),
which applies causal theory, perturbs context features, isolates spurious
correlations, and performs an attention-driven correction guided by face
features to mitigate context bias. Experimental results on the CAER-S dataset
demonstrate the effectiveness of AGCD-Net, achieving state-of-the-art
performance and highlighting the importance of causal debiasing for robust
emotion recognition in complex settings.

</details>


### [242] [Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching](https://arxiv.org/abs/2507.09256)
*Junyu Chen,Yihua Gao,Mingyuan Ge,Mingyong Li*

Main category: cs.CV

TL;DR: AAHR framework improves image-text matching by addressing semantic ambiguities and leveraging high-order relations through dynamic clustering, GNN, and momentum contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with high-order associations and semantic ambiguities, especially with soft positive/negative samples, limiting shared knowledge learning.

Method: AAHR uses dynamic clustering prototype contrastive learning, global/local feature extraction, GNN for semantic interactions, and momentum contrastive learning.

Result: AAHR outperforms state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets.

Conclusion: AAHR enhances accuracy and efficiency in image-text matching by mitigating ambiguities and leveraging high-order relations.

Abstract: Image-text matching is crucial for bridging the semantic gap between computer
vision and natural language processing. However, existing methods still face
challenges in handling high-order associations and semantic ambiguities among
similar instances. These ambiguities arise from subtle differences between soft
positive samples (semantically similar but incorrectly labeled) and soft
negative samples (locally matched but globally inconsistent), creating matching
uncertainties. Furthermore, current methods fail to fully utilize the
neighborhood relationships among semantically similar instances within training
batches, limiting the model's ability to learn high-order shared knowledge.
This paper proposes the Ambiguity-Aware and High-order Relation learning
framework (AAHR) to address these issues. AAHR constructs a unified
representation space through dynamic clustering prototype contrastive learning,
effectively mitigating the soft positive sample problem. The framework
introduces global and local feature extraction mechanisms and an adaptive
aggregation network, significantly enhancing full-grained semantic
understanding capabilities. Additionally, AAHR employs intra-modal and
inter-modal correlation matrices to investigate neighborhood relationships
among sample instances thoroughly. It incorporates GNN to enhance semantic
interactions between instances. Furthermore, AAHR integrates momentum
contrastive learning to expand the negative sample set. These combined
strategies significantly improve the model's ability to discriminate between
features. Experimental results demonstrate that AAHR outperforms existing
state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,
considerably improving the accuracy and efficiency of image-text matching. The
code and model checkpoints for this research are available at
https://github.com/Image-Text-Matching/AAHR .

</details>


### [243] [SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation](https://arxiv.org/abs/2507.09266)
*JianHe Low,Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: A segment-aware visual tokenization framework for gloss-free SLT reduces input sequence length by 50%, lowers memory usage, and improves scalability. It introduces token-to-token contrastive alignment and dual-level supervision, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the scalability and computational demands of gloss-free SLT methods while maintaining performance.

Method: Segment-aware visual tokenization converts video into discrete tokens, reducing sequence length. Token-to-token contrastive alignment and dual-level supervision bridge visual and linguistic modalities.

Result: Achieves up to 50% shorter input sequences, 2.67x lower memory usage, and outperforms state-of-the-art on PHOENIX14T.

Conclusion: The proposed framework offers scalable, efficient, and high-performing gloss-free SLT without gloss-level supervision.

Abstract: Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving
strong performances without relying on gloss annotations. However, these gains
have often come with increased model complexity and high computational demands,
raising concerns about scalability, especially as large-scale sign language
datasets become more common. We propose a segment-aware visual tokenization
framework that leverages sign segmentation to convert continuous video into
discrete, sign-informed visual tokens. This reduces input sequence length by up
to 50% compared to prior methods, resulting in up to 2.67x lower memory usage
and better scalability on larger datasets. To bridge the visual and linguistic
modalities, we introduce a token-to-token contrastive alignment objective,
along with a dual-level supervision that aligns both language embeddings and
intermediate hidden states. This improves fine-grained cross-modal alignment
without relying on gloss-level supervision. Our approach notably exceeds the
performance of state-of-the-art methods on the PHOENIX14T benchmark, while
significantly reducing sequence length. Further experiments also demonstrate
our improved performance over prior work under comparable sequence-lengths,
validating the potential of our tokenization and alignment strategies.

</details>


### [244] [Cross Knowledge Distillation between Artificial and Spiking Neural Networks](https://arxiv.org/abs/2507.09269)
*Shuhan Ye,Yuanbin Qian,Chong Wang,Sunqi Lin,Jiazhen Xu,Jiangbo Qian,Yuqi Li*

Main category: cs.CV

TL;DR: The paper proposes Cross Knowledge Distillation (CKD) to improve Spiking Neural Networks (SNNs) performance by leveraging RGB data and ANNs, addressing cross-modality and cross-architecture challenges.


<details>
  <summary>Details</summary>
Motivation: SNNs lag behind ANNs due to limited annotated event-based datasets and immature architectures. The goal is to enhance SNNs using RGB data and ANNs via knowledge distillation.

Method: CKD uses semantic similarity, sliding replacement for cross-modality, and indirect phased knowledge distillation for cross-architecture challenges.

Result: CKD outperforms state-of-the-art methods on neuromorphic datasets like N-Caltech101 and CEP-DVS.

Conclusion: CKD effectively bridges the gap between SNNs and ANNs, demonstrating superior performance on event-based datasets.

Abstract: Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in
computer vision domain due to their high biological plausibility, event-driven
characteristic and energy-saving efficiency. Still, limited annotated
event-based datasets and immature SNN architectures result in their performance
inferior to that of Artificial Neural Networks (ANNs). To enhance the
performance of SNNs on their optimal data format, DVS data, we explore using
RGB data and well-performing ANNs to implement knowledge distillation. In this
case, solving cross-modality and cross-architecture challenges is necessary. In
this paper, we propose cross knowledge distillation (CKD), which not only
leverages semantic similarity and sliding replacement to mitigate the
cross-modality challenge, but also uses an indirect phased knowledge
distillation to mitigate the cross-architecture challenge. We validated our
method on main-stream neuromorphic datasets, including N-Caltech101 and
CEP-DVS. The experimental results show that our method outperforms current
State-of-the-Art methods. The code will be available at
https://github.com/ShawnYE618/CKD

</details>


### [245] [Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models](https://arxiv.org/abs/2507.09279)
*Anita Kriz,Elizabeth Laura Janes,Xing Shen,Tal Arbel*

Main category: cs.CV

TL;DR: Prompt4Trust is a reinforcement learning framework for improving confidence calibration in multimodal large language models (MLLMs) for healthcare, enhancing trustworthiness and accuracy.


<details>
  <summary>Details</summary>
Motivation: MLLMs in healthcare face issues with prompt sensitivity and overconfidence in incorrect responses, which is critical for clinical reliability.

Method: A lightweight LLM is trained to generate context-aware auxiliary prompts to calibrate confidence in downstream MLLMs, prioritizing clinical safety.

Result: Achieves state-of-the-art medical VQA performance on PMC-VQA and shows zero-shot generalization to larger MLLMs.

Conclusion: Prompt4Trust improves trustworthiness and accuracy in MLLMs for safety-critical healthcare applications.

Abstract: Multimodal large language models (MLLMs) hold considerable promise for
applications in healthcare. However, their deployment in safety-critical
settings is hindered by two key limitations: (i) sensitivity to prompt design,
and (ii) a tendency to generate incorrect responses with high confidence. As
clinicians may rely on a model's stated confidence to gauge the reliability of
its predictions, it is especially important that when a model expresses high
confidence, it is also highly accurate. We introduce Prompt4Trust, the first
reinforcement learning (RL) framework for prompt augmentation targeting
confidence calibration in MLLMs. A lightweight LLM is trained to produce
context-aware auxiliary prompts that guide a downstream task MLLM to generate
responses in which the expressed confidence more accurately reflects predictive
accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically
prioritizes aspects of calibration most critical for safe and trustworthy
clinical decision-making. Beyond improvements driven by this clinically
motivated calibration objective, our proposed method also improves task
accuracy, achieving state-of-the-art medical visual question answering (VQA)
performance on the PMC-VQA benchmark, which is composed of multiple-choice
questions spanning diverse medical imaging modalities. Moreover, our framework
trained with a small downstream task MLLM showed promising zero-shot
generalization to larger MLLMs in our experiments, suggesting the potential for
scalable calibration without the associated computational costs. This work
demonstrates the potential of automated yet human-aligned prompt engineering
for improving the the trustworthiness of MLLMs in safety critical settings. Our
codebase can be found at https://github.com/xingbpshen/vccrl-llm.

</details>


### [246] [Generative Latent Kernel Modeling for Blind Motion Deblurring](https://arxiv.org/abs/2507.09285)
*Chenhao Ding,Jiangtao Zhang,Zongsheng Yue,Hui Wang,Qian Zhao,Deyu Meng*

Main category: cs.CV

TL;DR: A novel framework for blind motion deblurring (BMD) uses a GAN-based kernel generator and initializer to improve kernel estimation, reducing sensitivity to initialization and enhancing performance.


<details>
  <summary>Details</summary>
Motivation: Deep prior-based BMD methods suffer from high non-convexity and sensitivity to initial blur kernel, limiting their effectiveness.

Method: Pre-train a GAN-based kernel generator and initializer to encode kernel priors and provide better initialization. Integrates plug-and-play with existing BMD methods.

Result: Achieves state-of-the-art performance on benchmark datasets, including blind non-uniform motion deblurring without extra priors.

Conclusion: The proposed framework effectively addresses initialization sensitivity in BMD and improves performance, with potential for broader application.

Abstract: Deep prior-based approaches have demonstrated remarkable success in blind
motion deblurring (BMD) recently. These methods, however, are often limited by
the high non-convexity of the underlying optimization process in BMD, which
leads to extreme sensitivity to the initial blur kernel. To address this issue,
we propose a novel framework for BMD that leverages a deep generative model to
encode the kernel prior and induce a better initialization for the blur kernel.
Specifically, we pre-train a kernel generator based on a generative adversarial
network (GAN) to aptly characterize the kernel's prior distribution, as well as
a kernel initializer to provide a well-informed and high-quality starting point
for kernel estimation. By combining these two components, we constrain the BMD
solution within a compact latent kernel manifold, thus alleviating the
aforementioned sensitivity for kernel initialization. Notably, the kernel
generator and initializer are designed to be easily integrated with existing
BMD methods in a plug-and-play manner, enhancing their overall performance.
Furthermore, we extend our approach to tackle blind non-uniform motion
deblurring without the need for additional priors, achieving state-of-the-art
performance on challenging benchmark datasets. The source code is available at
https://github.com/dch0319/GLKM-Deblur.

</details>


### [247] [Supercharging Floorplan Localization with Semantic Rays](https://arxiv.org/abs/2507.09291)
*Yuval Grader,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: A semantic-aware floorplan localization framework improves accuracy by jointly estimating depth and semantic rays, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current floorplan localization techniques ignore rich semantic details like windows and doors, focusing only on depth-based structural cues.

Method: The framework constructs a structural-semantic probability volume in a coarse-to-fine manner, refining high-probability regions for precise 2D location and orientation angle prediction.

Result: Outperforms state-of-the-art methods on benchmarks, with significant recall improvements and the ability to incorporate metadata like room labels for further gains.

Conclusion: The semantic-aware approach enhances localization accuracy and efficiency, leveraging both structural and semantic floorplan details.

Abstract: Floorplans provide a compact representation of the building's structure,
revealing not only layout information but also detailed semantics such as the
locations of windows and doors. However, contemporary floorplan localization
techniques mostly focus on matching depth-based structural cues, ignoring the
rich semantics communicated within floorplans. In this work, we introduce a
semantic-aware localization framework that jointly estimates depth and semantic
rays, consolidating over both for predicting a structural-semantic probability
volume. Our probability volume is constructed in a coarse-to-fine manner: We
first sample a small set of rays to obtain an initial low-resolution
probability volume. We then refine these probabilities by performing a denser
sampling only in high-probability regions and process the refined values for
predicting a 2D location and orientation angle. We conduct an evaluation on two
standard floorplan localization benchmarks. Our experiments demonstrate that
our approach substantially outperforms state-of-the-art methods, achieving
significant improvements in recall metrics compared to prior works. Moreover,
we show that our framework can easily incorporate additional metadata such as
room labels, enabling additional gains in both accuracy and efficiency.

</details>


### [248] [Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection](https://arxiv.org/abs/2507.09294)
*Rui Tang,Haochen Yin,Guankun Wang,Long Bai,An Wang,Huxin Gao,Jiazheng Wang,Hongliang Ren*

Main category: cs.CV

TL;DR: Geo-RepNet integrates RGB and depth data for surgical phase recognition, outperforming existing methods in complex surgical scenes.


<details>
  <summary>Details</summary>
Motivation: High visual similarity in surgical phases and lack of structural cues in RGB images necessitate the use of depth information for better recognition.

Method: Proposes Geo-RepNet, a geometry-aware framework with Depth-Guided Geometric Prior Generation (DGPG) and Geometry-Enhanced Multi-scale Attention (GEMA) modules, built on a RepVGG backbone.

Result: Achieves state-of-the-art performance on a nine-phase ESD dataset, demonstrating robustness and computational efficiency.

Conclusion: Depth information significantly enhances surgical phase recognition, and Geo-RepNet provides an effective solution for complex surgical environments.

Abstract: Surgical phase recognition plays a critical role in developing intelligent
assistance systems for minimally invasive procedures such as Endoscopic
Submucosal Dissection (ESD). However, the high visual similarity across
different phases and the lack of structural cues in RGB images pose significant
challenges. Depth information offers valuable geometric cues that can
complement appearance features by providing insights into spatial relationships
and anatomical structures. In this paper, we pioneer the use of depth
information for surgical phase recognition and propose Geo-RepNet, a
geometry-aware convolutional framework that integrates RGB image and depth
information to enhance recognition performance in complex surgical scenes.
Built upon a re-parameterizable RepVGG backbone, Geo-RepNet incorporates the
Depth-Guided Geometric Prior Generation (DGPG) module that extracts geometry
priors from raw depth maps, and the Geometry-Enhanced Multi-scale Attention
(GEMA) to inject spatial guidance through geometry-aware cross-attention and
efficient multi-scale aggregation. To evaluate the effectiveness of our
approach, we construct a nine-phase ESD dataset with dense frame-level
annotations from real-world ESD videos. Extensive experiments on the proposed
dataset demonstrate that Geo-RepNet achieves state-of-the-art performance while
maintaining robustness and high computational efficiency under complex and
low-texture surgical environments.

</details>


### [249] [ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation](https://arxiv.org/abs/2507.09299)
*Abdulvahap Mutlu,Şengül Doğan,Türker Tuncer*

Main category: cs.CV

TL;DR: ViT-ProtoNet integrates Vision Transformers (ViTs) into Prototypical Networks for few-shot image classification, outperforming CNN-based methods and competing with transformer-based ones.


<details>
  <summary>Details</summary>
Motivation: To leverage the underutilized representational power of ViTs in few-shot classification by integrating them into the Prototypical Network framework.

Method: ViT-ProtoNet uses a ViT-Small backbone to average class conditional token embeddings from support examples, creating robust prototypes for novel categories in 5-shot settings.

Result: ViT-ProtoNet achieves up to 3.2% higher 5-shot accuracy than CNN-based methods and shows superior feature separability, performing competitively with transformer-based alternatives.

Conclusion: ViT-ProtoNet is a powerful, flexible approach for few-shot classification, setting a new baseline for transformer-based meta-learners.

Abstract: The remarkable representational power of Vision Transformers (ViTs) remains
underutilized in few-shot image classification. In this work, we introduce
ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical
Network framework. By averaging class conditional token embeddings from a
handful of support examples, ViT-ProtoNet constructs robust prototypes that
generalize to novel categories under 5-shot settings. We conduct an extensive
empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100,
CUB-200, and CIFAR-FS, including overlapped support variants to assess
robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based
prototypical counterparts, achieving up to a 3.2\% improvement in 5-shot
accuracy and demonstrating superior feature separability in latent space.
Furthermore, it outperforms or is competitive with transformer-based
competitors using a more lightweight backbone. Comprehensive ablations examine
the impact of transformer depth, patch size, and fine-tuning strategy. To
foster reproducibility, we release code and pretrained weights. Our results
establish ViT-ProtoNet as a powerful, flexible approach for few-shot
classification and set a new baseline for transformer-based meta-learners.

</details>


### [250] [DAA*: Deep Angular A Star for Image-based Path Planning](https://arxiv.org/abs/2507.09305)
*Zhiwei Xu*

Main category: cs.CV

TL;DR: The paper introduces Deep Angular A* (DAA*), a method incorporating Path Angular Freedom (PAF) to improve path smoothness and similarity in imitation learning, outperforming existing methods in path optimality and similarity metrics.


<details>
  <summary>Details</summary>
Motivation: Path smoothness is often neglected in imitation learning from expert demonstrations, limiting path quality. This work aims to enhance path similarity and optimality by addressing this gap.

Method: DAA* integrates PAF into A* to balance path smoothness and shortening. PAF explores move angle effects on path expansion, optimizing both heuristic distance (shortening) and PAF (smoothing).

Result: DAA* outperforms Neural A* and TransPath in path similarity (SPR, ASIM, PSIM) and length, with improvements up to 9.0%, 6.9%, and 3.9% respectively.

Conclusion: DAA* effectively improves path imitation learning by balancing smoothness and optimality, though with a minor trade-off in search efficiency.

Abstract: Path smoothness is often overlooked in path imitation learning from expert
demonstrations. In this paper, we introduce a novel learning method, termed
deep angular A* (DAA*), by incorporating the proposed path angular freedom
(PAF) into A* to improve path similarity through adaptive path smoothness. The
PAF aims to explore the effect of move angles on path node expansion by finding
the trade-off between their minimum and maximum values, allowing for high
adaptiveness for imitation learning. DAA* improves path optimality by closely
aligning with the reference path through joint optimization of path shortening
and smoothing, which correspond to heuristic distance and PAF, respectively.
Throughout comprehensive evaluations on 7 datasets, including 4 maze datasets,
2 video-game datasets, and a real-world drone-view dataset containing 2
scenarios, we demonstrate remarkable improvements of our DAA* over neural A* in
path similarity between the predicted and reference paths with a shorter path
length when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,
and 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path
loss and path probability map loss, DAA* significantly outperforms the
state-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also
discuss the minor trade-off between path optimality and search efficiency where
applicable.

</details>


### [251] [AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning](https://arxiv.org/abs/2507.09308)
*Zile Wang,Hao Yu,Jiabo Zhan,Chun Yuan*

Main category: cs.CV

TL;DR: ALPHA introduces a benchmark and ALPHAVAE, an RGBA VAE, improving RGBA image generation with better metrics and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack benchmarks and models for RGBA (transparent/layered) image generation.

Method: ALPHAVAE extends a pretrained RGB VAE with an alpha channel, trained on 8K images using a composite objective.

Result: ALPHAVAE outperforms prior methods (+4.9 dB PSNR, +3.2% SSIM) and enables superior transparent image generation.

Conclusion: ALPHA and ALPHAVAE advance RGBA image synthesis, offering a scalable and efficient solution.

Abstract: Recent advances in latent diffusion models have achieved remarkable results
in high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress
and reconstruct pixel data at low computational cost. However, the generation
of transparent or layered content (RGBA image) remains largely unexplored, due
to the lack of large-scale benchmarks. In this work, we propose ALPHA, the
first comprehensive RGBA benchmark that adapts standard RGB metrics to
four-channel images via alpha blending over canonical backgrounds. We further
introduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB
VAE by incorporating a dedicated alpha channel. The model is trained with a
composite objective that combines alpha-blended pixel reconstruction,
patch-level fidelity, perceptual consistency, and dual KL divergence
constraints to ensure latent fidelity across both RGB and alpha
representations. Our RGBA VAE, trained on only 8K images in contrast to 1M used
by prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase
in SSIM over LayerDiffuse in reconstruction. It also enables superior
transparent image generation when fine-tuned within a latent diffusion
framework. Our code, data, and models are released on
https://github.com/o0o0o00o0/AlphaVAE for reproducibility.

</details>


### [252] [An Enhanced Classification Method Based on Adaptive Multi-Scale Fusion for Long-tailed Multispectral Point Clouds](https://arxiv.org/abs/2412.11407)
*TianZhu Liu,BangYan Hu,YanFeng Gu,Xian Li,Aleksandra Pižurica*

Main category: cs.CV

TL;DR: Proposes an adaptive multi-scale fusion method for classifying multispectral point clouds (MPC) with long-tailed distributions, addressing sparse labels, scale variations, and class imbalance.


<details>
  <summary>Details</summary>
Motivation: Existing MPC classification methods struggle with outdoor datasets due to sparse labels, scale differences, and long-tailed distributions.

Method: Uses grid-balanced sampling for training, multi-scale feature fusion for feature learning, and adaptive hybrid loss for classification.

Result: Outperforms state-of-the-art methods on three MPC datasets.

Conclusion: The method effectively addresses key challenges in MPC classification for outdoor scenes.

Abstract: Multispectral point cloud (MPC) captures 3D spatial-spectral information from
the observed scene, which can be used for scene understanding and has a wide
range of applications. However, most of the existing classification methods
were extensively tested on indoor datasets, and when applied to outdoor
datasets they still face problems including sparse labeled targets, differences
in land-covers scales, and long-tailed distributions. To address the above
issues, an enhanced classification method based on adaptive multi-scale fusion
for MPCs with long-tailed distributions is proposed. In the training set
generation stage, a grid-balanced sampling strategy is designed to reliably
generate training samples from sparse labeled datasets. In the feature learning
stage, a multi-scale feature fusion module is proposed to fuse shallow features
of land-covers at different scales, addressing the issue of losing fine
features due to scale variations in land-covers. In the classification stage,
an adaptive hybrid loss module is devised to utilize multi-classification heads
with adaptive weights to balance the learning ability of different classes,
improving the classification performance of small classes due to various-scales
and long-tailed distributions in land-covers. Experimental results on three MPC
datasets demonstrate the effectiveness of the proposed method compared with the
state-of-the-art methods.

</details>


### [253] [ProactiveBench: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models](https://arxiv.org/abs/2507.09313)
*Yueqian Wang,Xiaojun Meng,Yifan Wang,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: ProactiveBench is introduced as the first benchmark for evaluating proactive interaction in multimodal dialogue systems, with PAUC as a new metric accounting for temporal dynamics, showing better alignment with human preferences than traditional metrics.


<details>
  <summary>Details</summary>
Motivation: Users expect multimodal systems to be more initiative, requiring evaluation methods that account for proactive interaction timing.

Method: ProactiveBench is developed as a benchmark, and PAUC is proposed as a metric to evaluate temporal dynamics of responses.

Result: PAUC aligns better with human preferences than traditional metrics, providing a more accurate assessment of proactive interaction.

Conclusion: PAUC offers a more faithful evaluation of user experience in proactive scenarios, advancing research in multimodal dialogue systems.

Abstract: With the growing research focus on multimodal dialogue systems, the
capability for proactive interaction is gradually gaining recognition. As an
alternative to conventional turn-by-turn dialogue, users increasingly expect
multimodal systems to be more initiative, for example, by autonomously
determining the timing of multi-turn responses in real time during video
playback. To facilitate progress in this emerging area, we introduce
ProactiveBench, the first comprehensive benchmark to evaluate a system's
ability to engage in proactive interaction. Since model responses are generated
at varying timestamps, we further propose PAUC, the first metric that accounts
for the temporal dynamics of model responses. This enables a more accurate
evaluation of systems operating in proactive settings. Through extensive
benchmarking of various baseline systems on ProactiveBench and a user study of
human preferences, we show that PAUC is in better agreement with human
preferences than traditional evaluation metrics, which typically only consider
the textual content of responses. These findings demonstrate that PAUC provides
a more faithful assessment of user experience in proactive interaction
scenarios. Project homepage:
https://github.com/yellow-binary-tree/ProactiveBench

</details>


### [254] [Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition](https://arxiv.org/abs/2507.09323)
*Kaixuan Cong,Yifan Wang,Rongkun Xue,Yuyang Jiang,Yiming Feng,Jing Yang*

Main category: cs.CV

TL;DR: The paper introduces DICCAE, an encoder for fine-grained audio-video alignment that dynamically adjusts confusion loss to improve distinguishing similar activities, achieving 65.5% top-1 accuracy on VGGSound.


<details>
  <summary>Details</summary>
Motivation: Existing audio-video pre-training lacks focus on distinguishing easily confused classes, limiting model performance in human activity recognition.

Method: Proposes DICCAE, which dynamically adjusts confusion loss based on inter-class confusion degrees, and includes a cluster-guided self-supervised pre-training strategy.

Result: Achieves 65.5% top-1 accuracy on VGGSound, with ablation studies validating module necessity.

Conclusion: DICCAE effectively enhances audio-video representation alignment and class distinction, demonstrating strong performance and feature quality.

Abstract: Humans do not understand individual events in isolation; rather, they
generalize concepts within classes and compare them to others. Existing
audio-video pre-training paradigms only focus on the alignment of the overall
audio-video modalities, without considering the reinforcement of distinguishing
easily confused classes through cognitive induction and contrast during
training. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder
(DICCAE), an encoder that aligns audio-video representations at a fine-grained,
category-level. DICCAE addresses category confusion by dynamically adjusting
the confusion loss based on inter-class confusion degrees, thereby enhancing
the model's ability to distinguish between similar activities. To further
extend the application of DICCAE, we also introduce a novel training framework
that incorporates both audio and video modalities, as well as their fusion. To
mitigate the scarcity of audio-video data in the human activity recognition
task, we propose a cluster-guided audio-video self-supervised pre-training
strategy for DICCAE. DICCAE achieves near state-of-the-art performance on the
VGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its
feature representation quality through extensive ablation studies, validating
the necessity of each module.

</details>


### [255] [Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging](https://arxiv.org/abs/2507.08052)
*Mazen Ali,António Pereira,Fabio Gentile,Aser Cortines,Sam Mugel,Román Orús,Stelios P. Neophytides,Michalis Mavrovouniotis*

Main category: cs.CV

TL;DR: The study evaluates machine learning methods for cloud and cloud shadow masking in hyperspectral imaging, finding CNNs with feature reduction most efficient.


<details>
  <summary>Details</summary>
Motivation: To improve preprocessing for high-quality hyperspectral satellite data by assessing machine learning approaches.

Method: Evaluated gradient boosting (XGBoost, LightGBM) and CNNs, focusing on accuracy, storage, and inference speed.

Result: All models exceeded 93% accuracy; CNNs with feature reduction (597 parameters) balanced accuracy, efficiency, and deployment feasibility.

Conclusion: Lightweight AI models like CNNs are promising for real-time hyperspectral processing and on-board satellite AI systems.

Abstract: Cloud and cloud shadow masking is a crucial preprocessing step in
hyperspectral satellite imaging, enabling the extraction of high-quality,
analysis-ready data. This study evaluates various machine learning approaches,
including gradient boosting methods such as XGBoost and LightGBM as well as
convolutional neural networks (CNNs). All boosting and CNN models achieved
accuracies exceeding 93%. Among the investigated models, the CNN with feature
reduction emerged as the most efficient, offering a balance of high accuracy,
low storage requirements, and rapid inference times on both CPUs and GPUs.
Variations of this version, with only up to 597 trainable parameters,
demonstrated the best trade-off in terms of deployment feasibility, accuracy,
and computational efficiency. These results demonstrate the potential of
lightweight artificial intelligence (AI) models for real-time hyperspectral
image processing, supporting the development of on-board satellite AI systems
for space-based applications.

</details>


### [256] [Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding](https://arxiv.org/abs/2507.09334)
*Wencan Huang,Daizong Liu,Wei Hu*

Main category: cs.CV

TL;DR: Fast3D is a plug-and-play framework for pruning redundant visual tokens in 3D MLLMs, improving computational efficiency without altering model parameters.


<details>
  <summary>Details</summary>
Motivation: Addressing computational inefficiency in 3D MLLMs due to excessive object-centric visual tokens, which hinders practical deployment.

Method: Proposes two innovations: Global Attention Prediction (GAP) for token importance estimation and Sample-Adaptive visual token Pruning (SAP) for dynamic pruning ratios.

Result: Validated across five benchmarks, Fast3D effectively improves efficiency, especially under high pruning ratios.

Conclusion: Fast3D offers a practical solution for accelerating 3D MLLMs by efficiently pruning redundant tokens.

Abstract: While 3D Multi-modal Large Language Models (MLLMs) demonstrate remarkable
scene understanding capabilities, their practical deployment faces critical
challenges due to computational inefficiency. The key bottleneck stems from
processing excessive object-centric visual tokens required for comprehensive 3D
scene representation. Although visual token pruning has shown promise in
accelerating 2D MLLMs, its applicability to 3D domains remains largely
unexplored due to fundamental disparities in token structures. In this paper,
we reveal two critical insights: (1) Significant redundancy exists in
object-level 3D token representations, analogous to patch-level redundancy in
2D systems; (2) Global attention patterns exhibit strong predictive power for
identifying non-essential tokens in 3D contexts. Building on these
observations, we propose Fast3D, a plug-and-play visual token pruning framework
for 3D MLLMs featuring two technical innovations: (1) Global Attention
Prediction (GAP), where a lightweight neural network learns to predict the
global attention distributions of the target model, enabling efficient token
importance estimation for precise pruning guidance; (2) Sample-Adaptive visual
token Pruning (SAP), which introduces dynamic token budgets through
attention-based complexity assessment, automatically adjusting layer-wise
pruning ratios based on input characteristics. Both of these two techniques
operate without modifying the parameters of the target model. Extensive
evaluations across five benchmarks validate the effectiveness of Fast3D,
particularly under high visual token pruning ratios. Code is available at
https://github.com/wencan25/Fast3D

</details>


### [257] [Simplifying Traffic Anomaly Detection with Video Foundation Models](https://arxiv.org/abs/2507.09338)
*Svetlana Orlova,Tommie Kerssies,Brunó B. Englert,Gijs Dubbelman*

Main category: cs.CV

TL;DR: Simple Video ViTs with strong pre-training outperform complex TAD methods, with self-supervised MVM and DAPT being most effective.


<details>
  <summary>Details</summary>
Motivation: To determine if simple architectures with advanced pre-training can outperform complex TAD methods.

Method: Uses plain Video ViTs with various pre-training strategies (weakly-, fully-supervised, self-supervised MVM, and DAPT).

Result: Simple models match or surpass specialized TAD methods, with self-supervised MVM and DAPT being most effective.

Conclusion: Pre-training is key; simple, efficient TAD models can achieve strong performance without complex architectures.

Abstract: Recent methods for ego-centric Traffic Anomaly Detection (TAD) often rely on
complex multi-stage or multi-representation fusion architectures, yet it
remains unclear whether such complexity is necessary. Recent findings in visual
perception suggest that foundation models, enabled by advanced pre-training,
allow simple yet flexible architectures to outperform specialized designs.
Therefore, in this work, we investigate an architecturally simple encoder-only
approach using plain Video Vision Transformers (Video ViTs) and study how
pre-training enables strong TAD performance. We find that: (i) strong
pre-training enables simple encoder-only models to match or even surpass the
performance of specialized state-of-the-art TAD methods, while also being
significantly more efficient; (ii) although weakly- and fully-supervised
pre-training are advantageous on standard benchmarks, we find them less
effective for TAD. Instead, self-supervised Masked Video Modeling (MVM)
provides the strongest signal; and (iii) Domain-Adaptive Pre-Training (DAPT) on
unlabeled driving videos further improves downstream performance, without
requiring anomalous examples. Our findings highlight the importance of
pre-training and show that effective, efficient, and scalable TAD models can be
built with minimal architectural complexity. We release our code,
domain-adapted encoders, and fine-tuned models to support future work:
https://github.com/tue-mps/simple-tad.

</details>


### [258] [Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture](https://arxiv.org/abs/2507.09375)
*Sourish Suri,Yifei Shao*

Main category: cs.CV

TL;DR: A CNN-based system automates detection of eight crop diseases from leaf images, achieving ~90% training accuracy and ~60% validation accuracy, with a treatment recommendation module and mobile deployment.


<details>
  <summary>Details</summary>
Motivation: Crop diseases hinder agricultural productivity and food security, especially in large-scale farming where early detection is often delayed or inaccurate.

Method: The system uses a CNN with three convolutional layers, preprocessing (resizing, normalization, augmentation), and TensorFlow/Keras for training. It includes a treatment recommendation module and is deployed on a mobile platform.

Result: High training accuracy (~90%) but validation accuracy (~60%) indicates minor overfitting. The system provides actionable treatment recommendations and is accessible via mobile.

Conclusion: The research offers a scalable, accessible tool for precision agriculture, reducing manual inspection and promoting sustainable disease management through deep learning.

Abstract: Crop diseases present a significant barrier to agricultural productivity and
global food security, especially in large-scale farming where early
identification is often delayed or inaccurate. This research introduces a
Convolutional Neural Network (CNN)-based image classification system designed
to automate the detection and classification of eight common crop diseases
using leaf imagery. The methodology involves a complete deep learning pipeline:
image acquisition from a large, labeled dataset, preprocessing via resizing,
normalization, and augmentation, and model training using TensorFlow with
Keras' Sequential API. The CNN architecture comprises three convolutional
layers with increasing filter sizes and ReLU activations, followed by max
pooling, flattening, and fully connected layers, concluding with a softmax
output for multi-class classification. The system achieves high training
accuracy (~90%) and demonstrates reliable performance on unseen data, although
a validation accuracy of ~60% suggests minor overfitting. Notably, the model
integrates a treatment recommendation module, providing actionable guidance by
mapping each detected disease to suitable pesticide or fungicide interventions.
Furthermore, the solution is deployed on an open-source, mobile-compatible
platform, enabling real-time image-based diagnostics for farmers in remote
areas. This research contributes a scalable and accessible tool to the field of
precision agriculture, reducing reliance on manual inspection and promoting
sustainable disease management practices. By merging deep learning with
practical agronomic support, this work underscores the potential of CNNs to
transform crop health monitoring and enhance food production resilience on a
global scale.

</details>


### [259] [GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups](https://arxiv.org/abs/2507.09410)
*Bernie Boscoe,Shawn Johnson,Andrea Osborn,Chandler Campbell,Karen Mager*

Main category: cs.CV

TL;DR: A guide for small research groups on processing camera trap data using a low-resource ML/AI pipeline, addressing challenges like data volume, labeling accuracy, and workflow integration.


<details>
  <summary>Details</summary>
Motivation: To help small research groups with limited resources and computational expertise manage and analyze large camera trap datasets efficiently.

Method: Develops a low-resource, on-premise pipeline incorporating ML/AI tools for data transmission, inference, and evaluation.

Result: Provides practical solutions for processing camera trap data, enabling meaningful insights despite resource constraints.

Conclusion: The pipeline makes ML/AI tools accessible for small groups, improving wildlife research efficiency.

Abstract: Camera traps have long been used by wildlife researchers to monitor and study
animal behavior, population dynamics, habitat use, and species diversity in a
non-invasive and efficient manner. While data collection from the field has
increased with new tools and capabilities, methods to develop, process, and
manage the data, especially the adoption of ML/AI tools, remain challenging.
These challenges include the sheer volume of data generated, the need for
accurate labeling and annotation, variability in environmental conditions
affecting data quality, and the integration of ML/AI tools into existing
workflows that often require domain-specific customization and computational
resources. This paper provides a guide to a low-resource pipeline to process
camera trap data on-premise, incorporating ML/AI capabilities tailored for
small research groups with limited resources and computational expertise. By
focusing on practical solutions, the pipeline offers accessible approaches for
data transmission, inference, and evaluation, enabling researchers to discover
meaningful insights from their ever-increasing camera trap datasets.

</details>


### [260] [Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data](https://arxiv.org/abs/2507.09420)
*Timothy Chase Jr,Karthik Dantu*

Main category: cs.CV

TL;DR: A novel system for real-time celestial terrain feature tracking using lightweight neural networks and improved domain adaptation for spacecraft autonomy.


<details>
  <summary>Details</summary>
Motivation: Address limitations of traditional photoclinometry-based methods (high cost, slow processing, poor generalization) and computational challenges of learning-based approaches for spacecraft.

Method: Proposes lightweight neural networks for real-time execution, improved domain adaptation for detection, and attention alignment for robust landmark description.

Result: Superior performance compared to state-of-the-art techniques in landmark tracking.

Conclusion: The system enhances spacecraft autonomy by enabling efficient, real-time celestial terrain feature tracking with minimal training data.

Abstract: The detection and tracking of celestial surface terrain features are crucial
for autonomous spaceflight applications, including Terrain Relative Navigation
(TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data
collection. Traditional photoclinometry-based pipelines often rely on extensive
a priori imaging and offline processing, constrained by the computational
limitations of radiation-hardened systems. While historically effective, these
approaches typically increase mission costs and duration, operate at low
processing rates, and have limited generalization. Recently, learning-based
computer vision has gained popularity to enhance spacecraft autonomy and
overcome these limitations. While promising, emerging techniques frequently
impose computational demands exceeding the capabilities of typical spacecraft
hardware for real-time operation and are further challenged by the scarcity of
labeled training data for diverse extraterrestrial environments. In this work,
we present novel formulations for in-situ landmark tracking via detection and
description. We utilize lightweight, computationally efficient neural network
architectures designed for real-time execution on current-generation spacecraft
flight processors. For landmark detection, we propose improved domain
adaptation methods that enable the identification of celestial terrain features
with distinct, cheaply acquired training data. Concurrently, for landmark
description, we introduce a novel attention alignment formulation that learns
robust feature representations that maintain correspondence despite significant
landmark viewpoint variations. Together, these contributions form a unified
system for landmark tracking that demonstrates superior performance compared to
existing state-of-the-art techniques.

</details>


### [261] [Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions](https://arxiv.org/abs/2507.09446)
*Yuanhong Zheng,Ruixuan Yu,Jian Sun*

Main category: cs.CV

TL;DR: A computationally efficient model for 3D multi-person motion prediction simplifies spatial and temporal interactions, achieving state-of-the-art performance with reduced costs.


<details>
  <summary>Details</summary>
Motivation: The complexity of modeling dependencies on individual movements and interactions between agents, along with high computational costs, drives the need for an efficient solution.

Method: Lightweight dual branches learn local/global representations separately, with a cross-level interaction block and spatial inter-person distance embedding for enhanced modeling.

Result: State-of-the-art performance on CMU-Mocap, MuPoTS-3D, and 3DPW datasets with significantly reduced computational cost.

Conclusion: The proposed model efficiently addresses the challenges of multi-person motion prediction, balancing performance and computational efficiency.

Abstract: 3D multi-person motion prediction is a highly complex task, primarily due to
the dependencies on both individual past movements and the interactions between
agents. Moreover, effectively modeling these interactions often incurs
substantial computational costs. In this work, we propose a computationally
efficient model for multi-person motion prediction by simplifying spatial and
temporal interactions. Our approach begins with the design of lightweight dual
branches that learn local and global representations for individual and
multiple persons separately. Additionally, we introduce a novel cross-level
interaction block to integrate the spatial and temporal representations from
both branches. To further enhance interaction modeling, we explicitly
incorporate the spatial inter-person distance embedding. With above efficient
temporal and spatial design, we achieve state-of-the-art performance for
multiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while
significantly reducing the computational cost. Code is available at
https://github.com/Yuanhong-Zheng/EMPMP.

</details>


### [262] [SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation](https://arxiv.org/abs/2507.09459)
*Zhihan Kang,Boyu Wang*

Main category: cs.CV

TL;DR: SegVec3D is a 3D point cloud instance segmentation framework combining attention, embedding learning, and cross-modal alignment for unsupervised segmentation and zero-shot retrieval.


<details>
  <summary>Details</summary>
Motivation: To unify instance segmentation and multimodal understanding with minimal supervision and practical deployability.

Method: Hierarchical feature extraction, contrastive clustering for unsupervised segmentation, and cross-modal alignment with natural language.

Result: Outperforms methods like Mask3D and ULIP by integrating segmentation and multimodal tasks.

Conclusion: SegVec3D effectively unifies 3D instance segmentation and multimodal understanding with minimal supervision.

Abstract: We propose SegVec3D, a novel framework for 3D point cloud instance
segmentation that integrates attention mechanisms, embedding learning, and
cross-modal alignment. The approach builds a hierarchical feature extractor to
enhance geometric structure modeling and enables unsupervised instance
segmentation via contrastive clustering. It further aligns 3D data with natural
language queries in a shared semantic space, supporting zero-shot retrieval.
Compared to recent methods like Mask3D and ULIP, our method uniquely unifies
instance segmentation and multimodal understanding with minimal supervision and
practical deployability.

</details>


### [263] [CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning](https://arxiv.org/abs/2507.09471)
*Lingfeng He,De Cheng,Zhiheng Ma,Huaijie Wang,Dingwen Zhang,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: CKAA improves robustness in continual learning by aligning feature subspaces and adaptively aggregating task-specific knowledge, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address ambiguous decisions in PEFT-based CL due to misaligned feature subspaces from independently trained sub-modules.

Method: Introduces Dual-level Knowledge Alignment (DKA) for feature distribution alignment and Task-Confidence-guided Mixture of Adapters (TC-MoA) for adaptive knowledge aggregation.

Result: CKAA outperforms existing PEFT-based CL methods in experiments.

Conclusion: CKAA effectively mitigates misleading task-id issues, enhancing model robustness in continual learning.

Abstract: Continual Learning (CL) empowers AI models to continuously learn from
sequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based
CL methods have garnered increasing attention due to their superior
performance. They typically allocate a unique sub-module for learning each
task, with a task recognizer to select the appropriate sub-modules for testing
images. However, due to the feature subspace misalignment from independently
trained sub-modules, these methods tend to produce ambiguous decisions under
misleading task-ids. To address this, we propose Cross-subspace Knowledge
Alignment and Aggregation (CKAA), a novel framework that enhances model
robustness against misleading task-ids through two key innovations: (1)
Dual-level Knowledge Alignment (DKA): By aligning intra-class feature
distributions across different subspaces and learning a robust global
classifier through a feature simulation process, DKA enables the model to
distinguish features from both correct and incorrect subspaces during training.
(2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference
scheme that adaptively aggregates task-specific knowledge from relevant
sub-modules based on task-confidence scores, avoiding overconfidence in
misleading task-id predictions. Extensive experiments demonstrate that CKAA
outperforms existing PEFT-based CL methods.

</details>


### [264] [HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space](https://arxiv.org/abs/2507.09487)
*Changli Wang,Fang Yin,Jiafeng Liu,Rui Wu*

Main category: cs.CV

TL;DR: HMID-Net integrates Masked Image Modeling (MIM) and knowledge distillation in hyperbolic space for efficient hierarchical learning, outperforming MERU and CLIP.


<details>
  <summary>Details</summary>
Motivation: To efficiently train models to capture and leverage visual-semantic hierarchies, which are naturally structured hierarchically.

Method: Proposes HMID-Net, combining MIM and knowledge distillation in hyperbolic space, with a custom distillation loss for effective knowledge transfer.

Result: Achieves success comparable to Euclidean space methods, excelling in image classification and retrieval tasks.

Conclusion: HMID-Net is a novel, efficient approach for hierarchical learning, surpassing existing models like MERU and CLIP.

Abstract: Visual and semantic concepts are often structured in a hierarchical manner.
For instance, textual concept `cat' entails all images of cats. A recent study,
MERU, successfully adapts multimodal learning techniques from Euclidean space
to hyperbolic space, effectively capturing the visual-semantic hierarchy.
However, a critical question remains: how can we more efficiently train a model
to capture and leverage this hierarchy? In this paper, we propose the
\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel
and efficient method that integrates Masked Image Modeling (MIM) and knowledge
distillation techniques within hyperbolic space. To the best of our knowledge,
this is the first approach to leverage MIM and knowledge distillation in
hyperbolic space to train highly efficient models. In addition, we introduce a
distillation loss function specifically designed to facilitate effective
knowledge transfer in hyperbolic space. Our experiments demonstrate that MIM
and knowledge distillation techniques in hyperbolic space can achieve the same
remarkable success as in Euclidean space. Extensive evaluations show that our
method excels across a wide range of downstream tasks, significantly
outperforming existing models like MERU and CLIP in both image classification
and retrieval.

</details>


### [265] [GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just Glimpse at Them?](https://arxiv.org/abs/2507.09491)
*Yiyang Zhou,Linjie Li,Shi Qiu,Zhengyuan Yang,Yuyang Zhao,Siwei Han,Yangfan He,Kangqi Li,Haonian Ji,Zihao Zhao,Haibo Tong,Lijuan Wang,Huaxiu Yao*

Main category: cs.CV

TL;DR: GLIMPSE is a new benchmark for evaluating deep temporal reasoning in large vision-language models (LVLMs), requiring full video context understanding, unlike existing superficial benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks allow models to answer questions with minimal frame analysis, limiting assessment of true video understanding.

Method: GLIMPSE includes 3,269 videos and 4,342 visual-centric questions across 11 categories, designed to require full video context and temporal reasoning.

Result: Human accuracy on GLIMPSE is 94.82%, but the best LVLM (GPT-o3) only achieves 66.43%, showing a gap in deep video reasoning.

Conclusion: GLIMPSE highlights LVLMs' limitations in true video understanding, emphasizing the need for models to move beyond superficial frame-level analysis.

Abstract: Existing video benchmarks often resemble image-based benchmarks, with
question types like "What actions does the person perform throughout the
video?" or "What color is the woman's dress in the video?" For these, models
can often answer by scanning just a few key frames, without deep temporal
reasoning. This limits our ability to assess whether large vision-language
models (LVLMs) can truly think with videos rather than perform superficial
frame-level analysis. To address this, we introduce GLIMPSE, a benchmark
specifically designed to evaluate whether LVLMs can genuinely think with
videos. Unlike prior benchmarks, GLIMPSE emphasizes comprehensive video
understanding beyond static image cues. It consists of 3,269 videos and over
4,342 highly visual-centric questions across 11 categories, including
Trajectory Analysis, Temporal Reasoning, and Forensics Detection. All questions
are carefully crafted by human annotators and require watching the entire video
and reasoning over full video context-this is what we mean by thinking with
video. These questions cannot be answered by scanning selected frames or
relying on text alone. In human evaluations, GLIMPSE achieves 94.82% accuracy,
but current LVLMs face significant challenges. Even the best-performing model,
GPT-o3, reaches only 66.43%, highlighting that LVLMs still struggle to move
beyond surface-level reasoning to truly think with videos.

</details>


### [266] [SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification](https://arxiv.org/abs/2507.09492)
*Fuyin Ye,Erwen Yao,Jianyong Chen,Fengmei He,Junxiang Zhang,Lihao Ni*

Main category: cs.CV

TL;DR: Proposes SDTN and TRN for hyperspectral image classification, addressing high-dimensional data and labeled sample scarcity with tensor decomposition and regularization, achieving high accuracy and reduced computational complexity.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with high-dimensional data, spectral-spatial redundancy, and scarcity of labeled samples, leading to suboptimal performance in hyperspectral image classification.

Method: Introduces SDTN for dynamic tensor rank adjustment and TRN, a lightweight network integrating SDTN features for multi-scale spectral-spatial feature capture.

Result: Experiments on PaviaU datasets show improved accuracy and reduced model parameters compared to state-of-the-art methods.

Conclusion: The proposed framework is effective for real-time deployment in resource-constrained environments, maintaining high classification accuracy with lower computational complexity.

Abstract: Hyperspectral image classification plays a pivotal role in precision
agriculture, providing accurate insights into crop health monitoring, disease
detection, and soil analysis. However, traditional methods struggle with
high-dimensional data, spectral-spatial redundancy, and the scarcity of labeled
samples, often leading to suboptimal performance. To address these challenges,
we propose the Self-Adaptive Tensor- Regularized Network (SDTN), which combines
tensor decomposition with regularization mechanisms to dynamically adjust
tensor ranks, ensuring optimal feature representation tailored to the
complexity of the data. Building upon SDTN, we propose the Tensor-Regularized
Network (TRN), which integrates the features extracted by SDTN into a
lightweight network capable of capturing spectral-spatial features at multiple
scales. This approach not only maintains high classification accuracy but also
significantly reduces computational complexity, making the framework highly
suitable for real-time deployment in resource-constrained environments.
Experiments on PaviaU datasets demonstrate significant improvements in accuracy
and reduced model parameters compared to state-of-the-art methods.

</details>


### [267] [Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations](https://arxiv.org/abs/2507.09500)
*Yiwen Liang,Hui Chen,Yizhe Xiong,Zihan Zhou,Mengyao Lyu,Zijia Lin,Shuaicheng Niu,Sicheng Zhao,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: ReTA improves reliability in Test-Time Adaptation for VLMs by addressing entropy unreliability and inflexible decision boundaries with CER and DDC strategies.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with distribution shifts in downstream tasks without labeled data, and existing cache-based TTA methods face reliability issues due to unreliable entropy and rigid decision boundaries.

Method: Proposes ReTA with Consistency-aware Entropy Reweighting (CER) for better cache quality and Diversity-driven Distribution Calibration (DDC) for adaptive decision boundaries.

Result: ReTA outperforms state-of-the-art methods, especially under challenging real-world distribution shifts.

Conclusion: ReTA enhances reliability in TTA for VLMs, addressing key challenges and improving performance under distribution shifts.

Abstract: Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but
struggle with distribution shifts in downstream tasks when labeled data is
unavailable, which has motivated the development of Test-Time Adaptation (TTA)
to improve VLMs' performance during inference without annotations. Among
various TTA approaches, cache-based methods show promise by preserving
historical knowledge from low-entropy samples in a dynamic cache and fostering
efficient adaptation. However, these methods face two critical reliability
challenges: (1) entropy often becomes unreliable under distribution shifts,
causing error accumulation in the cache and degradation in adaptation
performance; (2) the final predictions may be unreliable due to inflexible
decision boundaries that fail to accommodate large downstream shifts. To
address these challenges, we propose a Reliable Test-time Adaptation (ReTA)
method that integrates two complementary strategies to enhance reliability from
two perspectives. First, to mitigate the unreliability of entropy as a sample
selection criterion for cache construction, we introduce Consistency-aware
Entropy Reweighting (CER), which incorporates consistency constraints to weight
entropy during cache updating. While conventional approaches rely solely on low
entropy for cache prioritization and risk introducing noise, our method
leverages predictive consistency to maintain a high-quality cache and
facilitate more robust adaptation. Second, we present Diversity-driven
Distribution Calibration (DDC), which models class-wise text embeddings as
multivariate Gaussian distributions, enabling adaptive decision boundaries for
more accurate predictions across visually diverse content. Extensive
experiments demonstrate that ReTA consistently outperforms state-of-the-art
methods, particularly under challenging real-world distribution shifts.

</details>


### [268] [Online Micro-gesture Recognition Using Data Augmentation and Spatial-Temporal Attention](https://arxiv.org/abs/2507.09512)
*Pengyu Liu,Kun Li,Fei Wang,Yanyan Wei,Junhui She,Dan Guo*

Main category: cs.CV

TL;DR: HFUT-VUT's solution for micro-gesture recognition in videos uses data augmentation and spatial-temporal attention, achieving a 38.03 F1 score and outperforming prior methods by 37.9%.


<details>
  <summary>Details</summary>
Motivation: The task is challenging due to the need to precisely locate and categorize spontaneous micro-gestures in videos, which differ significantly from other human actions.

Method: Proposed hand-crafted data augmentation and spatial-temporal attention to improve classification and localization.

Result: Achieved an F1 score of 38.03, surpassing the previous state-of-the-art by 37.9%, ranking first in the challenge.

Conclusion: The method effectively addresses the challenges of micro-gesture recognition, demonstrating superior performance in the IJCAI 2025 MiGA Challenge.

Abstract: In this paper, we introduce the latest solution developed by our team,
HFUT-VUT, for the Micro-gesture Online Recognition track of the IJCAI 2025 MiGA
Challenge. The Micro-gesture Online Recognition task is a highly challenging
problem that aims to locate the temporal positions and recognize the categories
of multiple micro-gesture instances in untrimmed videos. Compared to
traditional temporal action detection, this task places greater emphasis on
distinguishing between micro-gesture categories and precisely identifying the
start and end times of each instance. Moreover, micro-gestures are typically
spontaneous human actions, with greater differences than those found in other
human actions. To address these challenges, we propose hand-crafted data
augmentation and spatial-temporal attention to enhance the model's ability to
classify and localize micro-gestures more accurately. Our solution achieved an
F1 score of 38.03, outperforming the previous state-of-the-art by 37.9%. As a
result, our method ranked first in the Micro-gesture Online Recognition track.

</details>


### [269] [QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models](https://arxiv.org/abs/2507.09514)
*Tien-Yu Chi,Hung-Yueh Chiang,Diana Marculescu,Kai-Chiang Wu*

Main category: cs.CV

TL;DR: QuarterMap is a post-training pruning method for SSMs like VMamba, removing redundant spatial activations to improve throughput without retraining, achieving up to 11% speedup with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Spatial redundancy in four-directional scans of SSMs like VMamba limits efficiency. QuarterMap aims to address this without retraining.

Method: QuarterMap prunes redundant activations before scanning and restores dimensions via nearest-neighbor upsampling.

Result: On ImageNet-1K, it achieves up to 11% speedup with <0.9% accuracy drop; similar gains on ADE20K and MedMamba.

Conclusion: QuarterMap is a plug-and-play tool for SSMs, improving deployment-time efficiency without compromising transferability.

Abstract: State space models (SSMs) reduce the quadratic complexity of transformers by
leveraging linear recurrence. Recently, VMamba has emerged as a strong
SSM-based vision backbone, yet remains bottlenecked by spatial redundancy in
its four-directional scan. We propose QuarterMap, a post-training activation
pruning method that removes redundant spatial activations before scanning and
restores dimensions via nearest-neighbor upsampling. Our method improves
throughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11%
speedup on VMamba with less than 0.9% accuracy drop, and yields similar gains
on ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a
domain-specific model that shares the same four-directional scanning structure,
where it consistently improves throughput while preserving accuracy across
multiple medical imaging tasks. Compared to token merging methods like ToMe,
QuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our
method offers a plug-and-play tool for deployment-time efficiency without
compromising transferability.

</details>


### [270] [When Schrödinger Bridge Meets Real-World Image Dehazing with Unpaired Training](https://arxiv.org/abs/2507.09524)
*Yunwei Lan,Zhigao Cui,Xin Luo,Chang Liu,Nian Wang,Menglin Zhang,Yanzhao Su,Dong Liu*

Main category: cs.CV

TL;DR: DehazeSB is a novel unpaired dehazing framework using Schrödinger Bridge and optimal transport theory to improve image quality and detail preservation.


<details>
  <summary>Details</summary>
Motivation: Existing GAN-based unpaired dehazing methods struggle with limited transport mapping capabilities, reducing their effectiveness.

Method: DehazeSB leverages optimal transport theory for direct distribution bridging, detail-preserving regularization, and prompt learning with CLIP models.

Result: The method outperforms others on real-world datasets, generating high-quality dehazed images.

Conclusion: DehazeSB effectively addresses limitations of current methods, offering superior performance in unpaired dehazing.

Abstract: Recent advancements in unpaired dehazing, particularly those using GANs, show
promising performance in processing real-world hazy images. However, these
methods tend to face limitations due to the generator's limited transport
mapping capability, which hinders the full exploitation of their effectiveness
in unpaired training paradigms. To address these challenges, we propose
DehazeSB, a novel unpaired dehazing framework based on the Schr\"odinger
Bridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges
the distributions between hazy and clear images. This enables optimal transport
mappings from hazy to clear images in fewer steps, thereby generating
high-quality results. To ensure the consistency of structural information and
details in the restored images, we introduce detail-preserving regularization,
which enforces pixel-level alignment between hazy inputs and dehazed outputs.
Furthermore, we propose a novel prompt learning to leverage pre-trained CLIP
models in distinguishing hazy images and clear ones, by learning a haze-aware
vision-language alignment. Extensive experiments on multiple real-world
datasets demonstrate our method's superiority. Code:
https://github.com/ywxjm/DehazeSB.

</details>


### [271] [VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization](https://arxiv.org/abs/2507.09531)
*Son Nguyen,Giang Nguyen,Hung Dao,Thao Do,Daeyoung Kim*

Main category: cs.CV

TL;DR: VDInstruct, a multimodal large language model (MLLM), improves Key Information Extraction (KIE) by separating spatial detection from semantic extraction, using content-aware tokenization to reduce redundancy and achieve SOTA results.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs perform poorly on dense documents and suffer from inefficient vision tokenization, leading to redundant computation and memory issues.

Method: VDInstruct employs content-aware tokenization, generating tokens proportional to document complexity, and uses a three-stage training paradigm.

Result: The model achieves SOTA on KIE benchmarks, reducing image tokens by 3.6x and outperforming baselines like DocOwl 1.5 by +5.5 F1 in zero-shot evaluations.

Conclusion: Content-aware tokenization with explicit layout modeling is a promising approach for document understanding, with public release of data, code, and model weights.

Abstract: Key Information Extraction (KIE) underpins the understanding of visual
documents (e.g., receipts and contracts) by extracting precise semantic content
and accurately capturing spatial structure. Yet existing multimodal large
language models (MLLMs) often perform poorly on dense documents and rely on
vision tokenization approaches that scale with image size, leading to redundant
computation and memory inefficiency. To address these challenges, we introduce
VDInstruct, an MLLM that separates spatial region detection from semantic
feature extraction. Central to our model is a content-aware tokenization
strategy: rather than fragmenting the entire image uniformly, it generates
tokens in proportion to document complexity, preserving critical structure
while eliminating wasted tokens. Leveraging a three-stage training paradigm,
our model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching
or exceeding the accuracy of leading approaches while reducing the number of
image tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses
strong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its
robustness to unseen documents. These findings show that content-aware
tokenization combined with explicit layout modeling offers a promising
direction forward for document understanding. Data, source code, and model
weights will be made publicly available.

</details>


### [272] [DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection](https://arxiv.org/abs/2507.09541)
*Zihao Xiong,Fei Zhou,Fengyi Wu,Shuai Yuan,Maixia Fu,Zhenming Peng,Jian Yang,Yimian Dai*

Main category: cs.CV

TL;DR: DRPCA-Net, a deep unfolding network, integrates sparsity-aware priors into a learnable architecture for infrared small target detection, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between performance and interpretability in deep learning models for infrared small target detection by leveraging intrinsic sparsity priors.

Method: Proposes Dynamic RPCA Network (DRPCA-Net) with a dynamic unfolding mechanism and Dynamic Residual Group (DRG) module for adaptive parameter generation and contextual variation capture.

Result: Significantly outperforms existing methods in detection accuracy on multiple public infrared datasets.

Conclusion: DRPCA-Net successfully combines model-based paradigms with deep learning, enhancing robustness and generalization for infrared small target detection.

Abstract: Infrared small target detection plays a vital role in remote sensing,
industrial monitoring, and various civilian applications. Despite recent
progress powered by deep learning, many end-to-end convolutional models tend to
pursue performance by stacking increasingly complex architectures, often at the
expense of interpretability, parameter efficiency, and generalization. These
models typically overlook the intrinsic sparsity prior of infrared small
targets--an essential cue that can be explicitly modeled for both performance
and efficiency gains. To address this, we revisit the model-based paradigm of
Robust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network
(DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware
prior into a learnable architecture. Unlike conventional deep unfolding methods
that rely on static, globally learned parameters, DRPCA-Net introduces a
dynamic unfolding mechanism via a lightweight hypernetwork. This design enables
the model to adaptively generate iteration-wise parameters conditioned on the
input scene, thereby enhancing its robustness and generalization across diverse
backgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to
better capture contextual variations within the background, leading to more
accurate low-rank estimation and improved separation of small targets.
Extensive experiments on multiple public infrared datasets demonstrate that
DRPCA-Net significantly outperforms existing state-of-the-art methods in
detection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net.

</details>


### [273] [SeqCSIST: Sequential Closely-Spaced Infrared Small Target Unmixing](https://arxiv.org/abs/2507.09556)
*Ximeng Zhai,Bohan Xu,Yaohong Chen,Hao Wang,Kehua Guo,Yimian Dai*

Main category: cs.CV

TL;DR: The paper introduces a novel task of Sequential CSIST Unmixing for detecting closely-spaced infrared small targets, proposes a dataset (SeqCSIST) and toolkit, and presents DeRefNet, a deep learning framework with a TDFA module, achieving a 5.3% mAP improvement.


<details>
  <summary>Details</summary>
Motivation: The challenge of detecting closely-spaced infrared small targets (CSIST) as sub-pixel points due to hardware limitations and the lack of public datasets motivated this work.

Method: The authors propose DeRefNet, a model-driven deep learning framework with a Temporal Deformable Feature Alignment (TDFA) module for inter-frame information aggregation, and introduce the SeqCSIST dataset and toolkit.

Result: DeRefNet outperforms state-of-the-art methods by improving mean Average Precision (mAP) by 5.3% on the SeqCSIST dataset.

Conclusion: This work pioneers the CSIST Unmixing task in a multi-frame paradigm, providing a dataset, toolkit, and a high-performing method (DeRefNet).

Abstract: Due to the limitation of the optical lens focal length and the resolution of
the infrared detector, distant Closely-Spaced Infrared Small Target (CSIST)
groups typically appear as mixing spots in the infrared image. In this paper,
we propose a novel task, Sequential CSIST Unmixing, namely detecting all
targets in the form of sub-pixel localization from a highly dense CSIST group.
However, achieving such precise detection is an extremely difficult challenge.
In addition, the lack of high-quality public datasets has also restricted the
research progress. To this end, firstly, we contribute an open-source
ecosystem, including SeqCSIST, a sequential benchmark dataset, and a toolkit
that provides objective evaluation metrics for this special task, along with
the implementation of 23 relevant methods. Furthermore, we propose the
Deformable Refinement Network (DeRefNet), a model-driven deep learning
framework that introduces a Temporal Deformable Feature Alignment (TDFA) module
enabling adaptive inter-frame information aggregation. To the best of our
knowledge, this work is the first endeavor to address the CSIST Unmixing task
within a multi-frame paradigm. Experiments on the SeqCSIST dataset demonstrate
that our method outperforms the state-of-the-art approaches with mean Average
Precision (mAP) metric improved by 5.3\%. Our dataset and toolkit are available
from https://github.com/GrokCV/SeqCSIST.

</details>


### [274] [EHPE: A Segmented Architecture for Enhanced Hand Pose Estimation](https://arxiv.org/abs/2507.09560)
*Bolun Zheng,Xinjie Liu,Qianyu Zhang,Canjin Wang,Fangni Chen,Mingen Xu*

Main category: cs.CV

TL;DR: The paper proposes EHPE, a segmented architecture for 3D hand pose estimation, focusing on TIP and wrist joints to reduce error accumulation and improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook the importance of TIP and wrist joints, leading to error accumulation and degraded pose estimation quality.

Method: EHPE uses a two-stage approach: TW-stage for TIP and wrist joint extraction, and PG-stage for refining remaining joints with a dual-branch network.

Result: EHPE achieves state-of-the-art performance on two benchmarks.

Conclusion: The segmented architecture effectively addresses error accumulation, enhancing overall hand pose estimation accuracy.

Abstract: 3D hand pose estimation has garnered great attention in recent years due to
its critical applications in human-computer interaction, virtual reality, and
related fields. The accurate estimation of hand joints is essential for
high-quality hand pose estimation. However, existing methods neglect the
importance of Distal Phalanx Tip (TIP) and Wrist in predicting hand joints
overall and often fail to account for the phenomenon of error accumulation for
distal joints in gesture estimation, which can cause certain joints to incur
larger errors, resulting in misalignments and artifacts in the pose estimation
and degrading the overall reconstruction quality. To address this challenge, we
propose a novel segmented architecture for enhanced hand pose estimation
(EHPE). We perform local extraction of TIP and wrist, thus alleviating the
effect of error accumulation on TIP prediction and further reduce the
predictive errors for all joints on this basis. EHPE consists of two key
stages: In the TIP and Wrist Joints Extraction stage (TW-stage), the positions
of the TIP and wrist joints are estimated to provide an initial accurate joint
configuration; In the Prior Guided Joints Estimation stage (PG-stage), a
dual-branch interaction network is employed to refine the positions of the
remaining joints. Extensive experiments on two widely used benchmarks
demonstrate that EHPE achieves state-of-the-arts performance. Code is available
at https://github.com/SereinNout/EHPE.

</details>


### [275] [Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges](https://arxiv.org/abs/2507.09562)
*Yidong Jiang*

Main category: cs.CV

TL;DR: This paper surveys prompt engineering techniques for the Segment Anything Model (SAM), analyzing methodologies, applications, and challenges, and highlights its evolution and impact across domains.


<details>
  <summary>Details</summary>
Motivation: The critical role of prompt engineering in SAM's success is underexplored, prompting a need for a comprehensive survey to organize and analyze this emerging field.

Method: The paper systematically reviews and categorizes existing work on prompt engineering for SAM, covering methodologies, applications, and challenges.

Result: The survey reveals the evolution of prompt engineering from simple to multimodal approaches, enabling SAM's adaptation in diverse domains like medical imaging and remote sensing.

Conclusion: The paper fills a literature gap by providing a structured framework for advancing prompt engineering in segmentation foundation models, identifying key challenges and future directions.

Abstract: The Segment Anything Model (SAM) has revolutionized image segmentation
through its innovative prompt-based approach, yet the critical role of prompt
engineering in its success remains underexplored. This paper presents the first
comprehensive survey focusing specifically on prompt engineering techniques for
SAM and its variants. We systematically organize and analyze the rapidly
growing body of work in this emerging field, covering fundamental
methodologies, practical applications, and key challenges. Our review reveals
how prompt engineering has evolved from simple geometric inputs to
sophisticated multimodal approaches, enabling SAM's adaptation across diverse
domains including medical imaging and remote sensing. We identify unique
challenges in prompt optimization and discuss promising research directions.
This survey fills an important gap in the literature by providing a structured
framework for understanding and advancing prompt engineering in foundation
models for segmentation.

</details>


### [276] [MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models](https://arxiv.org/abs/2507.09574)
*Haozhe Zhao,Zefan Cai,Shuzheng Si,Liang Chen,Jiuxiang Gu,Wen Xiao,Junjie Hu*

Main category: cs.CV

TL;DR: MENTOR is an autoregressive framework for multimodal image generation, addressing limitations of current models with a two-stage training approach for better alignment and controllability.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models lack precise visual control, struggle with multimodal inputs, and require extensive training.

Method: MENTOR uses an autoregressive image generator with two-stage training: multimodal alignment and instruction tuning for fine-grained token-level alignment.

Result: MENTOR outperforms baselines in concept preservation and prompt following, with superior image reconstruction and training efficiency.

Conclusion: MENTOR offers a scalable, efficient solution for multimodal image generation with improved controllability and performance.

Abstract: Recent text-to-image models produce high-quality results but still struggle
with precise visual control, balancing multimodal inputs, and requiring
extensive training for complex multimodal image generation. To address these
limitations, we propose MENTOR, a novel autoregressive (AR) framework for
efficient Multimodal-conditioned Tuning for Autoregressive multimodal image
generation. MENTOR combines an AR image generator with a two-stage training
paradigm, enabling fine-grained, token-level alignment between multimodal
inputs and image outputs without relying on auxiliary adapters or
cross-attention modules. The two-stage training consists of: (1) a multimodal
alignment stage that establishes robust pixel- and semantic-level alignment,
followed by (2) a multimodal instruction tuning stage that balances the
integration of multimodal inputs and enhances generation controllability.
Despite modest model size, suboptimal base components, and limited training
resources, MENTOR achieves strong performance on the DreamBench++ benchmark,
outperforming competitive baselines in concept preservation and prompt
following. Additionally, our method delivers superior image reconstruction
fidelity, broad task adaptability, and improved training efficiency compared to
diffusion-based methods. Dataset, code, and models are available at:
https://github.com/HaozheZhao/MENTOR

</details>


### [277] [WordCraft: Interactive Artistic Typography with Attention Awareness and Noise Blending](https://arxiv.org/abs/2507.09573)
*Zhe Wang,Jingbo Zhang,Tianyi Wei,Wanchao Su,Can Wang*

Main category: cs.CV

TL;DR: WordCraft is an interactive artistic typography system using diffusion models for automated, high-quality character stylization with localized edits and iterative refinement.


<details>
  <summary>Details</summary>
Motivation: Existing generative models lack interactivity, localized edits, and multi-character support, limiting creative workflows.

Method: WordCraft integrates diffusion models with a training-free regional attention mechanism and noise blending for precise, multi-region generation and refinement. It also uses a large language model for prompt interpretation.

Result: The system synthesizes high-quality, stylized typography for single- and multi-character inputs across languages, enhancing interactivity.

Conclusion: WordCraft advances artistic typography by enabling flexible, intent-driven generation and diverse creative workflows.

Abstract: Artistic typography aims to stylize input characters with visual effects that
are both creative and legible. Traditional approaches rely heavily on manual
design, while recent generative models, particularly diffusion-based methods,
have enabled automated character stylization. However, existing solutions
remain limited in interactivity, lacking support for localized edits, iterative
refinement, multi-character composition, and open-ended prompt interpretation.
We introduce WordCraft, an interactive artistic typography system that
integrates diffusion models to address these limitations. WordCraft features a
training-free regional attention mechanism for precise, multi-region generation
and a noise blending that supports continuous refinement without compromising
visual quality. To support flexible, intent-driven generation, we incorporate a
large language model to parse and structure both concrete and abstract user
prompts. These components allow our framework to synthesize high-quality,
stylized typography across single- and multi-character inputs across multiple
languages, supporting diverse user-centered workflows. Our system significantly
enhances interactivity in artistic typography synthesis, opening up creative
possibilities for artists and designers.

</details>


### [278] [Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation](https://arxiv.org/abs/2507.09577)
*Ming Yin,Fu Wang,Xujiong Ye,Yanda Meng,Zeyu Fu*

Main category: cs.CV

TL;DR: MA-SAM2 improves surgical video segmentation by addressing SAM2's limitations with context-aware and occlusion-resilient memory models, achieving better performance without extra training.


<details>
  <summary>Details</summary>
Motivation: Surgical video segmentation is crucial for improving surgery quality, but SAM2's limitations hinder performance in complex, long videos due to rapid instrument movement and occlusion.

Method: MA-SAM2 introduces training-free context-aware and occlusion-resilient memory models, using multi-target, single-loop, one-prompt inference for efficiency.

Result: MA-SAM2 outperforms SAM2 by 4.36% and 6.1% on EndoVis2017 and EndoVis2018 datasets, respectively.

Conclusion: MA-SAM2 is a robust, efficient solution for surgical video segmentation, suitable for practical applications.

Abstract: Surgical video segmentation is a critical task in computer-assisted surgery,
essential for enhancing surgical quality and patient outcomes. Recently, the
Segment Anything Model 2 (SAM2) framework has demonstrated remarkable
advancements in both image and video segmentation. However, the inherent
limitations of SAM2's greedy selection memory design are amplified by the
unique properties of surgical videos-rapid instrument movement, frequent
occlusion, and complex instrument-tissue interaction-resulting in diminished
performance in the segmentation of complex, long videos. To address these
challenges, we introduce Memory Augmented (MA)-SAM2, a training-free video
object segmentation strategy, featuring novel context-aware and
occlusion-resilient memory models. MA-SAM2 exhibits strong robustness against
occlusions and interactions arising from complex instrument movements while
maintaining accuracy in segmenting objects throughout videos. Employing a
multi-target, single-loop, one-prompt inference further enhances the efficiency
of the tracking process in multi-instrument videos. Without introducing any
additional parameters or requiring further training, MA-SAM2 achieved
performance improvements of 4.36% and 6.1% over SAM2 on the EndoVis2017 and
EndoVis2018 datasets, respectively, demonstrating its potential for practical
surgical applications.

</details>


### [279] [ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models](https://arxiv.org/abs/2507.09876)
*Yongheng Zhang,Xu Liu,Ruihan Tao,Qiguang Chen,Hao Fei,Wanxiang Che,Libo Qin*

Main category: cs.CV

TL;DR: The paper introduces ViTCoT, a video-text interleaved reasoning paradigm, addressing the gap in current methods that overlook visual modality in video reasoning. It includes a benchmark (ViTIB) and shows improved performance over text-only CoT.


<details>
  <summary>Details</summary>
Motivation: Current video reasoning methods rely heavily on text, ignoring visual content, unlike human reasoning which integrates both. This gap motivates the development of ViTCoT.

Method: The authors propose ViTCoT, a paradigm integrating visual and textual reasoning, and create ViTIB, a benchmark using MLLMs for key-video selection and manual verification.

Result: ViTCoT outperforms text-only CoT and activates more neuron values in MLLMs, demonstrating enhanced video understanding.

Conclusion: ViTCoT offers a more intuitive and effective approach to video reasoning by combining visual and textual modalities, validated by improved performance and neural activation.

Abstract: Video understanding plays a vital role in bridging low-level visual signals
with high-level cognitive reasoning, and is fundamental to applications such as
autonomous driving, embodied AI, and the broader pursuit of AGI. The rapid
development of large language models (LLMs), particularly those utilizing
Chain-of-Thought (CoT) technology, has significantly advanced video reasoning
capabilities. However, current approaches primarily depend on textual
information for reasoning, overlooking the visual modality in the actual video
reasoning process. In contrast, humans naturally re-examine visual content
while reasoning. Motivated by this, we introduce a novel video reasoning
paradigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive
and cognitively aligned reasoning. To the end, first, we construct the
Video-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for
key-video selection and manually verified. Furthermore, we extensively explore
the potential of the ViTCoT paradigm in the video understanding field.
Extensive experiments demonstrate that ViTCoT significantly enhances
performance compared to the traditional text-only CoT paradigm and effectively
activates more neuron values in MLLMs.

</details>


### [280] [Demystifying Flux Architecture](https://arxiv.org/abs/2507.09595)
*Or Greenberg*

Main category: cs.CV

TL;DR: FLUX.1 is a state-of-the-art text-to-image model, reverse-engineered for clarity due to lack of official documentation.


<details>
  <summary>Details</summary>
Motivation: To support FLUX's adoption by demystifying its architecture and training setup through reverse-engineering.

Method: Reverse-engineering the model's source code to analyze its architecture and training setup.

Result: FLUX outperforms models like Midjourney and DALL-E 3 in text-to-image generation.

Conclusion: The report provides unofficial insights into FLUX's design, aiding future research and development.

Abstract: FLUX.1 is a diffusion-based text-to-image generation model developed by Black
Forest Labs, designed to achieve faithful text-image alignment while
maintaining high image quality and diversity. FLUX is considered
state-of-the-art in text-to-image generation, outperforming popular models such
as Midjourney, DALL-E 3, Stable Diffusion 3 (SD3), and SDXL. Although publicly
available as open source, the authors have not released official technical
documentation detailing the model's architecture or training setup. This report
summarizes an extensive reverse-engineering effort aimed at demystifying FLUX's
architecture directly from its source code, to support its adoption as a
backbone for future research and development. This document is an unofficial
technical report and is not published or endorsed by the original developers or
their affiliated institutions.

</details>


### [281] [Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive](https://arxiv.org/abs/2507.09612)
*You Huang,Lichao Chen,Jiayi Ji,Liujuan Cao,Shengchuan Zhang,Rongrong Ji*

Main category: cs.CV

TL;DR: Inter2Former optimizes dense-token processing for interactive segmentation, balancing accuracy and speed on CPU devices with dynamic computation strategies.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between accuracy (dense-token methods) and speed (SAM) in interactive segmentation by improving dense-token processing efficiency.

Method: Introduces Dynamic Prompt Embedding (DPE), Dynamic Hybrid Attention (DHA), Hybrid Mixture of Experts (HMoE), and Dynamic Local Upsampling (DLU) to optimize computation.

Result: Achieves state-of-the-art performance on high-precision IS benchmarks with efficient CPU processing.

Conclusion: Inter2Former successfully bridges the gap between accuracy and speed in interactive segmentation, making it practical for real-world applications.

Abstract: Interactive segmentation (IS) improves annotation efficiency by segmenting
target regions from user prompts, with widespread applications in real-world
scenarios. Current approaches face a critical trade-off: dense-token methods
achieve superior accuracy and detail preservation but suffer from prohibitively
slow processing on CPU devices, while the Segment Anything Model (SAM) advances
the field with sparse prompt tokens for fast inference but compromises
segmentation quality. In this paper, we propose Inter2Former to address this
challenge by optimizing computation allocation in dense-token processing, which
introduces four key enhancements. First, we propose Dynamic Prompt Embedding
(DPE) that adaptively processes only regions of interest while avoiding
additional overhead from background tokens. Second, we introduce Dynamic Hybrid
Attention (DHA), which leverages previous segmentation masks to route tokens
through either full attention (O(N2)) for boundary regions or our proposed
efficient BSQ attention (O(N)) for non-boundary regions. Third, we develop
Hybrid Mixture of Experts (HMoE), which applies similar adaptive computation
strategies in FFN modules with CPU-optimized parallel processing. Finally, we
present Dynamic Local Upsampling (DLU), a reverse operation of DPE, which
localizes objects with a lightweight MLP and performs fine-grained upsampling
only in detected regions. Experimental results on high-precision IS benchmarks
demonstrate that Inter2Former achieves SOTA performance with high efficiency on
CPU devices.

</details>


### [282] [Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect](https://arxiv.org/abs/2507.10013)
*Tom Kouwenhoven,Kiana Shahrasbi,Tessa Verhoef*

Main category: cs.CV

TL;DR: The paper re-evaluates the bouba-kiki effect in vision-and-language models (VLMs), finding they lack consistent human-like cross-modal associations.


<details>
  <summary>Details</summary>
Motivation: To assess if VLMs integrate cross-modal information like humans, using the bouba-kiki effect as a test case.

Method: Evaluated CLIP variants (ResNet, ViT) using prompt-based probabilities and Grad-CAM for visual attention in shape-word matching.

Result: VLMs do not consistently show the bouba-kiki effect; their responses differ significantly from human data.

Conclusion: VLMs' cross-modal understanding is limited, highlighting gaps in their alignment with human cognition.

Abstract: Recent advances in multimodal models have raised questions about whether
vision-and-language models (VLMs) integrate cross-modal information in ways
that reflect human cognition. One well-studied test case in this domain is the
bouba-kiki effect, where humans reliably associate pseudowords like "bouba"
with round shapes and "kiki" with jagged ones. Given the mixed evidence found
in prior studies for this effect in VLMs, we present a comprehensive
re-evaluation focused on two variants of CLIP, ResNet and Vision Transformer
(ViT), given their centrality in many state-of-the-art VLMs. We apply two
complementary methods closely modelled after human experiments: a prompt-based
evaluation that uses probabilities as model preference, and we use Grad-CAM as
a novel way to interpret visual attention in shape-word matching tasks. Our
findings show that these models do not consistently exhibit the bouba-kiki
effect. While ResNet shows a preference for round shapes, overall performance
across both models lacks the expected associations. Moreover, direct comparison
with prior human data on the same task shows that the models' responses fall
markedly short of the robust, modality-integrated behaviour characteristic of
human cognition. These results contribute to the ongoing debate about the
extent to which VLMs truly understand cross-modal concepts, highlighting
limitations in their internal representations and alignment with human
intuitions.

</details>


### [283] [Towards Fine-Grained Adaptation of CLIP via a Self-Trained Alignment Score](https://arxiv.org/abs/2507.09615)
*Eman Ali,Sathira Silva,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: FAIR improves fine-grained unsupervised adaptation in VLMs by dynamically aligning image and text features, outperforming SOTA methods by 2.78%.


<details>
  <summary>Details</summary>
Motivation: Existing methods for unsupervised adaptation in VLMs either use fixed alignment scores or costly pseudo-labeling, failing to capture fine-grained distinctions.

Method: FAIR introduces Class Description Anchors (CDA) and Learned Alignment Score (LAS) to dynamically align features and refine pseudo-labels.

Result: FAIR achieves a 2.78% performance gain over SOTA methods across 13 fine-grained datasets.

Conclusion: FAIR enhances fine-grained classification in VLMs through dynamic alignment and refined pseudo-labeling.

Abstract: Vision-language models (VLMs) like CLIP excel in zero-shot learning by
aligning image and text representations through contrastive pretraining.
Existing approaches to unsupervised adaptation (UA) for fine-grained
classification with VLMs either rely on fixed alignment scores that cannot
capture evolving, subtle class distinctions or use computationally expensive
pseudo-labeling strategies that limit scalability. In contrast, we show that
modeling fine-grained cross-modal interactions during adaptation produces more
accurate, class-discriminative pseudo-labels and substantially improves
performance over state-of-the-art (SOTA) methods. We introduce Fine-grained
Alignment and Interaction Refinement (FAIR), an innovative approach that
dynamically aligns localized image features with descriptive language
embeddings through a set of Class Description Anchors (CDA). This enables the
definition of a Learned Alignment Score (LAS), which incorporates CDA as an
adaptive classifier, facilitating cross-modal interactions to improve
self-training in unsupervised adaptation. Furthermore, we propose a
self-training weighting mechanism designed to refine pseudo-labels in the
presence of inter-class ambiguities. Our approach, FAIR, delivers a substantial
performance boost in fine-grained unsupervised adaptation, achieving a notable
overall gain of 2.78% across 13 fine-grained datasets compared to SOTA methods.

</details>


### [284] [Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection](https://arxiv.org/abs/2507.09619)
*Yilin Lu,Jianghang Lin,Linhuang Xie,Kai Zhao,Yansong Qu,Shengchuan Zhang,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: GAA is a few-shot anomaly synthesis framework using a pretrained latent diffusion model to generate realistic, aligned anomaly-mask pairs, improving localization and classification tasks.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly synthesis methods suffer from low realism, poor mask alignment, and weak generalization due to scarce anomaly samples.

Method: GAA uses Localized Concept Decomposition and Adaptive Multi-Round Anomaly Clustering for semantic and spatial control, along with region-guided mask generation and quality filtering.

Result: GAA outperforms on MVTec AD and LOCO datasets in anomaly synthesis quality and downstream tasks.

Conclusion: GAA effectively addresses limitations of existing methods, enhancing anomaly inspection in industrial manufacturing.

Abstract: Anomaly inspection plays a vital role in industrial manufacturing, but the
scarcity of anomaly samples significantly limits the effectiveness of existing
methods in tasks such as localization and classification. While several anomaly
synthesis approaches have been introduced for data augmentation, they often
struggle with low realism, inaccurate mask alignment, and poor generalization.
To overcome these limitations, we propose Generate Aligned Anomaly (GAA), a
region-guided, few-shot anomaly image-mask pair generation framework. GAA
leverages the strong priors of a pretrained latent diffusion model to generate
realistic, diverse, and semantically aligned anomalies using only a small
number of samples. The framework first employs Localized Concept Decomposition
to jointly model the semantic features and spatial information of anomalies,
enabling flexible control over the type and location of anomalies. It then
utilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained
semantic clustering of anomaly concepts, thereby enhancing the consistency of
anomaly representations. Subsequently, a region-guided mask generation strategy
ensures precise alignment between anomalies and their corresponding masks,
while a low-quality sample filtering module is introduced to further improve
the overall quality of the generated samples. Extensive experiments on the
MVTec AD and LOCO datasets demonstrate that GAA achieves superior performance
in both anomaly synthesis quality and downstream tasks such as localization and
classification.

</details>


### [285] [FaceLLM: A Multimodal Large Language Model for Face Understanding](https://arxiv.org/abs/2507.10300)
*Hatef Otroshi Shahreza,Sébastien Marcel*

Main category: cs.CV

TL;DR: FaceLLM is a multimodal large language model specialized for facial image understanding, trained using weakly supervised data generated by ChatGPT from the FairFace dataset. It outperforms generic MLLMs on face-centric tasks.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs lack domain-specific training for facial cues, limiting their performance on tasks requiring detailed facial understanding.

Method: Proposes a weakly supervised pipeline using ChatGPT to generate annotated question-answer pairs (FairFaceGPT) from the FairFace dataset, then trains FaceLLM on this data.

Result: FaceLLM achieves state-of-the-art performance on face-centric tasks, demonstrating the effectiveness of synthetic supervision.

Conclusion: FaceLLM sets a precedent for domain-specialized MLLMs and trustworthy human-centric AI, with publicly available datasets and models.

Abstract: Multimodal large language models (MLLMs) have shown remarkable performance in
vision-language tasks. However, existing MLLMs are primarily trained on generic
datasets, limiting their ability to reason on domain-specific visual cues such
as those in facial images. In particular, tasks that require detailed
understanding of facial structure, expression, emotion, and demographic
features remain underexplored by MLLMs due to the lack of large-scale annotated
face image-text datasets. In this work, we introduce FaceLLM, a multimodal
large language model trained specifically for facial image understanding. To
construct the training data, we propose a novel weakly supervised pipeline that
uses ChatGPT with attribute-aware prompts to generate high-quality
question-answer pairs based on images from the FairFace dataset. The resulting
corpus, called FairFaceGPT, covers a diverse set of attributes including
expression, pose, skin texture, and forensic information. Our experiments
demonstrate that FaceLLM improves the performance of MLLMs on various
face-centric tasks and achieves state-of-the-art performance. This work
highlights the potential of synthetic supervision via language models for
building domain-specialized MLLMs, and sets a precedent for trustworthy,
human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM
models are publicly available in the project page.

</details>


### [286] [Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI](https://arxiv.org/abs/2507.09630)
*Shomukh Qari,Maha A. Thafar*

Main category: cs.CV

TL;DR: An AI framework using MaxViT for multiclass stroke classification from CT scans achieved 98% accuracy, integrating XAI for transparency.


<details>
  <summary>Details</summary>
Motivation: Early and accurate stroke diagnosis is critical for patient outcomes, especially in emergencies, but current methods lack transparency and trust.

Method: Used MaxViT and other transformer variants for classification, with data augmentation and synthetic image generation to address class imbalance.

Result: MaxViT with augmentation achieved 98% accuracy and F1-score, outperforming other models.

Conclusion: The study provides a trustworthy, interpretable AI tool for stroke diagnosis, enhancing clinical practice and emergency care.

Abstract: Stroke is one of the leading causes of death globally, making early and
accurate diagnosis essential for improving patient outcomes, particularly in
emergency settings where timely intervention is critical. CT scans are the key
imaging modality because of their speed, accessibility, and cost-effectiveness.
This study proposed an artificial intelligence framework for multiclass stroke
classification (ischemic, hemorrhagic, and no stroke) using CT scan images from
a dataset provided by the Republic of Turkey's Ministry of Health. The proposed
method adopted MaxViT, a state-of-the-art Vision Transformer, as the primary
deep learning model for image-based stroke classification, with additional
transformer variants (vision transformer, transformer-in-transformer, and
ConvNext). To enhance model generalization and address class imbalance, we
applied data augmentation techniques, including synthetic image generation. The
MaxViT model trained with augmentation achieved the best performance, reaching
an accuracy and F1-score of 98.00%, outperforming all other evaluated models
and the baseline methods. The primary goal of this study was to distinguish
between stroke types with high accuracy while addressing crucial issues of
transparency and trust in artificial intelligence models. To achieve this,
Explainable Artificial Intelligence (XAI) was integrated into the framework,
particularly Grad-CAM++. It provides visual explanations of the model's
decisions by highlighting relevant stroke regions in the CT scans and
establishing an accurate, interpretable, and clinically applicable solution for
early stroke detection. This research contributed to the development of a
trustworthy AI-assisted diagnostic tool for stroke, facilitating its
integration into clinical practice and enhancing access to timely and optimal
stroke diagnosis in emergency departments, thereby saving more lives.

</details>


### [287] [Devanagari Handwritten Character Recognition using Convolutional Neural Network](https://arxiv.org/abs/2507.10398)
*Diksha Mehta,Prateek Mehta*

Main category: cs.CV

TL;DR: The paper presents a deep learning approach for recognizing handwritten Devanagari characters, achieving high accuracy (96.36% testing, 99.55% training) using convolutional neural networks.


<details>
  <summary>Details</summary>
Motivation: The lack of proper digitization tools for the Devanagari script and the growing need for applications like search engines and social media drive this research.

Method: The study uses two deep convolutional neural network layers on the Devanagari Handwritten Character Dataset (DHCD), with 36 classes and 1700 images per class for training/testing.

Result: The method achieves 96.36% accuracy in testing and 99.55% in training.

Conclusion: The proposed technique effectively enhances recognition rates for Devanagari handwritten characters, demonstrating its potential for practical applications.

Abstract: Handwritten character recognition is getting popular among researchers
because of its possible applications in facilitating technological search
engines, social media, recommender systems, etc. The Devanagari script is one
of the oldest language scripts in India that does not have proper digitization
tools. With the advancement of computing and technology, the task of this
research is to extract handwritten Hindi characters from an image of Devanagari
script with an automated approach to save time and obsolete data. In this
paper, we present a technique to recognize handwritten Devanagari characters
using two deep convolutional neural network layers. This work employs a
methodology that is useful to enhance the recognition rate and configures a
convolutional neural network for effective Devanagari handwritten text
recognition (DHTR). This approach uses the Devanagari handwritten character
dataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each
of these classes has 1700 images for training and testing purposes. This
approach obtains promising results in terms of accuracy by achieving 96.36%
accuracy in testing and 99.55% in training time.

</details>


### [288] [Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams](https://arxiv.org/abs/2507.09640)
*Leonor Fernandes,Tiago Gonçalves,João Matos,Luis Filipe Nakayama,Jaime S. Cardoso*

Main category: cs.CV

TL;DR: The paper evaluates AI models for diabetic retinopathy (DR) prediction, focusing on fairness and bias mitigation using disentanglement. It highlights performance disparities and mixed results of disentanglement across models.


<details>
  <summary>Details</summary>
Motivation: To address concerns about fairness and generalization in AI-based DR diagnosis, ensuring equitable healthcare solutions.

Method: Three models (ConvNeXt V2, DINOv2, Swin V2) were trained on macula images to predict DR and sensitive attributes (age, gender/sex). Fairness was assessed, and disentanglement was applied to mitigate bias.

Result: High DR prediction performance (up to 94% AUROC) and reasonable SA prediction (91% for age, 77% for gender/sex). Disentanglement improved DINOv2 but harmed others. Fairness gaps (e.g., 10% AUROC between age groups) were observed.

Conclusion: Fairness in medical AI is complex; disentanglement's effectiveness varies by model. Ensuring equitable healthcare requires addressing bias and model-specific challenges.

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss in working-age
adults. While screening reduces the risk of blindness, traditional imaging is
often costly and inaccessible. Artificial intelligence (AI) algorithms present
a scalable diagnostic solution, but concerns regarding fairness and
generalization persist. This work evaluates the fairness and performance of
image-trained models in DR prediction, as well as the impact of disentanglement
as a bias mitigation technique, using the diverse mBRSET fundus dataset. Three
models, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to
predict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness
was assessed between subgroups of SAs, and disentanglement was applied to
reduce bias. All models achieved high DR prediction performance in diagnosing
(up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77%
AUROC, respectively). Fairness assessment suggests disparities, such as a 10%
AUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction
had varying results, depending on the model selected. Disentanglement improved
DINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2
and Swin V2 (7% and 3%, respectively). These findings highlight the complexity
of disentangling fine-grained features in fundus imaging and emphasize the
importance of fairness in medical imaging AI to ensure equitable and reliable
healthcare solutions.

</details>


### [289] [Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources](https://arxiv.org/abs/2507.10403)
*Daniele Rege Cambrin,Lorenzo Vaiani,Giuseppe Gallipoli,Luca Cagliero,Paolo Garza*

Main category: cs.CV

TL;DR: CrisisLandMark introduces a large-scale corpus of SAR and multispectral images with structured text annotations, and CLOSP/GeoCLOSP frameworks improve retrieval by aligning optical and SAR data using text and geographic context.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image retrieval systems often ignore non-RGB sensor data like SAR and multispectral imagery, limiting their utility for applications like disaster response and climate monitoring.

Method: Developed CrisisLandMark corpus with 647,000+ Sentinel-1 SAR and Sentinel-2 multispectral images paired with structured text. Introduced CLOSP for contrastive pretraining to align optical and SAR data via text, and GeoCLOSP to integrate geographic coordinates.

Result: CLOSP improves retrieval nDGC by 54% over existing models. GeoCLOSP excels in location-dependent tasks, balancing generality and specificity.

Conclusion: Integrating diverse sensor data and geographic context is key to maximizing remote sensing archive potential.

Abstract: Retrieving relevant imagery from vast satellite archives is crucial for
applications like disaster response and long-term climate monitoring. However,
most text-to-image retrieval systems are limited to RGB data, failing to
exploit the unique physical information captured by other sensors, such as the
all-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the
spectral signatures in optical multispectral data. To bridge this gap, we
introduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1
SAR and Sentinel-2 multispectral images paired with structured textual
annotations for land cover, land use, and crisis events harmonized from
authoritative land cover systems (CORINE and Dynamic World) and crisis-specific
sources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),
a novel framework that uses text as a bridge to align unpaired optical and SAR
images into a unified embedding space. Our experiments show that CLOSP achieves
a new state-of-the-art, improving retrieval nDGC by 54% over existing models.
Additionally, we find that the unified training strategy overcomes the inherent
difficulty of interpreting SAR imagery by transferring rich semantic knowledge
from the optical domain with indirect interaction. Furthermore, GeoCLOSP, which
integrates geographic coordinates into our framework, creates a powerful
trade-off between generality and specificity: while the CLOSP excels at general
semantic tasks, the GeoCLOSP becomes a specialized expert for retrieving
location-dependent crisis events and rare geographic features. This work
highlights that the integration of diverse sensor data and geographic context
is essential for unlocking the full potential of remote sensing archives.

</details>


### [290] [EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR](https://arxiv.org/abs/2507.09649)
*Zhengyuan Peng,Jianqing Xu,Shen Li,Jiazhen Ji,Yuge Huang,Jingyun Zhang,Jinmin Li,Shouhong Ding,Rizen Guo,Xin Tan,Lizhuang Ma*

Main category: cs.CV

TL;DR: EyeSeg is an uncertainty-aware eye segmentation framework for AR/VR, addressing challenges like motion blur and domain gaps by modeling uncertainties, outperforming existing methods in segmentation and gaze estimation.


<details>
  <summary>Details</summary>
Motivation: Accurate gaze estimation in AR/VR requires robust eye segmentation, but existing methods struggle with motion blur, eyelid occlusion, and domain gaps.

Method: EyeSeg uses Bayesian uncertainty learning to model segmentation uncertainties, outputting an uncertainty score and segmentation result for robust gaze estimation.

Result: EyeSeg improves segmentation metrics (MIoU, E1, F1, ACC) and outperforms existing methods, especially under challenging conditions.

Conclusion: EyeSeg effectively addresses key challenges in eye segmentation for AR/VR, enhancing gaze estimation robustness and performance.

Abstract: Human-machine interaction through augmented reality (AR) and virtual reality
(VR) is increasingly prevalent, requiring accurate and efficient gaze
estimation which hinges on the accuracy of eye segmentation to enable smooth
user experiences. We introduce EyeSeg, a novel eye segmentation framework
designed to overcome key challenges that existing approaches struggle with:
motion blur, eyelid occlusion, and train-test domain gaps. In these situations,
existing models struggle to extract robust features, leading to suboptimal
performance. Noting that these challenges can be generally quantified by
uncertainty, we design EyeSeg as an uncertainty-aware eye segmentation
framework for AR/VR wherein we explicitly model the uncertainties by performing
Bayesian uncertainty learning of a posterior under the closed set prior.
Theoretically, we prove that a statistic of the learned posterior indicates
segmentation uncertainty levels and empirically outperforms existing methods in
downstream tasks, such as gaze estimation. EyeSeg outputs an uncertainty score
and the segmentation result, weighting and fusing multiple gaze estimates for
robustness, which proves to be effective especially under motion blur, eyelid
occlusion and cross-domain challenges. Moreover, empirical results suggest that
EyeSeg achieves segmentation improvements of MIoU, E1, F1, and ACC surpassing
previous approaches. The code is publicly available at
https://github.com/JethroPeng/EyeSeg.

</details>


### [291] [VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation](https://arxiv.org/abs/2507.09672)
*Xinyu Zhang,Zhonghao Ye,Jingwei Zhang,Xiang Tian,Zhisheng Liang,Shipeng Yu*

Main category: cs.CV

TL;DR: VST-Pose is a WiFi-based human pose estimation framework using deep learning, achieving high accuracy with a novel dual-stream architecture and velocity modeling.


<details>
  <summary>Details</summary>
Motivation: To provide a non-visual, privacy-preserving solution for accurate and continuous human pose estimation in indoor environments.

Method: Introduces ViSTA-Former, a spatiotemporal attention backbone with dual-stream architecture, and integrates a velocity modeling branch for fine-grained motion representation.

Result: Achieves 92.2% accuracy on PCK@50, outperforming existing methods by 8.3%, and shows robustness on the MMFi dataset.

Conclusion: VST-Pose offers a reliable, privacy-aware solution for indoor human motion analysis, with code publicly available.

Abstract: WiFi-based human pose estimation has emerged as a promising non-visual
alternative approaches due to its pene-trability and privacy advantages. This
paper presents VST-Pose, a novel deep learning framework for accurate and
continuous pose estimation using WiFi channel state information. The proposed
method introduces ViSTA-Former, a spatiotemporal attention backbone with
dual-stream architecture that adopts a dual-stream architecture to separately
capture temporal dependencies and structural relationships among body joints.
To enhance sensitivity to subtle human motions, a velocity modeling branch is
integrated into the framework, which learns short-term keypoint dis-placement
patterns and improves fine-grained motion representation. We construct a 2D
pose dataset specifically designed for smart home care scenarios and
demonstrate that our method achieves 92.2% accuracy on the PCK@50 metric,
outperforming existing methods by 8.3% in PCK@50 on the self-collected dataset.
Further evaluation on the public MMFi dataset confirms the model's robustness
and effectiveness in 3D pose estimation tasks. The proposed system provides a
reliable and privacy-aware solution for continuous human motion analysis in
indoor environments. Our codes are available in
https://github.com/CarmenQing/VST-Pose.

</details>


### [292] [Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model](https://arxiv.org/abs/2507.09681)
*Osher Rafaeli,Tal Svoray,Ariel Nahlieli*

Main category: cs.CV

TL;DR: A framework for high-resolution DEM estimation using prompt-based monocular depth estimation, achieving 100x resolution gain and robust generalization across diverse landscapes.


<details>
  <summary>Details</summary>
Motivation: High-resolution elevation data is crucial for hydrology, urban studies, and ecosystem monitoring, but existing methods like super-resolution and monocular depth estimation have limitations.

Method: Uses low-resolution SRTM data as prompts with high-resolution NAIP imagery, fine-tuning a vision transformer encoder with LiDAR-derived DEMs for tasks like DEM estimation, void filling, and updating.

Result: Achieves 30-cm resolution (100x gain), <5 m MAE relative to LiDAR, and improves over SRTM by up to 18%. Suitable for hydrological and environmental studies.

Conclusion: The framework offers scalable, high-resolution DEM estimation with public code and models, advancing global elevation mapping.

Abstract: High-resolution elevation estimations are essential to understand catchment
and hillslope hydrology, study urban morphology and dynamics, and monitor the
growth, decline, and mortality of terrestrial ecosystems. Various deep learning
approaches (e.g., super-resolution techniques, monocular depth estimation) have
been developed to create high-resolution Digital Elevation Models (DEMs).
However, super-resolution techniques are limited by the upscaling factor, and
monocular depth estimation lacks global elevation context, making its
conversion to a seamless DEM restricted. The recently introduced technique of
prompt-based monocular depth estimation has opened new opportunities to extract
estimates of absolute elevation in a global context. We present here a
framework for the estimation of high-resolution DEMs as a new paradigm for
absolute global elevation mapping. It is exemplified using low-resolution
Shuttle Radar Topography Mission (SRTM) elevation data as prompts and
high-resolution RGB imagery from the National Agriculture Imagery Program
(NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived
DEMs and employs a versatile prompting strategy, enabling tasks such as DEM
estimation, void filling, and updating. Our framework achieves a 100x
resolution gain (from 30-m to 30-cm), surpassing prior methods by an order of
magnitude. Evaluations across three diverse U.S. landscapes show robust
generalization, capturing urban structures and fine-scale terrain features with
< 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological
analysis confirms suitability for hazard and environmental studies. We
demonstrate scalability by applying the framework to large regions in the U.S.
and Israel. All code and pretrained models are publicly available at:
https://osherr1996.github.io/prompt2dem_propage/.

</details>


### [293] [ExpStar: Towards Automatic Commentary Generation for Multi-discipline Scientific Experiments](https://arxiv.org/abs/2507.09693)
*Jiali Chen,Yujie Jia,Zihan Wu,Jinyu Yang,Jianpeng Chen,Xusen Hei,Jiayuan Xie,Yi Cai,Qing Li*

Main category: cs.CV

TL;DR: The paper introduces ExpStar, a model for automatic experiment commentary generation, and ExpInstruct, a dataset supporting this task. ExpStar outperforms 14 leading LMMs, demonstrating its effectiveness.


<details>
  <summary>Details</summary>
Motivation: Human teachers spend significant time preparing experiment commentary. The paper aims to automate this process using AI.

Method: Constructs ExpInstruct dataset (7K commentaries across 21 subjects) and proposes ExpStar, a retrieval-augmented model for commentary generation.

Result: ExpStar outperforms 14 leading LMMs, showcasing its superiority.

Conclusion: ExpStar advances AI-assisted scientific experiment instruction, with potential for broader applications.

Abstract: Experiment commentary is crucial in describing the experimental procedures,
delving into underlying scientific principles, and incorporating
content-related safety guidelines. In practice, human teachers rely heavily on
subject-specific expertise and invest significant time preparing such
commentary. To address this challenge, we introduce the task of automatic
commentary generation across multi-discipline scientific experiments. While
recent progress in large multimodal models (LMMs) has demonstrated promising
capabilities in video understanding and reasoning, their ability to generate
fine-grained and insightful experiment commentary remains largely
underexplored. In this paper, we make the following contributions: (i) We
construct \textit{ExpInstruct}, the first dataset tailored for experiment
commentary generation, featuring over 7\textit{K} step-level commentaries
across 21 scientific subjects from 3 core disciplines (\ie, science, healthcare
and engineering). Each sample includes procedural descriptions along with
potential scientific principles (\eg, chemical equations and physical laws) and
safety guidelines. (ii) We propose ExpStar, an automatic experiment commentary
generation model that leverages a retrieval-augmented mechanism to adaptively
access, evaluate, and utilize external knowledge. (iii) Extensive experiments
show that our ExpStar substantially outperforms 14 leading LMMs, which
highlights the superiority of our dataset and model. We believe that ExpStar
holds great potential for advancing AI-assisted scientific experiment
instruction.

</details>


### [294] [EmbRACE-3K: Embodied Reasoning and Action in Complex Environments](https://arxiv.org/abs/2507.10548)
*Mingxian Lin,Wei Huang,Yitang Li,Chengjie Jiang,Kui Wu,Fangwei Zhong,Shengju Qian,Xin Wang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: The paper introduces EmRACE-3K, a dataset for evaluating embodied reasoning in vision-language models (VLMs), highlighting their limitations in interactive environments and proposing a fine-tuning approach to improve performance.


<details>
  <summary>Details</summary>
Motivation: Current VLMs excel in passive tasks but struggle in embodied settings requiring active interaction and spatial reasoning. This gap motivates the creation of EmRACE-3K to benchmark and enhance VLM capabilities.

Method: The authors develop EmRACE-3K, a dataset of 3,000 tasks in photorealistic environments, and use it to benchmark VLMs. They fine-tune Qwen2.5-VL-7B with supervised and reinforcement learning.

Result: Zero-shot VLMs perform poorly (<20% success). Fine-tuning Qwen2.5-VL-7B significantly improves performance across exploration, spatial reasoning, and multi-stage tasks.

Conclusion: EmRACE-3K effectively benchmarks and improves VLMs for embodied reasoning, demonstrating the need for specialized datasets and training methods in interactive environments.

Abstract: Recent advanced vision-language models(VLMs) have demonstrated strong
performance on passive, offline image and video understanding tasks. However,
their effectiveness in embodied settings, which require online interaction and
active scene understanding remains limited. In such scenarios, an agent
perceives the environment from a first-person perspective, with each action
dynamically shaping subsequent observations. Even state-of-the-art models such
as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment
interactions, exhibiting clear limitations in spatial reasoning and
long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset
of over 3,000 language-guided tasks situated in diverse, photorealistic
environments constructed using Unreal Engine and the UnrealCV-Zoo framework.
The tasks encompass a wide range of embodied challenges, including navigation,
object manipulation, and multi-stage goal execution. Each task unfolds as a
multi-step trajectory, pairing first-person visual observations with high-level
instructions, grounded actions, and natural language rationales that express
the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to
evaluate the embodied reasoning capabilities of VLMs across three key
dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage
Goal Execution. In zero-shot settings, all models achieve success rates below
20%, underscoring the challenge posed by our benchmark and the current
limitations of VLMs in interactive environments. To demonstrate the utility of
EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning
followed by reinforcement learning. This approach yields substantial
improvements across all three challenge categories, highlighting the dataset's
effectiveness in enabling the development of embodied reasoning capabilities.

</details>


### [295] [Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI](https://arxiv.org/abs/2507.09702)
*Phat Nguyen,Ngai-Man Cheung*

Main category: cs.CV

TL;DR: The paper presents a systematic taxonomy and comparative study of token compression methods for Vision Transformers (ViTs), highlighting gaps in existing research and evaluating their effectiveness on standard and compact ViT architectures.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a unified survey on token compression techniques and their limited evaluation on compact ViTs, which are crucial for edge devices.

Method: The study categorizes token compression approaches (e.g., pruning, merging) and evaluates them on both standard and compact ViT models.

Result: Token compression methods are effective for standard ViTs but often underperform on compact designs.

Conclusion: The findings provide insights for adapting token optimization techniques to compact transformers, guiding future research for edge AI applications.

Abstract: Token compression techniques have recently emerged as powerful tools for
accelerating Vision Transformer (ViT) inference in computer vision. Due to the
quadratic computational complexity with respect to the token sequence length,
these methods aim to remove less informative tokens before the attention layers
to improve inference throughput. While numerous studies have explored various
accuracy-efficiency trade-offs on large-scale ViTs, two critical gaps remain.
First, there is a lack of unified survey that systematically categorizes and
compares token compression approaches based on their core strategies (e.g.,
pruning, merging, or hybrid) and deployment settings (e.g., fine-tuning vs.
plug-in). Second, most benchmarks are limited to standard ViT models (e.g.,
ViT-B, ViT-L), leaving open the question of whether such methods remain
effective when applied to structurally compressed transformers, which are
increasingly deployed on resource-constrained edge devices. To address these
gaps, we present the first systematic taxonomy and comparative study of token
compression methods, and we evaluate representative techniques on both standard
and compact ViT architectures. Our experiments reveal that while token
compression methods are effective for general-purpose ViTs, they often
underperform when directly applied to compact designs. These findings not only
provide practical insights but also pave the way for future research on
adapting token optimization techniques to compact transformer-based networks
for edge AI and AI agent applications.

</details>


### [296] [Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation](https://arxiv.org/abs/2507.09748)
*Yu Lei,Bingde Liu,Qingsong Xie,Haonan Lu,Zhijie Deng*

Main category: cs.CV

TL;DR: The paper introduces Linearized Lookahead Variational Score Distillation ($L^2$-VSD) to improve text-to-3D generation by addressing convergence issues in VSD through optimization order adjustment and linearization.


<details>
  <summary>Details</summary>
Motivation: To address slow and ill-posed convergence in Variational Score Distillation (VSD) for text-to-3D generation, caused by mismatching between LoRA and 3D distributions.

Method: Proposes $L^2$-VSD, which adjusts optimization order and uses a linearized variant of the model for score distillation, leveraging forward-mode autodiff.

Result: $L^2$-VSD outperforms prior score distillation methods and integrates seamlessly into VSD-based frameworks.

Conclusion: The method effectively improves generation quality and stability in text-to-3D tasks.

Abstract: Text-to-3D generation based on score distillation of pre-trained 2D diffusion
models has gained increasing interest, with variational score distillation
(VSD) as a remarkable example. VSD proves that vanilla score distillation can
be improved by introducing an extra score-based model, which characterizes the
distribution of images rendered from 3D models, to correct the distillation
gradient. Despite the theoretical foundations, VSD, in practice, is likely to
suffer from slow and sometimes ill-posed convergence. In this paper, we perform
an in-depth investigation of the interplay between the introduced score model
and the 3D model, and find that there exists a mismatching problem between LoRA
and 3D distributions in practical implementation. We can simply adjust their
optimization order to improve the generation quality. By doing so, the score
model looks ahead to the current 3D state and hence yields more reasonable
corrections. Nevertheless, naive lookahead VSD may suffer from unstable
training in practice due to the potential over-fitting. To address this, we
propose to use a linearized variant of the model for score distillation, giving
rise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD).
$L^2$-VSD can be realized efficiently with forward-mode autodiff
functionalities of existing deep learning libraries. Extensive experiments
validate the efficacy of $L^2$-VSD, revealing its clear superiority over prior
score distillation-based methods. We also show that our method can be
seamlessly incorporated into any other VSD-based text-to-3D framework.

</details>


### [297] [Pairwise Alignment & Compatibility for Arbitrarily Irregular Image Fragments](https://arxiv.org/abs/2507.09767)
*Ofir Itzhak Shahar,Gur Elkin,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: The paper introduces a hybrid geometric and pictorial method for optimal fragment alignment in jigsaw puzzles, handling realistic shapes and erosion, and demonstrates improved performance on a new dataset.


<details>
  <summary>Details</summary>
Motivation: Existing fragment-reconstruction methods struggle with realistic geometric properties and rely on restricted fragment shapes, limiting their applicability to real-life puzzles.

Method: A hybrid (geometric and pictorial) approach for computing optimal alignment of fragment pairs, without shape or content assumptions, using a new dataset and erosion model.

Result: State-of-the-art neighborhood-level precision and recall on the RePAIR 2D dataset, showing compatibility performance improvements.

Conclusion: The proposed hybrid method effectively handles realistic fragment properties and erosion, advancing fragment-reconstruction algorithms.

Abstract: Pairwise compatibility calculation is at the core of most
fragments-reconstruction algorithms, in particular those designed to solve
different types of the jigsaw puzzle problem. However, most existing approaches
fail, or aren't designed to deal with fragments of realistic geometric
properties one encounters in real-life puzzles. And in all other cases,
compatibility methods rely strongly on the restricted shapes of the fragments.
In this paper, we propose an efficient hybrid (geometric and pictorial)
approach for computing the optimal alignment for pairs of fragments, without
any assumptions about their shapes, dimensions, or pictorial content. We
introduce a new image fragments dataset generated via a novel method for image
fragmentation and a formal erosion model that mimics real-world archaeological
erosion, along with evaluation metrics for the compatibility task. We then
embed our proposed compatibility into an archaeological puzzle-solving
framework and demonstrate state-of-the-art neighborhood-level precision and
recall on the RePAIR 2D dataset, directly reflecting compatibility performance
improvements.

</details>


### [298] [NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection](https://arxiv.org/abs/2507.09795)
*Amirhossein Ansari,Ke Wang,Pulei Xiong*

Main category: cs.CV

TL;DR: NegRefine improves zero-shot OOD detection by refining negative labels and addressing multi-label matching issues.


<details>
  <summary>Details</summary>
Motivation: Existing negative label-based methods struggle with false OOD detection due to subcategories/proper nouns and multi-label ambiguity.

Method: NegRefine filters subcategory/proper noun labels and uses a dynamic scoring function for multi-label matching.

Result: NegRefine achieves robust separation between in-distribution and OOD samples on benchmarks like ImageNet-1K.

Conclusion: NegRefine offers a refined approach for zero-shot OOD detection, addressing key limitations of prior methods.

Abstract: Recent advancements in Vision-Language Models like CLIP have enabled
zero-shot OOD detection by leveraging both image and textual label information.
Among these, negative label-based methods such as NegLabel and CSP have shown
promising results by utilizing a lexicon of words to define negative labels for
distinguishing OOD samples. However, these methods suffer from detecting
in-distribution samples as OOD due to negative labels that are subcategories of
in-distribution labels or proper nouns. They also face limitations in handling
images that match multiple in-distribution and negative labels. We propose
NegRefine, a novel negative label refinement framework for zero-shot OOD
detection. By introducing a filtering mechanism to exclude subcategory labels
and proper nouns from the negative label set and incorporating a
multi-matching-aware scoring function that dynamically adjusts the
contributions of multiple labels matching an image, NegRefine ensures a more
robust separation between in-distribution and OOD samples. We evaluate
NegRefine on large-scale benchmarks, including ImageNet-1K. Source code is
available at https://github.com/ah-ansari/NegRefine.

</details>


### [299] [VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding](https://arxiv.org/abs/2507.09815)
*Younggun Kim,Ahmed S. Abdelrahman,Mohamed Abdel-Aty*

Main category: cs.CV

TL;DR: VRU-Accident is a benchmark to evaluate multimodal large language models (MLLMs) in safety-critical VRU scenarios, revealing gaps in reasoning about accident causes and preventability.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of standardized benchmarks for evaluating MLLMs in VRU safety scenarios, crucial for autonomous driving systems.

Method: Developed VRU-Accident with 1K dashcam videos, 6K QA pairs, and 1K dense descriptions, focusing on VRU-vehicle accidents. Evaluated 17 MLLMs on VQA and captioning tasks.

Result: MLLMs perform well on visual attributes but struggle with reasoning about accident causes, types, and preventability.

Conclusion: VRU-Accident highlights the need for improved MLLM reasoning in safety-critical scenarios, providing a foundation for future research.

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, is a critical challenge for autonomous driving systems, as crashes
involving VRUs often result in severe or fatal consequences. While multimodal
large language models (MLLMs) have shown promise in enhancing scene
understanding and decision making in autonomous vehicles, there is currently no
standardized benchmark to quantitatively evaluate their reasoning abilities in
complex, safety-critical scenarios involving VRUs. To address this gap, we
present VRU-Accident, a large-scale vision-language benchmark designed to
evaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident
comprises 1K real-world dashcam accident videos, annotated with 6K
multiple-choice question-answer pairs across six safety-critical categories
(with 24K candidate options and 3.4K unique answer choices), as well as 1K
dense scene descriptions. Unlike prior works, our benchmark focuses explicitly
on VRU-vehicle accidents, providing rich, fine-grained annotations that capture
both spatial-temporal dynamics and causal semantics of accidents. To assess the
current landscape of MLLMs, we conduct a comprehensive evaluation of 17
state-of-the-art models on the multiple-choice VQA task and on the dense
captioning task. Our findings reveal that while MLLMs perform reasonably well
on visually grounded attributes, they face significant challenges in reasoning
and describing accident causes, types, and preventability.

</details>


### [300] [Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models](https://arxiv.org/abs/2507.09830)
*Shuhao Fu,Philip J. Kellman,Hongjing Lu*

Main category: cs.CV

TL;DR: The paper compares human and deep learning models in recognizing 3D objects from sparse point clouds, finding that visual transformers (point transformer) better mimic human performance than convolutional models due to hierarchical abstraction.


<details>
  <summary>Details</summary>
Motivation: To understand whether deep learning models develop 3D shape representations similar to human vision for object recognition.

Method: Conducted human experiments manipulating point density, object orientation, and local geometric structure, and compared performance with DGCNN and point transformer models.

Result: Humans performed consistently well, while the point transformer model matched human performance better than the convolutional model, attributed to its hierarchical abstraction mechanism.

Conclusion: The point transformer model's hierarchical abstraction aligns more closely with human 3D shape representation, suggesting its superiority over convolutional models for mimicking human vision.

Abstract: Both humans and deep learning models can recognize objects from 3D shapes
depicted with sparse visual information, such as a set of points randomly
sampled from the surfaces of 3D objects (termed a point cloud). Although deep
learning models achieve human-like performance in recognizing objects from 3D
shapes, it remains unclear whether these models develop 3D shape
representations similar to those used by human vision for object recognition.
We hypothesize that training with 3D shapes enables models to form
representations of local geometric structures in 3D shapes. However, their
representations of global 3D object shapes may be limited. We conducted two
human experiments systematically manipulating point density and object
orientation (Experiment 1), and local geometric structure (Experiment 2).
Humans consistently performed well across all experimental conditions. We
compared two types of deep learning models, one based on a convolutional neural
network (DGCNN) and the other on visual transformers (point transformer), with
human performance. We found that the point transformer model provided a better
account of human performance than the convolution-based model. The advantage
mainly results from the mechanism in the point transformer model that supports
hierarchical abstraction of 3D shapes.

</details>


### [301] [A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends](https://arxiv.org/abs/2507.09861)
*Yihao Ding,Siwen Luo,Yue Dai,Yanbei Jiang,Zechuan Li,Geoffrey Martin,Yifan Peng*

Main category: cs.CV

TL;DR: The paper surveys Multimodal Large Language Models (MLLMs) in Visually-Rich Document Understanding (VRDU), covering feature fusion, training methods, datasets, and future challenges.


<details>
  <summary>Details</summary>
Motivation: The need to automate processing of documents with complex visual, textual, and layout information drives the study of VRDU.

Method: The survey reviews MLLM-based VRDU, focusing on feature encoding/fusion, training paradigms, and datasets.

Result: MLLMs show promise in VRDU, with advancements in feature fusion, training strategies, and dataset usage.

Conclusion: Challenges remain in VRDU, but future directions aim to improve efficiency, generalizability, and robustness.

Abstract: Visually-Rich Document Understanding (VRDU) has emerged as a critical field,
driven by the need to automatically process documents containing complex
visual, textual, and layout information. Recently, Multimodal Large Language
Models (MLLMs) have shown remarkable potential in this domain, leveraging both
Optical Character Recognition (OCR)-dependent and OCR-free frameworks to
extract and interpret information in document images. This survey reviews
recent advancements in MLLM-based VRDU, highlighting three core components: (1)
methods for encoding and fusing textual, visual, and layout features; (2)
training paradigms, including pretraining strategies, instruction-response
tuning, and the trainability of different model modules; and (3) datasets
utilized for pretraining, instruction-tuning, and supervised fine-tuning.
Finally, we discuss the challenges and opportunities in this evolving field and
propose future directions to advance the efficiency, generalizability, and
robustness of VRDU systems.

</details>


### [302] [SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation](https://arxiv.org/abs/2507.09862)
*Youliang Zhang,Zhaoyang Li,Duomin Wang,Jiahe Zhang,Deyu Zhou,Zixin Yin,Xili Dai,Gang Yu,Xiu Li*

Main category: cs.CV

TL;DR: The paper introduces SpeakerVid-5M, a large-scale dataset for audio-visual dyadic interactive virtual human research, with diverse interaction types and quality tiers, and provides a baseline model and benchmark.


<details>
  <summary>Details</summary>
Motivation: To address the lack of large-scale, high-quality datasets for audio-visual dyadic interactive virtual human generation, hindering progress in this emerging field.

Method: The dataset (SpeakerVid-5M) is structured by interaction type (dialogue, single, listening, multi-turn) and quality (pre-training subset, high-quality SFT subset). An AR-based video chat baseline and benchmark (VidChatBench) are also provided.

Result: SpeakerVid-5M contains 5.2M video clips (8,743 hours) with diverse interaction scenarios. The dataset supports various 2D virtual human tasks.

Conclusion: The release of SpeakerVid-5M and its tools (baseline, benchmark) aims to advance research in audio-visual dyadic interactive virtual humans.

Abstract: The rapid development of large-scale models has catalyzed significant
breakthroughs in the digital human domain. These advanced methodologies offer
high-fidelity solutions for avatar driving and rendering, leading academia to
focus on the next major challenge: audio-visual dyadic interactive virtual
human. To facilitate research in this emerging area, we present SpeakerVid-5M
dataset, the first large-scale, high-quality dataset designed for audio-visual
dyadic interactive virtual human generation. Totaling over 8,743 hours,
SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It
covers diverse scales and interaction types, including monadic talking,
listening, and dyadic conversations. Crucially, the dataset is structured along
two key dimensions: interaction type and data quality. First, it is categorized
into four types (dialogue branch, single branch, listening branch and
multi-turn branch) based on the interaction scenario. Second, it is stratified
into a large-scale pre-training subset and a curated, high-quality subset for
Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of
2D virtual human tasks. In addition, we provide an autoregressive (AR)-based
video chat baseline trained on this data, accompanied by a dedicated set of
metrics and test data to serve as a benchmark VidChatBench for future work.
Both the dataset and the corresponding data processing code will be publicly
released. Project page: https://dorniwang.github.io/SpeakerVid-5M/

</details>


### [303] [OpenHuman4D: Open-Vocabulary 4D Human Parsing](https://arxiv.org/abs/2507.09880)
*Keito Suzuki,Bang Du,Runfa Blark Li,Kunyao Chen,Lei Wang,Peng Liu,Ning Bi,Truong Nguyen*

Main category: cs.CV

TL;DR: A 4D human parsing framework is introduced to reduce inference time and enable open-vocabulary capabilities, outperforming previous methods with 93.3% faster processing.


<details>
  <summary>Details</summary>
Motivation: Existing human part segmentation methods are limited by closed-set datasets and slow inference, hindering their practicality in virtual and extended reality applications.

Method: The framework uses mask-based video object tracking, a Mask Validation module for new targets, and a 4D Mask Fusion module for robust embedding fusion.

Result: Achieves up to 93.3% acceleration compared to prior methods and supports open-vocabulary parsing.

Conclusion: The proposed method effectively addresses limitations in dynamic 3D human representation, offering faster and more flexible parsing.

Abstract: Understanding dynamic 3D human representation has become increasingly
critical in virtual and extended reality applications. However, existing human
part segmentation methods are constrained by reliance on closed-set datasets
and prolonged inference times, which significantly restrict their
applicability. In this paper, we introduce the first 4D human parsing framework
that simultaneously addresses these challenges by reducing the inference time
and introducing open-vocabulary capabilities. Building upon state-of-the-art
open-vocabulary 3D human parsing techniques, our approach extends the support
to 4D human-centric video with three key innovations: 1) We adopt mask-based
video object tracking to efficiently establish spatial and temporal
correspondences, avoiding the necessity of segmenting all frames. 2) A novel
Mask Validation module is designed to manage new target identification and
mitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating
memory-conditioned attention and logits equalization for robust embedding
fusion. Extensive experiments demonstrate the effectiveness and flexibility of
the proposed method on 4D human-centric parsing tasks, achieving up to 93.3%
acceleration compared to the previous state-of-the-art method, which was
limited to parsing fixed classes.

</details>


### [304] [Counterfactual Visual Explanation via Causally-Guided Adversarial Steering](https://arxiv.org/abs/2507.09881)
*Yiran Qiao,Disheng Liu,Yiren Lu,Yu Yin,Mengnan Du,Jing Ma*

Main category: cs.CV

TL;DR: CECAS introduces a causally-guided adversarial method for generating high-quality counterfactual visual explanations by avoiding spurious correlations.


<details>
  <summary>Details</summary>
Motivation: Existing counterfactual visual explanation methods overlook causal relationships, leading to unintended alterations and poor-quality explanations.

Method: CECAS integrates a causal perspective into adversarial methods to generate counterfactuals without unwanted perturbations.

Result: Outperforms state-of-the-art methods, achieving a balanced trade-off in validity, sparsity, proximity, and realism.

Conclusion: CECAS provides superior counterfactual explanations by addressing causal relationships and spurious correlations.

Abstract: Recent work on counterfactual visual explanations has contributed to making
artificial intelligence models more explainable by providing visual
perturbation to flip the prediction. However, these approaches neglect the
causal relationships and the spurious correlations behind the image generation
process, which often leads to unintended alterations in the counterfactual
images and renders the explanations with limited quality. To address this
challenge, we introduce a novel framework CECAS, which first leverages a
causally-guided adversarial method to generate counterfactual explanations. It
innovatively integrates a causal perspective to avoid unwanted perturbations on
spurious factors in the counterfactuals. Extensive experiments demonstrate that
our method outperforms existing state-of-the-art approaches across multiple
benchmark datasets and ultimately achieves a balanced trade-off among various
aspects of validity, sparsity, proximity, and realism.

</details>


### [305] [MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention](https://arxiv.org/abs/2507.09885)
*Zhanjiang Yang,Lijun Sun,Jiawei Dong,Xiaoxin An,Yang Liu,Meng Li*

Main category: cs.CV

TL;DR: MCGA is a two-stage method for hyperspectral image (HSI) reconstruction from RGB images, using multi-scale VQ-VAE and attention mechanisms for improved accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods neglect the challenge of transitioning from low-dimensional (RGB) to high-dimensional (HSI) data, leading to suboptimal results.

Method: MCGA first learns spectral patterns via a multi-scale VQ-VAE, then refines the RGB-to-HSI mapping using a Mixture of Codebooks (MoC) and attention mechanisms.

Result: MCGA achieves state-of-the-art performance in HSI reconstruction, validated through extensive experiments.

Conclusion: The proposed method effectively addresses the limitations of direct RGB-to-HSI mapping, offering a robust and efficient solution.

Abstract: Reconstructing hyperspectral images (HSI) from RGB images is a cost-effective
solution for various vision-based applications. However, most existing
learning-based hyperspectral reconstruction methods directly learn the
RGB-to-HSI mapping using complex attention mechanisms, neglecting the inherent
challenge of transitioning from low-dimensional to high-dimensional
information. To address this limitation, we propose a two-stage approach, MCGA,
which first learns spectral patterns before estimating the mapping. In the
first stage, a multi-scale VQ-VAE learns representations from heterogeneous HSI
datasets, extracting a Mixture of Codebooks (MoC). In the second stage, the
RGB-to-HSI mapping is refined by querying features from the MoC to replace
latent HSI representations, incorporating prior knowledge rather than forcing a
direct high-dimensional transformation. To further enhance reconstruction
quality, we introduce Grayscale-Aware Attention and Quantized Self-Attention,
which adaptively adjust feature map intensities to meet hyperspectral
reconstruction requirements. This physically motivated attention mechanism
ensures lightweight and efficient HSI recovery. Moreover, we propose an
entropy-based Test-Time Adaptation strategy to improve robustness in real-world
scenarios. Extensive experiments demonstrate that our method, MCGA, achieves
state-of-the-art performance. The code and models will be released at
https://github.com/Fibonaccirabbit/MCGA

</details>


### [306] [Measuring the Impact of Rotation Equivariance on Aerial Object Detection](https://arxiv.org/abs/2507.09896)
*Xiuyu Wu,Xinhao Wang,Xiubin Zhu,Lan Yang,Jiyuan Liu,Xingchen Hu*

Main category: cs.CV

TL;DR: The paper proposes MessDet, a strictly rotation-equivariant detector for aerial images, achieving state-of-the-art performance with low parameters.


<details>
  <summary>Details</summary>
Motivation: Rotation equivariance is crucial for aerial object detection, but existing methods rely on approximations. The need for strict rotation equivariance is unclear.

Method: Implemented a strictly rotation-equivariant backbone and neck network, and introduced a multi-branch head to reduce parameters and improve accuracy.

Result: MessDet outperforms on DOTA-v1.0, DOTA-v1.5, and DIOR-R datasets with low parameter count.

Conclusion: Strict rotation equivariance enhances aerial object detection, and the multi-branch head improves efficiency and accuracy.

Abstract: Due to the arbitrary orientation of objects in aerial images, rotation
equivariance is a critical property for aerial object detectors. However,
recent studies on rotation-equivariant aerial object detection remain scarce.
Most detectors rely on data augmentation to enable models to learn
approximately rotation-equivariant features. A few detectors have constructed
rotation-equivariant networks, but due to the breaking of strict rotation
equivariance by typical downsampling processes, these networks only achieve
approximately rotation-equivariant backbones. Whether strict rotation
equivariance is necessary for aerial image object detection remains an open
question. In this paper, we implement a strictly rotation-equivariant backbone
and neck network with a more advanced network structure and compare it with
approximately rotation-equivariant networks to quantitatively measure the
impact of rotation equivariance on the performance of aerial image detectors.
Additionally, leveraging the inherently grouped nature of rotation-equivariant
features, we propose a multi-branch head network that reduces the parameter
count while improving detection accuracy. Based on the aforementioned
improvements, this study proposes the Multi-branch head rotation-equivariant
single-stage Detector (MessDet), which achieves state-of-the-art performance on
the challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and DIOR-R with an
exceptionally low parameter count.

</details>


### [307] [IGD: Instructional Graphic Design with Multimodal Layer Generation](https://arxiv.org/abs/2507.09910)
*Yadong Qu,Shancheng Fang,Yuxin Wang,Xiaorui Wang,Zhineng Chen,Hongtao Xie,Yongdong Zhang*

Main category: cs.CV

TL;DR: IGD is a new method for automated graphic design using natural language instructions, combining parametric rendering and image asset generation for editable, scalable outputs.


<details>
  <summary>Details</summary>
Motivation: Current graphic design methods lack creativity, intelligence, and editable flexibility, making automation impractical.

Method: IGD uses a multimodal approach with MLLM for attribute prediction and layout, plus a diffusion model for image asset generation, enabling end-to-end training.

Result: IGD achieves superior performance, offering scalable and extensible solutions for complex graphic design tasks.

Conclusion: IGD introduces a novel, practical approach to automated graphic design with editable outputs and broad applicability.

Abstract: Graphic design visually conveys information and data by creating and
combining text, images and graphics. Two-stage methods that rely primarily on
layout generation lack creativity and intelligence, making graphic design still
labor-intensive. Existing diffusion-based methods generate non-editable graphic
design files at image level with poor legibility in visual text rendering,
which prevents them from achieving satisfactory and practical automated graphic
design. In this paper, we propose Instructional Graphic Designer (IGD) to
swiftly generate multimodal layers with editable flexibility with only natural
language instructions. IGD adopts a new paradigm that leverages parametric
rendering and image asset generation. First, we develop a design platform and
establish a standardized format for multi-scenario design files, thus laying
the foundation for scaling up data. Second, IGD utilizes the multimodal
understanding and reasoning capabilities of MLLM to accomplish attribute
prediction, sequencing and layout of layers. It also employs a diffusion model
to generate image content for assets. By enabling end-to-end training, IGD
architecturally supports scalability and extensibility in complex graphic
design tasks. The superior experimental results demonstrate that IGD offers a
new solution for graphic design.

</details>


### [308] [Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios](https://arxiv.org/abs/2507.09915)
*Siyue Yao,Mingjie Sun,Eng Gee Lim,Ran Yi,Baojiang Zhong,Moncef Gabbouj*

Main category: cs.CV

TL;DR: Crucial-Diff is a domain-agnostic framework that synthesizes high-quality training samples to address data scarcity, improving detection and segmentation performance by targeting model weaknesses.


<details>
  <summary>Details</summary>
Motivation: Data scarcity in fields like medical, industry, and autonomous driving leads to overfitting and imbalance, hindering model performance. Existing synthetic samples lack diversity and fail to address model weaknesses.

Method: Crucial-Diff integrates SAFE (Scene Agnostic Feature Extractor) for unified feature extraction and WASM (Weakness Aware Sample Miner) to generate hard-to-detect samples using downstream model feedback.

Result: Achieves 83.63% pixel-level AP and 78.12% F1-MAX on MVTec, and 81.64% mIoU and 87.69% mDice on polyp dataset.

Conclusion: Crucial-Diff effectively mitigates data scarcity by generating diverse, high-quality samples, outperforming existing methods.

Abstract: The scarcity of data in various scenarios, such as medical, industry and
autonomous driving, leads to model overfitting and dataset imbalance, thus
hindering effective detection and segmentation performance. Existing studies
employ the generative models to synthesize more training samples to mitigate
data scarcity. However, these synthetic samples are repetitive or simplistic
and fail to provide "crucial information" that targets the downstream model's
weaknesses. Additionally, these methods typically require separate training for
different objects, leading to computational inefficiencies. To address these
issues, we propose Crucial-Diff, a domain-agnostic framework designed to
synthesize crucial samples. Our method integrates two key modules. The Scene
Agnostic Feature Extractor (SAFE) utilizes a unified feature extractor to
capture target information. The Weakness Aware Sample Miner (WASM) generates
hard-to-detect samples using feedback from the detection results of downstream
model, which is then fused with the output of SAFE module. Together, our
Crucial-Diff framework generates diverse, high-quality training data, achieving
a pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset,
Crucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code will be
released after acceptance.

</details>


### [309] [Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis](https://arxiv.org/abs/2507.09950)
*Shubham Shukla,Kunal Sonalkar*

Main category: cs.CV

TL;DR: The paper evaluates GPT-4o-mini and Gemini 2.0 Flash for zero-shot fashion attribute recognition, finding Gemini 2.0 Flash superior with a 56.79% F1 score.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' performance in fine-grained fashion attribute recognition, impacting e-commerce product discovery.

Method: Zero-shot evaluation using the DeepFashion-MultiModal dataset, focusing on image inputs across 18 fashion attributes.

Result: Gemini 2.0 Flash outperformed GPT-4o-mini (56.79% vs. 43.28% macro F1 score).

Conclusion: LLMs show promise for fashion attribution but need domain-specific fine-tuning; the study sets a foundation for future fashion AI research.

Abstract: The fashion retail business is centered around the capacity to comprehend
products. Product attribution helps in comprehending products depending on the
business process. Quality attribution improves the customer experience as they
navigate through millions of products offered by a retail website. It leads to
well-organized product catalogs. In the end, product attribution directly
impacts the 'discovery experience' of the customer. Although large language
models (LLMs) have shown remarkable capabilities in understanding multimodal
data, their performance on fine-grained fashion attribute recognition remains
under-explored. This paper presents a zero-shot evaluation of state-of-the-art
LLMs that balance performance with speed and cost efficiency, mainly
GPT-4o-mini and Gemini 2.0 Flash. We have used the dataset
DeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to
evaluate these models in the attribution tasks of fashion products. Our study
evaluates these models across 18 categories of fashion attributes, offering
insight into where these models excel. We only use images as the sole input for
product information to create a constrained environment. Our analysis shows
that Gemini 2.0 Flash demonstrates the strongest overall performance with a
macro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a
macro F1 score of 43.28%. Through detailed error analysis, our findings provide
practical insights for deploying these LLMs in production e-commerce product
attribution-related tasks and highlight the need for domain-specific
fine-tuning approaches. This work also lays the groundwork for future research
in fashion AI and multimodal attribute extraction.

</details>


### [310] [4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion](https://arxiv.org/abs/2507.09953)
*Zifei Wang,Zian Mao,Xiaoya He,Xi Huang,Haoran Zhang,Chun Cheng,Shufen Chu,Tingzheng Hou,Xiaoqin Zeng,Yujun Xie*

Main category: cs.CV

TL;DR: The paper introduces a method combining multi-image super-resolution (MISR) and a CNN to achieve atomic-scale resolution in electron microscopy for beam-sensitive materials, overcoming radiation damage limitations.


<details>
  <summary>Details</summary>
Motivation: Radiation damage in electron microscopy limits its use for beam-sensitive materials like proteins and 2D materials. The goal is to achieve high-resolution imaging without exceeding dose limits.

Method: The method fuses multiple low-resolution, sub-pixel-shifted views and enhances reconstruction with a CNN. A dual-path, attention-guided network is developed for 4D-STEM to achieve atomic-scale resolution from ultra-low-dose data.

Result: The method provides robust atomic-scale visualization for beam-sensitive specimens, matching conventional ptychography resolution under ultra-low-dose conditions.

Conclusion: This work expands 4D-STEM capabilities, offering a generalizable solution for structural analysis of radiation-vulnerable materials.

Abstract: While electron microscopy offers crucial atomic-resolution insights into
structure-property relationships, radiation damage severely limits its use on
beam-sensitive materials like proteins and 2D materials. To overcome this
challenge, we push beyond the electron dose limits of conventional electron
microscopy by adapting principles from multi-image super-resolution (MISR) that
have been widely used in remote sensing. Our method fuses multiple
low-resolution, sub-pixel-shifted views and enhances the reconstruction with a
convolutional neural network (CNN) that integrates features from synthetic,
multi-angle observations. We developed a dual-path, attention-guided network
for 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose
data. This provides robust atomic-scale visualization across amorphous,
semi-crystalline, and crystalline beam-sensitive specimens. Systematic
evaluations on representative materials demonstrate comparable spatial
resolution to conventional ptychography under ultra-low-dose conditions. Our
work expands the capabilities of 4D-STEM, offering a new and generalizable
method for the structural analysis of radiation-vulnerable materials.

</details>


### [311] [Uncertainty Quantification for Incomplete Multi-View Data Using Divergence Measures](https://arxiv.org/abs/2507.09980)
*Zhipeng Xue,Yan Zhang,Ming Li,Chun Li,Yue Liu,Fei Yu*

Main category: cs.CV

TL;DR: KPHD-Net improves multi-view classification and clustering by using Hölder divergence for uncertainty estimation and integrating Dempster-Shafer evidence theory for reliable fusion.


<details>
  <summary>Details</summary>
Motivation: Current methods lack reliability in multi-view integration due to noisy data and ignore domain gaps between modalities.

Method: KPHD-Net uses variational Dirichlet distributions and Hölder divergence for uncertainty estimation, combined with Dempster-Shafer evidence theory and Kalman filtering.

Result: KPHD-Net outperforms state-of-the-art methods in accuracy, robustness, and reliability.

Conclusion: The proposed method ensures reliable multi-view learning with theoretical guarantees and superior performance.

Abstract: Existing multi-view classification and clustering methods typically improve
task accuracy by leveraging and fusing information from different views.
However, ensuring the reliability of multi-view integration and final decisions
is crucial, particularly when dealing with noisy or corrupted data. Current
methods often rely on Kullback-Leibler (KL) divergence to estimate uncertainty
of network predictions, ignoring domain gaps between different modalities. To
address this issue, KPHD-Net, based on H\"older divergence, is proposed for
multi-view classification and clustering tasks. Generally, our KPHD-Net employs
a variational Dirichlet distribution to represent class probability
distributions, models evidences from different views, and then integrates it
with Dempster-Shafer evidence theory (DST) to improve uncertainty estimation
effects. Our theoretical analysis demonstrates that Proper H\"older divergence
offers a more effective measure of distribution discrepancies, ensuring
enhanced performance in multi-view learning. Moreover, Dempster-Shafer evidence
theory, recognized for its superior performance in multi-view fusion tasks, is
introduced and combined with the Kalman filter to provide future state
estimations. This integration further enhances the reliability of the final
fusion results. Extensive experiments show that the proposed KPHD-Net
outperforms the current state-of-the-art methods in both classification and
clustering tasks regarding accuracy, robustness, and reliability, with
theoretical guarantees.

</details>


### [312] [Latent Diffusion Models with Masked AutoEncoders](https://arxiv.org/abs/2507.09984)
*Junho Lee,Jeongwoo Shin,Hyungwook Choi,Joonseok Lee*

Main category: cs.CV

TL;DR: The paper analyzes autoencoders in Latent Diffusion Models (LDMs), identifies key properties, and proposes Variational Masked AutoEncoders (VMAEs) to improve image generation.


<details>
  <summary>Details</summary>
Motivation: Existing autoencoders in LDMs lack simultaneous satisfaction of latent smoothness, perceptual compression, and reconstruction quality.

Method: Proposes VMAEs, leveraging hierarchical features from Masked AutoEncoder, and integrates them into LDMs as LDMAEs.

Result: LDMAEs show improved image generation quality and computational efficiency.

Conclusion: VMAEs address limitations of existing autoencoders, enhancing LDM performance.

Abstract: In spite of remarkable potential of the Latent Diffusion Models (LDMs) in
image generation, the desired properties and optimal design of the autoencoders
have been underexplored. In this work, we analyze the role of autoencoders in
LDMs and identify three key properties: latent smoothness, perceptual
compression quality, and reconstruction quality. We demonstrate that existing
autoencoders fail to simultaneously satisfy all three properties, and propose
Variational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical
features maintained by Masked AutoEncoder. We integrate VMAEs into the LDM
framework, introducing Latent Diffusion Models with Masked AutoEncoders
(LDMAEs). Through comprehensive experiments, we demonstrate significantly
enhanced image generation quality and computational efficiency.

</details>


### [313] [3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving](https://arxiv.org/abs/2507.09993)
*Yixun Zhang,Lizhi Wang,Junjun Zhao,Wending Zhao,Feng Zhou,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: 3DGAA is a novel adversarial attack framework using 3D Gaussian Splatting to optimize geometry and appearance for robust, physically realistic attacks on autonomous driving systems.


<details>
  <summary>Details</summary>
Motivation: Existing 2D/3D attacks lack balance between physical realism and robustness, limiting their effectiveness in real-world scenarios.

Method: 3DGAA jointly optimizes geometric (shape, scale, rotation) and appearance (color, opacity) attributes using 3DGS, with physical filtering and augmentation modules for realism and generalization.

Result: 3DGAA reduces detection mAP from 87.21% to 7.38%, outperforming existing methods and maintaining high transferability across conditions.

Conclusion: 3DGAA sets a new benchmark for physically realizable adversarial attacks, proving practical for evaluating autonomous driving perception safety.

Abstract: Camera-based object detection systems play a vital role in autonomous
driving, yet they remain vulnerable to adversarial threats in real-world
environments. While existing 2D and 3D physical attacks typically optimize
texture, they often struggle to balance physical realism and attack robustness.
In this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel
adversarial object generation framework that leverages the full 14-dimensional
parameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry
and appearance in physically realizable ways. Unlike prior works that rely on
patches or texture, 3DGAA jointly perturbs both geometric attributes (shape,
scale, rotation) and appearance attributes (color, opacity) to produce
physically realistic and transferable adversarial objects. We further introduce
a physical filtering module to preserve geometric fidelity, and a physical
augmentation module to simulate complex physical scenarios, thus enhancing
attack generalization under real-world conditions. We evaluate 3DGAA on both
virtual benchmarks and physical-world setups using miniature vehicle models.
Experimental results show that 3DGAA achieves to reduce the detection mAP from
87.21% to 7.38%, significantly outperforming existing 3D physical attacks.
Moreover, our method maintains high transferability across different physical
conditions, demonstrating a new state-of-the-art in physically realizable
adversarial attacks. These results validate 3DGAA as a practical attack
framework for evaluating the safety of perception systems in autonomous
driving.

</details>


### [314] [Leveraging Swin Transformer for enhanced diagnosis of Alzheimer's disease using multi-shell diffusion MRI](https://arxiv.org/abs/2507.09996)
*Quentin Dessain,Nicolas Delinte,Bernard Hanseeuw,Laurence Dricot,Benoît Macq*

Main category: cs.CV

TL;DR: A vision transformer-based deep learning framework using multi-shell dMRI data achieves high accuracy in early Alzheimer's disease and amyloid detection.


<details>
  <summary>Details</summary>
Motivation: To support early diagnosis of Alzheimer's disease and amyloid accumulation using microstructural information from multi-shell dMRI data.

Method: A Swin Transformer model processes multi-shell dMRI data, integrating Low-Rank Adaptation for limited labeled data. Metrics from DTI and NODDI are projected onto 2D planes for transfer learning.

Result: Achieved 95.2% balanced accuracy for Alzheimer's dementia vs. normal, and 77.2% for amyloid-positive cases. Clinically relevant brain regions were identified via Grad-CAM.

Conclusion: The framework shows promise for early Alzheimer's detection and amyloid pathology, aiding biomarker-driven diagnostics in data-limited settings.

Abstract: Objective: This study aims to support early diagnosis of Alzheimer's disease
and detection of amyloid accumulation by leveraging the microstructural
information available in multi-shell diffusion MRI (dMRI) data, using a vision
transformer-based deep learning framework.
  Methods: We present a classification pipeline that employs the Swin
Transformer, a hierarchical vision transformer model, on multi-shell dMRI data
for the classification of Alzheimer's disease and amyloid presence. Key metrics
from DTI and NODDI were extracted and projected onto 2D planes to enable
transfer learning with ImageNet-pretrained models. To efficiently adapt the
transformer to limited labeled neuroimaging data, we integrated Low-Rank
Adaptation. We assessed the framework on diagnostic group prediction
(cognitively normal, mild cognitive impairment, Alzheimer's disease dementia)
and amyloid status classification.
  Results: The framework achieved competitive classification results within the
scope of multi-shell dMRI-based features, with the best balanced accuracy of
95.2% for distinguishing cognitively normal individuals from those with
Alzheimer's disease dementia using NODDI metrics. For amyloid detection, it
reached 77.2% balanced accuracy in distinguishing amyloid-positive mild
cognitive impairment/Alzheimer's disease dementia subjects from
amyloid-negative cognitively normal subjects, and 67.9% for identifying
amyloid-positive individuals among cognitively normal subjects. Grad-CAM-based
explainability analysis identified clinically relevant brain regions, including
the parahippocampal gyrus and hippocampus, as key contributors to model
predictions.
  Conclusion: This study demonstrates the promise of diffusion MRI and
transformer-based architectures for early detection of Alzheimer's disease and
amyloid pathology, supporting biomarker-driven diagnostics in data-limited
biomedical settings.

</details>


### [315] [Vision-Based Anti Unmanned Aerial Technology: Opportunities and Challenges](https://arxiv.org/abs/2507.10006)
*Guanghai Ding,Yihua Ren,Yuting Liu,Qijun Zhao,Shuiwang Li*

Main category: cs.CV

TL;DR: The paper reviews Anti-UAV tracking technologies, discusses challenges, compiles datasets, analyzes vision-based algorithms, and suggests future research directions.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of UAV technology necessitates efficient Anti-UAV tracking for applications like public safety and environmental monitoring.

Method: The paper reviews current technologies, compiles datasets, and analyzes vision-based and vision-fusion-based algorithms.

Result: It provides insights into challenges, available datasets, and recent algorithms in Anti-UAV tracking.

Conclusion: The paper outlines future research directions to advance Anti-UAV tracking technologies.

Abstract: With the rapid advancement of UAV technology and its extensive application in
various fields such as military reconnaissance, environmental monitoring, and
logistics, achieving efficient and accurate Anti-UAV tracking has become
essential. The importance of Anti-UAV tracking is increasingly prominent,
especially in scenarios such as public safety, border patrol, search and
rescue, and agricultural monitoring, where operations in complex environments
can provide enhanced security. Current mainstream Anti-UAV tracking
technologies are primarily centered around computer vision techniques,
particularly those that integrate multi-sensor data fusion with advanced
detection and tracking algorithms. This paper first reviews the characteristics
and current challenges of Anti-UAV detection and tracking technologies. Next,
it investigates and compiles several publicly available datasets, providing
accessible links to support researchers in efficiently addressing related
challenges. Furthermore, the paper analyzes the major vision-based and
vision-fusion-based Anti-UAV detection and tracking algorithms proposed in
recent years. Finally, based on the above research, this paper outlines future
research directions, aiming to provide valuable insights for advancing the
field.

</details>


### [316] [Binomial Self-Compensation: Mechanism and Suppression of Motion Error in Phase-Shifting Profilometry](https://arxiv.org/abs/2507.10009)
*Geyou Zhang,Kai Liu,Ce Zhu*

Main category: cs.CV

TL;DR: The paper introduces I-BSC, an improved method over P-BSC, to reduce motion errors in dynamic 3D scanning by weighting fringe images instead of phase frames, achieving faster computation and better error convergence.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of PSP in dynamic measurements due to object motion, and improve upon P-BSC's computational overhead and error accumulation.

Method: Proposes I-BSC, which weights and sums homogeneous fringe images (instead of phase frames) and computes the arctangent function once, reducing complexity and error.

Result: I-BSC outperforms existing methods in reducing motion error, achieves quasi-single-shot frame rates, and accelerates computation significantly compared to P-BSC.

Conclusion: I-BSC is a superior solution for dynamic 3D scanning, offering high efficiency, reduced computational load, and faster error convergence.

Abstract: Phase shifting profilometry (PSP) is widely used in high-precision 3D
scanning due to its high accuracy, robustness, and pixel-wise handling.
However, a fundamental assumption of PSP that the object should remain static
does not hold in dynamic measurement, making PSP susceptible to object motion.
To address this challenge, our proposed solution, phase-sequential binomial
self-compensation (P-BSC), sums successive motion-affected phase frames
weighted by binomial coefficients. This approach exponentially reduces the
motion error in a pixel-wise and frame-wise loopable manner. Despite its
efficacy, P-BSC suffers from high computational overhead and error accumulation
due to its reliance on multi-frame phase calculations and weighted summations.
Inspired by P-BSC, we propose an image-sequential binomial self-compensation
(I-BSC) to weight sum the homogeneous fringe images instead of successive phase
frames, which generalizes the BSC concept from phase sequences to image
sequences. I-BSC computes the arctangent function only once, resolving both
limitations in P-BSC. Extensive analysis, simulations, and experiments show
that 1) the proposed BSC outperforms existing methods in reducing motion error
while achieving a quasi-single-shot frame rate, i.e., depth map frame rate
equals to the camera's acquisition rate, enabling 3D reconstruction with high
pixel-depth-temporal resolution; 2) compared to P-BSC, our I-BSC reduces the
computational complexity by one polynomial order, thereby accelerating the
computational frame rate by several to dozen times, while also reaching faster
motion error convergence.

</details>


### [317] [(Almost) Free Modality Stitching of Foundation Models](https://arxiv.org/abs/2507.10015)
*Jaisidh Singh,Diganta Misra,Boris Knyazev,Antonio Orvieto*

Main category: cs.CV

TL;DR: Hyma proposes a hypernetwork-based solution for efficient selection of uni-modal models and connector training in multi-modal systems, reducing search costs significantly.


<details>
  <summary>Details</summary>
Motivation: The complexity and computational cost of selecting and aligning uni-modal models for multi-modal systems motivate the need for an efficient solution.

Method: Hyma leverages hypernetworks to jointly train connector modules for multiple uni-modal model combinations, optimizing selection and alignment.

Result: Hyma reduces search costs by 10x while maintaining performance comparable to grid search across diverse benchmarks.

Conclusion: Hyma offers a scalable and efficient approach for multi-modal model alignment, addressing computational challenges in model selection and connector training.

Abstract: Foundation multi-modal models are often designed by stitching of multiple
existing pretrained uni-modal models: for example, an image classifier with an
autoregressive text model. This stitching process is performed by training a
connector module that aims to align the representation-representation or
representation-input spaces of these uni-modal models. However, given the
complexity of training such connectors on large scale web-based datasets
coupled with the ever-increasing number of available pretrained uni-modal
models, the task of uni-modal models selection and subsequent connector module
training becomes computationally demanding. To address this under-studied
critical problem, we propose Hypernetwork Model Alignment (Hyma), a novel
all-in-one solution for optimal uni-modal model selection and connector
training by leveraging hypernetworks. Specifically, our framework utilizes the
parameter prediction capability of a hypernetwork to obtain jointly trained
connector modules for $N \times M$ combinations of uni-modal models. In our
experiments, Hyma reduces the optimal uni-modal model pair search cost by
$10\times$ (averaged across all experiments), while matching the ranking and
trained connector performance obtained via grid search across a suite of
diverse multi-modal benchmarks.

</details>


### [318] [Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies](https://arxiv.org/abs/2507.10029)
*Seokeon Choi,Sunghyun Park,Hyoungwoo Park,Jeongho Kim,Sungrack Yun*

Main category: cs.CV

TL;DR: A selective optimization framework combines BP-low and ZO-high for memory-efficient, high-quality fine-tuning of text-to-image diffusion models, guided by timestep-aware probabilistic selection.


<details>
  <summary>Details</summary>
Motivation: To enable memory-efficient personalization of text-to-image diffusion models on edge devices while preserving privacy and computational constraints.

Method: Proposes a framework using BP-low for target-specific adaptation and ZO-high for high-resolution refinement, dynamically selected via a timestep-aware function.

Result: Achieves competitive performance with reduced memory usage, enabling scalable on-device personalization without increased latency.

Conclusion: The framework effectively balances memory efficiency and quality, making it suitable for edge device applications.

Abstract: Memory-efficient personalization is critical for adapting text-to-image
diffusion models while preserving user privacy and operating within the limited
computational resources of edge devices. To this end, we propose a selective
optimization framework that adaptively chooses between backpropagation on
low-resolution images (BP-low) and zeroth-order optimization on high-resolution
images (ZO-high), guided by the characteristics of the diffusion process. As
observed in our experiments, BP-low efficiently adapts the model to
target-specific features, but suffers from structural distortions due to
resolution mismatch. Conversely, ZO-high refines high-resolution details with
minimal memory overhead but faces slow convergence when applied without prior
adaptation. By complementing both methods, our framework leverages BP-low for
effective personalization while using ZO-high to maintain structural
consistency, achieving memory-efficient and high-quality fine-tuning. To
maximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware
probabilistic function that dynamically selects the appropriate optimization
strategy based on diffusion timesteps. This function mitigates the overfitting
from BP-low at high timesteps, where structural information is critical, while
ensuring ZO-high is applied more effectively as training progresses.
Experimental results demonstrate that our method achieves competitive
performance while significantly reducing memory consumption, enabling scalable,
high-quality on-device personalization without increasing inference latency.

</details>


### [319] [LifelongPR: Lifelong knowledge fusion for point cloud place recognition based on replay and prompt learning](https://arxiv.org/abs/2507.10034)
*Xianghong Zou,Jianping Li,Zhe Chen,Zhen Cao,Zhen Dong,Qiegen Liu,Bisheng Yang*

Main category: cs.CV

TL;DR: LifelongPR is a continual learning framework for point cloud place recognition (PCPR) that addresses catastrophic forgetting and domain shifts, improving performance and scalability.


<details>
  <summary>Details</summary>
Motivation: Existing PCPR models suffer from catastrophic forgetting and poor adaptability to dynamic environments, limiting their practicality.

Method: Proposes a replay sample selection method and a prompt learning-based CL framework with a lightweight prompt module and two-stage training.

Result: Achieves 6.50% improvement in mIR@1, 7.96% in mR@1, and 8.95% reduction in F compared to SOTA methods.

Conclusion: LifelongPR effectively enhances PCPR performance and scalability, validated by experiments on large-scale datasets.

Abstract: Point cloud place recognition (PCPR) plays a crucial role in photogrammetry
and robotics applications such as autonomous driving, intelligent
transportation, and augmented reality. In real-world large-scale deployments of
a positioning system, PCPR models must continuously acquire, update, and
accumulate knowledge to adapt to diverse and dynamic environments, i.e., the
ability known as continual learning (CL). However, existing PCPR models often
suffer from catastrophic forgetting, leading to significant performance
degradation in previously learned scenes when adapting to new environments or
sensor types. This results in poor model scalability, increased maintenance
costs, and system deployment difficulties, undermining the practicality of
PCPR. To address these issues, we propose LifelongPR, a novel continual
learning framework for PCPR, which effectively extracts and fuses knowledge
from sequential point cloud data. First, to alleviate the knowledge loss, we
propose a replay sample selection method that dynamically allocates sample
sizes according to each dataset's information quantity and selects spatially
diverse samples for maximal representativeness. Second, to handle domain
shifts, we design a prompt learning-based CL framework with a lightweight
prompt module and a two-stage training strategy, enabling domain-specific
feature adaptation while minimizing forgetting. Comprehensive experiments on
large-scale public and self-collected datasets are conducted to validate the
effectiveness of the proposed method. Compared with state-of-the-art (SOTA)
methods, our method achieves 6.50% improvement in mIR@1, 7.96% improvement in
mR@1, and an 8.95% reduction in F. The code and pre-trained models are publicly
available at https://github.com/zouxianghong/LifelongPR.

</details>


### [320] [CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books](https://arxiv.org/abs/2507.10053)
*Marc Serra Ortega,Emanuele Vivoli,Artemis Llabrés,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: CoSMo is a multimodal Transformer for comic book Page Stream Segmentation (PSS), outperforming baselines and large vision-language models. It leverages visual features and multimodal inputs for ambiguity resolution, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Automated content understanding in comic books requires accurate PSS for tasks like character analysis and metadata enrichment. Existing methods lack performance, motivating the development of CoSMo.

Method: CoSMo is a multimodal Transformer tested in vision-only and multimodal variants. It uses a curated 20,800-page annotated dataset for training and evaluation.

Result: CoSMo outperforms traditional baselines and larger models in F1-Macro, Panoptic Quality, and stream-level metrics. Visual features dominate macro-structure, while multimodal inputs resolve ambiguities.

Conclusion: CoSMo sets a new state-of-the-art for comic PSS, enabling scalable comic book analysis and downstream applications.

Abstract: This paper introduces CoSMo, a novel multimodal Transformer for Page Stream
Segmentation (PSS) in comic books, a critical task for automated content
understanding, as it is a necessary first stage for many downstream tasks like
character analysis, story indexing, or metadata enrichment. We formalize PSS
for this unique medium and curate a new 20,800-page annotated dataset. CoSMo,
developed in vision-only and multimodal variants, consistently outperforms
traditional baselines and significantly larger general-purpose vision-language
models across F1-Macro, Panoptic Quality, and stream-level metrics. Our
findings highlight the dominance of visual features for comic PSS
macro-structure, yet demonstrate multimodal benefits in resolving challenging
ambiguities. CoSMo establishes a new state-of-the-art, paving the way for
scalable comic book analysis.

</details>


### [321] [Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning](https://arxiv.org/abs/2507.10056)
*A. K. M. Shoriful Islam,Md. Rakib Hassan,Macbah Uddin,Md. Shahidur Rahman*

Main category: cs.CV

TL;DR: A lightweight ML approach detects poultry diseases via fecal images using multi-color space features and achieves high accuracy with low resource usage.


<details>
  <summary>Details</summary>
Motivation: Poultry farming is vulnerable to diseases; a cost-effective, scalable solution is needed for low-resource settings.

Method: Multi-color space feature extraction (RGB, HSV, LAB), descriptors (color histograms, LBP, wavelet transforms, edge detectors), PCA/XGBoost for dimensionality reduction, and ANN classifier.

Result: 95.85% accuracy, no GPU needed, 638s execution time in Google Colab, comparable to deep learning models but with lower resource usage.

Conclusion: The method is a cost-effective, interpretable, and scalable alternative to deep learning for real-time poultry disease detection.

Abstract: Poultry farming is a vital component of the global food supply chain, yet it
remains highly vulnerable to infectious diseases such as coccidiosis,
salmonellosis, and Newcastle disease. This study proposes a lightweight machine
learning-based approach to detect these diseases by analyzing poultry fecal
images. We utilize multi-color space feature extraction (RGB, HSV, LAB) and
explore a wide range of color, texture, and shape-based descriptors, including
color histograms, local binary patterns (LBP), wavelet transforms, and edge
detectors. Through a systematic ablation study and dimensionality reduction
using PCA and XGBoost feature selection, we identify a compact global feature
set that balances accuracy and computational efficiency. An artificial neural
network (ANN) classifier trained on these features achieved 95.85% accuracy
while requiring no GPU and only 638 seconds of execution time in Google Colab.
Compared to deep learning models such as Xception and MobileNetV3, our proposed
model offers comparable accuracy with drastically lower resource usage. This
work demonstrates a cost-effective, interpretable, and scalable alternative to
deep learning for real-time poultry disease detection in low-resource
agricultural settings.

</details>


### [322] [MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second](https://arxiv.org/abs/2507.10065)
*Chenguo Lin,Yuchen Lin,Panwang Pan,Yifan Yu,Honglei Yan,Katerina Fragkiadaki,Yadong Mu*

Main category: cs.CV

TL;DR: MoVieS is a fast feed-forward model for 4D dynamic novel view synthesis from monocular videos, unifying appearance, geometry, and motion in a single framework.


<details>
  <summary>Details</summary>
Motivation: To enable unified modeling of dynamic 3D scenes for tasks like view synthesis, reconstruction, and 3D tracking without heavy task-specific supervision.

Method: Uses pixel-aligned grids of Gaussian primitives with explicit supervision of time-varying motion.

Result: Achieves competitive performance with significant speedups, supporting zero-shot applications like scene flow estimation.

Conclusion: MoVieS effectively bridges novel view synthesis and dynamic geometry reconstruction, offering versatility and efficiency.

Abstract: We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic
novel views from monocular videos in one second. MoVieS represents dynamic 3D
scenes using pixel-aligned grids of Gaussian primitives, explicitly supervising
their time-varying motion. This allows, for the first time, the unified
modeling of appearance, geometry and motion, and enables view synthesis,
reconstruction and 3D point tracking within a single learning-based framework.
By bridging novel view synthesis with dynamic geometry reconstruction, MoVieS
enables large-scale training on diverse datasets with minimal dependence on
task-specific supervision. As a result, it also naturally supports a wide range
of zero-shot applications, such as scene flow estimation and moving object
segmentation. Extensive experiments validate the effectiveness and efficiency
of MoVieS across multiple tasks, achieving competitive performance while
offering several orders of magnitude speedups.

</details>


### [323] [Frequency Regulation for Exposure Bias Mitigation in Diffusion Models](https://arxiv.org/abs/2507.10072)
*Meng Yu,Kun Zhan*

Main category: cs.CV

TL;DR: The paper addresses exposure bias in diffusion models by analyzing energy reduction in noisy images and introduces a frequency-domain regulation method to improve generative quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion models suffer from exposure bias, which impacts their generative capabilities. The paper aims to mitigate this by analyzing energy patterns in noisy images.

Method: The authors use wavelet transforms to separately regulate low- and high-frequency subbands, leveraging observed energy reduction patterns. The method is training-free and plug-and-play.

Result: The proposed method significantly improves generative quality across various diffusion models and provides a robust solution to exposure bias.

Conclusion: The frequency-domain regulation mechanism effectively addresses exposure bias in diffusion models, enhancing their performance without requiring additional training.

Abstract: Diffusion models exhibit impressive generative capabilities but are
significantly impacted by exposure bias. In this paper, we make a key
observation: the energy of the predicted noisy images decreases during the
diffusion process. Building on this, we identify two important findings: 1) The
reduction in energy follows distinct patterns in the low-frequency and
high-frequency subbands; 2) This energy reduction results in amplitude
variations between the network-reconstructed clean data and the real clean
data. Based on the first finding, we introduce a frequency-domain regulation
mechanism utilizing wavelet transforms, which separately adjusts the low- and
high-frequency subbands. Leveraging the second insight, we provide a more
accurate analysis of exposure bias in the two subbands. Our method is
training-free and plug-and-play, significantly improving the generative quality
of various diffusion models and providing a robust solution to exposure bias
across different model architectures. The source code is available at
https://github.com/kunzhan/wpp.

</details>


### [324] [A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area](https://arxiv.org/abs/2507.10084)
*Haonan Chen,Xin Tong*

Main category: cs.CV

TL;DR: A two-stage transfer learning strategy using SegFormer improves water body segmentation in remote sensing, boosting IoU from 25.50% to 64.84% in challenging terrains like Tibet.


<details>
  <summary>Details</summary>
Motivation: Address domain shift and small sample sizes in remote sensing image water body segmentation, especially in unique environments like Tibet.

Method: Two-stage transfer learning: pre-train on a diverse source domain, then fine-tune on target domain data using SegFormer.

Result: Improved IoU from 25.50% (direct transfer) to 64.84% in the Zhada Tulin area, resolving domain discrepancy issues.

Conclusion: The strategy effectively tackles domain shift and small datasets, offering a high-precision solution for remote sensing tasks.

Abstract: To address the prevalent challenges of domain shift and small sample sizes in
remote sensing image water body segmentation, this study proposes and validates
a two-stage transfer learning strategy based on the SegFormer model. The
approach begins by training a foundational segmentation model on a diverse
source domain, where it achieves an Intersection over Union (IoU) of 68.80% on
its validation set, followed by fine-tuning on data from the distinct target
domain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by
highly complex topography and spectral features -- the experimental results
demonstrate that this strategy significantly boosts the IoU for the water body
segmentation task from 25.50% (for direct transfer) to 64.84%. This not only
effectively resolves the model performance degradation caused by domain
discrepancy but also provides an effective technical paradigm for
high-precision thematic information extraction in data-scarce and
environmentally unique remote sensing scenarios.

</details>


### [325] [FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text](https://arxiv.org/abs/2507.10095)
*Bingchao Wang,Zhiwei Ning,Jianyu Ding,Xuanang Gao,Yin Li,Dongsheng Jiang,Jie Yang,Wei Liu*

Main category: cs.CV

TL;DR: FIX-CLIP enhances CLIP for long-text tasks with dual-branch training, regional prompts, and hierarchical alignment, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: CLIP struggles with long-text inputs (>77 tokens), limiting its effectiveness in under-stream tasks.

Method: Proposes FIX-CLIP with dual-branch training, regional prompts, and hierarchical feature alignment. Uses 30M images and MLLM-synthesized captions.

Result: Achieves state-of-the-art performance on long and short-text retrieval. Works well with diffusion models for long-text input.

Conclusion: FIX-CLIP effectively addresses CLIP's long-text limitations and excels in downstream applications.

Abstract: CLIP has shown promising performance across many short-text tasks in a
zero-shot manner. However, limited by the input length of the text encoder,
CLIP struggles on under-stream tasks with long-text inputs (>77 tokens). To
remedy this issue, we propose FIX-CLIP which includes three novel modules: (1)
A dual-branch training pipeline that aligns short and long texts with masked
and raw images respectively, which boosts the long-text representation while
preserving the short-text ability. (2) Multiple learnable regional prompts with
unidirectional masks in Transformer layers for regional information extraction.
(3) A hierarchical feature alignment module in the intermediate encoder layers
to promote the consistency of multi-scale features. Furthermore, we collect 30M
images and utilize existing MLLMs to synthesize long-text captions for
training. Extensive experiments show that FIX-CLIP achieves state-of-the-art
performance on both long-text and short-text retrieval benchmarks. For
downstream applications, we reveal that FIX-CLIP's text encoder delivers
promising performance in a plug-and-play manner for diffusion models with
long-text input.

</details>


### [326] [Glance-MCMT: A General MCMT Framework with Glance Initialization and Progressive Association](https://arxiv.org/abs/2507.10115)
*Hamidreza Hashempoor*

Main category: cs.CV

TL;DR: A framework for multi-camera multi-target tracking using trajectory and appearance cues, with global ID assignment and 3D spatial validation.


<details>
  <summary>Details</summary>
Motivation: To ensure consistent global identity assignment across multiple camera views for accurate multi-target tracking.

Method: Uses BoT-SORT for single-camera tracking, initializes global IDs via trajectory-feature matching, and employs a prioritized global matching strategy for new tracklets. 3D positions are estimated for spatial validation.

Result: Achieves consistent global identity assignment across views by leveraging trajectory and appearance cues.

Conclusion: The proposed MCMT framework effectively manages global identity assignment and spatial validation for multi-target tracking.

Abstract: We propose a multi-camera multi-target (MCMT) tracking framework that ensures
consistent global identity assignment across views using trajectory and
appearance cues. The pipeline starts with BoT-SORT-based single-camera
tracking, followed by an initial glance phase to initialize global IDs via
trajectory-feature matching. In later frames, new tracklets are matched to
existing global identities through a prioritized global matching strategy. New
global IDs are only introduced when no sufficiently similar trajectory or
feature match is found. 3D positions are estimated using depth maps and
calibration for spatial validation.

</details>


### [327] [DEARLi: Decoupled Enhancement of Recognition and Localization for Semi-supervised Panoptic Segmentation](https://arxiv.org/abs/2507.10118)
*Ivan Martinović,Josip Šarić,Marin Oršić,Matej Kristan,Siniša Šegvić*

Main category: cs.CV

TL;DR: A novel semi-supervised panoptic approach (DEARLi) uses foundation models to enhance recognition and localization, outperforming state-of-the-art methods with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Pixel-level annotation is costly, and existing semi-supervised methods lack effective mechanisms to exploit foundation models for label scarcity.

Method: DEARLi combines unsupervised mask-transformer consistency with CLIP's zero-shot classification for recognition and SAM pseudo-labels for localization.

Result: Achieves 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images, outperforming state-of-the-art methods with 8x less GPU memory.

Conclusion: DEARLi excels in challenging semi-supervised scenarios, offering a scalable solution for segmentation tasks with limited labeled data.

Abstract: Pixel-level annotation is expensive and time-consuming. Semi-supervised
segmentation methods address this challenge by learning models on few labeled
images alongside a large corpus of unlabeled images. Although foundation models
could further account for label scarcity, effective mechanisms for their
exploitation remain underexplored. We address this by devising a novel
semi-supervised panoptic approach fueled by two dedicated foundation models. We
enhance recognition by complementing unsupervised mask-transformer consistency
with zero-shot classification of CLIP features. We enhance localization by
class-agnostic decoder warm-up with respect to SAM pseudo-labels. The resulting
decoupled enhancement of recognition and localization (DEARLi) particularly
excels in the most challenging semi-supervised scenarios with large taxonomies
and limited labeled data. Moreover, DEARLi outperforms the state of the art in
semi-supervised semantic segmentation by a large margin while requiring 8x less
GPU memory, in spite of being trained only for the panoptic objective. We
observe 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images. The
source code is available at https://github.com/helen1c/DEARLi.

</details>


### [328] [Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion](https://arxiv.org/abs/2507.10127)
*Md Abulkalam Azad,John Nyberg,Håvard Dalen,Bjørnar Grenne,Lasse Lovstakken,Andreas Østvik*

Main category: cs.CV

TL;DR: The paper explores SOTA point tracking methods for echocardiography, identifies directional motion bias, and proposes refined training and a lightweight network to improve tracking accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Accurate motion estimation in echocardiography is crucial for cardiac function measurements, but existing methods struggle with intricate cardiac motion.

Method: The study refines training procedures with tailored augmentations and introduces a lightweight network using multi-scale cost volumes to address motion bias.

Result: Fine-tuning improves performance, with EchoTracker achieving 60.7% higher position accuracy and 61.5% lower trajectory error. Some SOTA models underperform the proposed simple model.

Conclusion: The refined methods enhance tracking robustness and clinical reproducibility, aligning better with expert-validated tools.

Abstract: Accurate motion estimation for tracking deformable tissues in
echocardiography is essential for precise cardiac function measurements. While
traditional methods like block matching or optical flow struggle with intricate
cardiac motion, modern point tracking approaches remain largely underexplored
in this domain. This work investigates the potential of state-of-the-art (SOTA)
point tracking methods for ultrasound, with a focus on echocardiography.
Although these novel approaches demonstrate strong performance in general
videos, their effectiveness and generalizability in echocardiography remain
limited. By analyzing cardiac motion throughout the heart cycle in real B-mode
ultrasound videos, we identify that a directional motion bias across different
views is affecting the existing training strategies. To mitigate this, we
refine the training procedure and incorporate a set of tailored augmentations
to reduce the bias and enhance tracking robustness and generalization through
impartial cardiac motion. We also propose a lightweight network leveraging
multi-scale cost volumes from spatial context alone to challenge the advanced
spatiotemporal point tracking models. Experiments demonstrate that fine-tuning
with our strategies significantly improves models' performances over their
baselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker
boosts overall position accuracy by 60.7% and reduces median trajectory error
by 61.5% across heart cycle phases. Interestingly, several point tracking
models fail to outperform our proposed simple model in terms of tracking
accuracy and generalization, reflecting their limitations when applied to
echocardiography. Nevertheless, clinical evaluation reveals that these methods
improve GLS measurements, aligning more closely with expert-validated,
semi-automated tools and thus demonstrating better reproducibility in
real-world applications.

</details>


### [329] [Deep Recurrence for Dynamical Segmentation Models](https://arxiv.org/abs/2507.10143)
*David Calhas,Arlindo L. Oliveira*

Main category: cs.CV

TL;DR: A feedback mechanism inspired by predictive coding is introduced in a U-Net architecture, improving performance in noisy conditions and data efficiency compared to feedforward models.


<details>
  <summary>Details</summary>
Motivation: Biological vision systems use feedback for iterative refinement, but artificial neural networks often lack this, limiting their adaptability and robustness.

Method: A recurrent feedback loop is added to a U-Net, with softmax projection and exponential decay for stability. Tested on synthetic segmentation tasks.

Result: Feedback outperforms feedforward models in noise and with limited data, achieving above-random performance with just two training examples.

Conclusion: Feedback enhances robustness and data efficiency, suggesting a direction for more biologically inspired neural architectures.

Abstract: While biological vision systems rely heavily on feedback connections to
iteratively refine perception, most artificial neural networks remain purely
feedforward, processing input in a single static pass. In this work, we propose
a predictive coding inspired feedback mechanism that introduces a recurrent
loop from output to input, allowing the model to refine its internal state over
time. We implement this mechanism within a standard U-Net architecture and
introduce two biologically motivated operations, softmax projection and
exponential decay, to ensure stability of the feedback loop. Through controlled
experiments on a synthetic segmentation task, we show that the feedback model
significantly outperforms its feedforward counterpart in noisy conditions and
generalizes more effectively with limited supervision. Notably, feedback
achieves above random performance with just two training examples, while the
feedforward model requires at least four. Our findings demonstrate that
feedback enhances robustness and data efficiency, and offer a path toward more
adaptive and biologically inspired neural architectures. Code is available at:
github.com/DCalhas/feedback_segmentation.

</details>


### [330] [SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis](https://arxiv.org/abs/2507.10171)
*Youngmin Kim,Giyeong Oh,Kwangsoo Youm,Youngjae Yu*

Main category: cs.CV

TL;DR: SlumpGuard is an AI-powered video system for real-time concrete workability assessment, replacing manual slump tests to improve accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional slump testing is manual, time-consuming, and inconsistent, limiting real-time monitoring.

Method: Proposes SlumpGuard, an AI system analyzing concrete flow from truck chutes via video for automated workability assessment.

Result: Demonstrates effectiveness in real-world deployment, enabling full-batch inspection without manual intervention.

Conclusion: SlumpGuard is a practical solution for modern concrete quality assurance, enhancing accuracy and efficiency.

Abstract: Concrete workability is essential for construction quality, with the slump
test being the most common on-site method for its assessment. However,
traditional slump testing is manual, time-consuming, and prone to
inconsistency, limiting its applicability for real-time monitoring. To address
these challenges, we propose SlumpGuard, an AI-powered, video-based system that
automatically analyzes concrete flow from the truck chute to assess workability
in real time. Our system enables full-batch inspection without manual
intervention, improving both the accuracy and efficiency of quality control. We
present the system design, a the construction of a dedicated dataset, and
empirical results from real-world deployment, demonstrating the effectiveness
of SlumpGuard as a practical solution for modern concrete quality assurance.

</details>


### [331] [Minimizing the Pretraining Gap: Domain-aligned Text-Based Person Retrieval](https://arxiv.org/abs/2507.10195)
*Shuyu Yang,Yaxiong Wang,Yongrui Li,Li Zhu,Zhedong Zheng*

Main category: cs.CV

TL;DR: The paper introduces a dual-level domain adaptation method for text-based person retrieval, addressing the gap between synthetic pretraining and real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns and high annotation costs make synthetic data popular for pretraining, but domain gaps (lighting, color, viewpoint) hinder performance.

Method: Proposes a pipeline with Domain-aware Diffusion (DaD) for image-level adaptation and Multi-granularity Relation Alignment (MRA) for region-level alignment.

Result: Achieves state-of-the-art results on CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets.

Conclusion: The dual-level adaptation effectively bridges the domain gap, improving retrieval performance.

Abstract: In this work, we focus on text-based person retrieval, which aims to identify
individuals based on textual descriptions. Given the significant privacy issues
and the high cost associated with manual annotation, synthetic data has become
a popular choice for pretraining models, leading to notable advancements.
However, the considerable domain gap between synthetic pretraining datasets and
real-world target datasets, characterized by differences in lighting, color,
and viewpoint, remains a critical obstacle that hinders the effectiveness of
the pretrain-finetune paradigm. To bridge this gap, we introduce a unified
text-based person retrieval pipeline considering domain adaptation at both
image and region levels. In particular, it contains two primary components,
i.e., Domain-aware Diffusion (DaD) for image-level adaptation and
Multi-granularity Relation Alignment (MRA) for region-level adaptation. As the
name implies, Domain-aware Diffusion is to migrate the distribution of images
from the pretraining dataset domain to the target real-world dataset domain,
e.g., CUHK-PEDES. Subsequently, MRA performs a meticulous region-level
alignment by establishing correspondences between visual regions and their
descriptive sentences, thereby addressing disparities at a finer granularity.
Extensive experiments show that our dual-level adaptation method has achieved
state-of-the-art results on the CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets,
outperforming existing methodologies. The dataset, model, and code are
available at https://github.com/Shuyu-XJTU/MRA.

</details>


### [332] [A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images](https://arxiv.org/abs/2507.10202)
*Jaeseong Lee,Yeeun Choi,Heechan Choi,Hanjung Kim,Seonjoo Kim*

Main category: cs.CV

TL;DR: ECP is a training-free, task-agnostic framework to improve MLLM performance on high-resolution images by leveraging coarse predictions for localization and preserving fine details.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with fine-grained localization in high-resolution images due to fixed resolution fine-tuning, leading to poor generalization or loss of details.

Method: Proposes Extract Candidate then Predict (ECP), a two-stage framework: first identifies candidate regions using coarse predictions, then refines predictions based on these regions.

Result: Achieves +21.3%, +5.8%, +5.2% absolute improvement on 4K GUI grounding and 4K, 8K MLLM perception tasks.

Conclusion: ECP effectively enhances MLLM performance on high-resolution images without additional training, preserving fine-grained details.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in vision-language understanding, reasoning, and generation.
However, they struggle with tasks requiring fine-grained localization and
reasoning in high-resolution images. This constraint stems from the fact that
MLLMs are fine-tuned with fixed image resolution to align with the pre-trained
image encoder used in MLLM. Consequently, feeding high-resolution images
directly into MLLMs leads to poor generalization due to a train-test resolution
discrepancy, while downsampling these images-although ensuring
consistency-compromises fine-grained visual details and ultimately degrades
performance. To address this challenge, we propose Extract Candidate then
Predict (ECP), a novel training-free, task-agnostic two-stage framework
designed to enhance MLLM performance on high-resolution images. The key
intuition behind ECP is that while MLLMs struggle with high-resolution images,
their predictions on downsampled images still contain implicit localization
cues. By first identifying candidate region using the coarse prediction and
then predicting the final output based on candidate region, ECP effectively
preserves fine-grained details while mitigating the challenges posed by
high-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K
MLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared
to baseline respectively, demonstrating its effectiveness. Code is available at
https://github.com/yenncye/ECP.

</details>


### [333] [Improving Multimodal Learning via Imbalanced Learning](https://arxiv.org/abs/2507.10203)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: The paper proposes Asymmetric Representation Learning (ARL) to optimize multimodal learning by imbalanced dependency on modalities, inversely proportional to their variances, improving performance without extra parameters.


<details>
  <summary>Details</summary>
Motivation: Multimodal learning often underperforms unimodal learning due to imbalanced optimization. Existing gradient balancing methods are suboptimal.

Method: ARL uses auxiliary regularizers to calculate modality variances, re-weighting optimization inversely to variance ratios, and jointly optimizes prediction bias with multimodal loss.

Result: Extensive experiments show ARL's effectiveness and versatility across datasets.

Conclusion: ARL improves multimodal learning by imbalanced optimization, validated by experiments, and is parameter-efficient.

Abstract: Multimodal learning often encounters the under-optimized problem and may
perform worse than unimodal learning. Existing approaches attribute this issue
to imbalanced learning across modalities and tend to address it through
gradient balancing. However, this paper argues that balanced learning is not
the optimal setting for multimodal learning. With bias-variance analysis, we
prove that imbalanced dependency on each modality obeying the inverse ratio of
their variances contributes to optimal performance. To this end, we propose the
Asymmetric Representation Learning(ARL) strategy to assist multimodal learning
via imbalanced optimization. ARL introduces auxiliary regularizers for each
modality encoder to calculate their prediction variance. ARL then calculates
coefficients via the unimodal variance to re-weight the optimization of each
modality, forcing the modality dependence ratio to be inversely proportional to
the modality variance ratio. Moreover, to minimize the generalization error,
ARL further introduces the prediction bias of each modality and jointly
optimizes them with multimodal loss. Notably, all auxiliary regularizers share
parameters with the multimodal model and rely only on the modality
representation. Thus the proposed ARL strategy introduces no extra parameters
and is independent of the structures and fusion methods of the multimodal
model. Finally, extensive experiments on various datasets validate the
effectiveness and versatility of ARL. Code is available at
\href{https://github.com/shicaiwei123/ICCV2025-ARL}{https://github.com/shicaiwei123/ICCV2025-ARL}

</details>


### [334] [Is Micro-expression Ethnic Leaning?](https://arxiv.org/abs/2507.10209)
*Huai-Qian Khor,Yante Li,Xingxun Jiang,Guoying Zhao*

Main category: cs.CV

TL;DR: The paper challenges Ekman's universality of emotions by investigating ethnic influences on micro-expression recognition, proposing an ethnically aware framework.


<details>
  <summary>Details</summary>
Motivation: To explore the role of ethnicity in emotional expression, countering the assumption of universal emotions across cultures.

Method: Constructed a cross-cultural micro-expression database, annotated ethnic labels, and compared mono-ethnicity vs. stereo-ethnicity in controlled experiments.

Result: Found ethnic bias in micro-expression recognition and proposed an ethnically aware framework to address it.

Conclusion: Ethnicity significantly influences emotional expression, and integrating ethnic context improves micro-expression recognition.

Abstract: How much does ethnicity play its part in emotional expression? Emotional
expression and micro-expression research probe into understanding human
psychological responses to emotional stimuli, thereby revealing substantial
hidden yet authentic emotions that can be useful in the event of diagnosis and
interviews. While increased attention had been provided to micro-expression
analysis, the studies were done under Ekman's assumption of emotion
universality, where emotional expressions are identical across cultures and
social contexts. Our computational study uncovers some of the influences of
ethnic background in expression analysis, leading to an argument that the
emotional universality hypothesis is an overgeneralization from the perspective
of manual psychological analysis. In this research, we propose to investigate
the level of influence of ethnicity in a simulated micro-expression scenario.
We construct a cross-cultural micro-expression database and algorithmically
annotate the ethnic labels to facilitate the investigation. With the ethnically
annotated dataset, we perform a prima facie study to compare mono-ethnicity and
stereo-ethnicity in a controlled environment, which uncovers a certain
influence of ethnic bias via an experimental way. Building on this finding, we
propose a framework that integrates ethnic context into the emotional feature
learning process, yielding an ethnically aware framework that recognises
ethnicity differences in micro-expression recognition. For improved
understanding, qualitative analyses have been done to solidify the preliminary
investigation into this new realm of research. Code is publicly available at
https://github.com/IcedDoggie/ICMEW2025_EthnicMER

</details>


### [335] [Boosting Multimodal Learning via Disentangled Gradient Learning](https://arxiv.org/abs/2507.10213)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: The paper identifies optimization conflicts in multimodal learning, proposes a disentangled gradient learning (DGL) framework to improve performance, and validates its effectiveness across various tasks.


<details>
  <summary>Details</summary>
Motivation: Multimodal learning underperforms unimodal learning due to gradient conflicts between modality encoders and fusion modules, which existing methods fail to address.

Method: The DGL framework decouples optimization by truncating and replacing gradients: using unimodal loss for modality encoders and removing unimodal gradients from fusion modules.

Result: Experiments show DGL improves performance across diverse modalities, tasks, and frameworks with dense cross-modal interaction.

Conclusion: DGL effectively resolves optimization conflicts in multimodal learning, enhancing performance beyond unimodal baselines.

Abstract: Multimodal learning often encounters the under-optimized problem and may have
worse performance than unimodal learning. Existing methods attribute this
problem to the imbalanced learning between modalities and rebalance them
through gradient modulation. However, they fail to explain why the dominant
modality in multimodal models also underperforms that in unimodal learning. In
this work, we reveal the optimization conflict between the modality encoder and
modality fusion module in multimodal models. Specifically, we prove that the
cross-modal fusion in multimodal models decreases the gradient passed back to
each modality encoder compared with unimodal models. Consequently, the
performance of each modality in the multimodal model is inferior to that in the
unimodal model. To this end, we propose a disentangled gradient learning (DGL)
framework to decouple the optimization of the modality encoder and modality
fusion module in the multimodal model. DGL truncates the gradient
back-propagated from the multimodal loss to the modality encoder and replaces
it with the gradient from unimodal loss. Besides, DGL removes the gradient
back-propagated from the unimodal loss to the modality fusion module. This
helps eliminate the gradient interference between the modality encoder and
modality fusion module while ensuring their respective optimization processes.
Finally, extensive experiments on multiple types of modalities, tasks, and
frameworks with dense cross-modal interaction demonstrate the effectiveness and
versatility of the proposed DGL. Code is available at
\href{https://github.com/shicaiwei123/ICCV2025-GDL}{https://github.com/shicaiwei123/ICCV2025-GDL}

</details>


### [336] [From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level Controllable Human Image Generation](https://arxiv.org/abs/2507.10217)
*Jeongho Kim,Sunghyun Park,Hyoungwoo Park,Sungrack Yun,Jaegul Choo,Seokeon Cho*

Main category: cs.CV

TL;DR: Wardrobe Polyptych LoRA introduces a part-level controllable model for personalized human image generation, reducing computational costs while improving fidelity and consistency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for personalized human image generation are computationally expensive and impractical for real-time applications, requiring either inference-time fine-tuning or large-scale dataset training.

Method: The approach trains only LoRA layers, conditions generation on the subject's wardrobe, uses spatial references, and introduces a selective subject region loss to improve fidelity and consistency.

Result: The method outperforms existing techniques in fidelity and consistency, enabling realistic and identity-preserving full-body synthesis without additional inference parameters.

Conclusion: Wardrobe Polyptych LoRA offers a practical and efficient solution for personalized human image generation, addressing the limitations of current approaches.

Abstract: Recent diffusion models achieve personalization by learning specific
subjects, allowing learned attributes to be integrated into generated images.
However, personalized human image generation remains challenging due to the
need for precise and consistent attribute preservation (e.g., identity,
clothing details). Existing subject-driven image generation methods often
require either (1) inference-time fine-tuning with few images for each new
subject or (2) large-scale dataset training for generalization. Both approaches
are computationally expensive and impractical for real-time applications. To
address these limitations, we present Wardrobe Polyptych LoRA, a novel
part-level controllable model for personalized human image generation. By
training only LoRA layers, our method removes the computational burden at
inference while ensuring high-fidelity synthesis of unseen subjects. Our key
idea is to condition the generation on the subject's wardrobe and leverage
spatial references to reduce information loss, thereby improving fidelity and
consistency. Additionally, we introduce a selective subject region loss, which
encourages the model to disregard some of reference images during training. Our
loss ensures that generated images better align with text prompts while
maintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no
additional parameters at the inference stage and performs generation using a
single model trained on a few training samples. We construct a new dataset and
benchmark tailored for personalized human image generation. Extensive
experiments show that our approach significantly outperforms existing
techniques in fidelity and consistency, enabling realistic and
identity-preserving full-body synthesis.

</details>


### [337] [Straighten Viscous Rectified Flow via Noise Optimization](https://arxiv.org/abs/2507.10218)
*Jimin Dai,Jiexi Yan,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: VRFNO improves Reflow by optimizing noise couplings and enhancing trajectory distinction, achieving better image generation in fewer steps.


<details>
  <summary>Details</summary>
Motivation: Reflow's limitations in generating high-quality images quickly due to a distribution gap between its deterministic couplings and real images.

Method: VRFNO integrates an encoder and neural velocity field, introducing a historical velocity term and noise optimization via reparameterization.

Result: VRFNO outperforms Reflow, achieving state-of-the-art performance in one-step and few-step generation tasks.

Conclusion: VRFNO effectively addresses Reflow's shortcomings, offering a superior alternative for efficient high-quality image generation.

Abstract: The Reflow operation aims to straighten the inference trajectories of the
rectified flow during training by constructing deterministic couplings between
noises and images, thereby improving the quality of generated images in
single-step or few-step generation. However, we identify critical limitations
in Reflow, particularly its inability to rapidly generate high-quality images
due to a distribution gap between images in its constructed deterministic
couplings and real images. To address these shortcomings, we propose a novel
alternative called Straighten Viscous Rectified Flow via Noise Optimization
(VRFNO), which is a joint training framework integrating an encoder and a
neural velocity field. VRFNO introduces two key innovations: (1) a historical
velocity term that enhances trajectory distinction, enabling the model to more
accurately predict the velocity of the current trajectory, and (2) the noise
optimization through reparameterization to form optimized couplings with real
images which are then utilized for training, effectively mitigating errors
caused by Reflow's limitations. Comprehensive experiments on synthetic data and
real datasets with varying resolutions show that VRFNO significantly mitigates
the limitations of Reflow, achieving state-of-the-art performance in both
one-step and few-step generation tasks.

</details>


### [338] [Spatial Lifting for Dense Prediction](https://arxiv.org/abs/2507.10222)
*Mingzhi Xu,Yizhe Zhang*

Main category: cs.CV

TL;DR: Spatial Lifting (SL) lifts 2D inputs to higher dimensions (e.g., 3D) for dense prediction tasks, reducing model parameters and inference costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency and accuracy in dense prediction tasks by leveraging higher-dimensional processing.

Method: Lifts 2D inputs (e.g., images) to higher dimensions (e.g., 3D) and processes them with networks like 3D U-Net.

Result: Achieves competitive performance on 19 benchmarks (13 segmentation, 6 depth estimation) with 98% fewer parameters and lower inference costs.

Conclusion: SL offers a new paradigm for efficient, accurate, and reliable dense prediction in vision tasks.

Abstract: We present Spatial Lifting (SL), a novel methodology for dense prediction
tasks. SL operates by lifting standard inputs, such as 2D images, into a
higher-dimensional space and subsequently processing them using networks
designed for that higher dimension, such as a 3D U-Net. Counterintuitively,
this dimensionality lifting allows us to achieve good performance on benchmark
tasks compared to conventional approaches, while reducing inference costs and
significantly lowering the number of model parameters. The SL framework
produces intrinsically structured outputs along the lifted dimension. This
emergent structure facilitates dense supervision during training and enables
robust, near-zero-additional-cost prediction quality assessment at test time.
We validate our approach across 19 benchmark datasets (13 for semantic
segmentation and 6 for depth estimation), demonstrating competitive dense
prediction performance while reducing the model parameter count by over 98% (in
the U-Net case) and lowering inference costs. Spatial Lifting introduces a new
vision modeling paradigm that offers a promising path toward more efficient,
accurate, and reliable deep networks for dense prediction tasks in vision.

</details>


### [339] [ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users](https://arxiv.org/abs/2507.10223)
*Xiangyu Yin,Boyuan Yang,Weichen Liu,Qiyao Xue,Abrar Alamri,Goeran Fiedler,Wei Gao*

Main category: cs.CV

TL;DR: The paper introduces ProGait, a dataset for vision-based gait analysis of prosthetic legs, supporting tasks like segmentation, pose estimation, and gait analysis. It includes 412 video clips from amputees and benchmark models showing improved performance.


<details>
  <summary>Details</summary>
Motivation: Gait analysis is crucial for prosthetic leg optimization, but vision-based ML struggles with prosthesis detection due to unique appearances and movement patterns. ProGait aims to address this gap.

Method: The authors created the ProGait dataset with 412 video clips from four above-knee amputees, capturing their gait with prosthetic legs. They also provided benchmark tasks and fine-tuned baseline models.

Result: The baseline models outperformed pre-trained vision models, demonstrating better generalizability for prosthesis-specific tasks.

Conclusion: ProGait offers a valuable resource for vision-based gait analysis of prosthetic legs, with practical applications and improved model performance.

Abstract: Prosthetic legs play a pivotal role in clinical rehabilitation, allowing
individuals with lower-limb amputations the ability to regain mobility and
improve their quality of life. Gait analysis is fundamental for optimizing
prosthesis design and alignment, directly impacting the mobility and life
quality of individuals with lower-limb amputations. Vision-based machine
learning (ML) methods offer a scalable and non-invasive solution to gait
analysis, but face challenges in correctly detecting and analyzing prosthesis,
due to their unique appearances and new movement patterns. In this paper, we
aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait,
to support multiple vision tasks including Video Object Segmentation, 2D Human
Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from
four above-knee amputees when testing multiple newly-fitted prosthetic legs
through walking trials, and depicts the presence, contours, poses, and gait
patterns of human subjects with transfemoral prosthetic legs. Alongside the
dataset itself, we also present benchmark tasks and fine-tuned baseline models
to illustrate the practical application and performance of the ProGait dataset.
We compared our baseline models against pre-trained vision models,
demonstrating improved generalizability when applying the ProGait dataset for
prosthesis-specific tasks. Our code is available at
https://github.com/pittisl/ProGait and dataset at
https://huggingface.co/datasets/ericyxy98/ProGait.

</details>


### [340] [Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection](https://arxiv.org/abs/2507.10225)
*Jinglun Li,Kaixun Jiang,Zhaoyu Chen,Bo Lin,Yao Tang,Weifeng Ge,Wenqiang Zhang*

Main category: cs.CV

TL;DR: SynOOD leverages foundation models to generate synthetic OOD data for fine-tuning CLIP, improving boundary-level discrimination and achieving state-of-the-art performance on ImageNet.


<details>
  <summary>Details</summary>
Motivation: Challenging OOD samples close to InD data cause misclassification; foundation models offer a solution.

Method: Uses iterative in-painting with MLLM prompts and noise adjustments to create boundary-aligned OOD samples, then fine-tunes CLIP.

Result: Achieves 2.80% AUROC improvement and 11.13% FPR95 reduction on ImageNet.

Conclusion: SynOOD effectively enhances OOD detection with minimal overhead, outperforming existing methods.

Abstract: Pre-trained vision-language models have exhibited remarkable abilities in
detecting out-of-distribution (OOD) samples. However, some challenging OOD
samples, which lie close to in-distribution (InD) data in image feature space,
can still lead to misclassification. The emergence of foundation models like
diffusion models and multimodal large language models (MLLMs) offers a
potential solution to this issue. In this work, we propose SynOOD, a novel
approach that harnesses foundation models to generate synthetic, challenging
OOD data for fine-tuning CLIP models, thereby enhancing boundary-level
discrimination between InD and OOD samples. Our method uses an iterative
in-painting process guided by contextual prompts from MLLMs to produce nuanced,
boundary-aligned OOD samples. These samples are refined through noise
adjustments based on gradients from OOD scores like the energy score,
effectively sampling from the InD/OOD boundary. With these carefully
synthesized images, we fine-tune the CLIP image encoder and negative label
features derived from the text encoder to strengthen connections between
near-boundary OOD samples and a set of negative labels. Finally, SynOOD
achieves state-of-the-art performance on the large-scale ImageNet benchmark,
with minimal increases in parameters and runtime. Our approach significantly
surpasses existing methods, improving AUROC by 2.80% and reducing FPR95 by
11.13%. Codes are available in https://github.com/Jarvisgivemeasuit/SynOOD.

</details>


### [341] [Navigating the Challenges of AI-Generated Image Detection in the Wild: What Truly Matters?](https://arxiv.org/abs/2507.10236)
*Despina Konstantinidou,Dimitrios Karageorgiou,Christos Koutlis,Olga Papadopoulou,Emmanouil Schinas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: The paper addresses weaknesses in AI-Generated Image Detection (AID) models, introducing ITW-SM dataset and identifying key factors for real-world performance, achieving a 26.87% AUC improvement.


<details>
  <summary>Details</summary>
Motivation: The rise of generative technologies poses challenges in digital trust, necessitating robust AID methods to detect increasingly realistic AI-generated images.

Method: The study evaluates AID models using the ITW-SM dataset, analyzing backbone architecture, training data, pre-processing, and augmentation.

Result: Modifications based on the analysis improve AID models' AUC by 26.87% in real-world conditions.

Conclusion: The paper highlights critical factors for enhancing AID performance and demonstrates significant improvements in real-world detection.

Abstract: The rapid advancement of generative technologies presents both unprecedented
creative opportunities and significant challenges, particularly in maintaining
social trust and ensuring the integrity of digital information. Following these
concerns, the challenge of AI-Generated Image Detection (AID) becomes
increasingly critical. As these technologies become more sophisticated, the
quality of AI-generated images has reached a level that can easily deceive even
the most discerning observers. Our systematic evaluation highlights a critical
weakness in current AI-Generated Image Detection models: while they perform
exceptionally well on controlled benchmark datasets, they struggle
significantly with real-world variations. To assess this, we introduce ITW-SM,
a new dataset of real and AI-generated images collected from major social media
platforms. In this paper, we identify four key factors that influence AID
performance in real-world scenarios: backbone architecture, training data
composition, pre-processing strategies and data augmentation combinations. By
systematically analyzing these components, we shed light on their impact on
detection efficacy. Our modifications result in an average AUC improvement of
26.87% across various AID models under real-world conditions.

</details>


### [342] [Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks](https://arxiv.org/abs/2507.10239)
*Ben Hamscher,Edgar Heinert,Annika Mütze,Kira Maag,Matthias Rottmann*

Main category: cs.CV

TL;DR: Style transfer augmentation in semantic segmentation reduces texture bias and enhances robustness to image corruptions and adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: To investigate if style transfer can reduce texture bias and improve robustness in semantic segmentation, similar to its effects in image classification.

Method: Style transfer with random Voronoi cell-based areas, applied to train semantic segmentation DNNs, focusing on shape over texture.

Result: Reduced texture bias and increased robustness to corruptions and adversarial attacks across CNN and transformer architectures on Cityscapes and PASCAL Context.

Conclusion: Style transfer augmentation is effective for reducing texture dependence and improving robustness in semantic segmentation, generalizing across datasets and architectures.

Abstract: Recent research has investigated the shape and texture biases of deep neural
networks (DNNs) in image classification which influence their generalization
capabilities and robustness. It has been shown that, in comparison to regular
DNN training, training with stylized images reduces texture biases in image
classification and improves robustness with respect to image corruptions. In an
effort to advance this line of research, we examine whether style transfer can
likewise deliver these two effects in semantic segmentation. To this end, we
perform style transfer with style varying across artificial image areas. Those
random areas are formed by a chosen number of Voronoi cells. The resulting
style-transferred data is then used to train semantic segmentation DNNs with
the objective of reducing their dependence on texture cues while enhancing
their reliance on shape-based features. In our experiments, it turns out that
in semantic segmentation, style transfer augmentation reduces texture bias and
strongly increases robustness with respect to common image corruptions as well
as adversarial attacks. These observations hold for convolutional neural
networks and transformer architectures on the Cityscapes dataset as well as on
PASCAL Context, showing the generality of the proposed method.

</details>


### [343] [Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures](https://arxiv.org/abs/2507.10265)
*Xinlong Ding,Hongwei Yu,Jiawei Li,Feifan Li,Yu Shang,Bochao Zou,Huimin Ma,Jiansheng Chen*

Main category: cs.CV

TL;DR: The paper introduces the Kaleidoscopic Background Attack (KBA) to disrupt camera pose estimation by using symmetric background textures, enhancing attack effectiveness with a novel loss function.


<details>
  <summary>Details</summary>
Motivation: Background textures in sparse input scenarios can degrade pose estimation accuracy, prompting the need for adversarial attacks like KBA.

Method: KBA uses identical segments to create symmetric discs, optimized with a projected orientation consistency loss.

Result: Optimized adversarial backgrounds successfully attack multiple pose estimation models.

Conclusion: KBA demonstrates the vulnerability of pose estimation to adversarial backgrounds, highlighting a need for robust solutions.

Abstract: Camera pose estimation is a fundamental computer vision task that is
essential for applications like visual localization and multi-view stereo
reconstruction. In the object-centric scenarios with sparse inputs, the
accuracy of pose estimation can be significantly influenced by background
textures that occupy major portions of the images across different viewpoints.
In light of this, we introduce the Kaleidoscopic Background Attack (KBA), which
uses identical segments to form discs with multi-fold radial symmetry. These
discs maintain high similarity across different viewpoints, enabling effective
attacks on pose estimation models even with natural texture segments.
Additionally, a projected orientation consistency loss is proposed to optimize
the kaleidoscopic segments, leading to significant enhancement in the attack
effectiveness. Experimental results show that optimized adversarial
kaleidoscopic backgrounds can effectively attack various camera pose estimation
models.

</details>


### [344] [FTCFormer: Fuzzy Token Clustering Transformer for Image Classification](https://arxiv.org/abs/2507.10283)
*Muyi Bao,Changyu Zeng,Yifan Wang,Zhengni Yang,Zimu Wang,Guangliang Cheng,Jun Qi,Wei Wang*

Main category: cs.CV

TL;DR: FTCFormer introduces a clustering-based downsampling module to dynamically generate vision tokens based on semantic meanings, improving feature representation and performance across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Current transformer architectures use uniform grid-based vision tokens, ignoring semantic meanings of image regions, leading to suboptimal feature representations.

Method: FTCFormer uses a clustering-based downsampling module with DPC-FKNN for clustering center determination, SCS for token assignment, and Cmerge for token merging.

Result: FTCFormer outperforms TCFormer, achieving gains of 1.43% on fine-grained, 1.09% on natural, 0.97% on medical, and 0.55% on remote sensing datasets.

Conclusion: FTCFormer effectively addresses the limitations of uniform token embedding by leveraging semantic clustering, enhancing performance across various vision tasks.

Abstract: Transformer-based deep neural networks have achieved remarkable success
across various computer vision tasks, largely attributed to their long-range
self-attention mechanism and scalability. However, most transformer
architectures embed images into uniform, grid-based vision tokens, neglecting
the underlying semantic meanings of image regions, resulting in suboptimal
feature representations. To address this issue, we propose Fuzzy Token
Clustering Transformer (FTCFormer), which incorporates a novel clustering-based
downsampling module to dynamically generate vision tokens based on the semantic
meanings instead of spatial positions. It allocates fewer tokens to less
informative regions and more to represent semantically important regions,
regardless of their spatial adjacency or shape irregularity. To further enhance
feature extraction and representation, we propose a Density Peak
Clustering-Fuzzy K-Nearest Neighbor (DPC-FKNN) mechanism for clustering center
determination, a Spatial Connectivity Score (SCS) for token assignment, and a
channel-wise merging (Cmerge) strategy for token merging. Extensive experiments
on 32 datasets across diverse domains validate the effectiveness of FTCFormer
on image classification, showing consistent improvements over the TCFormer
baseline, achieving gains of improving 1.43% on five fine-grained datasets,
1.09% on six natural image datasets, 0.97% on three medical datasets and 0.55%
on four remote sensing datasets. The code is available at:
https://github.com/BaoBao0926/FTCFormer/tree/main.

</details>


### [345] [Show and Polish: Reference-Guided Identity Preservation in Face Video Restoration](https://arxiv.org/abs/2507.10293)
*Wenkang Han,Wang Lin,Yiyun Zhou,Qi Liu,Shulei Wang,Chang Yao,Jingyuan Chen*

Main category: cs.CV

TL;DR: IP-FVR is a novel method for face video restoration that uses a reference image for identity conditioning, employs cross-attention mechanisms, and introduces feedback learning and blending strategies to minimize identity drift, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods fail to preserve identity-specific features in severely degraded face videos, often producing generic results. IP-FVR aims to address this by leveraging reference images and advanced techniques for identity consistency.

Method: IP-FVR uses decoupled cross-attention for identity conditioning, feedback learning for intra-clip drift, exponential blending for inter-clip drift, and multi-stream negative prompts to enhance restoration.

Result: IP-FVR outperforms existing methods in quality and identity preservation on synthetic and real-world datasets.

Conclusion: IP-FVR shows significant potential for practical face video restoration, offering detailed and identity-consistent results.

Abstract: Face Video Restoration (FVR) aims to recover high-quality face videos from
degraded versions. Traditional methods struggle to preserve fine-grained,
identity-specific features when degradation is severe, often producing
average-looking faces that lack individual characteristics. To address these
challenges, we introduce IP-FVR, a novel method that leverages a high-quality
reference face image as a visual prompt to provide identity conditioning during
the denoising process. IP-FVR incorporates semantically rich identity
information from the reference image using decoupled cross-attention
mechanisms, ensuring detailed and identity consistent results. For intra-clip
identity drift (within 24 frames), we introduce an identity-preserving feedback
learning method that combines cosine similarity-based reward signals with
suffix-weighted temporal aggregation. This approach effectively minimizes drift
within sequences of frames. For inter-clip identity drift, we develop an
exponential blending strategy that aligns identities across clips by
iteratively blending frames from previous clips during the denoising process.
This method ensures consistent identity representation across different clips.
Additionally, we enhance the restoration process with a multi-stream negative
prompt, guiding the model's attention to relevant facial attributes and
minimizing the generation of low-quality or incorrect features. Extensive
experiments on both synthetic and real-world datasets demonstrate that IP-FVR
outperforms existing methods in both quality and identity preservation,
showcasing its substantial potential for practical applications in face video
restoration.

</details>


### [346] [DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs](https://arxiv.org/abs/2507.10302)
*Jiahe Zhao,Rongkun Zheng,Yi Wang,Helin Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: DisCo introduces a novel visual encapsulation method for video MLLMs, addressing semantic indistinctness and temporal incoherence with two modules: VCD for semantic clarity and TFC for temporal consistency. It outperforms existing methods in benchmarks and improves token efficiency.


<details>
  <summary>Details</summary>
Motivation: Linear projectors in video MLLMs cause semantic indistinctness and temporal incoherence. Resamplers show potential but lack effective solutions, prompting the need for DisCo.

Method: DisCo uses a Visual Concept Discriminator (VCD) for semantic distinctness and a Temporal Focus Calibrator (TFC) for temporal coherence.

Result: DisCo outperforms state-of-the-art methods in video understanding benchmarks and achieves higher token efficiency.

Conclusion: DisCo effectively addresses key challenges in video MLLMs, offering improved performance and efficiency.

Abstract: In video Multimodal Large Language Models (video MLLMs), the visual
encapsulation process plays a pivotal role in converting video contents into
representative tokens for LLM input. While linear projectors are widely
employed for encapsulation, they introduce semantic indistinctness and temporal
incoherence when applied to videos. Conversely, the structure of resamplers
shows promise in tackling these challenges, but an effective solution remains
unexplored. Drawing inspiration from resampler structures, we introduce DisCo,
a novel visual encapsulation method designed to yield semantically distinct and
temporally coherent visual tokens for video MLLMs. DisCo integrates two key
components: (1) A Visual Concept Discriminator (VCD) module, assigning unique
semantics for visual tokens by associating them in pair with discriminative
concepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring
consistent temporal focus of visual tokens to video elements across every video
frame. Through extensive experiments on multiple video MLLM frameworks, we
demonstrate that DisCo remarkably outperforms previous state-of-the-art methods
across a variety of video understanding benchmarks, while also achieving higher
token efficiency thanks to the reduction of semantic indistinctness. The code:
https://github.com/ZJHTerry18/DisCo.

</details>


### [347] [Contrastive Pretraining with Dual Visual Encoders for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2507.10306)
*Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: A two-phase, dual visual encoder framework for gloss-free Sign Language Translation (SLT) outperforms single-stream variants and achieves top BLEU-4 scores.


<details>
  <summary>Details</summary>
Motivation: Early SLT systems rely on costly gloss annotations, which are limited in capturing continuous signing complexity. This work aims to bypass gloss annotations for more efficient and accurate translation.

Method: Proposes a dual visual encoder framework with contrastive visual-language pretraining. Two visual backbones align outputs with each other and text embeddings. Features are fused for downstream SLT using an encoder-decoder model.

Result: The dual encoder architecture outperforms single-stream variants and achieves the highest BLEU-4 score on the Phoenix-2014T benchmark among gloss-free SLT approaches.

Conclusion: The proposed gloss-free SLT framework is effective, leveraging dual encoders and contrastive pretraining to improve translation accuracy without relying on gloss annotations.

Abstract: Sign Language Translation (SLT) aims to convert sign language videos into
spoken or written text. While early systems relied on gloss annotations as an
intermediate supervision, such annotations are costly to obtain and often fail
to capture the full complexity of continuous signing. In this work, we propose
a two-phase, dual visual encoder framework for gloss-free SLT, leveraging
contrastive visual-language pretraining. During pretraining, our approach
employs two complementary visual backbones whose outputs are jointly aligned
with each other and with sentence-level text embeddings via a contrastive
objective. During the downstream SLT task, we fuse the visual features and
input them into an encoder-decoder model. On the Phoenix-2014T benchmark, our
dual encoder architecture consistently outperforms its single stream variants
and achieves the highest BLEU-4 score among existing gloss-free SLT approaches.

</details>


### [348] [Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching](https://arxiv.org/abs/2507.10318)
*Yuhan Liu,Jingwen Fu,Yang Wu,Kangyi Wu,Pengna Li,Jiayi Wu,Sanping Zhou,Jingmin Xin*

Main category: cs.CV

TL;DR: The paper introduces IMD, a framework using diffusion models for image feature matching, addressing misalignment issues in foundation models and improving performance in multi-instance scenarios.


<details>
  <summary>Details</summary>
Motivation: Previous works ignored misalignment when using foundation models for feature matching, leading to suboptimal performance, especially in multi-instance cases.

Method: IMD integrates generative-based diffusion models for instance-level details and introduces a cross-image interaction prompting module for bidirectional information flow.

Result: IMD achieves state-of-the-art performance, with a 12% improvement on the new IMIM benchmark for multi-instance scenarios.

Conclusion: The IMD framework effectively mitigates misalignment in foundation models for feature matching, demonstrating superior performance in multi-instance tasks.

Abstract: Leveraging the vision foundation models has emerged as a mainstream paradigm
that improves the performance of image feature matching. However, previous
works have ignored the misalignment when introducing the foundation models into
feature matching. The misalignment arises from the discrepancy between the
foundation models focusing on single-image understanding and the cross-image
understanding requirement of feature matching. Specifically, 1) the embeddings
derived from commonly used foundation models exhibit discrepancies with the
optimal embeddings required for feature matching; 2) lacking an effective
mechanism to leverage the single-image understanding ability into cross-image
understanding. A significant consequence of the misalignment is they struggle
when addressing multi-instance feature matching problems. To address this, we
introduce a simple but effective framework, called IMD (Image feature Matching
with a pre-trained Diffusion model) with two parts: 1) Unlike the dominant
solutions employing contrastive-learning based foundation models that emphasize
global semantics, we integrate the generative-based diffusion models to
effectively capture instance-level details. 2) We leverage the prompt mechanism
in generative model as a natural tunnel, propose a novel cross-image
interaction prompting module to facilitate bidirectional information
interaction between image pairs. To more accurately measure the misalignment,
we propose a new benchmark called IMIM, which focuses on multi-instance
scenarios. Our proposed IMD establishes a new state-of-the-art in commonly
evaluated benchmarks, and the superior improvement 12% in IMIM indicates our
method efficiently mitigates the misalignment.

</details>


### [349] [Text Embedding Knows How to Quantize Text-Guided Diffusion Models](https://arxiv.org/abs/2507.10340)
*Hongjae Lee,Myungjun Son,Dongjea Kang,Seung-Won Jung*

Main category: cs.CV

TL;DR: QLIP is a novel quantization method for diffusion models that uses text prompts to guide bit precision selection, improving efficiency and image quality.


<details>
  <summary>Details</summary>
Motivation: The computational complexity of diffusion models limits their use in resource-constrained environments, and existing quantization methods ignore input conditions like text prompts.

Method: QLIP leverages text prompts to dynamically select bit precision for each layer and time step, and integrates with existing quantization methods.

Result: QLIP reduces computational complexity and enhances generated image quality across multiple datasets.

Conclusion: QLIP effectively addresses the limitations of diffusion models by incorporating text prompts into quantization, offering a scalable solution for resource-efficient image generation.

Abstract: Despite the success of diffusion models in image generation tasks such as
text-to-image, the enormous computational complexity of diffusion models limits
their use in resource-constrained environments. To address this, network
quantization has emerged as a promising solution for designing efficient
diffusion models. However, existing diffusion model quantization methods do not
consider input conditions, such as text prompts, as an essential source of
information for quantization. In this paper, we propose a novel quantization
method dubbed Quantization of Language-to-Image diffusion models using text
Prompts (QLIP). QLIP leverages text prompts to guide the selection of bit
precision for every layer at each time step. In addition, QLIP can be
seamlessly integrated into existing quantization methods to enhance
quantization efficiency. Our extensive experiments demonstrate the
effectiveness of QLIP in reducing computational complexity and improving the
quality of the generated images across various datasets.

</details>


### [350] [FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans](https://arxiv.org/abs/2507.10343)
*Hugo Norrby,Gabriel Färm,Kevin Hernandez-Diaz,Fernando Alonso-Fernandez*

Main category: cs.CV

TL;DR: FGSSNet is a multi-headed feature-guided semantic segmentation architecture for wall segmentation on floorplans, outperforming vanilla U-Net by injecting domain-specific features.


<details>
  <summary>Details</summary>
Motivation: To improve generalization in wall segmentation by leveraging domain-specific features like texture and wall width.

Method: Uses a U-Net backbone with a multi-headed feature extractor trained on wall patches to guide segmentation via injected latent features.

Result: Experiments show FGSSNet outperforms vanilla U-Net, validating the feature-guided approach.

Conclusion: FGSSNet's feature injection enhances wall segmentation, proving its effectiveness over traditional methods.

Abstract: We introduce FGSSNet, a novel multi-headed feature-guided semantic
segmentation (FGSS) architecture designed to improve the generalization ability
of wall segmentation on floorplans. FGSSNet features a U-Net segmentation
backbone with a multi-headed dedicated feature extractor used to extract
domain-specific feature maps which are injected into the latent space of U-Net
to guide the segmentation process. This dedicated feature extractor is trained
as an encoder-decoder with selected wall patches, representative of the walls
present in the input floorplan, to produce a compressed latent representation
of wall patches while jointly trained to predict the wall width. In doing so,
we expect that the feature extractor encodes texture and width features of wall
patches that are useful to guide the wall segmentation process. Our experiments
show increased performance by the use of such injected features in comparison
to the vanilla U-Net, highlighting the validity of the proposed approach.

</details>


### [351] [Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter](https://arxiv.org/abs/2507.10355)
*Bo Jiang,Xueyang Ze,Beibei Wang,Xixi Wang,Xixi Wan,Bin Luo*

Main category: cs.CV

TL;DR: The paper proposes VRGAdapter, a Vertex Random Graph Adapter, to capture diverse textual descriptions and inter-class relationships in Vision-Language Models, enhancing downstream task performance.


<details>
  <summary>Details</summary>
Motivation: Existing deterministic textual adapters fail to capture the diversity in textual descriptions and inter-class relationships, limiting their effectiveness.

Method: VRGAdapter uses a Vertex Random Knowledge Graph (VRKG) to model diverse descriptions and inter-class relationships, followed by probabilistic message propagation and reparameterized sampling for adapter learning. An Uncertainty-guided Multi-branch Fusion (UMF) scheme is also introduced for robust ensemble prediction.

Result: Extensive experiments on benchmark datasets validate the effectiveness of VRGAdapter.

Conclusion: VRGAdapter offers a general and improved adapter solution for Vision-Language Models, outperforming traditional methods.

Abstract: Textual adapter-based tuning methods have shown significant potential in
transferring knowledge from pre-trained Vision-Language Models (VLMs) to
downstream tasks. Existing works generally employ the deterministic textual
feature adapter to refine each category textual representation. However, due to
inherent factors such as different attributes and contexts, there exists
significant diversity in textual descriptions for each category. Such
description diversity offers rich discriminative semantic knowledge that can
benefit downstream visual learning tasks. Obviously, traditional deterministic
adapter model cannot adequately capture this varied semantic information. Also,
it is desirable to exploit the inter-class relationships in VLM adapter. To
address these issues, we propose to exploit random graph model into VLM adapter
and develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first
models the inherent diverse descriptions of each category and inter-class
relationships of different categories simultaneously by leveraging a Vertex
Random Knowledge Graph (VRKG) model. Then, it employs probabilistic message
propagation on VRKG to learn context-aware distribution representation for each
class node. Finally, it adopts a reparameterized sampling function to achieve
textual adapter learning. Note that, VRGAdapter provides a more general adapter
solution that encompasses traditional graph-based adapter as a special case. In
addition, to enable more robust performance for downstream tasks, we also
introduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that
dynamically integrates multiple pre-trained models for ensemble prediction.
Extensive experiments on multiple benchmark datasets demonstrate the
effectiveness of our approach.

</details>


### [352] [Fine-Grained Zero-Shot Object Detection](https://arxiv.org/abs/2507.10358)
*Hongxu Ma,Chenbo Zhang,Lu Zhang,Jiaogen Zhou,Jihong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: The paper introduces Fine-Grained Zero-Shot Object Detection (FG-ZSD), addressing the challenge of detecting visually similar objects. It proposes the MSHC method and a new dataset, FGZSD-Birds, showing superior performance over existing models.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot object detection (ZSD) methods struggle with fine-grained scenarios where classes are visually similar, such as distinguishing bird species. This paper aims to bridge this gap.

Method: The MSHC method improves a two-stage detector with multi-level semantics-aware embedding alignment loss to align visual and semantic spaces tightly.

Result: The proposed method outperforms existing ZSD models on the newly created FGZSD-Birds dataset, which includes 1432 species.

Conclusion: The paper successfully addresses FG-ZSD, demonstrating the effectiveness of MSHC and the need for specialized datasets like FGZSD-Birds.

Abstract: Zero-shot object detection (ZSD) aims to leverage semantic descriptions to
localize and recognize objects of both seen and unseen classes. Existing ZSD
works are mainly coarse-grained object detection, where the classes are
visually quite different, thus are relatively easy to distinguish. However, in
real life we often have to face fine-grained object detection scenarios, where
the classes are too similar to be easily distinguished. For example, detecting
different kinds of birds, fishes, and flowers.
  In this paper, we propose and solve a new problem called Fine-Grained
Zero-Shot Object Detection (FG-ZSD for short), which aims to detect objects of
different classes with minute differences in details under the ZSD paradigm. We
develop an effective method called MSHC for the FG-ZSD task, which is based on
an improved two-stage detector and employs a multi-level semantics-aware
embedding alignment loss, ensuring tight coupling between the visual and
semantic spaces. Considering that existing ZSD datasets are not suitable for
the new FG-ZSD task, we build the first FG-ZSD benchmark dataset FGZSD-Birds,
which contains 148,820 images falling into 36 orders, 140 families, 579 genera
and 1432 species. Extensive experiments on FGZSD-Birds show that our method
outperforms existing ZSD models.

</details>


### [353] [Test-Time Canonicalization by Foundation Models for Robust Perception](https://arxiv.org/abs/2507.10375)
*Utkarsh Singhal,Ryan Feng,Stella X. Yu,Atul Prakash*

Main category: cs.CV

TL;DR: FOCAL is a test-time framework using internet-scale visual priors to enhance perception robustness without retraining or architectural changes.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on specialized architectures or predefined augmentations, limiting generalization to diverse transformations.

Method: FOCAL generates and optimizes candidate transformations toward visually typical views using foundation models.

Result: Improved robustness of CLIP and SAM across 2D/3D rotations, illumination shifts, and day-night variations.

Conclusion: FOCAL offers a scalable alternative to transform-specific training, achieving invariance without re-training.

Abstract: Real-world visual perception requires invariance to diverse transformations,
yet current methods rely heavily on specialized architectures or training on
predefined augmentations, limiting generalization. We propose FOCAL, a
test-time, data-driven framework that achieves robust perception by leveraging
internet-scale visual priors from foundation models. By generating and
optimizing candidate transformations toward visually typical, "canonical"
views, FOCAL enhances robustness without re-training or architectural changes.
Our experiments demonstrate improved robustness of CLIP and SAM across
challenging transformations, including 2D/3D rotations, illumination shifts
(contrast and color), and day-night variations. We also highlight potential
applications in active vision. Our approach challenges the assumption that
transform-specific training is necessary, instead offering a scalable path to
invariance. Our code is available at: https://github.com/sutkarsh/focal.

</details>


### [354] [Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks](https://arxiv.org/abs/2507.10381)
*Aaryam Sharma*

Main category: cs.CV

TL;DR: The paper proposes integrating topological data analysis (TDA) features with deep learning for remote sensing classification, improving model performance on datasets like EuroSAT and RESISC45.


<details>
  <summary>Details</summary>
Motivation: CNNs are biased towards texture-based features, limiting their effectiveness. TDA offers robust geometric descriptors, which can complement deep learning models.

Method: A TDA feature engineering pipeline is developed to extract topological features, which are integrated with a ResNet18 model for classification.

Result: The method boosts ResNet18's accuracy by 1.44% on EuroSAT (99.33%) and 1.82% on RESISC45, outperforming larger models like ResNet50 and XL Vision Transformers.

Conclusion: TDA features enhance deep learning models even on datasets without explicit topological structures, expanding TDA's applicability in satellite scene classification.

Abstract: Topological data analysis (TDA) is a relatively new field that is gaining
rapid adoption due to its robustness and ability to effectively describe
complex datasets by quantifying geometric information. In imaging contexts, TDA
typically models data as filtered cubical complexes from which we can extract
discriminative features using persistence homology. Meanwhile, convolutional
neural networks (CNNs) have been shown to be biased towards texture based local
features. To address this limitation, we propose a TDA feature engineering
pipeline and a simple method to integrate topological features with deep
learning models on remote sensing classification. Our method improves the
performance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving
99.33% accuracy, which surpasses all previously reported single-model
accuracies, including those with larger architectures, such as ResNet50 (2x
larger) and XL Vision Transformers (197x larger). We additionally show that our
method's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45
dataset. To our knowledge, this is the first application of TDA features in
satellite scene classification with deep learning. This demonstrates that TDA
features can be integrated with deep learning models, even on datasets without
explicit topological structures, thereby increasing the applicability of TDA. A
clean implementation of our method will be made publicly available upon
publication.

</details>


### [355] [Numerically Computing Galois Groups of Minimal Problems](https://arxiv.org/abs/2507.10407)
*Timothy Duff*

Main category: cs.CV

TL;DR: Analysis of solving parametric algebraic systems in algebra, numerical computation, and computer vision, focusing on RanSaC applications.


<details>
  <summary>Details</summary>
Motivation: The problem of solving parametric algebraic systems is relevant to ISSAC attendees and arises in robust model-fitting (RanSaC) in computer vision.

Method: Overview of recent work (last 5+ years) to measure the intrinsic difficulty of solving such systems and develop practical solutions.

Result: Progress in understanding and solving parametric algebraic systems for practical applications.

Conclusion: The talk highlights advancements in addressing the challenges of parametric systems in algebra and computer vision.

Abstract: I discuss a seemingly unlikely confluence of topics in algebra, numerical
computation, and computer vision. The motivating problem is that of solving
multiples instances of a parametric family of systems of algebraic (polynomial
or rational function) equations. No doubt already of interest to ISSAC
attendees, this problem arises in the context of robust model-fitting paradigms
currently utilized by the computer vision community (namely "Random Sampling
and Consensus", aka "RanSaC".) This talk will give an overview of work in the
last 5+ years that aspires to measure the intrinsic difficulty of solving such
parametric systems, and makes strides towards practical solutions.

</details>


### [356] [Text-Visual Semantic Constrained AI-Generated Image Quality Assessment](https://arxiv.org/abs/2507.10432)
*Qiang Li,Qingsen Yan,Haojian Huang,Peng Wu,Haokui Zhang,Yanning Zhang*

Main category: cs.CV

TL;DR: SC-AGIQA is a novel framework for assessing AI-generated image quality by addressing semantic misalignment and detail perception issues through text-visual semantic constraints.


<details>
  <summary>Details</summary>
Motivation: Existing methods for evaluating AI-generated images face challenges like semantic misalignment and missing details, necessitating a more robust solution.

Method: SC-AGIQA integrates text-visual semantic constraints with two core modules: TSAM for semantic alignment using MLLMs and FFDPM for fine-grained degradation perception via frequency-domain analysis.

Result: SC-AGIQA outperforms state-of-the-art methods on benchmark datasets.

Conclusion: The proposed framework effectively enhances the evaluation of AI-generated images by improving text-image consistency and perceptual distortion assessment.

Abstract: With the rapid advancements in Artificial Intelligence Generated Image (AGI)
technology, the accurate assessment of their quality has become an increasingly
vital requirement. Prevailing methods typically rely on cross-modal models like
CLIP or BLIP to evaluate text-image alignment and visual quality. However, when
applied to AGIs, these methods encounter two primary challenges: semantic
misalignment and details perception missing. To address these limitations, we
propose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment
(SC-AGIQA), a unified framework that leverages text-visual semantic constraints
to significantly enhance the comprehensive evaluation of both text-image
consistency and perceptual distortion in AI-generated images. Our approach
integrates key capabilities from multiple models and tackles the aforementioned
challenges by introducing two core modules: the Text-assisted Semantic
Alignment Module (TSAM), which leverages Multimodal Large Language Models
(MLLMs) to bridge the semantic gap by generating an image description and
comparing it against the original prompt for a refined consistency check, and
the Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which
draws inspiration from Human Visual System (HVS) properties by employing
frequency domain analysis combined with perceptual sensitivity weighting to
better quantify subtle visual distortions and enhance the capture of
fine-grained visual quality details in images. Extensive experiments conducted
on multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing
state-of-the-art methods. The code is publicly available at
https://github.com/mozhu1/SC-AGIQA.

</details>


### [357] [4D-Animal: Freely Reconstructing Animatable 3D Animals from Videos](https://arxiv.org/abs/2507.10437)
*Shanshan Zhong,Jiawei Peng,Zehan Zheng,Zhongzhan Huang,Wufei Ma,Guofeng Zhang,Qihao Liu,Alan Yuille,Jieneng Chen*

Main category: cs.CV

TL;DR: 4D-Animal reconstructs animatable 3D animals from videos without sparse keypoints, using a dense feature network and hierarchical alignment for accurate results.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on unreliable, labor-intensive keypoints; 4D-Animal eliminates this need.

Method: Uses a dense feature network to map 2D representations to SMAL parameters and a hierarchical alignment strategy integrating multiple cues.

Result: Outperforms baselines, producing high-quality 3D assets useful for other tasks.

Conclusion: 4D-Animal is efficient, stable, and scalable for large applications.

Abstract: Existing methods for reconstructing animatable 3D animals from videos
typically rely on sparse semantic keypoints to fit parametric models. However,
obtaining such keypoints is labor-intensive, and keypoint detectors trained on
limited animal data are often unreliable. To address this, we propose
4D-Animal, a novel framework that reconstructs animatable 3D animals from
videos without requiring sparse keypoint annotations. Our approach introduces a
dense feature network that maps 2D representations to SMAL parameters,
enhancing both the efficiency and stability of the fitting process.
Furthermore, we develop a hierarchical alignment strategy that integrates
silhouette, part-level, pixel-level, and temporal cues from pre-trained 2D
visual models to produce accurate and temporally coherent reconstructions
across frames. Extensive experiments demonstrate that 4D-Animal outperforms
both model-based and model-free baselines. Moreover, the high-quality 3D assets
generated by our method can benefit other 3D tasks, underscoring its potential
for large-scale applications. The code is released at
https://github.com/zhongshsh/4D-Animal.

</details>


### [358] [CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding](https://arxiv.org/abs/2507.10449)
*Hongyong Han,Wei Wang,Gaowei Zhang,Mingjie Li,Yi Wang*

Main category: cs.CV

TL;DR: CoralVQA is the first large-scale VQA dataset for coral reef analysis, addressing domain-specific challenges and enabling vision-language reasoning for conservation efforts.


<details>
  <summary>Details</summary>
Motivation: Coral reefs require monitoring, but interpreting images is challenging due to domain expertise needs. VQA can bridge this gap, but lacks dedicated datasets.

Method: Developed CoralVQA with 12,805 coral images and 277,653 QA pairs using a semi-automatic pipeline with marine biologists.

Result: Evaluated LVLMs, revealing limitations and opportunities for coral-specific vision-language reasoning.

Conclusion: CoralVQA provides a benchmark for future LVLM development, supporting coral conservation.

Abstract: Coral reefs are vital yet vulnerable ecosystems that require continuous
monitoring to support conservation. While coral reef images provide essential
information in coral monitoring, interpreting such images remains challenging
due to the need for domain expertise. Visual Question Answering (VQA), powered
by Large Vision-Language Models (LVLMs), has great potential in user-friendly
interaction with coral reef images. However, applying VQA to coral imagery
demands a dedicated dataset that addresses two key challenges: domain-specific
annotations and multidimensional questions. In this work, we introduce
CoralVQA, the first large-scale VQA dataset for coral reef analysis. It
contains 12,805 real-world coral images from 67 coral genera collected from 3
oceans, along with 277,653 question-answer pairs that comprehensively assess
ecological and health-related conditions. To construct this dataset, we develop
a semi-automatic data construction pipeline in collaboration with marine
biologists to ensure both scalability and professional-grade data quality.
CoralVQA presents novel challenges and provides a comprehensive benchmark for
studying vision-language reasoning in the context of coral reef images. By
evaluating several state-of-the-art LVLMs, we reveal key limitations and
opportunities. These insights form a foundation for future LVLM development,
with a particular emphasis on supporting coral conservation efforts.

</details>


### [359] [RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening](https://arxiv.org/abs/2507.10461)
*Tao Tang,Chengxu Yang*

Main category: cs.CV

TL;DR: RAPNet introduces content-adaptive convolution (RAPConv) and dynamic feature fusion (PAN-DFF) to improve pansharpening by addressing local content variations and balancing spatial-spectral fidelity.


<details>
  <summary>Details</summary>
Motivation: CNNs in pansharpening lack adaptability to local content variations, limiting precision in spatial detail extraction.

Method: RAPNet uses RAPConv for spatially adaptive kernels and PAN-DFF with attention for optimal spatial-spectral balance.

Result: RAPNet outperforms existing methods on public datasets, validated by metrics and qualitative assessments.

Conclusion: RAPNet's adaptive components effectively enhance pansharpening, offering superior performance and flexibility.

Abstract: Pansharpening refers to the process of integrating a high resolution
panchromatic (PAN) image with a lower resolution multispectral (MS) image to
generate a fused product, which is pivotal in remote sensing. Despite the
effectiveness of CNNs in addressing this challenge, they are inherently
constrained by the uniform application of convolutional kernels across all
spatial positions, overlooking local content variations. To overcome this
issue, we introduce RAPNet, a new architecture that leverages content-adaptive
convolution. At its core, RAPNet employs the Receptive-field Adaptive
Pansharpening Convolution (RAPConv), designed to produce spatially adaptive
kernels responsive to local feature context, thereby enhancing the precision of
spatial detail extraction. Additionally, the network integrates the
Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an
attention mechanism to achieve an optimal balance between spatial detail
enhancement and spectral fidelity. Comprehensive evaluations on publicly
available datasets confirm that RAPNet delivers superior performance compared
to existing approaches, as demonstrated by both quantitative metrics and
qualitative assessments. Ablation analyses further substantiate the
effectiveness of the proposed adaptive components.

</details>


### [360] [RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction](https://arxiv.org/abs/2507.10470)
*Zhicun Yin,Junjie Chen,Ming Liu,Zhixin Wang,Fan Li,Renjing Pei,Xiaoming Li,Rynson W. H. Lau,Wangmeng Zuo*

Main category: cs.CV

TL;DR: RefSTAR introduces a novel blind facial image restoration method using reference images, focusing on selection, transfer, and reconstruction to improve identity preservation.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with identity preservation due to improper feature introduction on detailed textures.

Method: Proposes RefSTAR with modules for reference selection (RefSel), feature fusion, and reconstruction, using a new dataset (RefSel-HQ) and redesigned cycle consistency loss.

Result: Superior performance in identity preservation and reference feature transfer across various backbone models.

Conclusion: RefSTAR effectively addresses blind facial restoration challenges, with code and models available for further use.

Abstract: Blind facial image restoration is highly challenging due to unknown complex
degradations and the sensitivity of humans to faces. Although existing methods
introduce auxiliary information from generative priors or high-quality
reference images, they still struggle with identity preservation problems,
mainly due to improper feature introduction on detailed textures. In this
paper, we focus on effectively incorporating appropriate features from
high-quality reference images, presenting a novel blind facial image
restoration method that considers reference selection, transfer, and
reconstruction (RefSTAR). In terms of selection, we construct a reference
selection (RefSel) module. For training the RefSel module, we construct a
RefSel-HQ dataset through a mask generation pipeline, which contains annotating
masks for 10,000 ground truth-reference pairs. As for the transfer, due to the
trivial solution in vanilla cross-attention operations, a feature fusion
paradigm is designed to force the features from the reference to be integrated.
Finally, we propose a reference image reconstruction mechanism that further
ensures the presence of reference image features in the output image. The cycle
consistency loss is also redesigned in conjunction with the mask. Extensive
experiments on various backbone models demonstrate superior performance,
showing better identity preservation ability and reference feature transfer
quality. Source code, dataset, and pre-trained models are available at
https://github.com/yinzhicun/RefSTAR.

</details>


### [361] [GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space](https://arxiv.org/abs/2507.10473)
*David G. Shatwell,Ishan Rajendrakumar Dave,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: GT-Loc is a retrieval-based method that jointly predicts image capture time (hour/month) and geo-location (GPS) using aligned embeddings in a shared feature space, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Timestamp prediction and geo-localization are interdependent due to shared visual cues like brightness and shadows. Addressing this link improves accuracy in both tasks.

Method: GT-Loc uses separate encoders for images, time, and location, aligning embeddings in a shared space. It employs a temporal metric-learning objective for cyclical time modeling.

Result: GT-Loc surpasses previous time prediction methods and achieves competitive geo-localization results. The unified embedding space aids in compositional and text-based retrieval.

Conclusion: Jointly optimizing timestamp prediction and geo-localization enhances accuracy and enables versatile applications like retrieval and forensics.

Abstract: Timestamp prediction aims to determine when an image was captured using only
visual information, supporting applications such as metadata correction,
retrieval, and digital forensics. In outdoor scenarios, hourly estimates rely
on cues like brightness, hue, and shadow positioning, while seasonal changes
and weather inform date estimation. However, these visual cues significantly
depend on geographic context, closely linking timestamp prediction to
geo-localization. To address this interdependence, we introduce GT-Loc, a novel
retrieval-based method that jointly predicts the capture time (hour and month)
and geo-location (GPS coordinates) of an image. Our approach employs separate
encoders for images, time, and location, aligning their embeddings within a
shared high-dimensional feature space. Recognizing the cyclical nature of time,
instead of conventional contrastive learning with hard positives and negatives,
we propose a temporal metric-learning objective providing soft targets by
modeling pairwise time differences over a cyclical toroidal surface. We present
new benchmarks demonstrating that our joint optimization surpasses previous
time prediction methods, even those using the ground-truth geo-location as an
input during inference. Additionally, our approach achieves competitive results
on standard geo-localization tasks, and the unified embedding space facilitates
compositional and text-based image retrieval.

</details>


### [362] [Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation](https://arxiv.org/abs/2507.10474)
*Seyed Alireza Rahimi Azghadi,Truong-Thanh-Hung Nguyen,Helene Fournier,Monica Wachowicz,Rene Richard,Francis Palma,Hung Cao*

Main category: cs.CV

TL;DR: A framework combining semi-supervised federated learning, indoor localization, and vision-based systems achieves 99.99% accuracy for fall detection in older adults while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: The aging population faces high risks of falls, and timely detection can reduce medical costs and recovery time. Current systems must balance effectiveness, reliability, and privacy.

Method: The framework includes SF2D (semi-supervised federated learning), indoor localization/navigation, and vision-based detection. Wearable and edge devices identify falls, localize them, and use a robot-mounted camera for verification.

Result: SF2D achieves 99.19% accuracy, vision-based detection 96.3%, and navigation 95% success. Combined, the framework reaches 99.99% accuracy.

Conclusion: The proposed framework is highly reliable, safe, and privacy-preserving for fall detection in older adults.

Abstract: The aging population is growing rapidly, and so is the danger of falls in
older adults. A major cause of injury is falling, and detection in time can
greatly save medical expenses and recovery time. However, to provide timely
intervention and avoid unnecessary alarms, detection systems must be effective
and reliable while addressing privacy concerns regarding the user. In this
work, we propose a framework for detecting falls using several complementary
systems: a semi-supervised federated learning-based fall detection system
(SF2D), an indoor localization and navigation system, and a vision-based human
fall recognition system. A wearable device and an edge device identify a fall
scenario in the first system. On top of that, the second system uses an indoor
localization technique first to localize the fall location and then navigate a
robot to inspect the scenario. A vision-based detection system running on an
edge device with a mounted camera on a robot is used to recognize fallen
people. Each of the systems of this proposed framework achieves different
accuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to
99.19% accuracy, while the vision-based fallen people detection achieves 96.3%
accuracy. However, when we combine the accuracy of these two systems with the
accuracy of the navigation system (95% success rate), our proposed framework
creates a highly reliable performance for fall detection, with an overall
accuracy of 99.99%. Not only is the proposed framework safe for older adults,
but it is also a privacy-preserving solution for detecting falls.

</details>


### [363] [The Power of Certainty: How Confident Models Lead to Better Segmentation](https://arxiv.org/abs/2507.10490)
*Tugberk Erol,Tuba Caglikantar,Duygu Sarikaya*

Main category: cs.CV

TL;DR: A confidence-based self-distillation method is proposed for polyp segmentation, reducing resource usage while outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for polyp segmentation are over-parameterized, prone to overfitting, and resource-intensive.

Method: A confidence-based self-distillation approach using dynamic confidence coefficients and previous iteration data storage.

Result: Outperforms state-of-the-art models and generalizes well across diverse datasets.

Conclusion: The proposed method is efficient, effective, and resource-friendly for polyp segmentation.

Abstract: Deep learning models have been proposed for automatic polyp detection and
precise segmentation of polyps during colonoscopy procedures. Although these
state-of-the-art models achieve high performance, they often require a large
number of parameters. Their complexity can make them prone to overfitting,
particularly when trained on biased datasets, and can result in poor
generalization across diverse datasets. Knowledge distillation and
self-distillation are proposed as promising strategies to mitigate the
limitations of large, over-parameterized models. These approaches, however, are
resource-intensive, often requiring multiple models and significant memory
during training. We propose a confidence-based self-distillation approach that
outperforms state-of-the-art models by utilizing only previous iteration data
storage during training, without requiring extra computation or memory usage
during testing. Our approach calculates the loss between the previous and
current iterations within a batch using a dynamic confidence coefficient. To
evaluate the effectiveness of our approach, we conduct comprehensive
experiments on the task of polyp segmentation. Our approach outperforms
state-of-the-art models and generalizes well across datasets collected from
multiple clinical centers. The code will be released to the public once the
paper is accepted.

</details>


### [364] [BenchReAD: A systematic benchmark for retinal anomaly detection](https://arxiv.org/abs/2507.10492)
*Chenyu Lian,Hong-Yu Zhou,Zhanli Hu,Jing Qin*

Main category: cs.CV

TL;DR: The paper introduces a comprehensive benchmark for retinal anomaly detection, addressing gaps in existing methods and proposing a new approach (NFM-DRA) that improves performance.


<details>
  <summary>Details</summary>
Motivation: Current retinal anomaly detection lacks a public benchmark, leading to limited anomaly types, saturated test sets, and poor generalization. Existing methods also overlook labeled abnormal and unlabeled data.

Method: The paper proposes a benchmark and evaluates methods, finding a fully supervised approach (DRA) works best but struggles with unseen anomalies. NFM-DRA combines DRA with a Normal Feature Memory to improve performance.

Result: NFM-DRA achieves state-of-the-art performance by mitigating performance drops on unseen anomalies.

Conclusion: The introduced benchmark and NFM-DRA advance retinal anomaly detection, offering a systematic evaluation framework and improved methodology.

Abstract: Retinal anomaly detection plays a pivotal role in screening ocular and
systemic diseases. Despite its significance, progress in the field has been
hindered by the absence of a comprehensive and publicly available benchmark,
which is essential for the fair evaluation and advancement of methodologies.
Due to this limitation, previous anomaly detection work related to retinal
images has been constrained by (1) a limited and overly simplistic set of
anomaly types, (2) test sets that are nearly saturated, and (3) a lack of
generalization evaluation, resulting in less convincing experimental setups.
Furthermore, existing benchmarks in medical anomaly detection predominantly
focus on one-class supervised approaches (training only with negative samples),
overlooking the vast amounts of labeled abnormal data and unlabeled data that
are commonly available in clinical practice. To bridge these gaps, we introduce
a benchmark for retinal anomaly detection, which is comprehensive and
systematic in terms of data and algorithm. Through categorizing and
benchmarking previous methods, we find that a fully supervised approach
leveraging disentangled representations of abnormalities (DRA) achieves the
best performance but suffers from significant drops in performance when
encountering certain unseen anomalies. Inspired by the memory bank mechanisms
in one-class supervised learning, we propose NFM-DRA, which integrates DRA with
a Normal Feature Memory to mitigate the performance degradation, establishing a
new SOTA. The benchmark is publicly available at
https://github.com/DopamineLcy/BenchReAD.

</details>


### [365] [Cameras as Relative Positional Encoding](https://arxiv.org/abs/2507.10496)
*Ruilong Li,Brent Yi,Junchen Liu,Hang Gao,Yi Ma,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: The paper compares techniques for conditioning transformers on camera geometry in multi-view computer vision tasks, proposing a new method (PRoPE) that improves performance across various settings and tasks.


<details>
  <summary>Details</summary>
Motivation: To leverage geometric relationships between viewpoints for better 3D perception in multi-view transformers.

Method: Comparison of token-level raymap encodings, attention-level relative pose encodings, and the proposed PRoPE (Projective Positional Encoding) for camera conditioning.

Result: PRoPE improves performance in novel view synthesis, stereo depth estimation, and discriminative spatial cognition, even with out-of-distribution inputs.

Conclusion: Relative camera conditioning, especially with PRoPE, enhances transformer performance in multi-view tasks, generalizing well across settings and model sizes.

Abstract: Transformers are increasingly prevalent for multi-view computer vision tasks,
where geometric relationships between viewpoints are critical for 3D
perception. To leverage these relationships, multi-view transformers must use
camera geometry to ground visual tokens in 3D space. In this work, we compare
techniques for conditioning transformers on cameras: token-level raymap
encodings, attention-level relative pose encodings, and a new relative encoding
we propose -- Projective Positional Encoding (PRoPE) -- that captures complete
camera frustums, both intrinsics and extrinsics, as a relative positional
encoding. Our experiments begin by showing how relative camera conditioning
improves performance in feedforward novel view synthesis, with further gains
from PRoPE. This holds across settings: scenes with both shared and varying
intrinsics, when combining token- and attention-level conditioning, and for
generalization to inputs with out-of-distribution sequence lengths and camera
intrinsics. We then verify that these benefits persist for different tasks,
stereo depth estimation and discriminative spatial cognition, as well as larger
model sizes.

</details>


### [366] [National level satellite-based crop field inventories in smallholder landscapes](https://arxiv.org/abs/2507.10499)
*Philippe Rufin,Pauline Lucie Hammer,Leon-Friedrich Thomas,Sá Nogueira Lisboa,Natasha Ribeiro,Almeida Sitoe,Patrick Hostert,Patrick Meyfroidt*

Main category: cs.CV

TL;DR: The paper presents a method using high-resolution Earth observation data and deep transfer learning to map 21 million crop fields in Mozambique, achieving high accuracy and revealing insights into smallholder agriculture and field size variations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding of smallholder agriculture systems, particularly spatial distribution and field size, for better science-based policies.

Method: Integration of very high-resolution (1.5 m) Earth observation data with deep transfer learning to delineate crop fields at a national scale with minimal reference data.

Result: Produced a national dataset for Mozambique with 93% accuracy, median IoU of 0.81, and identified fragmented rural regions and field size variations.

Conclusion: Field size is a critical indicator for socio-economic and environmental outcomes in agriculture, highlighting its importance for policy and sustainability.

Abstract: The design of science-based policies to improve the sustainability of
smallholder agriculture is challenged by a limited understanding of fundamental
system properties, such as the spatial distribution of active cropland and
field size. We integrate very high spatial resolution (1.5 m) Earth observation
data and deep transfer learning to derive crop field delineations in complex
agricultural systems at the national scale, while maintaining minimum reference
data requirements and enhancing transferability. We provide the first
national-level dataset of 21 million individual fields for Mozambique (covering
~800,000 km2) for 2023. Our maps separate active cropland from non-agricultural
land use with an overall accuracy of 93% and balanced omission and commission
errors. Field-level spatial agreement reached median intersection over union
(IoU) scores of 0.81, advancing the state-of-the-art in large-area field
delineation in complex smallholder systems. The active cropland maps capture
fragmented rural regions with low cropland shares not yet identified in global
land cover or cropland maps. These regions are mostly located in agricultural
frontier regions which host 7-9% of the Mozambican population. Field size in
Mozambique is very low overall, with half of the fields being smaller than 0.16
ha, and 83% smaller than 0.5 ha. Mean field size at aggregate spatial
resolution (0.05{\deg}) is 0.32 ha, but it varies strongly across gradients of
accessibility, population density, and net forest cover change. This variation
reflects a diverse set of actors, ranging from semi-subsistence smallholder
farms to medium-scale commercial farming, and large-scale farming operations.
Our results highlight that field size is a key indicator relating to
socio-economic and environmental outcomes of agriculture (e.g., food
production, livelihoods, deforestation, biodiversity), as well as their
trade-offs.

</details>


### [367] [Quantize-then-Rectify: Efficient VQ-VAE Training](https://arxiv.org/abs/2507.10547)
*Borui Zhang,Qihang Rao,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: ReVQ transforms pre-trained VAEs into VQ-VAEs efficiently, reducing training costs by over 100x while maintaining competitive image reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: Training high-compression-rate VQ-VAEs is computationally expensive. ReVQ aims to leverage pre-trained VAEs to minimize this cost.

Method: ReVQ uses channel multi-group quantization to expand codebook capacity and a post rectifier to reduce quantization errors, enabling efficient VQ-VAE training.

Result: ReVQ compresses ImageNet images into 512 tokens with rFID = 1.06 and reduces training time to 22 hours on a single GPU, compared to 4.5 days on 32 GPUs for other methods.

Conclusion: ReVQ offers a highly efficient and effective solution for VQ-VAE training, balancing computational cost and reconstruction quality.

Abstract: Visual tokenizers are pivotal in multimodal large models, acting as bridges
between continuous inputs and discrete tokens. Nevertheless, training
high-compression-rate VQ-VAEs remains computationally demanding, often
necessitating thousands of GPU hours. This work demonstrates that a pre-trained
VAE can be efficiently transformed into a VQ-VAE by controlling quantization
noise within the VAE's tolerance threshold. We present
\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs
to enable rapid VQ-VAE training with minimal computational overhead. By
integrating \textbf{channel multi-group quantization} to enlarge codebook
capacity and a \textbf{post rectifier} to mitigate quantization errors, ReVQ
compresses ImageNet images into at most 512 tokens while sustaining competitive
reconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training
costs by over two orders of magnitude relative to state-of-the-art approaches:
ReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours,
whereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental
results show that ReVQ achieves superior efficiency-reconstruction trade-offs.

</details>


### [368] [Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder](https://arxiv.org/abs/2507.10552)
*Vladimir Iashin,Horace Lee,Dan Schofield,Andrew Zisserman*

Main category: cs.CV

TL;DR: A self-supervised method using DINOv2 trains Vision Transformers on unlabeled chimpanzee face crops, outperforming supervised baselines in re-identification without labeled data.


<details>
  <summary>Details</summary>
Motivation: Manual identification of individual animals from camera-trap footage is time-consuming, prompting the need for automated, scalable solutions.

Method: The study employs the DINOv2 framework to train Vision Transformers on automatically mined chimpanzee face crops, avoiding the need for labeled data.

Result: The method achieves strong open-set re-identification performance, surpassing supervised benchmarks on datasets like Bossou.

Conclusion: Self-supervised learning shows promise for scalable, non-invasive wildlife monitoring and population studies.

Abstract: Camera traps are revolutionising wildlife monitoring by capturing vast
amounts of visual data; however, the manual identification of individual
animals remains a significant bottleneck. This study introduces a fully
self-supervised approach to learning robust chimpanzee face embeddings from
unlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision
Transformers on automatically mined face crops, eliminating the need for
identity labels. Our method demonstrates strong open-set re-identification
performance, surpassing supervised baselines on challenging benchmarks such as
Bossou, despite utilising no labelled data during training. This work
underscores the potential of self-supervised learning in biodiversity
monitoring and paves the way for scalable, non-invasive population studies.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [369] [Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale](https://arxiv.org/abs/2507.08865)
*Javis AI Team,Amrendra Singh,Maulik Shah,Dharshan Sampath*

Main category: cs.CL

TL;DR: Spatial ModernBERT, a transformer model with spatial embeddings, is introduced for extracting tables and key-value pairs from financial documents, achieving high accuracy through token classification and post-processing.


<details>
  <summary>Details</summary>
Motivation: Extracting structured data from financial documents is crucial for business workflows like auditing and automated invoice processing.

Method: The model uses token classification across three heads (Label, Column, Row) and is pretrained on PubTables-1M, then fine-tuned on financial documents. Post-processing merges tokens and reconstructs layouts.

Result: Empirical evaluation shows the model effectively leverages textual and spatial cues for accurate extraction.

Conclusion: Spatial ModernBERT is robust for table and key-value extraction in financial documents.

Abstract: Extracting tables and key-value pairs from financial documents is essential
for business workflows such as auditing, data analytics, and automated invoice
processing. In this work, we introduce Spatial ModernBERT-a transformer-based
model augmented with spatial embeddings-to accurately detect and extract
tabular data and key-value fields from complex financial documents. We cast the
extraction task as token classification across three heads: (1) Label Head,
classifying each token as a label (e.g., PO Number, PO Date, Item Description,
Quantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;
(3) Row Head, distinguishing the start of item rows and header rows. The model
is pretrained on the PubTables-1M dataset, then fine-tuned on a financial
document dataset, achieving robust performance through cross-entropy loss on
each classification head. We propose a post-processing method to merge tokens
using B-I-IB tagging, reconstruct the tabular layout, and extract key-value
pairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages
both textual and spatial cues, facilitating highly accurate table and key-value
extraction in real-world financial documents.

</details>


### [370] [SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems](https://arxiv.org/abs/2507.08898)
*Wenliang Shan,Michael Fu,Rui Yang,Chakkrit,Tantithamthavorn*

Main category: cs.CL

TL;DR: SEALGuard is a multilingual guardrail for LLM systems, improving safety alignment across diverse languages by outperforming existing solutions like LlamaGuard.


<details>
  <summary>Details</summary>
Motivation: Existing guardrails like LlamaGuard struggle with multilingual unsafe inputs, leaving LLM systems vulnerable to prompts in low-resource languages.

Method: Adapts a multilingual language model using LoRA, constructs SEALSBench (260k+ prompts in 10 languages), and evaluates against state-of-the-art guardrails.

Result: SEALGuard improves Defense Success Rate by 48% over LlamaGuard, achieving top DSR, precision, and F1-score.

Conclusion: SEALGuard effectively addresses multilingual safety alignment gaps, enhancing LLM system safety.

Abstract: Safety alignment is critical for LLM-powered systems. While recent
LLM-powered guardrail approaches such as LlamaGuard achieve high detection
accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),
they struggle with multilingual unsafe inputs. This limitation leaves LLM
systems vulnerable to unsafe and jailbreak prompts written in low-resource
languages such as those in Southeast Asia. This paper introduces SEALGuard, a
multilingual guardrail designed to improve the safety alignment across diverse
languages. It aims to address the multilingual safety alignment gap of existing
guardrails and ensure effective filtering of unsafe and jailbreak prompts in
LLM-powered systems. We adapt a general-purpose multilingual language model
into a multilingual guardrail using low-rank adaptation (LoRA). We construct
SEALSBench, a large-scale multilingual safety alignment dataset containing over
260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.
We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on
this benchmark. Our findings show that multilingual unsafe and jailbreak
prompts substantially degrade the performance of the state-of-the-art
LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and
18%, respectively, compared to its performance on English-only prompts. In
contrast, SEALGuard outperforms existing guardrails in detecting multilingual
unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and
achieving the best DSR, precision, and F1-score. Our ablation study further
reveals the contributions of adaptation strategies and model size to the
overall performance of SEALGuard. SEALGuard advances the safety alignment of
LLM systems by introducing an effective multilingual guardrail.

</details>


### [371] [Evaluating LLMs in Medicine: A Call for Rigor, Transparency](https://arxiv.org/abs/2507.08916)
*Mahmoud Alwakeel,Aditya Nagori,Vijay Krishnamoorthy,Rishikesan Kamaleswaran*

Main category: cs.CL

TL;DR: The paper critiques the limitations of current datasets for evaluating LLMs in medical QA, advocating for standardized, rigorous, and clinically relevant frameworks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of clinical realism and transparency in datasets used to evaluate LLMs in medical question answering.

Method: Reviewed benchmark datasets (MedQA, MedMCQA, PubMedQA, MMLU) and alternative challenge questions for rigor, transparency, and clinical relevance.

Result: Existing datasets lack clinical realism and validation; challenge questions are limited in size and scope. Need for secure, representative datasets.

Conclusion: A standardized framework and collaboration among institutions are essential for unbiased, rigorous evaluation of LLMs in medicine.

Abstract: Objectives: To evaluate the current limitations of large language models
(LLMs) in medical question answering, focusing on the quality of datasets used
for their evaluation. Materials and Methods: Widely-used benchmark datasets,
including MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,
transparency, and relevance to clinical scenarios. Alternatives, such as
challenge questions in medical journals, were also analyzed to identify their
potential as unbiased evaluation tools. Results: Most existing datasets lack
clinical realism, transparency, and robust validation processes. Publicly
available challenge questions offer some benefits but are limited by their
small size, narrow scope, and exposure to LLM training. These gaps highlight
the need for secure, comprehensive, and representative datasets. Conclusion: A
standardized framework is critical for evaluating LLMs in medicine.
Collaborative efforts among institutions and policymakers are needed to ensure
datasets and methodologies are rigorous, unbiased, and reflective of clinical
complexities.

</details>


### [372] [From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation](https://arxiv.org/abs/2507.08924)
*Seokhee Hong,Sunkyoung Kim,Guijin Son,Soyeon Kim,Yeonjung Hong,Jinsik Lee*

Main category: cs.CL

TL;DR: Introduces two Korean expert-level benchmarks, KMMLU-Redux and KMMLU-Pro, for evaluating LLMs in real-world industrial and professional contexts.


<details>
  <summary>Details</summary>
Motivation: To provide robust benchmarks for LLMs that cover both academic and industrial domains, ensuring applicability in real-world scenarios.

Method: Developed KMMLU-Redux by refining the existing KMMLU (removing errors) and created KMMLU-Pro based on Korean National Professional Licensure exams.

Result: The benchmarks effectively represent industrial knowledge in Korea, as demonstrated by experiments.

Conclusion: The benchmarks are publicly released to aid in evaluating LLMs for real-world applicability in Korea.

Abstract: The development of Large Language Models (LLMs) requires robust benchmarks
that encompass not only academic domains but also industrial fields to
effectively evaluate their applicability in real-world scenarios. In this
paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,
reconstructed from the existing KMMLU, consists of questions from the Korean
National Technical Qualification exams, with critical errors removed to enhance
reliability. KMMLU-Pro is based on Korean National Professional Licensure exams
to reflect professional knowledge in Korea. Our experiments demonstrate that
these benchmarks comprehensively represent industrial knowledge in Korea. We
release our dataset publicly available.

</details>


### [373] [Self-Improving Model Steering](https://arxiv.org/abs/2507.08967)
*Rongyi Zhu,Yuhui Wang,Tanqiu Jiang,Jiacheng Liang,Ting Wang*

Main category: cs.CL

TL;DR: SIMS is a self-improving model-steering framework for LLMs that operates without external supervision, outperforming conventional methods in effectiveness and adaptability.


<details>
  <summary>Details</summary>
Motivation: Conventional model-steering methods depend on externally annotated data, limiting adaptability and effectiveness. SIMS aims to overcome these limitations by self-improving without external supervision.

Method: SIMS autonomously generates and refines contrastive samples through iterative self-improvement cycles, using strategies like prompt ranking and contrast sampling.

Result: SIMS outperforms existing methods in steering effectiveness and adaptability across diverse LLMs and benchmarks.

Conclusion: Self-improving model steering, as demonstrated by SIMS, is a promising direction for future research on inference-time LLM alignment.

Abstract: Model steering represents a powerful technique that dynamically aligns large
language models (LLMs) with human preferences during inference. However,
conventional model-steering methods rely heavily on externally annotated data,
not only limiting their adaptability to varying contexts but also tethering
their effectiveness to annotation quality. In this paper, we present SIMS, the
first self-improving model-steering framework that operates without relying on
external supervision. At its core, SIMS autonomously generates and refines
contrastive samples through iterative self-improvement cycles, enabling
adaptive, context-specific steering. Additionally, SIMS employs novel
strategies, including prompt ranking and contrast sampling, to further enhance
steering efficacy. Extensive evaluation across diverse LLMs and benchmarks
demonstrates that SIMS substantially outperforms existing methods in steering
effectiveness and adaptability, highlighting self-improving model steering as a
promising direction for future research on inference-time LLM alignment.

</details>


### [374] [Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR](https://arxiv.org/abs/2507.08969)
*Drew Walker,Jennifer Love,Swati Rajwal,Isabel C Walker,Hannah LF Cooper,Abeed Sarker,Melvin Livingston III*

Main category: cs.CL

TL;DR: The paper examines stigmatizing language in EHRs, finding higher rates among marginalized groups and certain healthcare providers.


<details>
  <summary>Details</summary>
Motivation: To identify and quantify stigmatizing language in EHRs and its association with patient demographics and provider types.

Method: Used lexicon matching and supervised learning to analyze MIMIC-III EHR data, with Poisson regression to assess predictors.

Result: Higher stigmatizing labels for Black/African American patients, those with government-run insurance, and certain conditions. Nurses and social workers used more stigmatizing language.

Conclusion: Stigmatizing language in EHRs disproportionately affects marginalized groups and varies by provider type, highlighting a need for intervention.

Abstract: Introduction: Electronic health records (EHR) are a critical medium through
which patient stigmatization is perpetuated among healthcare teams. Methods: We
identified linguistic features of doubt markers and stigmatizing labels in
MIMIC-III EHR via expanded lexicon matching and supervised learning
classifiers. Predictors of rates of linguistic features were assessed using
Poisson regression models. Results: We found higher rates of stigmatizing
labels per chart among patients who were Black or African American (RR: 1.16),
patients with Medicare/Medicaid or government-run insurance (RR: 2.46),
self-pay (RR: 2.12), and patients with a variety of stigmatizing disease and
mental health conditions. Patterns among doubt markers were similar, though
male patients had higher rates of doubt markers (RR: 1.25). We found increased
stigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),
with similar patterns of doubt markers. Discussion: Stigmatizing language
occurred at higher rates among historically stigmatized patients, perpetuated
by multiple provider types.

</details>


### [375] [Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery](https://arxiv.org/abs/2507.09011)
*Ana Chkhaidze,Reshanne R. Reeder,Connor Gag,Anastasia Kiyonaga,Seana Coulson*

Main category: cs.CL

TL;DR: Ganzflicker-induced hallucinations vary by imagery phenotype: strong imagers report complex content, weak imagers report simple patterns. Vision-language models better capture these differences than text-only models.


<details>
  <summary>Details</summary>
Motivation: To explore how differences in visual imagery (absent, typical, vivid) impact the complexity of internally generated visual experiences during Ganzflicker-induced hallucinations.

Method: Analyzed free-text descriptions of hallucinations from over 4,000 participants using natural language processing tools, comparing vision-language and text-only models.

Result: Strong imagers described complex, naturalistic content; weak imagers reported simple geometric patterns. Vision-language models outperformed text-only models in capturing these differences.

Conclusion: Individual variation in coordination between early visual and higher-order regions may explain differences in hallucination complexity across the imagery spectrum.

Abstract: A rapidly alternating red and black display known as Ganzflicker induces
visual hallucinations that reflect the generative capacity of the visual
system. Recent proposals regarding the imagery spectrum, that is, differences
in the visual system of individuals with absent imagery, typical imagery, and
vivid imagery, suggest these differences should impact the complexity of other
internally generated visual experiences. Here, we used tools from natural
language processing to analyze free-text descriptions of hallucinations from
over 4,000 participants, asking whether people with different imagery
phenotypes see different things in their mind's eye during Ganzflicker-induced
hallucinations. Strong imagers described complex, naturalistic content, while
weak imagers reported simple geometric patterns. Embeddings from vision
language models better captured these differences than text-only language
models, and participants with stronger imagery used language with richer
sensorimotor associations. These findings may reflect individual variation in
coordination between early visual areas and higher-order regions relevant for
the imagery spectrum.

</details>


### [376] [Lizard: An Efficient Linearization Framework for Large Language Models](https://arxiv.org/abs/2507.09025)
*Chien Van Nguyen,Ruiyi Zhang,Hanieh Deilamsalehy,Puneet Mathur,Viet Dac Lai,Haoliang Wang,Jayakumar Subramanian,Ryan A. Rossi,Trung Bui,Nikos Vlassis,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: Lizard is a framework that linearizes Transformer-based LLMs into subquadratic architectures for infinite-context generation, combining gated linear attention and sliding window attention for efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Transformer-based LLMs face memory and computational bottlenecks with increasing context lengths due to quadratic complexity in attention and KV cache growth.

Method: Lizard introduces a subquadratic attention mechanism with gating for adaptive memory control, hybrid global-local context capture, and a hardware-aware training algorithm.

Result: Lizard achieves near-lossless performance recovery of teacher models, outperforms prior linearization methods by 18 points on MMLU, and excels in associative recall tasks.

Conclusion: Lizard offers a flexible, efficient solution for infinite-context generation, balancing performance and computational efficiency.

Abstract: We propose Lizard, a linearization framework that transforms pretrained
Transformer-based Large Language Models (LLMs) into flexible, subquadratic
architectures for infinite-context generation. Transformer-based LLMs face
significant memory and computational bottlenecks as context lengths increase,
due to the quadratic complexity of softmax attention and the growing key-value
(KV) cache. Lizard addresses these limitations by introducing a subquadratic
attention mechanism that closely approximates softmax attention while
preserving the output quality. Unlike previous linearization methods, which are
often limited by fixed model structures and therefore exclude gating
mechanisms, Lizard incorporates a gating module inspired by recent
state-of-the-art linear models. This enables adaptive memory control, supports
constant-memory inference, offers strong length generalization, and allows more
flexible model design. Lizard combines gated linear attention for global
context compression with sliding window attention enhanced by meta memory,
forming a hybrid mechanism that captures both long-range dependencies and
fine-grained local interactions. Moreover, we introduce a hardware-aware
algorithm that accelerates the training speed of our models. Extensive
experiments show that Lizard achieves near-lossless recovery of the teacher
model's performance across standard language modeling tasks, while
significantly outperforming previous linearization methods. On the 5-shot MMLU
benchmark, Lizard improves over prior models by 18 points and shows significant
improvements on associative recall tasks.

</details>


### [377] [ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making](https://arxiv.org/abs/2507.09037)
*Bharadwaj Ravichandran,David Joy,Paul Elliott,Brian Hu,Jadie Adams,Christopher Funk,Emily Veenhuis,Anthony Hoogs,Arslan Basharat*

Main category: cs.CL

TL;DR: The paper introduces ALIGN, a system for dynamic personalization of LLM-based decision-makers through prompt-based alignment to fine-grained attributes, enabling qualitative and quantitative comparisons.


<details>
  <summary>Details</summary>
Motivation: Users have diverse values and preferences affecting decision-making, requiring novel LLM alignment and personalization methods beyond existing benchmarking tools.

Method: ALIGN system features robust configuration management, structured output generation, swappable LLM backbones, and a modular backend for algorithm integration.

Result: Quantitative analysis compares alignment approaches in public opinion surveys and medical triage, with the framework being open-source.

Conclusion: ALIGN enables research on reliable, responsible, and personalized LLM-based decision-makers.

Abstract: Large language models (LLMs) are increasingly being used as decision aids.
However, users have diverse values and preferences that can affect their
decision-making, which requires novel methods for LLM alignment and
personalization. Existing LLM comparison tools largely focus on benchmarking
tasks, such as knowledge-based question answering. In contrast, our proposed
ALIGN system focuses on dynamic personalization of LLM-based decision-makers
through prompt-based alignment to a set of fine-grained attributes. Key
features of our system include robust configuration management, structured
output generation with reasoning, and several algorithm implementations with
swappable LLM backbones, enabling different types of analyses. Our user
interface enables a qualitative, side-by-side comparison of LLMs and their
alignment to various attributes, with a modular backend for easy algorithm
integration. Additionally, we perform a quantitative analysis comparing
alignment approaches in two different domains: demographic alignment for public
opinion surveys and value alignment for medical triage decision-making. The
entire ALIGN framework is open source and will enable new research on reliable,
responsible, and personalized LLM-based decision-makers.

</details>


### [378] [OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique](https://arxiv.org/abs/2507.09075)
*Wasi Uddin Ahmad,Somshubra Majumdar,Aleksander Ficek,Sean Narenthiran,Mehrzad Samadi,Jocelyn Huang,Siddhartha Jain,Vahid Noroozi,Boris Ginsburg*

Main category: cs.CL

TL;DR: OpenCodeReasoning-II introduces a 2.5M question-solution-critique dataset for code reasoning, employing a two-stage fine-tuning strategy to enhance LLMs in code generation and critique, achieving competitive performance.


<details>
  <summary>Details</summary>
Motivation: The need for large-scale, high-quality datasets to advance reasoning-based LLMs in code generation and critique.

Method: A two-stage supervised fine-tuning strategy: first for code generation, then joint training for generation and critique.

Result: The fine-tuned Qwen2.5-Instruct models match or exceed prior open-weight models in code generation, with critique integration improving competitive coding performance.

Conclusion: OpenCodeReasoning-II and the extended LiveCodeBench benchmark significantly contribute to LLM evaluation and performance in code reasoning.

Abstract: Recent advancements in reasoning-based Large Language Models (LLMs),
particularly their potential through test-time scaling, have created
significant opportunities for distillation in code generation and critique.
However, progress in both areas fundamentally depends on large-scale,
high-quality datasets. In this work, we introduce OpenCodeReasoning-II, a
dataset consists of 2.5M question-solution-critique triples (approx. 35K unique
programming questions), making it nearly twice the size of the previous largest
publicly available code reasoning dataset. In this work, we employ a two-stage
supervised fine-tuning strategy. The first stage focuses on fine-tuning for
code generation, while the second stage involves the joint training of models
for both code generation and critique. Our resulting finetuned Qwen2.5-Instruct
models achieve performance in code generation that either exceeds or equals the
best prior open-weight distilled models. Notably, the integration of our code
generation and critique models leads to significant improvements in competitive
coding performance. Furthermore, we present an extension of the LiveCodeBench
benchmark to specifically support the C++ programming language, thereby
facilitating more comprehensive LLM evaluation using this benchmark.

</details>


### [379] [Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation](https://arxiv.org/abs/2507.09076)
*Jialong Mai,Xiaofen Xing,Yawei Li,Zhipeng Li,Jingyuan Xing,Xiangmin Xu*

Main category: cs.CL

TL;DR: The paper introduces a Dynamic Parameter Memory (DPM) mechanism to enhance speech emotion recognition (SER) in SLLMs by addressing context window limitations and preserving emotion continuity.


<details>
  <summary>Details</summary>
Motivation: Current SLLMs struggle with long audio sequences due to high frame rates and token compression methods that ignore emotion continuity.

Method: Proposes DPM, which encodes sentence-level context and emotions into a temporary LoRA module during inference, enabling unlimited-length audio processing.

Result: DPM significantly improves SER performance on the IEMOCAP dataset, achieving state-of-the-art results.

Conclusion: DPM effectively overcomes SLLM limitations for long audio sequences, enhancing emotion recognition in conversations.

Abstract: Recent research has focused on applying speech large language model (SLLM) to
improve speech emotion recognition (SER). However, the inherently high frame
rate in speech modality severely limits the signal processing and understanding
capabilities of SLLM. For example, a SLLM with a 4K context window can only
process 80 seconds of audio at 50Hz feature sampling rate before reaching its
capacity limit. Input token compression methods used in SLLM overlook the
continuity and inertia of emotions across multiple conversation turns. This
paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual
semantics and sentence-level emotion encoding, enabling processing of
unlimited-length audio with limited context windows in SLLM. Specifically, DPM
progressively encodes sentence-level information and emotions into a temporary
LoRA module during inference to effectively "memorize" the contextual
information. We trained an emotion SLLM as a backbone and incorporated our DPM
into inference for emotion recognition in conversation (ERC). Experimental
results on the IEMOCAP dataset show that DPM significantly improves the emotion
recognition capabilities of SLLM when processing long audio sequences,
achieving state-of-the-art performance.

</details>


### [380] [CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards](https://arxiv.org/abs/2507.09104)
*Taolin Zhang,Maosong Cao,Alexander Lam,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: CompassJudger-2 is a generalist judge model addressing limitations of current LLM-as-judge systems through task-driven, multi-domain data curation and verifiable rewards, achieving superior performance and introducing JudgerBenchV2 for standardized evaluation.


<details>
  <summary>Details</summary>
Motivation: Current judge models are narrowly specialized and lack robustness, limiting comprehensive evaluations of large language models.

Method: Task-driven, multi-domain data curation; supervised judgment tasks with verifiable rewards; rejection sampling for critical reasoning; refined learning objective with margin policy gradient loss.

Result: CompassJudger-2 outperforms benchmarks, with its 7B model matching larger models like DeepSeek-V3 and Qwen3-235B-A22B in accuracy.

Conclusion: The work advances robust, scalable LLM judgment, setting new performance and evaluation standards with CompassJudger-2 and JudgerBenchV2.

Abstract: Recently, the role of LLM-as-judge in evaluating large language models has
gained prominence. However, current judge models suffer from narrow
specialization and limited robustness, undermining their capacity for
comprehensive evaluations. In this work, we present CompassJudger-2, a novel
generalist judge model that overcomes these limitations via a task-driven,
multi-domain data curation strategy. Central to our approach is supervising
judgment tasks with verifiable rewards, guiding intrinsic critical reasoning
through rejection sampling to foster robust, generalizable judgment
capabilities. We introduce a refined learning objective with margin policy
gradient loss to enhance performance. Empirically, CompassJudger-2 achieves
superior results across multiple judge and reward benchmarks, and our 7B model
demonstrates competitive judgment accuracy with significantly larger models
like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a
comprehensive benchmark evaluating cross-domain judgment accuracy and rank
consistency to standardize judge model evaluation. These contributions advance
robust, scalable LLM judgment and establish new performance and evaluation
standards.

</details>


### [381] [OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering](https://arxiv.org/abs/2507.09155)
*Ali Vosoughi,Ayoub Shahnazari,Yufeng Xi,Zeliang Zhang,Griffin Hess,Chenliang Xu,Niaz Abdolrahim*

Main category: cs.CL

TL;DR: OPENXRD is an open-book pipeline for crystallography QA, using GPT-4.5-generated references to aid smaller models. It improves accuracy, especially for models with limited crystallography training.


<details>
  <summary>Details</summary>
Motivation: To address copyright issues with scanned textbooks and enhance smaller models' understanding of XRD by generating domain-specific references.

Method: Integrates textual prompts with GPT-4.5-generated concise content. Evaluated on 217 expert-level XRD questions using vision-language models under closed- and open-book conditions.

Result: Significant accuracy improvements in models using GPT-4.5 summaries, especially those with limited crystallography training.

Conclusion: OPENXRD demonstrates the utility of specialized open-book systems in materials science and lays groundwork for broader NLP tools in scientific fields.

Abstract: This work presents OPENXRD, an open-book pipeline designed for
crystallography question answering, which integrates textual prompts with
concise supporting content generated by GPT-4.5. Instead of using scanned
textbooks, which may lead to copyright issues, OPENXRD generates compact,
domain-specific references that help smaller models understand key concepts in
X-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217
expert-level XRD questions by comparing different vision-language models,
including GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,
under both closed-book (without supporting material) and open-book (with
supporting material) conditions. Our experimental results show significant
accuracy improvements in models that use the GPT-4.5-generated summaries,
particularly those with limited prior training in crystallography. OPENXRD uses
knowledge from larger models to fill knowledge gaps in crystallography and
shows that AI-generated texts can help smaller models reason more effectively
in scientific tasks. While the current version of OPENXRD focuses on text-based
inputs, we also explore future extensions such as adding real crystal diagrams
or diffraction patterns to improve interpretation in specialized materials
science contexts. Overall, OPENXRD shows that specialized open-book systems can
be useful in materials science and provides a foundation for broader natural
language processing (NLP) tools in critical scientific fields.

</details>


### [382] [PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning](https://arxiv.org/abs/2507.09157)
*Bhavinkumar Vinodbhai Kuwar,Bikrant Bikram Pratap Maurya,Priyanshu Gupta,Nitin Choudhury*

Main category: cs.CL

TL;DR: A lightweight model (PU-Lie) for deception detection in Diplomacy, combining BERT, linguistic/game features, and PU learning, achieves 0.60 macro F1 with 650x fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Detecting deception is challenging due to subtle language and class imbalance (5% deceptive). Traditional binary classifiers fail with rare labeled deceptive messages.

Method: Uses frozen BERT embeddings, interpretable features, and PU learning to focus on the rare deceptive class.

Result: Achieves 0.60 macro F1, outperforms 7 models, and reduces parameters by 650x.

Conclusion: PU learning, interpretability, and speaker-awareness are key. Accurate deception detection is prioritized over truthfulness.

Abstract: Detecting deception in strategic dialogues is a complex and high-stakes task
due to the subtlety of language and extreme class imbalance between deceptive
and truthful communications. In this work, we revisit deception detection in
the Diplomacy dataset, where less than 5% of messages are labeled deceptive. We
introduce a lightweight yet effective model combining frozen BERT embeddings,
interpretable linguistic and game-specific features, and a Positive-Unlabeled
(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is
tailored for situations where only a small portion of deceptive messages are
labeled, and the majority are unlabeled. Our model achieves a new best macro F1
of 0.60 while reducing trainable parameters by over 650x. Through comprehensive
evaluations and ablation studies across seven models, we demonstrate the value
of PU learning, linguistic interpretability, and speaker-aware representations.
Notably, we emphasize that in this problem setting, accurately detecting
deception is more critical than identifying truthful messages. This priority
guides our choice of PU learning, which explicitly models the rare but vital
deceptive class.

</details>


### [383] [RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking](https://arxiv.org/abs/2507.09174)
*Shuo Yang,Zijian Yu,Zhenzhe Ying,Yuqin Dai,Guoqing Wang,Jun Lan,Jinfeng Xu,Jinze Li,Edith C. H. Ngai*

Main category: cs.CL

TL;DR: RAMA is a retrieval-augmented multi-agent framework for verifying multimedia misinformation, excelling in handling ambiguous claims by leveraging web-based evidence and multi-agent reasoning.


<details>
  <summary>Details</summary>
Motivation: The rise of multimodal misinformation, especially ambiguous or context-lacking claims, challenges automated fact-checking systems.

Method: RAMA uses strategic query formulation, cross-verification evidence aggregation, and a multi-agent ensemble of multimodal LLMs.

Result: RAMA outperforms benchmarks, especially in resolving ambiguous claims by grounding verification in retrieved evidence.

Conclusion: Integrating web-based evidence and multi-agent reasoning is crucial for reliable multimedia verification, with RAMA offering a scalable solution.

Abstract: The rapid proliferation of multimodal misinformation presents significant
challenges for automated fact-checking systems, especially when claims are
ambiguous or lack sufficient context. We introduce RAMA, a novel
retrieval-augmented multi-agent framework designed for verifying multimedia
misinformation. RAMA incorporates three core innovations: (1) strategic query
formulation that transforms multimodal claims into precise web search queries;
(2) cross-verification evidence aggregation from diverse, authoritative
sources; and (3) a multi-agent ensemble architecture that leverages the
complementary strengths of multiple multimodal large language models and prompt
variants. Extensive experiments demonstrate that RAMA achieves superior
performance on benchmark datasets, particularly excelling in resolving
ambiguous or improbable claims by grounding verification in retrieved factual
evidence. Our findings underscore the necessity of integrating web-based
evidence and multi-agent reasoning for trustworthy multimedia verification,
paving the way for more reliable and scalable fact-checking solutions. RAMA
will be publicly available at https://github.com/kalendsyang/RAMA.git.

</details>


### [384] [Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models](https://arxiv.org/abs/2507.09185)
*Ameen Ali,Shahar Katz,Lior Wolf,Ivan Titov*

Main category: cs.CL

TL;DR: A fine-tuning method prunes dataset-specific neurons in LLMs to improve generalization, outperforming non-pruning methods.


<details>
  <summary>Details</summary>
Motivation: LLMs often rely on dataset-specific correlations, harming performance on novel tasks.

Method: Uses Integrated Gradients to identify and prune neurons tied to dataset-specific mechanisms, forcing reliance on generalizable representations.

Result: Outperforms prior adaptation methods on multiple-choice benchmarks.

Conclusion: Pruning dataset-specific neurons enhances LLM generalization.

Abstract: Large language models (LLMs) often develop learned mechanisms specialized to
specific datasets, such as reliance on domain-specific correlations, which
yield high-confidence predictions without generalizable reasoning. While
beneficial in one setting, these dataset-specific mechanisms typically degrade
performance when models encounter novel tasks or distributions. In this work,
we introduce a fine-tuning approach designed to enhance generalization by
identifying and pruning neurons associated with dataset-specific mechanisms in
transformer-based LLMs. Our method employs Integrated Gradients to quantify
each neuron's influence on high-confidence predictions, pinpointing those that
disproportionately contribute to dataset-specific performance without
supporting robust, transferable reasoning. Selectively pruning these neurons
compels the model to depend on generalizable representations. Evaluated across
multiple-choice benchmarks, our pruning-based fine-tuning significantly
enhances performance, surpassing prior (non-pruning) adaptation methods.

</details>


### [385] [Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training](https://arxiv.org/abs/2507.09205)
*Leiyu Pan,Bojian Xiong,Lei Yang,Renren Jin,Shaowei Zhang,Yue Chen,Ling Shi,Jiang Zhou,Junru Wu,Zhen Wang,Jianxiang Peng,Juesi Xiao,Tianyu Dong,Zhuowen Han,Zhuo Chen,Sangjee Dondrub,Caizang Tai,Haixing Zhao,Huaque Cairang,Suonan Cairang,Rou Te,Lengben Zhaxi,Gazang Zhaxi,Zhonglin Ye,Yuhui Zheng,Chunyan Peng,Secha Jia,Pema Tashi,Cizhen Jiacuo,Pema Dorjee,Hongkai Liu,Pema Yanggon,Tsehang Dorjee,Jiaxin Han,Qiongying Hu,Jilin Man,Huanke You,Yuqi Ren,Duo La,Deyi Xiong*

Main category: cs.CL

TL;DR: The paper introduces Banzhida, a multilingual large language model for Tibetan, addressing the lack of representation in existing models by curating the largest Tibetan pre-training corpus and creating new benchmarks.


<details>
  <summary>Details</summary>
Motivation: Tibetan is underrepresented in large language models due to scarce high-quality training data, prompting the need for tailored solutions.

Method: The authors curated a large Tibetan corpus, applied a dedicated cleaning pipeline, and pre/post-trained a multilingual base model into Banzhida. New benchmarks were also created.

Result: Banzhida outperforms similar-scale open-source models and Tibetan-tailored models across various tasks.

Conclusion: The work advances generative AI for Tibetan by addressing data scarcity and improving model performance.

Abstract: Large language models have achieved remarkable progress across many
languages. However, Tibetan, as a representative low-resource language, is
particularly underrepresented in existing models due to the scarcity of
high-quality training corpora. To address this gap, we curate the largest
Tibetan pre-training corpus to date, aggregating data from diverse sources and
applying a dedicated data cleaning and processing pipeline tailored for
Tibetan. With the curated data, we continue pre/post-training a multilingual
base model into Banzhida, a multilingual large language model that advances
generative AI for Tibetan. To evaluate the Tibetan capabilities of the model,
we create new high-quality Tibetan benchmarks, and complement them with
existing public benchmarks. Experimental results demonstrate that Banzhida
consistently and significantly outperforms both open-source models of similar
scale and Tibetan-tailored models across a wide range of tasks.

</details>


### [386] [MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis](https://arxiv.org/abs/2507.09225)
*Biagio Scalingi,Chiara Barattieri di San Pietro,Paolo Canal,Valentina Bambini*

Main category: cs.CL

TL;DR: The paper introduces MetaClimage, a database of climate change visual metaphors, and analyzes their impact on communication. Visual metaphors are harder to understand but more aesthetically pleasing, with no difference in efficacy or arousal compared to literal images. They elicit more tags and positive valence, suggesting deeper cognitive engagement.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on the impact of visual metaphors in climate change communication and provide a resource for future studies.

Method: Created MetaClimage, a database of visual metaphors paired with literal images, enriched with human ratings (difficulty, efficacy, artistic quality, emotional arousal) and NLP-derived semantic and emotion variables from tags.

Result: Visual metaphors were rated as more difficult yet aesthetically pleasing, with no efficacy or arousal difference. They elicited more tags, positive valence, and dominance, indicating deeper cognitive engagement.

Conclusion: Visual metaphors offer a trade-off between cognitive load and positive effects like aesthetic appreciation and deeper elaboration, aiding environmental communication strategies.

Abstract: Visual metaphors of climate change (e.g., melting glaciers depicted as a
melting ice grenade) are regarded as valuable tools for addressing the
complexity of environmental challenges. However, few studies have examined
their impact on communication, also due to scattered availability of material.
Here, we present a novel database of Metaphors of Climate Change in Images
(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal
images and enriched with human ratings. For each image, we collected values of
difficulty, efficacy, artistic quality, and emotional arousal from human
rating, as well as number of tags generated by participants to summarize the
message. Semantic and emotion variables were further derived from the tags via
Natural Language Processing. Visual metaphors were rated as more difficult to
understand, yet more aesthetically pleasant than literal images, but did not
differ in efficacy and arousal. The latter for visual metaphors, however, was
higher in participants with higher Need For Cognition. Furthermore, visual
metaphors received more tags, often referring to entities not depicted in the
image, and elicited words with more positive valence and greater dominance than
literal images. These results evidence the greater cognitive load of visual
metaphors, which nevertheless might induce positive effects such as deeper
cognitive elaboration and abstraction compared to literal stimuli. Furthermore,
while they are not deemed as more effective and arousing, visual metaphors seem
to generate superior aesthetic appreciation and a more positively valenced
experience. Overall, this study contributes to understanding the impact of
visual metaphors of climate change both by offering a database for future
research and by elucidating a cost-benefit trade-off to take into account when
shaping environmental communication.

</details>


### [387] [Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources](https://arxiv.org/abs/2507.09245)
*Deshan Sumanathilaka,Sameera Perera,Sachithya Dharmasiri,Maneesha Athukorala,Anuja Dilrukshi Herath,Rukshan Dias,Pasindu Gamage,Ruvan Weerasinghe,Y. H. P. P. Priyadarshana*

Main category: cs.CL

TL;DR: The Swa-bhasha Resource Hub offers data and tools for Romanized Sinhala to Sinhala transliteration, aiding NLP research and applications.


<details>
  <summary>Details</summary>
Motivation: To advance Sinhala NLP by providing accessible resources for transliteration research and applications.

Method: Collection and public release of datasets and tools, along with a comparative analysis of existing transliteration applications.

Result: A comprehensive hub of resources and tools for transliteration, supporting NLP advancements.

Conclusion: The hub significantly contributes to Sinhala NLP by facilitating research and application development in transliteration.

Abstract: The Swa-bhasha Resource Hub provides a comprehensive collection of data
resources and algorithms developed for Romanized Sinhala to Sinhala
transliteration between 2020 and 2025. These resources have played a
significant role in advancing research in Sinhala Natural Language Processing
(NLP), particularly in training transliteration models and developing
applications involving Romanized Sinhala. The current openly accessible data
sets and corresponding tools are made publicly available through this hub. This
paper presents a detailed overview of the resources contributed by the authors
and includes a comparative analysis of existing transliteration applications in
the domain.

</details>


### [388] [Psychology-Driven Enhancement of Humour Translation](https://arxiv.org/abs/2507.09259)
*Yuchen Su,Yonghua Zhu,Yang Chen,Diana Benavides-Prado,Michael Witbrock*

Main category: cs.CL

TL;DR: The paper introduces a Humour Decomposition Mechanism (HDM) to improve humour translation in LLMs, achieving significant gains in humour, fluency, and coherence.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with humour translation due to linguistic interference and lack of humour in outputs, hindering cross-cultural communication.

Method: Proposes HDM, a psychology-inspired method using Chain-of-Thought (CoT) and humour theory to enhance humour translation.

Result: Experiments show average gains of 7.75% in humour, 2.81% in fluency, and 6.13% in coherence.

Conclusion: HDM effectively improves humour translation quality in LLMs, bridging cultural gaps.

Abstract: Humour translation plays a vital role as a bridge between different cultures,
fostering understanding and communication. Although most existing Large
Language Models (LLMs) are capable of general translation tasks, these models
still struggle with humour translation, which is especially reflected through
linguistic interference and lacking humour in translated text. In this paper,
we propose a psychology-inspired Humour Decomposition Mechanism (HDM) that
utilises Chain-of-Thought (CoT) to imitate the ability of the human thought
process, stimulating LLMs to optimise the readability of translated humorous
texts. Moreover, we integrate humour theory in HDM to further enhance the
humorous elements in the translated text. Our automatic evaluation experiments
on open-source humour datasets demonstrate that our method significantly
improves the quality of humour translation, yielding average gains of 7.75\% in
humour, 2.81\% in fluency, and 6.13\% in coherence of the generated text.

</details>


### [389] [ClaritySpeech: Dementia Obfuscation in Speech](https://arxiv.org/abs/2507.09282)
*Dominika Woszczyk,Ranya Aloufi,Soteris Demetriou*

Main category: cs.CL

TL;DR: ClaritySpeech, a dementia obfuscation framework, improves speech clarity and privacy for dementia patients using ASR, text obfuscation, and zero-shot TTS, achieving better WER and speech quality.


<details>
  <summary>Details</summary>
Motivation: Dementia alters speech, creating communication and privacy challenges. Current ASR struggles with atypical speech, limiting accessibility.

Method: Integrates ASR, text obfuscation, and zero-shot TTS to correct dementia-affected speech without fine-tuning in low-data environments.

Result: 16% and 10% F1 score drop in adversarial settings, improved WER (0.73 to 0.08), and speech quality (1.65 to ~2.15) while maintaining 50% speaker similarity.

Conclusion: ClaritySpeech enhances privacy and accessibility for dementia patients by improving speech clarity and quality.

Abstract: Dementia, a neurodegenerative disease, alters speech patterns, creating
communication barriers and raising privacy concerns. Current speech
technologies, such as automatic speech transcription (ASR), struggle with
dementia and atypical speech, further challenging accessibility. This paper
presents a novel dementia obfuscation in speech framework, ClaritySpeech,
integrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to
correct dementia-affected speech while preserving speaker identity in low-data
environments without fine-tuning. Results show a 16% and 10% drop in mean F1
score across various adversarial settings and modalities (audio, text, fusion)
for ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We
also find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15
for ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and
accessibility.

</details>


### [390] [DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models](https://arxiv.org/abs/2507.09424)
*Cathy Jiao,Yijun Pan,Emily Xiao,Daisy Sheng,Niket Jain,Hanzhang Zhao,Ishita Dasgupta,Jiaqi W. Ma,Chenyan Xiong*

Main category: cs.CL

TL;DR: DATE-LM is a benchmark for evaluating data attribution methods in LLMs, covering tasks like data selection, toxicity filtering, and factual attribution. It reveals no single method excels in all tasks and highlights trade-offs with baselines.


<details>
  <summary>Details</summary>
Motivation: Gaps in systematic evaluation of data attribution methods for LLMs prompted the creation of DATE-LM.

Method: DATE-LM evaluates attribution methods through three tasks: training data selection, toxicity/bias filtering, and factual attribution.

Result: No single method dominates; performance varies by task and is sensitive to evaluation design.

Conclusion: DATE-LM provides a foundation for future research and includes a public leaderboard for community engagement.

Abstract: Data attribution methods quantify the influence of training data on model
outputs and are becoming increasingly relevant for a wide range of LLM research
and applications, including dataset curation, model interpretability, data
valuation. However, there remain critical gaps in systematic LLM-centric
evaluation of data attribution methods. To this end, we introduce DATE-LM (Data
Attribution Evaluation in Language Models), a unified benchmark for evaluating
data attribution methods through real-world LLM applications. DATE-LM measures
attribution quality through three key tasks -- training data selection,
toxicity/bias filtering, and factual attribution. Our benchmark is designed for
ease of use, enabling researchers to configure and run large-scale evaluations
across diverse tasks and LLM architectures. Furthermore, we use DATE-LM to
conduct a large-scale evaluation of existing data attribution methods. Our
findings show that no single method dominates across all tasks, data
attribution methods have trade-offs with simpler baselines, and method
performance is sensitive to task-specific evaluation design. Finally, we
release a public leaderboard for quick comparison of methods and to facilitate
community engagement. We hope DATE-LM serves as a foundation for future data
attribution research in LLMs.

</details>


### [391] [Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models](https://arxiv.org/abs/2507.09470)
*Mingchuan Yang,Ziyuan Huang*

Main category: cs.CL

TL;DR: Optimized DRAGON Longformer for clinical text classification, improving accuracy from 72.0% to 85.2% with domain-specific adjustments.


<details>
  <summary>Details</summary>
Motivation: Enhance performance of the DRAGON Longformer model for binary classification of medical case descriptions.

Method: Hyperparameter tuning, domain-specific preprocessing, architectural adjustments (e.g., increased sequence length, adjusted learning rates, extended epochs).

Result: Significant performance gains: accuracy (85.2%), precision (84.1%), recall (86.3%), F1-score (85.2%).

Conclusion: The optimized model is effective for clinical NLP, with potential for broad healthcare applications.

Abstract: This study explores the optimization of the DRAGON Longformer base model for
clinical text classification, specifically targeting the binary classification
of medical case descriptions. A dataset of 500 clinical cases containing
structured medical observations was used, with 400 cases for training and 100
for validation. Enhancements to the pre-trained
joeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter
tuning, domain-specific preprocessing, and architectural adjustments. Key
modifications involved increasing sequence length from 512 to 1024 tokens,
adjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5
to 8, and incorporating specialized medical terminology. The optimized model
achieved notable performance gains: accuracy improved from 72.0% to 85.2%,
precision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from
71.0% to 85.2%. Statistical analysis confirmed the significance of these
improvements (p < .001). The model demonstrated enhanced capability in
interpreting medical terminology, anatomical measurements, and clinical
observations. These findings contribute to domain-specific language model
research and offer practical implications for clinical natural language
processing applications. The optimized model's strong performance across
diverse medical conditions underscores its potential for broad use in
healthcare settings.

</details>


### [392] [The CoNLL-2013 Shared Task on Grammatical Error Correction](https://arxiv.org/abs/2507.09474)
*Hwee Tou Ng,Siew Mei Wu,Yuanbin Wu,Christian Hadiwinoto,Joel Tetreault*

Main category: cs.CL

TL;DR: Overview of the CoNLL-2013 shared task on grammatical error correction, including task definition, datasets, evaluation metrics, participant approaches, and results.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive summary of the CoNLL-2013 shared task, focusing on grammatical error correction, for researchers and practitioners.

Method: Describes the task setup, datasets, evaluation metrics, and summarizes participant approaches.

Result: Presents evaluation results of the participating teams' methods.

Conclusion: The paper serves as a detailed reference for the CoNLL-2013 shared task, highlighting methodologies and outcomes in grammatical error correction.

Abstract: The CoNLL-2013 shared task was devoted to grammatical error correction. In
this paper, we give the task definition, present the data sets, and describe
the evaluation metric and scorer used in the shared task. We also give an
overview of the various approaches adopted by the participating teams, and
present the evaluation results.

</details>


### [393] [Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs](https://arxiv.org/abs/2507.09477)
*Yangning Li,Weizhi Zhang,Yuyao Yang,Wei-Chieh Huang,Yaozu Wu,Junyu Luo,Yuanchen Bei,Henry Peng Zou,Xiao Luo,Yusheng Zhao,Chunkit Chan,Yankai Chen,Zhongfen Deng,Yinghui Li,Hai-Tao Zheng,Dongyuan Li,Renhe Jiang,Ming Zhang,Yangqiu Song,Philip S. Yu*

Main category: cs.CL

TL;DR: The paper surveys the integration of retrieval-augmented generation (RAG) and reasoning in LLMs, proposing a unified framework to enhance factuality and inference.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of RAG in multi-step inference and reasoning-oriented approaches in grounding facts, by synthesizing both under a reasoning-retrieval perspective.

Method: The paper maps how reasoning enhances RAG stages and how retrieved knowledge supports complex inference, introducing Synergized RAG-Reasoning frameworks.

Result: Emerging frameworks interleave search and reasoning, achieving state-of-the-art performance in knowledge-intensive tasks.

Conclusion: The paper outlines future research for deeper, more adaptive, trustworthy, and human-centric RAG-Reasoning systems.

Abstract: Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language
Models (LLMs) by injecting external knowledge, yet it falls short on problems
that demand multi-step inference; conversely, purely reasoning-oriented
approaches often hallucinate or mis-ground facts. This survey synthesizes both
strands under a unified reasoning-retrieval perspective. We first map how
advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,
we show how retrieved knowledge of different type supply missing premises and
expand context for complex inference (RAG-Enhanced Reasoning). Finally, we
spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs
iteratively interleave search and reasoning to achieve state-of-the-art
performance across knowledge-intensive benchmarks. We categorize methods,
datasets, and open challenges, and outline research avenues toward deeper
RAG-Reasoning systems that are more effective, multimodally-adaptive,
trustworthy, and human-centric. The collection is available at
https://github.com/DavidZWZ/Awesome-RAG-Reasoning.

</details>


### [394] [ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning](https://arxiv.org/abs/2507.09482)
*Changli Wang,Rui Wu,Fang Yin*

Main category: cs.CL

TL;DR: The paper introduces M2SaG, a multimodal sarcasm generation dataset, and ViSP, a framework combining PPO and contrastive learning to improve sarcasm generation. ViSP outperforms baselines, including large language models, and generates higher-quality sarcastic texts.


<details>
  <summary>Details</summary>
Motivation: Sarcasm generation is underexplored due to reliance on text and neglect of visual cues, as well as dataset limitations. The paper aims to address these gaps.

Method: Proposed ViSP framework integrates PPO for reward-guided text generation and contrastive learning to favor high-reward outputs.

Result: ViSP surpasses baselines, achieving higher Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity (0.768 vs. 0.739) than the original dataset.

Conclusion: ViSP effectively improves sarcasm generation quality, highlighting the potential of multimodal approaches and the limitations of text-only methods.

Abstract: Human emotions are complex, with sarcasm being a subtle and distinctive form.
Despite progress in sarcasm research, sarcasm generation remains underexplored,
primarily due to the overreliance on textual modalities and the neglect of
visual cues, as well as the mismatch between image content and sarcastic intent
in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm
generation dataset with 4,970 samples, each containing an image, a sarcastic
text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation
framework that integrates Proximal Policy Optimization (PPO) and contrastive
learning. PPO utilizes reward scores from DIP to steer the generation of
sarcastic texts, while contrastive learning encourages the model to favor
outputs with higher reward scores. These strategies improve overall generation
quality and produce texts with more pronounced sarcastic intent. We evaluate
ViSP across five metric sets and find it surpasses all baselines, including
large language models, underscoring their limitations in sarcasm generation.
Furthermore, we analyze the distributions of Sarcasm Scores and Factual
Incongruity for both M2SaG and the texts generated by ViSP. The generated texts
exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity
(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic
content than the original dataset. % The dataset and code will be publicly
available. Our dataset and code will be released at
\textit{https://github.com/wclapply/ViSP}.

</details>


### [395] [Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.09485)
*Junjie Liu,Yuanhe Tian,Yan Song*

Main category: cs.CL

TL;DR: The paper proposes an LLM-based ABSA approach with data augmentation, using reinforcement learning to improve augmented data quality, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: Challenges in ABSA include short text, small/unbalanced labeled data, and difficulty in learning context. Data augmentation with LLMs is promising but needs quality control.

Method: An LLM generates augmented training data from original data, creating a larger, balanced dataset. Reinforcement learning optimizes the augmentation process.

Result: Experiments on English ABSA benchmarks show the approach outperforms baselines and existing studies.

Conclusion: The proposed method effectively addresses ABSA challenges by leveraging LLM-based data augmentation and reinforcement learning, yielding improved performance.

Abstract: Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in
social media scenarios to identify the sentiment polarity of specific aspect
terms in a sentence. Although many existing studies leverage large language
models (LLMs) to perform ABSA due to their strong context understanding
capabilities, they still face challenges to learn the context information in
the running text because of the short text, as well as the small and unbalanced
labeled training data, where most data are labeled with positive sentiment.
Data augmentation (DA) is a feasible strategy for providing richer contextual
information, especially when using LLMs to create synthetic training data, but
faces challenges in ensuring a high quality of the augmented data.In this
paper, we propose an LLM-based ABSA approach with training data
augmentation.Specifically, an LLM is prompted to generate augmented training
data based on the original training data, so as to construct a new training
data with larger size and balanced label distributions to better train an ABSA
model. Meanwhile, in order to improve the quality of the augmented data, we
propose a reinforcement learning approach to optimize the data augmentation.
LLM.Experiment results and further analyses on English benchmark datasets for
ABSA demonstrate the effectiveness of our approach, where superior performance
is observed over strong baselines and most existing studies.

</details>


### [396] [GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities](https://arxiv.org/abs/2507.09497)
*Siyi Wu,Zeyu Wang,Xinyuan Song,Zhengpeng Zhou,Lifan Sun,Tianyu Shi*

Main category: cs.CL

TL;DR: GoalfyMax is a protocol-driven framework for multi-agent collaboration, addressing limitations of traditional AI systems with features like A2A communication, memory reuse, and dynamic safety validation, showing superior adaptability and coordination.


<details>
  <summary>Details</summary>
Motivation: Traditional single-purpose AI systems lack coordination, memory reuse, and task decomposition, limiting scalability in complex enterprise environments.

Method: GoalfyMax uses the Model Context Protocol (MCP) for A2A communication, Experience Pack (XP) for memory, and includes multi-turn dialogue, memory modules, and safety validation.

Result: Empirical results show GoalfyMax outperforms baselines in adaptability, coordination, and experience reuse on complex task benchmarks.

Conclusion: GoalfyMax is a scalable, future-ready foundation for multi-agent intelligent systems.

Abstract: Modern enterprise environments demand intelligent systems capable of handling
complex, dynamic, and multi-faceted tasks with high levels of autonomy and
adaptability. However, traditional single-purpose AI systems often lack
sufficient coordination, memory reuse, and task decomposition capabilities,
limiting their scalability in realistic settings. To address these challenges,
we present \textbf{GoalfyMax}, a protocol-driven framework for end-to-end
multi-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent
(A2A) communication layer built on the Model Context Protocol (MCP), allowing
independent agents to coordinate through asynchronous, protocol-compliant
interactions. It incorporates the Experience Pack (XP) architecture, a layered
memory system that preserves both task rationales and execution traces,
enabling structured knowledge retention and continual learning. Moreover, our
system integrates advanced features including multi-turn contextual dialogue,
long-short term memory modules, and dynamic safety validation, supporting
robust, real-time strategy adaptation. Empirical results on complex task
orchestration benchmarks and case study demonstrate that GoalfyMax achieves
superior adaptability, coordination, and experience reuse compared to baseline
frameworks. These findings highlight its potential as a scalable, future-ready
foundation for multi-agent intelligent systems.

</details>


### [397] [Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models](https://arxiv.org/abs/2507.09506)
*Junjie Wu,Gefei Gu,Yanan Zheng,Dit-Yan Yeung,Arman Cohan*

Main category: cs.CL

TL;DR: The paper introduces Ref-Long, a benchmark to evaluate long-context referencing in language models, revealing significant shortcomings even in advanced models like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Long-context referencing is underexplored despite its importance, prompting the need for a dedicated benchmark.

Method: Ref-Long requires models to identify document indexes referencing a key, emphasizing contextual relationships. Three subsets (synthetic to realistic) are constructed for evaluation.

Result: Experiments with 13 LCLMs show notable deficiencies in referencing, even in top models. Analyses include human evaluations and fine-tuning.

Conclusion: Ref-Long highlights challenges in long-context referencing and provides insights for future improvements.

Abstract: Long-context language models (LCLMs) have exhibited impressive capabilities
in long-context understanding tasks. Among these, long-context referencing -- a
crucial task that requires LCLMs to attribute items of interest to specific
parts of long-context data -- remains underexplored. To bridge this gap, this
paper proposes Referencing Evaluation for Long-context Language Models
(Ref-Long), a novel benchmark designed to assess the long-context referencing
capability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the
indexes of documents that reference a specific key, emphasizing contextual
relationships between the key and the documents over simple retrieval. Based on
the task design, we construct three subsets ranging from synthetic to realistic
scenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs
reveal significant shortcomings in long-context referencing, even among
advanced models like GPT-4o. To further investigate these challenges, we
conduct comprehensive analyses, including human evaluations, task format
adjustments, fine-tuning experiments, and error analyses, leading to several
key insights. Our data and code can be found in https://github.
com/wujunjie1998/Ref-Long.

</details>


### [398] [How Important is `Perfect' English for Machine Translation Prompts?](https://arxiv.org/abs/2507.09509)
*Patrícia Schmidtová,Niyati Bafna,Seth Aycock,Gianluca Vico,Wiktor Kamzela,Katharina Hämmerl,Vilém Zouhar*

Main category: cs.CL

TL;DR: The study evaluates how human and synthetic errors in prompts affect LLMs' performance in machine translation and evaluation, showing that prompt quality significantly impacts results.


<details>
  <summary>Details</summary>
Motivation: To understand how errors in user prompts influence the performance of large language models (LLMs) in machine translation and evaluation tasks.

Method: Systematic evaluation of LLMs' responses to various types of noise (humanly plausible and synthetic) in prompts, including quantitative and qualitative analysis.

Result: Prompt quality strongly affects translation performance; character-level and combined noise degrade performance more than phrasal noise. LLMs can still translate with overwhelming noise.

Conclusion: Lower prompt quality leads to poorer instruction following rather than direct translation quality impact, and LLMs show resilience to extreme noise.

Abstract: Large language models (LLMs) have achieved top results in recent machine
translation evaluations, but they are also known to be sensitive to errors and
perturbations in their prompts. We systematically evaluate how both humanly
plausible and synthetic errors in user prompts affect LLMs' performance on two
related tasks: Machine translation and machine translation evaluation. We
provide both a quantitative analysis and qualitative insights into how the
models respond to increasing noise in the user prompt.
  The prompt quality strongly affects the translation performance: With many
errors, even a good prompt can underperform a minimal or poor prompt without
errors. However, different noise types impact translation quality differently,
with character-level and combined noisers degrading performance more than
phrasal perturbations. Qualitative analysis reveals that lower prompt quality
largely leads to poorer instruction following, rather than directly affecting
translation quality itself. Further, LLMs can still translate in scenarios with
overwhelming random noise that would make the prompt illegible to humans.

</details>


### [399] [Adapting Definition Modeling for New Languages: A Case Study on Belarusian](https://arxiv.org/abs/2507.09536)
*Daniela Kazakouskaya,Timothee Mickus,Janine Siewert*

Main category: cs.CL

TL;DR: The paper explores adapting definition modeling to Belarusian, proposing a dataset of 43,150 definitions and showing minimal data is needed for adaptation, though gaps in automatic metrics exist.


<details>
  <summary>Details</summary>
Motivation: To assist lexicographers in documenting unsupported languages by leveraging existing models, focusing on Belarusian.

Method: Adapting existing definition modeling systems to Belarusian using a novel dataset of 43,150 definitions.

Result: Demonstrates that minimal data is required for adaptation, but highlights gaps in automatic metrics.

Conclusion: Definition modeling can be adapted to new languages like Belarusian with limited data, but current metrics may not fully capture performance.

Abstract: Definition modeling, the task of generating new definitions for words in
context, holds great prospect as a means to assist the work of lexicographers
in documenting a broader variety of lects and languages, yet much remains to be
done in order to assess how we can leverage pre-existing models for as-of-yet
unsupported languages. In this work, we focus on adapting existing models to
Belarusian, for which we propose a novel dataset of 43,150 definitions. Our
experiments demonstrate that adapting a definition modeling systems requires
minimal amounts of data, but that there currently are gaps in what automatic
metrics do capture.

</details>


### [400] [NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance](https://arxiv.org/abs/2507.09601)
*Hanwool Lee,Sara Yu,Yewon Hwang,Jonghyun Choi,Heejae Ahn,Sungbum Jung,Youngjae Yu*

Main category: cs.CL

TL;DR: NMIXX introduces cross-lingual financial embeddings for Korean and English, improving performance on financial benchmarks while highlighting tokenizer importance.


<details>
  <summary>Details</summary>
Motivation: General sentence embeddings fail to capture financial semantics in low-resource languages like Korean due to jargon, temporal shifts, and bilingual misalignment.

Method: NMIXX is fine-tuned with 18.8K high-confidence triplets (paraphrases, hard negatives, translations) and evaluated on KorFinSTS, a new financial benchmark.

Result: NMIXX outperforms baselines, achieving +0.10 (English) and +0.22 (Korean) Spearman's rho gains, with a trade-off in general STS performance.

Conclusion: Tokenizer design is crucial for low-resource cross-lingual adaptation. NMIXX and KorFinSTS provide tools for domain-specific multilingual learning.

Abstract: General-purpose sentence embedding models often struggle to capture
specialized financial semantics, especially in low-resource languages like
Korean, due to domain-specific jargon, temporal meaning shifts, and misaligned
bilingual vocabularies. To address these gaps, we introduce NMIXX (Neural
eMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual
embedding models fine-tuned with 18.8K high-confidence triplets that pair
in-domain paraphrases, hard negatives derived from a semantic-shift typology,
and exact Korean-English translations. Concurrently, we release KorFinSTS, a
1,921-pair Korean financial STS benchmark spanning news, disclosures, research
reports, and regulations, designed to expose nuances that general benchmarks
miss.
  When evaluated against seven open-license baselines, NMIXX's multilingual
bge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and
+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing
other models by the largest margin, while revealing a modest trade-off in
general STS performance. Our analysis further shows that models with richer
Korean token coverage adapt more effectively, underscoring the importance of
tokenizer design in low-resource, cross-lingual settings. By making both models
and the benchmark publicly available, we provide the community with robust
tools for domain-adapted, multilingual representation learning in finance.

</details>


### [401] [SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks](https://arxiv.org/abs/2507.09628)
*Salvatore Citraro,Edith Haim,Alessandra Carini,Cynthia S. Q. Siew,Giulio Rossetti,Massimo Stella*

Main category: cs.CL

TL;DR: SpreadPy is a Python library for simulating spreading activation in cognitive networks, aiding research in psychology, neuroscience, and education by linking network dynamics to cognitive and clinical phenomena.


<details>
  <summary>Details</summary>
Motivation: To provide a tool for testing structure-function relationships in cognitive processes and enable systematic investigations of activation dynamics in knowledge modeling.

Method: Numerical simulations of spreading activation in single-layer and multiplex networks, validated through case studies comparing results with grounded theories.

Result: Case studies demonstrate SpreadPy's utility in distinguishing math anxiety levels, analyzing cognitive load in creativity tasks, and correlating activation patterns with clinical impairments in aphasia.

Conclusion: SpreadPy offers a flexible, open-source framework for modeling cognitive processes, supporting reproducible research and mechanistic insights into individual differences and impairments.

Abstract: We introduce SpreadPy as a Python library for simulating spreading activation
in cognitive single-layer and multiplex networks. Our tool is designed to
perform numerical simulations testing structure-function relationships in
cognitive processes. By comparing simulation results with grounded theories in
knowledge modelling, SpreadPy enables systematic investigations of how
activation dynamics reflect cognitive, psychological and clinical phenomena. We
demonstrate the library's utility through three case studies: (1) Spreading
activation on associative knowledge networks distinguishes students with high
versus low math anxiety, revealing anxiety-related structural differences in
conceptual organization; (2) Simulations of a creativity task show that
activation trajectories vary with task difficulty, exposing how cognitive load
modulates lexical access; (3) In individuals with aphasia, simulated activation
patterns on lexical networks correlate with empirical error types (semantic vs.
phonological) during picture-naming tasks, linking network structure to
clinical impairments. SpreadPy's flexible framework allows researchers to model
these processes using empirically derived or theoretical networks, providing
mechanistic insights into individual differences and cognitive impairments. The
library is openly available, supporting reproducible research in psychology,
neuroscience, and education research.

</details>


### [402] [An Exploration of Knowledge Editing for Arabic](https://arxiv.org/abs/2507.09629)
*Basel Mousi,Nadir Durrani,Fahim Dalvi*

Main category: cs.CL

TL;DR: First study of Knowledge Editing (KE) in Arabic, evaluating four methods on Arabic benchmarks. Parameter-based methods struggle cross-lingually, while instruction-tuned methods perform better. LTE extended to multilingual settings improves editability and transfer.


<details>
  <summary>Details</summary>
Motivation: Explore KE behavior in morphologically rich languages like Arabic, which is underexamined compared to English.

Method: Evaluated ROME, MEMIT, ICE, and LTE on Arabic translations of ZsRE and Counterfact benchmarks, analyzing multilingual and cross-lingual settings using Llama-2-7B-chat. Extended LTE to multilingual training.

Result: Parameter-based methods struggle with cross-lingual generalization; instruction-tuned methods perform robustly. Joint Arabic-English training enhances LTE's editability and transfer.

Conclusion: Arabic KE benchmarks and multilingual LTE training data released to support future research, highlighting the importance of multilingual approaches for robust KE.

Abstract: While Knowledge Editing (KE) has been widely explored in English, its
behavior in morphologically rich languages like Arabic remains underexamined.
In this work, we present the first study of Arabic KE. We evaluate four methods
(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact
benchmarks, analyzing both multilingual and cross-lingual settings. Our
experiments on Llama-2-7B-chat show show that parameter-based methods struggle
with cross-lingual generalization, while instruction-tuned methods perform more
robustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show
that joint Arabic-English training improves both editability and transfer. We
release Arabic KE benchmarks and multilingual training for LTE data to support
future research.

</details>


### [403] [Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?](https://arxiv.org/abs/2507.09638)
*Pawitsapak Akarajaradwong,Chompakorn Chaksangchaichot,Pirat Pothavorn,Attapol Thamrongrattanarit-Rutherford,Ekapol Chuangsuwanich,Sarana Nutanong*

Main category: cs.CL

TL;DR: The paper introduces Group-Relative Policy Optimization (GRPO) to improve Thai legal question answering by enhancing law citation accuracy and response quality, using BGE-M3 embeddings for cost efficiency.


<details>
  <summary>Details</summary>
Motivation: The performance of Retrieval-Augmented Generation (RAG) systems on Thai legal QA is limited, especially for complex legal reasoning tasks.

Method: The approach uses GRPO and BGE-M3 embeddings to reduce computational costs and improve accuracy.

Result: Experiments show 90% citation-F1 gains and a 31% increase in joint quality metrics over instruction tuning, with enhanced robustness on complex tasks.

Conclusion: GRPO provides an effective, resource-efficient solution for improving Thai legal LLMs.

Abstract: The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal
question answering is still limited, especially for questions requiring
extensive, complex legal reasoning. To address these limitations, we introduce
an approach aligning LLMs toward improved law citation accuracy and better
response quality using Group-Relative Policy Optimization (GRPO). Our approach
leverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,
significantly reducing computational expenses up to 2.5x compared to large
language model judges. Experiments on the NitiBench benchmark demonstrate
substantial improvements: GRPO achieves up to 90% citation-F1 gains from the
base model and a 31% increase in joint quality metrics over instruction tuning.
Crucially, our method shows enhanced robustness on complex legal reasoning
tasks compared to instruction tuning, providing an effective and
resource-efficient solution for enhancing Thai legal LLMs.

</details>


### [404] [MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs](https://arxiv.org/abs/2507.09701)
*Shulin Huang,Linyi Yang,Yue Zhang*

Main category: cs.CL

TL;DR: MCEval is a multilingual framework evaluating cultural awareness and bias in large language models across 13 cultures and languages, revealing disparities and fairness issues.


<details>
  <summary>Details</summary>
Motivation: Large language models lack cross-cultural understanding, necessitating a robust evaluation framework to assess cultural biases and awareness.

Method: MCEval uses dynamic cultural question construction and causal analysis (Counterfactual Rephrasing and Confounder Rephrasing) to evaluate models.

Result: The framework generates 39,897 cultural awareness and 17,940 cultural bias instances, showing performance disparities and fairness issues.

Conclusion: MCEval is the first comprehensive multilingual cultural evaluation framework, highlighting the need for better language-culture alignment in LLMs.

Abstract: Large language models exhibit cultural biases and limited cross-cultural
understanding capabilities, particularly when serving diverse global user
populations. We propose MCEval, a novel multilingual evaluation framework that
employs dynamic cultural question construction and enables causal analysis
through Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive
evaluation spans 13 cultures and 13 languages, systematically assessing both
cultural awareness and cultural bias across different linguistic scenarios. The
framework provides 39,897 cultural awareness instances and 17,940 cultural bias
instances. Experimental results reveal performance disparities across different
linguistic scenarios, demonstrating that optimal cultural performance is not
only linked to training data distribution, but also is related to
language-culture alignment. The evaluation results also expose the fairness
issue, where approaches appearing successful in the English scenario create
substantial disadvantages. MCEval represents the first comprehensive
multilingual cultural evaluation framework that provides deeper insights into
LLMs' cultural understanding.

</details>


### [405] [Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces](https://arxiv.org/abs/2507.09709)
*Baturay Saglam,Paul Kassianik,Blaine Nelson,Sajana Weerawardhena,Yaron Singer,Amin Karbasi*

Main category: cs.CL

TL;DR: The paper explores the latent space geometry of LLMs, revealing that high-level semantic information is organized in low-dimensional, linearly separable subspaces, especially in deeper layers and structured reasoning prompts. This enables effective causal interventions and supports geometry-aware tools for content moderation.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs internally organize semantic representations and leverage this for improving model alignment and interpretability.

Method: A large-scale empirical study of hidden states in 11 decoder-only LLMs across 6 scientific topics and 12 layers, analyzing separability and interventions.

Result: Semantic information lies in low-dimensional, linearly separable subspaces, with deeper layers and structured prompts enhancing separability. Simple interventions (e.g., capturing chain-of-thought reasoning) are possible.

Conclusion: The findings support geometry-aware tools for detecting and mitigating harmful content, demonstrated by a lightweight MLP classifier for adversarial prompt detection.

Abstract: Understanding the latent space geometry of large language models (LLMs) is
key to interpreting their behavior and improving alignment. \baturay{However,
it remains unclear to what extent LLMs internally organize representations
related to semantic understanding. To investigate this, we conduct a
large-scale empirical study of hidden states in transformer-based LLMs,
analyzing 11 decoder-only models across 6 scientific topics and 12 layers each.
We find that high-level semantic information consistently lies in
low-dimensional subspaces that form linearly separable representations across
distinct domains. This separability becomes more pronounced in deeper layers
and under prompts that trigger structured reasoning or alignment
behaviors$\unicode{x2013}$even when surface content is unchanged. This geometry
enables simple yet effective causal interventions in hidden space; for example,
reasoning patterns like chain-of-thought can be captured by a single vector
direction. Together, these findings support the development of geometry-aware
tools that operate directly on latent representations to detect and mitigate
harmful or adversarial content, using methods such as transport-based defenses
that leverage this separability. As a proof of concept, we demonstrate this
potential by training a simple MLP classifier as a lightweight latent-space
guardrail, which detects adversarial and malicious prompts with high precision.

</details>


### [406] [Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding](https://arxiv.org/abs/2507.09758)
*Qi Feng,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

TL;DR: A self-adaptive curriculum learning method uses PLM-predicted difficulty scores to order training examples, outperforming random sampling in NLU tasks.


<details>
  <summary>Details</summary>
Motivation: Existing curriculum learning relies on manual difficulty metrics, which may not align with the model's perspective.

Method: Proposes a self-adaptive curriculum learning paradigm using PLM-predicted difficulty scores and explores training strategies (easy-to-hard, hard-to-easy, mixed).

Result: Faster convergence and improved performance on four NLU datasets compared to random sampling.

Conclusion: Self-adaptive curriculum learning based on PLM-predicted scores is effective for NLU tasks.

Abstract: Curriculum learning is a widely adopted training strategy in natural language
processing (NLP), where models are exposed to examples organized by increasing
difficulty to enhance learning efficiency and performance. However, most
existing approaches rely on manually defined difficulty metrics -- such as text
length -- which may not accurately reflect the model's own perspective. To
overcome this limitation, we present a self-adaptive curriculum learning
paradigm that prioritizes fine-tuning examples based on difficulty scores
predicted by pre-trained language models (PLMs) themselves. Building on these
scores, we explore various training strategies that differ in the ordering of
examples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed
sampling. We evaluate our method on four natural language understanding (NLU)
datasets covering both binary and multi-class classification tasks.
Experimental results show that our approach leads to faster convergence and
improved performance compared to standard random sampling.

</details>


### [407] [Te Ahorré Un Click: A Revised Definition of Clickbait and Detection in Spanish News](https://arxiv.org/abs/2507.09777)
*Gabriel Mordecki,Guillermo Moncecchi,Javier Couto*

Main category: cs.CL

TL;DR: The paper redefines clickbait as headlines omitting information to spark curiosity, introduces a refined dataset creation method, and releases TA1C, a Spanish clickbait dataset with strong baseline results.


<details>
  <summary>Details</summary>
Motivation: To clarify the definition of clickbait and reduce subjectivity in detection by focusing on the curiosity gap.

Method: Proposed a new definition of clickbait, refined dataset creation criteria, and created TA1C, a manually annotated Spanish dataset.

Result: Achieved 0.825 Fleiss' K agreement in annotations and 0.84 F1-score with baseline models.

Conclusion: The refined definition and dataset improve clickbait detection, with TA1C serving as a valuable resource for Spanish language research.

Abstract: We revise the definition of clickbait, which lacks current consensus, and
argue that the creation of a curiosity gap is the key concept that
distinguishes clickbait from other related phenomena such as sensationalism and
headlines that do not deliver what they promise or diverge from the article.
Therefore, we propose a new definition: clickbait is a technique for generating
headlines and teasers that deliberately omit part of the information with the
goal of raising the readers' curiosity, capturing their attention and enticing
them to click. We introduce a new approach to clickbait detection datasets
creation, by refining the concept limits and annotations criteria, minimizing
the subjectivity in the decision as much as possible. Following it, we created
and release TA1C (for Te Ahorr\'e Un Click, Spanish for Saved You A Click), the
first open source dataset for clickbait detection in Spanish. It consists of
3,500 tweets coming from 18 well known media sources, manually annotated and
reaching a 0.825 Fleiss' K inter annotator agreement. We implement strong
baselines that achieve 0.84 in F1-score.

</details>


### [408] [Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition](https://arxiv.org/abs/2507.09875)
*Qinyuan Ye,Robin Jia,Xiang Ren*

Main category: cs.CL

TL;DR: The paper investigates how large language models generalize to unseen tasks like off-by-one addition, revealing a function induction mechanism and parallel attention heads driving this ability.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind task-level generalization in large language models, using off-by-one addition as a case study.

Method: Circuit-style interpretability techniques (e.g., path patching) to analyze internal computations.

Result: Discovered a function induction mechanism, parallel attention heads governing the +1 function, and reuse of this mechanism in other tasks.

Conclusion: The findings provide insights into reusable and composable structures in language models enabling generalization.

Abstract: Large language models demonstrate the intriguing ability to perform unseen
tasks via in-context learning. However, it remains unclear what mechanisms
inside the model drive such task-level generalization. In this work, we
approach this question through the lens of off-by-one addition (i.e., 1+1=3,
2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function
as a second step. Leveraging circuit-style interpretability techniques such as
path patching, we analyze the models' internal computations behind their
notable performance and present three key findings. First, we uncover a
function induction mechanism that explains the model's generalization from
standard addition to off-by-one addition. This mechanism resembles the
structure of the induction head mechanism found in prior work and elevates it
to a higher level of abstraction. Second, we show that the induction of the +1
function is governed by multiple attention heads in parallel, each of which
emits a distinct piece of the +1 function. Finally, we find that this function
induction mechanism is reused in a broader range of tasks, including synthetic
tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8
addition. Overall, our findings offer deeper insights into how reusable and
composable structures within language models enable task-level generalization.

</details>


### [409] [Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking](https://arxiv.org/abs/2507.09935)
*Hai Toan Nguyen,Tien Dat Nguyen,Viet Ha Nguyen*

Main category: cs.CL

TL;DR: A novel framework improves RAG systems by using hierarchical text segmentation and clustering for better semantic chunking, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional chunking in RAG systems often lacks semantic coherence due to ignoring textual structure, limiting retrieval quality.

Method: Integrates hierarchical text segmentation and clustering to create meaningful chunks, using segment and cluster-level vectors for retrieval.

Result: Outperforms traditional chunking on NarrativeQA, QuALITY, and QASPER datasets.

Conclusion: The framework enhances RAG by improving semantic coherence and retrieval precision.

Abstract: Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies
for retrieval, which enhance large language models (LLMs) by enabling them to
access external knowledge, ensuring that the retrieved information is
up-to-date and domain-specific. However, traditional methods often fail to
create chunks that capture sufficient semantic meaning, as they do not account
for the underlying textual structure. This paper proposes a novel framework
that enhances RAG by integrating hierarchical text segmentation and clustering
to generate more meaningful and semantically coherent chunks. During inference,
the framework retrieves information by leveraging both segment-level and
cluster-level vector representations, thereby increasing the likelihood of
retrieving more precise and contextually relevant information. Evaluations on
the NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method
achieved improved results compared to traditional chunking techniques.

</details>


### [410] [Tiny Reward Models](https://arxiv.org/abs/2507.09973)
*Sarah Pan*

Main category: cs.CL

TL;DR: TinyRM, a small bidirectional masked language model (400M parameters), rivals larger models (175x bigger) in reasoning and safety preference tasks, using efficient tuning strategies like FLAN prompting and DoRA.


<details>
  <summary>Details</summary>
Motivation: Address the high inference costs of large decoder-based reward models in RLHF by proposing a smaller, efficient alternative.

Method: Combines FLAN-style prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to optimize performance with fewer resources.

Result: TinyRM matches or outperforms larger models on RewardBench, especially in reasoning tasks, with domain-specific tuning.

Conclusion: Lightweight bidirectional architectures like TinyRM show promise as scalable, efficient alternatives for preference modeling, though challenges remain in generalist models and conversational tasks.

Abstract: Large decoder-based language models have become the dominant architecture for
reward modeling in reinforcement learning from human feedback (RLHF). However,
as reward models are increasingly deployed in test-time strategies, their
inference costs become a growing concern. We present TinyRM, a family of small,
bidirectional masked language models (MLMs) with as few as 400 million
parameters, that rival the capabilities of models over 175 times larger on
reasoning and safety preference modeling tasks. TinyRM combines FLAN-style
prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to
achieve strong performance on RewardBench, despite using significantly fewer
resources. Our experiments suggest that small models benefit from
domain-specific tuning strategies, particularly in reasoning, where lightweight
finetuning methods are especially effective. While challenges remain in
building generalist models and conversational preference modeling, our
preliminary results highlight the promise of lightweight bidirectional
architectures as efficient, scalable alternatives for preference modeling.

</details>


### [411] [TextOmics-Guided Diffusion for Hit-like Molecular Generation](https://arxiv.org/abs/2507.09982)
*Hang Yuan,Chen Li,Wenjun Ma,Yuncheng Jiang*

Main category: cs.CL

TL;DR: TextOmics introduces a benchmark for aligning omics expressions with molecular textual descriptions, enabling hit-like molecular generation. ToDi, a generative framework, outperforms existing methods by leveraging multi-level associations and conditional diffusion.


<details>
  <summary>Details</summary>
Motivation: The lack of heterogeneous data and unified frameworks for integrating diverse molecular representations in drug discovery motivates the development of TextOmics and ToDi.

Method: TextOmics provides a dataset aligning omics expressions with textual descriptions. ToDi uses two encoders (OmicsEn and TextEn) and conditional diffusion (DiffGen) for controllable molecular generation.

Result: ToDi outperforms state-of-the-art methods and shows potential in zero-shot therapeutic molecular generation.

Conclusion: TextOmics and ToDi bridge the gap in molecular generation, offering a robust framework for target-specific drug discovery.

Abstract: Hit-like molecular generation with therapeutic potential is essential for
target-specific drug discovery. However, the field lacks heterogeneous data and
unified frameworks for integrating diverse molecular representations. To bridge
this gap, we introduce TextOmics, a pioneering benchmark that establishes
one-to-one correspondences between omics expressions and molecular textual
descriptions. TextOmics provides a heterogeneous dataset that facilitates
molecular generation through representations alignment. Built upon this
foundation, we propose ToDi, a generative framework that jointly conditions on
omics expressions and molecular textual descriptions to produce biologically
relevant, chemically valid, hit-like molecules. ToDi leverages two encoders
(OmicsEn and TextEn) to capture multi-level biological and semantic
associations, and develops conditional diffusion (DiffGen) for controllable
generation. Extensive experiments confirm the effectiveness of TextOmics and
demonstrate ToDi outperforms existing state-of-the-art approaches, while also
showcasing remarkable potential in zero-shot therapeutic molecular generation.
Sources are available at: https://github.com/hala-ToDi.

</details>


### [412] [Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media](https://arxiv.org/abs/2507.10008)
*Jun Li,Xiangmeng Wang,Haoyang Li,Yifei Yan,Hong Va Leong,Ling Feng,Nancy Xiaonan Yu,Qing Li*

Main category: cs.CL

TL;DR: A novel framework predicts suicide risk by analyzing dynamic interactions of risk and protective factors, outperforming existing models and offering interpretable insights for clinicians.


<details>
  <summary>Details</summary>
Motivation: Existing models focus only on risk factors and lack temporal dynamics, ignoring protective factors that mitigate suicide risk.

Method: Proposes a Protective Factor-Aware Dataset from Reddit posts and a Dynamic Factors Influence Learning approach to model fluctuating suicide risk.

Result: The model significantly outperforms state-of-the-art models and large language models, providing interpretable weights for clinical use.

Conclusion: The framework enhances suicide risk prediction by integrating dynamic risk and protective factors, aiding targeted interventions.

Abstract: Suicide is a critical global health issue that requires urgent attention.
Even though prior work has revealed valuable insights into detecting current
suicide risk on social media, little attention has been paid to developing
models that can predict subsequent suicide risk over time, limiting their
ability to capture rapid fluctuations in individuals' mental state transitions.
In addition, existing work ignores protective factors that play a crucial role
in suicide risk prediction, focusing predominantly on risk factors alone.
Protective factors such as social support and coping strategies can mitigate
suicide risk by moderating the impact of risk factors. Therefore, this study
proposes a novel framework for predicting subsequent suicide risk by jointly
learning the dynamic influence of both risk factors and protective factors on
users' suicide risk transitions. We propose a novel Protective Factor-Aware
Dataset, which is built from 12 years of Reddit posts along with comprehensive
annotations of suicide risk and both risk and protective factors. We also
introduce a Dynamic Factors Influence Learning approach that captures the
varying impact of risk and protective factors on suicide risk transitions,
recognizing that suicide risk fluctuates over time according to established
psychological theories. Our thorough experiments demonstrate that the proposed
model significantly outperforms state-of-the-art models and large language
models across three datasets. In addition, the proposed Dynamic Factors
Influence Learning provides interpretable weights, helping clinicians better
understand suicidal patterns and enabling more targeted intervention
strategies.

</details>


### [413] [GeLaCo: An Evolutionary Approach to Layer Compression](https://arxiv.org/abs/2507.10059)
*David Ponce,Thierry Etchegoyhen,Javier Del Ser*

Main category: cs.CL

TL;DR: GeLaCo introduces an evolutionary approach for compressing LLMs via layer collapse, outperforming existing methods in efficiency and quality.


<details>
  <summary>Details</summary>
Motivation: LLMs face deployment barriers due to high computational demands; model compression is needed to mitigate this.

Method: GeLaCo uses population-based search and a similarity fitness function for efficient compression space exploration, supporting single and multi-objective optimization.

Result: GeLaCo outperforms state-of-the-art methods in perplexity and generative evaluations.

Conclusion: GeLaCo provides a scalable and effective solution for LLM compression, balancing quality and efficiency.

Abstract: Large Language Models (LLM) have achieved remarkable performance across a
large number of tasks, but face critical deployment and usage barriers due to
substantial computational requirements. Model compression methods, which aim to
reduce model size while preserving its capacity, are an important means to
mitigate these issues. Promising approaches along these lines, such as
structured pruning, typically require costly empirical search for optimal
variants and may run the risk of ignoring better solutions. In this work we
introduce GeLaCo, an evolutionary approach to LLM compression via layer
collapse. Our approach supports an efficient exploration of the compression
solution space via population-based search and a module-wise similarity fitness
function capturing attention, feed-forward, and hidden state representations.
GeLaCo also supports both single and multi-objective evolutionary compression
search, establishing the first Pareto frontier along compression and quality
axes. We evaluate GeLaCo solutions via both perplexity-based and generative
evaluations over foundational and instruction-tuned models, outperforming
state-of-the-art alternatives.

</details>


### [414] [Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires](https://arxiv.org/abs/2507.10073)
*Simon Münker*

Main category: cs.CL

TL;DR: LLMs homogenize moral diversity, failing to represent cultural nuances, and larger models don't improve cultural fidelity. This challenges their use in social science and highlights AI alignment limitations.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs accurately represent diverse human moral values across cultures or merely average them, exposing gaps in AI alignment.

Method: Applied the Moral Foundations Questionnaire across 19 cultural contexts, comparing LLMs' outputs to human baseline data.

Result: LLMs systematically flatten moral diversity, with no consistent improvement from larger models.

Conclusion: Current AI alignment lacks cultural nuance; new objectives and metrics are needed to ensure diverse human values are represented.

Abstract: Are AI systems truly representing human values, or merely averaging across
them? Our study suggests a concerning reality: Large Language Models (LLMs)
fail to represent diverse cultural moral frameworks despite their linguistic
capabilities. We expose significant gaps between AI-generated and human moral
intuitions by applying the Moral Foundations Questionnaire across 19 cultural
contexts. Comparing multiple state-of-the-art LLMs' origins against human
baseline data, we find these models systematically homogenize moral diversity.
Surprisingly, increased model size doesn't consistently improve cultural
representation fidelity. Our findings challenge the growing use of LLMs as
synthetic populations in social science research and highlight a fundamental
limitation in current AI alignment approaches. Without data-driven alignment
beyond prompting, these systems cannot capture the nuanced, culturally-specific
moral intuitions. Our results call for more grounded alignment objectives and
evaluation metrics to ensure AI systems represent diverse human values rather
than flattening the moral landscape.

</details>


### [415] [Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning](https://arxiv.org/abs/2507.10085)
*Chenxi Huang,Shaotian Yan,Liang Xie,Binbin Lin,Sinan Fan,Yue Xin,Deng Cai,Chen Shen,Jieping Ye*

Main category: cs.CL

TL;DR: CRFT improves reasoning tasks by dynamically optimizing critical representations in a low-rank subspace, outperforming native ReFT and boosting few-shot performance.


<details>
  <summary>Details</summary>
Motivation: Native ReFT's fixed-position representation modifications are suboptimal for complex reasoning tasks, where critical representations significantly impact outputs.

Method: CRFT identifies and optimizes critical representations through information flow analysis, operating in a low-rank subspace while freezing the base model.

Result: Validated on eight benchmarks, CRFT enhances reasoning performance and boosts one-shot accuracy by 16.4%.

Conclusion: CRFT offers a lightweight, powerful alternative to traditional PEFT methods, unlocking representation-level optimization potential for reasoning tasks.

Abstract: Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient
Fine-Tuning (PEFT) method, has attracted widespread attention for significantly
improving parameter efficiency by editing representation space alone. In this
work, we investigate applying ReFT to complex reasoning tasks. However,
directly using the native ReFT method, which modifies fixed representations at
the beginning and end of each layer, yields suboptimal performance, as these
fixed-position representations have uncertain impact on the outputs. We observe
that, in complex reasoning tasks, there often exist certain critical
representations. These representations either integrate significant information
from preceding layers or regulate subsequent layer representations. Through
layer-by-layer propagation, they exert a substantial influence on the final
output. Naturally, fine-tuning these critical representations has the potential
to greatly enhance reasoning performance. Building upon these insights, we
propose Critical Representation Fine-Tuning (CRFT), a novel method that
identifies and optimizes these critical representations through information
flow analysis. CRFT operates within a supervised learning framework,
dynamically optimizing critical representations in a low-rank linear subspace
while freezing the base model. The effectiveness and efficiency of our method
are validated across eight benchmarks for arithmetic and commonsense reasoning,
using LLaMA and Mistral model families. Furthermore, our method also adapts
effectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work
highlights the untapped potential of representation-level optimization for CoT
reasoning, offering a lightweight yet powerful alternative to traditional PEFT
methods.

</details>


### [416] [Fusing Large Language Models with Temporal Transformers for Time Series Forecasting](https://arxiv.org/abs/2507.10098)
*Chen Su,Yuanhe Tian,Qinyu Liu,Jun Zhang,Yan Song*

Main category: cs.CL

TL;DR: A novel Transformer-based architecture combines LLMs and vanilla Transformers to improve time series forecasting by integrating high-level semantic patterns with temporal dynamics.


<details>
  <summary>Details</summary>
Motivation: LLMs excel in text tasks but underperform in time series forecasting due to gaps between text and numerical data. Vanilla Transformers lack semantic understanding.

Method: A hybrid model fuses representations from LLMs (semantic patterns) and vanilla Transformers (temporal dynamics) for better forecasting.

Result: The fused representation improves accuracy by combining historical temporal and semantic patterns.

Conclusion: The proposed architecture effectively bridges the gap between LLMs and Transformers, enhancing time series forecasting performance.

Abstract: Recently, large language models (LLMs) have demonstrated powerful
capabilities in performing various tasks and thus are applied by recent studies
to time series forecasting (TSF) tasks, which predict future values with the
given historical time series. Existing LLM-based approaches transfer knowledge
learned from text data to time series prediction using prompting or fine-tuning
strategies. However, LLMs are proficient at reasoning over discrete tokens and
semantic patterns but are not initially designed to model continuous numerical
time series data. The gaps between text and time series data lead LLMs to
achieve inferior performance to a vanilla Transformer model that is directly
trained on TSF data. However, the vanilla Transformers often struggle to learn
high-level semantic patterns. In this paper, we design a novel
Transformer-based architecture that complementarily leverages LLMs and vanilla
Transformers, so as to integrate the high-level semantic representations
learned by LLMs into the temporal information encoded by time series
Transformers, where a hybrid representation is obtained by fusing the
representations from the LLM and the Transformer. The resulting fused
representation contains both historical temporal dynamics and semantic
variation patterns, allowing our model to predict more accurate future values.
Experiments on benchmark datasets demonstrate the effectiveness of the proposed
approach.

</details>


### [417] [Task-Based Flexible Feature Distillation for LLMs](https://arxiv.org/abs/2507.10155)
*Khouloud Saadi,Di Wang*

Main category: cs.CL

TL;DR: Proposes a task-based feature distillation method for LLMs with varying hidden sizes, avoiding extra parameters and improving performance.


<details>
  <summary>Details</summary>
Motivation: Traditional feature distillation methods limit student architecture flexibility and degrade performance when aligning feature spaces with linear projectors.

Method: Identifies task-relevant hidden units in the teacher and directly distills their activations to the student, introducing no new parameters.

Result: Achieves up to 3% performance gain over linear projection baselines across tasks like classification and summarization.

Conclusion: The method offers a flexible, parameter-free solution for knowledge transfer between LLMs of different architectures.

Abstract: Knowledge Distillation (KD) in general and feature distillation in particular
are promising techniques for reducing the high computational demand of large
language models (LLMs). However, traditional feature KD methods typically
assume that the teacher and the student share the same hidden size, limiting
the flexibility of the student's architecture. A common solution to this
problem involves training a linear projector to align their feature spaces, but
this introduces additional parameters that must be learned from scratch and
often degrades performance on downstream tasks, especially in generative
settings. To address this issue, in this work, we propose a novel task-based
feature distillation method that enables knowledge transfer between teacher and
student models with different hidden layer dimensions, without introducing any
new parameters. Leveraging the insight that only a subset of LLM components
contribute significantly to a specific downstream task, our approach identifies
the most task-relevant hidden units in the teacher and directly distills their
activations to the student. Our method is flexible and easily integrates with
other distillation frameworks. Empirical results show consistent improvements
over prior approaches across diverse tasks, including classification,
instruction-following, and summarization, achieving up to a 3\% performance
gain over the linear projection baseline.

</details>


### [418] [Abusive text transformation using LLMs](https://arxiv.org/abs/2507.10177)
*Rohitash Chandra,Jiyong Choi*

Main category: cs.CL

TL;DR: The study explores using LLMs (Gemini, GPT-4o, DeepSeek, Groq) to transform abusive text into non-abusive versions while preserving intent, sentiment, and semantics. Groq's results differ significantly from others, while GPT-4o and DeepSeek-V3 show similarities.


<details>
  <summary>Details</summary>
Motivation: LLMs' effectiveness in classifying and transforming abusive text is underexplored. The study aims to leverage LLMs for this task while retaining text intent.

Method: Evaluate LLMs (Gemini, GPT-4o, DeepSeek, Groq) on abusive text transformation. Assess performance via sentiment and semantic analysis of raw and transformed datasets.

Result: Groq yields notably different results compared to other LLMs. GPT-4o and DeepSeek-V3 exhibit similarities in performance.

Conclusion: LLMs can transform abusive text effectively, with Groq standing out for its distinct results, while GPT-4o and DeepSeek-V3 perform similarly.

Abstract: Although Large Language Models (LLMs) have demonstrated significant
advancements in natural language processing tasks, their effectiveness in the
classification and transformation of abusive text into non-abusive versions
remains an area for exploration. In this study, we aim to use LLMs to transform
abusive text (tweets and reviews) featuring hate speech and swear words into
non-abusive text, while retaining the intent of the text. We evaluate the
performance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and
Groq, on their ability to identify abusive text. We them to transform and
obtain a text that is clean from abusive and inappropriate content but
maintains a similar level of sentiment and semantics, i.e. the transformed text
needs to maintain its message. Afterwards, we evaluate the raw and transformed
datasets with sentiment analysis and semantic analysis. Our results show Groq
provides vastly different results when compared with other LLMs. We have
identified similarities between GPT-4o and DeepSeek-V3.

</details>


### [419] [Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects](https://arxiv.org/abs/2507.10216)
*Renad Al-Monef,Hassan Alhuzali,Nora Alturayeif,Ashwag Alasmari*

Main category: cs.CL

TL;DR: The paper introduces Absher, a benchmark for evaluating LLMs on Saudi dialects, revealing gaps in cultural and contextual understanding.


<details>
  <summary>Details</summary>
Motivation: Assess LLMs' understanding of regional dialects and cultural nuances in Arabic NLP, focusing on Saudi Arabia.

Method: Developed Absher, a benchmark with 18,000+ multiple-choice questions across six categories, evaluated on state-of-the-art LLMs.

Result: Notable performance gaps in cultural inference and contextual understanding tasks.

Conclusion: Highlights the need for dialect-aware training and culturally aligned evaluation methods for LLMs in Arabic applications.

Abstract: As large language models (LLMs) become increasingly central to Arabic NLP
applications, evaluating their understanding of regional dialects and cultural
nuances is essential, particularly in linguistically diverse settings like
Saudi Arabia. This paper introduces \texttt{Absher}, a comprehensive benchmark
specifically designed to assess LLMs performance across major Saudi dialects.
\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six
distinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,
Cultural Interpretation, and Location Recognition. These questions are derived
from a curated dataset of dialectal words, phrases, and proverbs sourced from
various regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,
including multilingual and Arabic-specific models. We also provide detailed
insights into their capabilities and limitations. Our results reveal notable
performance gaps, particularly in tasks requiring cultural inference or
contextual understanding. Our findings highlight the urgent need for
dialect-aware training and culturally aligned evaluation methodologies to
improve LLMs performance in real-world Arabic applications.

</details>


### [420] [Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation](https://arxiv.org/abs/2507.10326)
*Muzhaffar Hazman,Minh-Khoi Pham,Shweta Soundararajan,Goncalo Mordido,Leonardo Custode,David Lynch,Giorgio Cruciata,Yucheng Shi,Hongmeng Song,Wang Chao,Pan Yue,Aleksandar Milenovic,Alexandros Agapitos*

Main category: cs.CL

TL;DR: An evolutionary search approach for automated discrete prompt optimization outperforms state-of-the-art methods on smaller LLMs for complex tasks.


<details>
  <summary>Details</summary>
Motivation: Current prompt engineering methods are evaluated on simple tasks and large models, but complex tasks and smaller models require more robust solutions.

Method: A two-phase approach: grammar-guided genetic programming for prompt synthesis, followed by local search for fine-tuning.

Result: Outperforms PromptWizard, OPRO, and RL-Prompt on smaller LLMs in challenging tasks, with minimal performance degradation.

Conclusion: The proposed method is effective for optimizing prompts in complex scenarios, especially for smaller models.

Abstract: Prompt engineering has proven to be a crucial step in leveraging pretrained
large language models (LLMs) in solving various real-world tasks. Numerous
solutions have been proposed that seek to automate prompt engineering by using
the model itself to edit prompts. However, the majority of state-of-the-art
approaches are evaluated on tasks that require minimal prompt templates and on
very large and highly capable LLMs. In contrast, solving complex tasks that
require detailed information to be included in the prompt increases the amount
of text that needs to be optimised. Furthermore, smaller models have been shown
to be more sensitive to prompt design. To address these challenges, we propose
an evolutionary search approach to automated discrete prompt optimisation
consisting of two phases. In the first phase, grammar-guided genetic
programming is invoked to synthesise prompt-creating programmes by searching
the space of programmes populated by function compositions of syntactic,
dictionary-based and LLM-based prompt-editing functions. In the second phase,
local search is applied to explore the neighbourhoods of best-performing
programmes in an attempt to further fine-tune their performance. Our approach
outperforms three state-of-the-art prompt optimisation approaches,
PromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose
LLMs in four domain-specific challenging tasks. We also illustrate several
examples where these benchmark methods suffer relatively severe performance
degradation, while our approach improves performance in almost all task-model
combinations, only incurring minimal degradation when it does not.

</details>


### [421] [Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach](https://arxiv.org/abs/2507.10330)
*Mohammed Bouri,Adnane Saoud*

Main category: cs.CL

TL;DR: The paper introduces Growth Bound Matrices (GBM) to enhance NLP model robustness against adversarial attacks, focusing on LSTM, S4, and CNN architectures, and demonstrates improved performance over baselines.


<details>
  <summary>Details</summary>
Motivation: Despite progress in NLP, models are still vulnerable to adversarial attacks like synonym substitutions, especially in recurrent networks and state space models (SSMs), which are understudied.

Method: A novel regularization technique using Growth Bound Matrices (GBM) is proposed to mitigate input perturbations' impact on model outputs, tested on LSTM, S4, and CNN architectures.

Result: Experiments show the method improves adversarial robustness by up to 8.8% over baselines, outperforming state-of-the-art defenses.

Conclusion: The GBM technique effectively enhances NLP model robustness and generalizes well on clean text, providing the first systematic analysis of SSM (S4) robustness.

Abstract: Despite advancements in Natural Language Processing (NLP), models remain
vulnerable to adversarial attacks, such as synonym substitutions. While prior
work has focused on improving robustness for feed-forward and convolutional
architectures, the robustness of recurrent networks and modern state space
models (SSMs), such as S4, remains understudied. These architectures pose
unique challenges due to their sequential processing and complex parameter
dynamics. In this paper, we introduce a novel regularization technique based on
Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the
impact of input perturbations on model outputs. We focus on computing the GBM
for three architectures: Long Short-Term Memory (LSTM), State Space models
(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance
resilience against word substitution attacks, (2) improve generalization on
clean text, and (3) providing the first systematic analysis of SSM (S4)
robustness. Extensive experiments across multiple architectures and benchmark
datasets demonstrate that our method improves adversarial robustness by up to
8.8% over existing baselines. These results highlight the effectiveness of our
approach, outperforming several state-of-the-art methods in adversarial
defense. Codes are available at https://github.com/BouriMohammed/GBM

</details>


### [422] [Using AI to replicate human experimental results: a motion study](https://arxiv.org/abs/2507.10342)
*Rosa Illan Castillo,Javier Valenzuela*

Main category: cs.CL

TL;DR: The paper demonstrates that large language models (LLMs) like GPT-4 can reliably replicate human judgments in linguistic research, particularly in affective meanings of temporal expressions, with strong correlations between human and AI responses.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability of LLMs in replicating nuanced human judgments in linguistic research, specifically for affective meanings in temporal expressions.

Method: Four psycholinguistic studies were conducted with human participants and replicated using an LLM, comparing responses on emergent meanings, valence shifts, verb choice, and sentence-emoji associations.

Result: Strong correlations (Spearman's rho = .73-.96) between human and AI responses were found, with minor divergences not affecting interpretative outcomes.

Conclusion: LLMs can augment human-based linguistic research, offering scalability without compromising validity, and serve as credible collaborators in linguistic inquiry.

Abstract: This paper explores the potential of large language models (LLMs) as reliable
analytical tools in linguistic research, focusing on the emergence of affective
meanings in temporal expressions involving manner-of-motion verbs. While LLMs
like GPT-4 have shown promise across a range of tasks, their ability to
replicate nuanced human judgements remains under scrutiny. We conducted four
psycholinguistic studies (on emergent meanings, valence shifts, verb choice in
emotional contexts, and sentence-emoji associations) first with human
participants and then replicated the same tasks using an LLM. Results across
all studies show a striking convergence between human and AI responses, with
statistical analyses (e.g., Spearman's rho = .73-.96) indicating strong
correlations in both rating patterns and categorical choices. While minor
divergences were observed in some cases, these did not alter the overall
interpretative outcomes. These findings offer compelling evidence that LLMs can
augment traditional human-based experimentation, enabling broader-scale studies
without compromising interpretative validity. This convergence not only
strengthens the empirical foundation of prior human-based findings but also
opens possibilities for hypothesis generation and data expansion through AI.
Ultimately, our study supports the use of LLMs as credible and informative
collaborators in linguistic inquiry.

</details>


### [423] [Meanings are like Onions: a Layered Approach to Metaphor Processing](https://arxiv.org/abs/2507.10354)
*Silvia Cappa,Anna Sofia Lippolis,Stefano Zoia*

Main category: cs.CL

TL;DR: A stratified model for metaphor processing is proposed, treating meaning as a multi-layered structure (content analysis, conceptual blending, pragmatic intentionality) to enhance computational metaphor interpretation.


<details>
  <summary>Details</summary>
Motivation: To address the complexity of metaphorical meaning beyond flat mappings, integrating cognitive and pragmatic dimensions for richer computational understanding.

Method: A three-level framework: (1) content analysis, (2) conceptual blending, (3) pragmatic intentionality, unifying these layers for formal computational representation.

Result: The model enables deeper, context-sensitive metaphor reasoning in computational systems, moving beyond surface associations.

Conclusion: The stratified approach provides a cognitively grounded and pragmatic framework for advanced metaphor processing in computational contexts.

Abstract: Metaphorical meaning is not a flat mapping between concepts, but a complex
cognitive phenomenon that integrates multiple levels of interpretation. In this
paper, we propose a stratified model of metaphor processing that treats meaning
as an onion: a multi-layered structure comprising (1) content analysis, (2)
conceptual blending, and (3) pragmatic intentionality. This three-dimensional
framework allows for a richer and more cognitively grounded approach to
metaphor interpretation in computational systems. At the first level, metaphors
are annotated through basic conceptual elements. At the second level, we model
conceptual combinations, linking components to emergent meanings. Finally, at
the third level, we introduce a pragmatic vocabulary to capture speaker intent,
communicative function, and contextual effects, aligning metaphor understanding
with pragmatic theories. By unifying these layers into a single formal
framework, our model lays the groundwork for computational methods capable of
representing metaphorical meaning beyond surface associations, toward deeper,
more context-sensitive reasoning.

</details>


### [424] [From Sequence to Structure: Uncovering Substructure Reasoning in Transformers](https://arxiv.org/abs/2507.10435)
*Xinnan Dai,Kai Yang,Jay Revolinsky,Kai Guo,Aoran Wang,Bohang Zhang,Jiliang Tang*

Main category: cs.CL

TL;DR: The paper explores how decoder-only Transformers understand graph structures, introducing Induced Substructure Filtration (ISF) to explain substructure extraction in LLMs.


<details>
  <summary>Details</summary>
Motivation: To understand how sequence-based Transformers interpret graph structures embedded in text and perform substructure extraction tasks.

Method: Empirical and theoretical analysis of substructure extraction, introducing ISF to capture substructure identification in multi-layer transformers.

Result: LLMs exhibit consistent internal dynamics for substructure extraction, and Transformers can handle diverse graph types, including molecular graphs.

Conclusion: The findings provide insights into how sequence-based Transformers perform substructure extraction, highlighting their broader capabilities in graph reasoning.

Abstract: Recent studies suggest that large language models (LLMs) possess the
capability to solve graph reasoning tasks. Notably, even when graph structures
are embedded within textual descriptions, LLMs can still effectively answer
related questions. This raises a fundamental question: How can a decoder-only
Transformer architecture understand underlying graph structures? To address
this, we start with the substructure extraction task, interpreting the inner
mechanisms inside the transformers and analyzing the impact of the input
queries. Specifically, through both empirical results and theoretical analysis,
we present Induced Substructure Filtration (ISF), a perspective that captures
the substructure identification in the multi-layer transformers. We further
validate the ISF process in LLMs, revealing consistent internal dynamics across
layers. Building on these insights, we explore the broader capabilities of
Transformers in handling diverse graph types. Specifically, we introduce the
concept of thinking in substructures to efficiently extract complex composite
patterns, and demonstrate that decoder-only Transformers can successfully
extract substructures from attributed graphs, such as molecular graphs.
Together, our findings offer a new insight on how sequence-based Transformers
perform the substructure extraction task over graph data.

</details>


### [425] [Referential ambiguity and clarification requests: comparing human and LLM behaviour](https://arxiv.org/abs/2507.10445)
*Chris Madge,Matthew Purver,Massimo Poesio*

Main category: cs.CL

TL;DR: The paper investigates LLMs' ability to ask clarification questions in task-oriented dialogues, comparing human and LLM behaviors in handling ambiguity. A new corpus is created for this purpose, revealing weak human-LLM correlation and differing tendencies in question types. Reasoning in LLMs improves question frequency and relevancy.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs handle ambiguity in dialogues compared to humans, and whether their ability to ask clarification questions is linked to reasoning.

Method: A new corpus combining existing annotations from the Minecraft Dialogue Corpus is used to compare human and LLM behaviors in generating clarification questions. Different reasoning approaches in LLMs are tested.

Result: Humans rarely ask clarification questions for referential ambiguity but often for task uncertainty, while LLMs do the opposite. Reasoning in LLMs increases question frequency and relevancy.

Conclusion: LLMs' ability to ask clarification questions is influenced by reasoning, but their behavior differs significantly from humans, suggesting further research is needed to align their responses with human-like dialogue strategies.

Abstract: In this work we examine LLMs' ability to ask clarification questions in
task-oriented dialogues that follow the asynchronous
instruction-giver/instruction-follower format. We present a new corpus that
combines two existing annotations of the Minecraft Dialogue Corpus -- one for
reference and ambiguity in reference, and one for SDRT including clarifications
-- into a single common format providing the necessary information to
experiment with clarifications and their relation to ambiguity. With this
corpus we compare LLM actions with original human-generated clarification
questions, examining how both humans and LLMs act in the case of ambiguity. We
find that there is only a weak link between ambiguity and humans producing
clarification questions in these dialogues, and low correlation between humans
and LLMs. Humans hardly ever produce clarification questions for referential
ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce
more clarification questions for referential ambiguity, but less so for task
uncertainty. We question if LLMs' ability to ask clarification questions is
predicated on their recent ability to simulate reasoning, and test this with
different reasoning approaches, finding that reasoning does appear to increase
question frequency and relevancy.

</details>


### [426] [From BERT to Qwen: Hate Detection across architectures](https://arxiv.org/abs/2507.10468)
*Ariadna Mon,Saúl Fenollosa,Jon Lecumberri*

Main category: cs.CL

TL;DR: The study compares classic bidirectional transformer encoders and ultra-large autoregressive LLMs for hate-speech detection to verify if scale improves performance.


<details>
  <summary>Details</summary>
Motivation: Online platforms face challenges in balancing hate-speech detection and avoiding over-censorship. The potential of ultra-large LLMs for deeper context-awareness in this task is unverified.

Method: Benchmarking classic bidirectional transformer encoders and next-generation LLMs on curated corpora of online interactions labeled for hate speech.

Result: The study evaluates whether the extra scale of LLMs translates to better hate-speech detection in real-world text.

Conclusion: The research aims to clarify if ultra-large LLMs outperform classic models in practical hate-speech detection.

Abstract: Online platforms struggle to curb hate speech without over-censoring
legitimate discourse. Early bidirectional transformer encoders made big
strides, but the arrival of ultra-large autoregressive LLMs promises deeper
context-awareness. Whether this extra scale actually improves practical
hate-speech detection on real-world text remains unverified. Our study puts
this question to the test by benchmarking both model families, classic encoders
and next-generation LLMs, on curated corpora of online interactions for
hate-speech detection (Hate or No Hate).

</details>


### [427] [MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking](https://arxiv.org/abs/2507.10472)
*Mohamed T. Younes,Omar Walid,Mai Hassan,Ali Hamdi*

Main category: cs.CL

TL;DR: MLAR is a novel RPA framework using LLMs to automate resume screening, outperforming leading RPA platforms in speed and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing bottlenecks in traditional recruitment processes like resume screening and candidate shortlisting due to time and resource constraints.

Method: Uses LLMs in three layers: job posting analysis, resume parsing, and similarity matching, integrated into RPA pipelines.

Result: Processed 2,400 resumes at 5.4 seconds each, reducing time by 16.9% and 17.1% compared to Automation Anywhere and UiPath.

Conclusion: MLAR offers an efficient, accurate, and scalable solution for modern recruitment needs.

Abstract: This paper introduces an innovative Applicant Tracking System (ATS) enhanced
by a novel Robotic process automation (RPA) framework or as further referred to
as MLAR. Traditional recruitment processes often encounter bottlenecks in
resume screening and candidate shortlisting due to time and resource
constraints. MLAR addresses these challenges employing Large Language Models
(LLMs) in three distinct layers: extracting key characteristics from job
postings in the first layer, parsing applicant resume to identify education,
experience, skills in the second layer, and similarity matching in the third
layer. These features are then matched through advanced semantic algorithms to
identify the best candidates efficiently. Our approach integrates seamlessly
into existing RPA pipelines, automating resume parsing, job matching, and
candidate notifications. Extensive performance benchmarking shows that MLAR
outperforms the leading RPA platforms, including UiPath and Automation
Anywhere, in high-volume resume-processing tasks. When processing 2,400
resumes, MLAR achieved an average processing time of 5.4 seconds per resume,
reducing processing time by approximately 16.9% compared to Automation Anywhere
and 17.1% compared to UiPath. These results highlight the potential of MLAR to
transform recruitment workflows by providing an efficient, accurate, and
scalable solution tailored to modern hiring needs.

</details>


### [428] [Can You Detect the Difference?](https://arxiv.org/abs/2507.10475)
*İsmail Tarım,Aytuğ Onan*

Main category: cs.CL

TL;DR: The paper compares diffusion-generated (LLaDA) and autoregressive-generated (LLaMA) text, showing LLaDA mimics human text better, challenging current detectors. It calls for diffusion-aware detection methods.


<details>
  <summary>Details</summary>
Motivation: The rise of LLMs necessitates reliable detection of AI-generated text, but current methods (focused on AR models) may fail for diffusion-based outputs.

Method: A systematic comparison of 2,000 samples of LLaDA (diffusion) and LLaMA (AR) text using metrics like perplexity, burstiness, lexical diversity, readability, and BLEU/ROUGE scores.

Result: LLaDA closely resembles human text in perplexity and burstiness, leading to high false negatives in AR-oriented detectors. LLaMA has lower perplexity but reduced lexical fidelity. Single metrics fail to distinguish diffusion outputs.

Conclusion: The study underscores the need for diffusion-aware detectors and suggests hybrid models, diffusion-specific stylometric signatures, and robust watermarking as future directions.

Abstract: The rapid advancement of large language models (LLMs) has raised concerns
about reliably detecting AI-generated text. Stylometric metrics work well on
autoregressive (AR) outputs, but their effectiveness on diffusion-based models
is unknown. We present the first systematic comparison of diffusion-generated
text (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,
burstiness, lexical diversity, readability, and BLEU/ROUGE scores show that
LLaDA closely mimics human text in perplexity and burstiness, yielding high
false-negative rates for AR-oriented detectors. LLaMA shows much lower
perplexity but reduced lexical fidelity. Relying on any single metric fails to
separate diffusion outputs from human writing. We highlight the need for
diffusion-aware detectors and outline directions such as hybrid models,
diffusion-specific stylometric signatures, and robust watermarking.

</details>


### [429] [Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation](https://arxiv.org/abs/2507.10524)
*Sangmin Bae,Yujin Kim,Reza Bayat,Sungnyun Kim,Jiyoun Ha,Tal Schuster,Adam Fisch,Hrayr Harutyunyan,Ziwei Ji,Aaron Courville,Se-Young Yun*

Main category: cs.CL

TL;DR: Mixture-of-Recursions (MoR) combines parameter sharing and adaptive computation in a Recursive Transformer, improving efficiency and performance across model scales.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational and memory costs of scaling language models by unifying parameter sharing and adaptive computation.

Method: MoR uses shared layers across recursion steps and lightweight routers for adaptive token-level recursion depth, optimizing attention computation and memory usage.

Result: MoR outperforms baselines in perplexity, few-shot accuracy, and throughput, forming a new Pareto frontier for efficiency and performance.

Conclusion: MoR effectively achieves large-model quality without the associated costs, offering a scalable solution for efficient language modeling.

Abstract: Scaling language models unlocks impressive capabilities, but the accompanying
computational and memory demands make both training and deployment expensive.
Existing efficiency efforts typically target either parameter sharing or
adaptive computation, leaving open the question of how to attain both
simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework
that combines the two axes of efficiency inside a single Recursive Transformer.
MoR reuses a shared stack of layers across recursion steps to achieve parameter
efficiency, while lightweight routers enable adaptive token-level thinking by
dynamically assigning different recursion depths to individual tokens. This
allows MoR to focus quadratic attention computation only among tokens still
active at a given recursion depth, further improving memory access efficiency
by selectively caching only their key-value pairs. Beyond these core
mechanisms, we also propose a KV sharing variant that reuses KV pairs from the
first recursion, specifically designed to decrease prefill latency and memory
footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms
a new Pareto frontier: at equal training FLOPs and smaller model sizes, it
significantly lowers validation perplexity and improves few-shot accuracy,
while delivering higher throughput compared with vanilla and existing recursive
baselines. These gains demonstrate that MoR is an effective path towards
large-model quality without incurring large-model cost.

</details>


### [430] [CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks](https://arxiv.org/abs/2507.10535)
*Hongchao Jiang,Yiming Chen,Yushi Cao,Hung-yi Lee,Robby T. Tan*

Main category: cs.CL

TL;DR: CodeJudgeBench evaluates LLMs as judges in coding tasks, finding thinking models outperform non-thinking ones, but with notable randomness and sensitivity to response order. Pairwise comparison and retaining comments improve judging.


<details>
  <summary>Details</summary>
Motivation: The effectiveness of LLM-as-a-Judge in coding tasks is underexplored due to lack of benchmarks, prompting the creation of CodeJudgeBench.

Method: Introduces CodeJudgeBench to evaluate 26 LLM-as-a-Judge models across code generation, repair, and unit test generation tasks.

Result: Thinking models outperform non-thinking ones, but judgments are inconsistent and sensitive to response order. Pairwise comparison and full responses enhance performance.

Conclusion: LLM-as-a-Judge shows promise but requires reliability improvements; optimal prompting strategies include pairwise comparison and retaining full responses.

Abstract: Large Language Models (LLMs) have significantly advanced the state-of-the-art
in various coding tasks. Beyond directly answering user queries, LLMs can also
serve as judges, assessing and comparing the quality of responses generated by
other models. Such an evaluation capability is crucial both for benchmarking
different LLMs and for improving response quality through response ranking.
However, despite the growing adoption of the LLM-as-a-Judge paradigm, its
effectiveness in coding scenarios remains underexplored due to the absence of
dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a
benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge
models across three critical coding tasks: code generation, code repair, and
unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge
models, we find that recent thinking models significantly outperform
non-thinking models on our carefully designed code judging tasks. Notably, even
relatively small thinking models, such as Qwen3-8B, can outperform specially
trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still
exhibit significant randomness in their judgment of coding tasks. For pairwise
judging tasks, simply changing the order in which responses are presented can
substantially impact accuracy. In addition, when judging code and unit tests
written by different LLMs, LLM-as-a-Judge models also show variance in
performance. This sensitivity raises concerns about the reliability and
consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal
prompting strategies for LLM-as-a-Judge. We find that using pair-wise
comparison outperforms scalar point-wise judging. Furthermore, retaining
comments and reasoning in the full, unprocessed LLM response leads to improved
judge performance.

</details>


### [431] [REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once](https://arxiv.org/abs/2507.10541)
*Zhuoshi Pan,Qizhi Pei,Yu Li,Qiyao Sun,Zinan Tang,H. Vicky Zhao,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: REST is a stress-testing framework for Large Reasoning Models (LRMs) that evaluates multi-context reasoning, revealing performance gaps not seen in single-question benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LRMs focus on isolated problem-solving, missing real-world complexities like multi-context pressure and cross-problem interference.

Method: REST concurrently exposes LRMs to multiple problems, testing contextual priority allocation, interference resistance, and cognitive load management.

Result: SOTA models like DeepSeek-R1 show significant performance drops under REST, which also highlights mechanistic insights like the "overthinking trap."

Conclusion: REST offers a cost-efficient, realistic evaluation paradigm for LRMs, reducing reliance on human annotation and better reflecting real-world demands.

Abstract: Recent Large Reasoning Models (LRMs) have achieved remarkable progress on
task-specific benchmarks, yet their evaluation methods remain constrained by
isolated problem-solving paradigms. Existing benchmarks predominantly assess
single-question reasoning through sequential testing, resulting critical
limitations: (1) vulnerability to data contamination and less challenging
(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual
creation of new questions with large human efforts, (2) failure to evaluate
models under multi-context pressure, a key requirement for real-world
deployment. To bridge this gap, we present REST (Reasoning Evaluation through
Simultaneous Testing), a stress-testing framework that concurrently exposes
LRMs to multiple problems simultaneously. Beyond basic reasoning, REST
specifically evaluates several under-tested capabilities: contextual priority
allocation, cross-problem interference resistance, and dynamic cognitive load
management. Our evaluation reveals several striking findings: Even
state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance
degradation under stress testing. Crucially, REST demonstrates stronger
discriminative power than existing benchmarks, revealing pronounced performance
differences among models that exhibit similar, near-ceiling performance under
single-question evaluations. Some key mechanistic insights emerge from our
analysis: (1) the "overthinking trap" is a critical factor contributing to the
performance degradation; (2) the models trained with "long2short" technique
preserve more accuracy of their single-problem performance under REST,
outperforming standard-trained counterparts. These results establish REST as a
cost-efficient, future-proof evaluation paradigm that better reflects
real-world reasoning demands while reducing reliance on continuous human
annotation.

</details>

<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]
- [cs.LG](#cs.LG) [Total: 53]
- [cs.CV](#cs.CV) [Total: 85]
- [cs.CL](#cs.CL) [Total: 103]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents](https://arxiv.org/abs/2506.21669)
*Wanxin Tian,Shijie Zhang,Kevin Zhang,Xiaowei Chi,Yulin Luo,Junyu Lu,Chunkai Fan,Qiang Zhou,Yiming Zhao,Ning Liu Siyu Lin,Zhiyuan Qin,Xiaozhu Ju,Shanghang Zhang,Jian Tang*

Main category: cs.AI

TL;DR: SEEA-R1 is a reinforcement fine-tuning framework for self-evolving embodied agents, addressing sparse rewards and generalization with Tree-GRPO and MGRM, achieving state-of-the-art performance on ALFWorld.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement fine-tuning (RFT) methods lack effective intermediate rewards and generalization for embodied intelligence, limiting self-evolution in multi-modal tasks.

Method: Proposes SEEA-R1 with Tree-GRPO for dense intermediate rewards via Monte Carlo Tree Search and MGRM for cross-task/scene reward generalization.

Result: Achieves 85.07% (textual) and 36.19% (multi-modal) on ALFWorld, outperforming GPT-4o and baselines, with 80.3% without environmental rewards.

Conclusion: SEEA-R1 demonstrates scalable self-evolving capabilities, promising for future embodied intelligence research.

Abstract: Self-evolution, the ability of agents to autonomously improve their reasoning
and behavior, is essential for the embodied domain with long-horizon,
real-world tasks. Despite current advancements in reinforcement fine-tuning
(RFT) showing strong performance in enhancing reasoning in LLMs, its potential
to enable self-evolving embodied intelligence with multi-modal interactions
remains largely unexplored. Specifically, reinforcement fine-tuning faces two
fundamental obstacles in embodied settings: (i) the lack of accessible
intermediate rewards in multi-step reasoning tasks limits effective learning
signals, and (ii) reliance on hand-crafted reward functions restricts
generalization to novel tasks and environments. To address these challenges, we
present Self-Evolving Embodied Agents-R1, SEEA-R1, the first RFT framework
designed for enabling the self-evolving capabilities of embodied agents.
Specifically, to convert sparse delayed rewards into denser intermediate
signals that improve multi-step reasoning, we propose Tree-based group relative
policy optimization (Tree-GRPO), which integrates Monte Carlo Tree Search into
GRPO. To generalize reward estimation across tasks and scenes, supporting
autonomous adaptation and reward-driven self-evolution, we further introduce
Multi-modal Generative Reward Model (MGRM). To holistically evaluate the
effectiveness of SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing
state-of-the-art methods with scores of 85.07% (textual) and 36.19%
(multi-modal), outperforming prior models including GPT-4o. SEEA-R1 also
achieves scores of 80.3% without environmental reward, surpassing all
open-source baselines and highlighting its scalability as a self-evolving
embodied agent. Additional experiments and qualitative analysis further support
the potential of SEEA-R1 for future research in scalable embodied intelligence.

</details>


### [2] [Hierarchical Reasoning Model](https://arxiv.org/abs/2506.21734)
*Guan Wang,Jin Li,Yuhao Sun,Xing Chen,Changling Liu,Yue Wu,Meng Lu,Sen Song,Yasin Abbasi Yadkori*

Main category: cs.AI

TL;DR: The paper introduces the Hierarchical Reasoning Model (HRM), a recurrent architecture inspired by human brain processing, which outperforms current LLMs in reasoning tasks with minimal data and no pre-training.


<details>
  <summary>Details</summary>
Motivation: Current LLMs rely on brittle Chain-of-Thought techniques with high data and latency costs. HRM aims to mimic human hierarchical reasoning for efficient, stable, and scalable solutions.

Method: HRM uses two recurrent modules: a high-level module for abstract planning and a low-level module for detailed computations, enabling complex reasoning in a single forward pass without intermediate supervision.

Result: HRM achieves near-perfect performance on tasks like Sudoku and maze pathfinding with only 27M parameters and 1000 samples, outperforming larger models on the ARC benchmark.

Conclusion: HRM represents a significant leap toward universal computation and general-purpose reasoning, offering efficiency and scalability without extensive data or pre-training.

Abstract: Reasoning, the process of devising and executing complex goal-oriented action
sequences, remains a critical challenge in AI. Current large language models
(LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from
brittle task decomposition, extensive data requirements, and high latency.
Inspired by the hierarchical and multi-timescale processing in the human brain,
we propose the Hierarchical Reasoning Model (HRM), a novel recurrent
architecture that attains significant computational depth while maintaining
both training stability and efficiency. HRM executes sequential reasoning tasks
in a single forward pass without explicit supervision of the intermediate
process, through two interdependent recurrent modules: a high-level module
responsible for slow, abstract planning, and a low-level module handling rapid,
detailed computations. With only 27 million parameters, HRM achieves
exceptional performance on complex reasoning tasks using only 1000 training
samples. The model operates without pre-training or CoT data, yet achieves
nearly perfect performance on challenging tasks including complex Sudoku
puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms
much larger models with significantly longer context windows on the Abstraction
and Reasoning Corpus (ARC), a key benchmark for measuring artificial general
intelligence capabilities. These results underscore HRM's potential as a
transformative advancement toward universal computation and general-purpose
reasoning systems.

</details>


### [3] [THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?](https://arxiv.org/abs/2506.21763)
*Xin Wang,Jiyao Liu,Yulong Xiao,Junzhi Ning,Lihao Liu,Junjun He,Botian Shi,Kaicheng Yu*

Main category: cs.AI

TL;DR: THE-Tree is a computational framework that constructs verifiable, causally-linked evolution trees from scientific literature to evaluate AI-generated propositions for novelty and accuracy.


<details>
  <summary>Details</summary>
Motivation: The challenge of rigorously evaluating AI-generated scientific propositions due to inadequate existing validation methods (e.g., LLM hallucinations, lack of domain knowledge, unstructured surveys).

Method: THE-Tree uses a search algorithm and a "Think-Verbalize-Cite-Verify" process where LLMs propose advancements, cite literature, and validate links for coherence and evidence.

Result: THE-Tree improves graph completion by 8-14%, predicts future developments with 10% higher accuracy, and boosts evaluation performance by nearly 100% when combined with other methods.

Conclusion: THE-Tree addresses the bottleneck of evaluating AI-generated scientific ideas by providing structured, verifiable, and causally-linked historical data, demonstrating significant improvements over traditional methods.

Abstract: Large Language Models (LLMs) are accelerating scientific idea generation, but
rigorously evaluating these numerous, often superficial, AI-generated
propositions for novelty and factual accuracy is a critical bottleneck; manual
verification is too slow.Existing validation methods are inadequate: LLMs as
standalone verifiers may hallucinate and lack domain knowledge (our findings
show ~60\% unawareness of relevant papers in specific domains), while
traditional citation networks lack explicit causality and narrative surveys are
unstructured.This underscores a core challenge: the absence of structured,
verifiable, and causally-linked historical data of scientific evolution.To
address this,we introduce \textbf{THE-Tree} (\textbf{T}echnology
\textbf{H}istory \textbf{E}volution Tree), a computational framework that
constructs such domain-specific evolution trees from scientific
literature.THE-Tree employs a search algorithm to explore evolutionary paths.
During its node expansion, it utilizes a novel "Think-Verbalize-Cite-Verify"
process: an LLM proposes potential advancements and cites supporting
literature. Critically, each proposed evolutionary link is then validated for
logical coherence and evidential support by a recovered natural language
inference mechanism that interrogates the cited literature, ensuring that each
step is grounded.We construct and validate 88 THE-Trees across diverse domains
and release a benchmark dataset including up to 71k fact verifications covering
27k papers to foster further research.Experiments demonstrate that i) in graph
completion, our THE-Tree improves hit@1 by 8\% to 14\% across multiple models
compared to traditional citation networks; ii) for predicting future scientific
developments, it improves hit@1 metric by nearly 10\%; and iii) when combined
with other methods, it boosts the performance of evaluating important
scientific papers by almost 100\%.

</details>


### [4] [MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models](https://arxiv.org/abs/2506.21784)
*Yifan Liu,Xishun Liao,Haoxuan Ma,Jonathan Liu,Rohan Jadhav,Jiaqi Ma*

Main category: cs.AI

TL;DR: MobiVerse is a hybrid framework combining lightweight domain-specific generators and LLMs for scalable, context-aware mobility simulations, demonstrated in Westwood, LA.


<details>
  <summary>Details</summary>
Motivation: Addressing gaps in mobility simulation platforms for algorithm development, policy implementation, and evaluation at scale.

Method: Hybrid framework using domain-specific generators for base activity chains and LLMs for dynamic adjustments.

Result: Efficiently simulated 53,000 agents, enabling dynamic responses to environmental changes while maintaining computational efficiency.

Conclusion: MobiVerse bridges mobility simulation gaps, offering a customizable, realistic platform for planning and operations.

Abstract: Understanding and modeling human mobility patterns is crucial for effective
transportation planning and urban development. Despite significant advances in
mobility research, there remains a critical gap in simulation platforms that
allow for algorithm development, policy implementation, and comprehensive
evaluation at scale. Traditional activity-based models require extensive data
collection and manual calibration, machine learning approaches struggle with
adaptation to dynamic conditions, and treding agent-based Large Language Models
(LLMs) implementations face computational constraints with large-scale
simulations. To address these challenges, we propose MobiVerse, a hybrid
framework leverages the efficiency of lightweight domain-specific generator for
generating base activity chains with the adaptability of LLMs for context-aware
modifications. A case study was conducted in Westwood, Los Angeles, where we
efficiently generated and dynamically adjusted schedules for the whole
population of approximately 53,000 agents on a standard PC. Our experiments
demonstrate that MobiVerse successfully enables agents to respond to
environmental feedback, including road closures, large gathering events like
football games, and congestion, through our hybrid framework. Its modular
design facilitates testing various mobility algorithms at both transportation
system and agent levels. Results show our approach maintains computational
efficiency while enhancing behavioral realism. MobiVerse bridges the gap in
mobility simulation by providing a customizable platform for mobility systems
planning and operations with benchmark algorithms. Code and videos are
available at https://github.com/ucla-mobility/MobiVerse.

</details>


### [5] [CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation](https://arxiv.org/abs/2506.21805)
*Nicolas Bougie,Narimasa Watanabe*

Main category: cs.AI

TL;DR: CitySim is an urban simulator using large language models to model human behavior with nuanced intentions, adaptive behaviors, and realistic schedules, outperforming prior rigid methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations of rigid, hand-crafted rules in simulating human behavior by leveraging human-level intelligence from large language models for more realistic urban simulations.

Method: Agents generate daily schedules recursively, balancing mandatory activities, habits, and situational factors, while equipped with beliefs, long-term goals, and spatial memory.

Result: CitySim aligns better with real human behavior at micro and macro levels, demonstrated through experiments like crowd density estimation and well-being assessment.

Conclusion: CitySim is a scalable, flexible testbed for understanding and forecasting urban phenomena, offering improved realism over traditional methods.

Abstract: Modeling human behavior in urban environments is fundamental for social
science, behavioral studies, and urban planning. Prior work often rely on
rigid, hand-crafted rules, limiting their ability to simulate nuanced
intentions, plans, and adaptive behaviors. Addressing these challenges, we
envision an urban simulator (CitySim), capitalizing on breakthroughs in
human-level intelligence exhibited by large language models. In CitySim, agents
generate realistic daily schedules using a recursive value-driven approach that
balances mandatory activities, personal habits, and situational factors. To
enable long-term, lifelike simulations, we endow agents with beliefs, long-term
goals, and spatial memory for navigation. CitySim exhibits closer alignment
with real humans than prior work, both at micro and macro levels. Additionally,
we conduct insightful experiments by modeling tens of thousands of agents and
evaluating their collective behaviors under various real-world scenarios,
including estimating crowd density, predicting place popularity, and assessing
well-being. Our results highlight CitySim as a scalable, flexible testbed for
understanding and forecasting urban phenomena.

</details>


### [6] [Interactive Multi-Objective Probabilistic Preference Learning with Soft and Hard Bounds](https://arxiv.org/abs/2506.21887)
*Edward Chen,Sang T. Truong,Natalie Dullerud,Sanmi Koyejo,Carlos Guestrin*

Main category: cs.AI

TL;DR: Active-MoSH is an interactive framework for high-stakes decision-making, combining soft-hard bounds with preference learning and active sampling to refine Pareto-optimal solutions while building user trust.


<details>
  <summary>Details</summary>
Motivation: High-stakes decisions require balancing competing objectives with expensive evaluations, and current methods lack systematic ways to refine preferences or ensure trust in the final decision.

Method: Active-MoSH integrates soft-hard bounds with probabilistic preference learning and active sampling. Its global component, T-MoSH, uses sensitivity analysis to identify overlooked high-value points.

Result: Active-MoSH improves convergence, enhances decision-maker trust, and enables expressive preference articulation, validated through synthetic and real-world applications.

Conclusion: Active-MoSH effectively addresses the challenges of high-stakes decision-making by combining local refinement with global trust-building, outperforming existing methods.

Abstract: High-stakes decision-making involves navigating multiple competing objectives
with expensive evaluations. For instance, in brachytherapy, clinicians must
balance maximizing tumor coverage (e.g., an aspirational target or soft bound
of >95% coverage) against strict organ dose limits (e.g., a non-negotiable hard
bound of <601 cGy to the bladder), with each plan evaluation being
resource-intensive. Selecting Pareto-optimal solutions that match implicit
preferences is challenging, as exhaustive Pareto frontier exploration is
computationally and cognitively prohibitive, necessitating interactive
frameworks to guide users. While decision-makers (DMs) often possess domain
knowledge to narrow the search via such soft-hard bounds, current methods often
lack systematic approaches to iteratively refine these multi-faceted preference
structures. Critically, DMs must trust their final decision, confident they
haven't missed superior alternatives; this trust is paramount in
high-consequence scenarios. We present Active-MoSH, an interactive local-global
framework designed for this process. Its local component integrates soft-hard
bounds with probabilistic preference learning, maintaining distributions over
DM preferences and bounds for adaptive Pareto subset refinement. This is guided
by an active sampling strategy optimizing exploration-exploitation while
minimizing cognitive burden. To build DM trust, Active-MoSH's global component,
T-MoSH, leverages multi-objective sensitivity analysis to identify potentially
overlooked, high-value points beyond immediate feedback. We demonstrate
Active-MoSH's performance benefits through diverse synthetic and real-world
applications. A user study on AI-generated image selection further validates
our hypotheses regarding the framework's ability to improve convergence,
enhance DM trust, and provide expressive preference articulation, enabling more
effective DMs.

</details>


### [7] [AlphaBeta is not as good as you think: a new probabilistic model to better analyze deterministic game-solving algorithms](https://arxiv.org/abs/2506.21996)
*Raphaël Boige,Amine Boumaza,Bruno Scherrer*

Main category: cs.AI

TL;DR: The paper critiques traditional game-solving algorithm analysis for oversimplifying game-tree structures and introduces a new model with ancestor dependency to better reflect real-world complexity. It provides recursive formulas for algorithm complexities, revealing practical differences like AlphaBeta's slowdown compared to Scout.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional models that assume independence of leaf values, which strip games of structural complexity and produce trivial instances, the paper aims to introduce a more realistic model.

Method: The paper proposes a new probabilistic model for game-trees with ancestor dependency, enabling adjustable difficulty while retaining analytical tractability. Recursive formulas for algorithm complexities (e.g., AlphaBeta, Scout) are derived.

Result: Deep finite trees show practical differences: AlphaBeta has a larger constant multiplicative factor than Scout, leading to slowdowns, despite asymptotic convergence to identical branching factors.

Conclusion: The new framework provides rigorous analytical tools for understanding game-solving algorithms under a more realistic and challenging model, advancing the field beyond oversimplified assumptions.

Abstract: Deterministic game-solving algorithms are conventionally analyzed in the
light of their average-case complexity against a distribution of random
game-trees, where leaf values are independently sampled from a fixed
distribution. This simplified model enables uncluttered mathematical analysis,
revealing two key properties: root value distributions asymptotically collapse
to a single fixed value for finite-valued trees, and all reasonable algorithms
achieve global optimality. However, these findings are artifacts of the model's
design-its long criticized independence assumption strips games of structural
complexity, producing trivial instances where no algorithm faces meaningful
challenges. To address this limitation, we introduce a new probabilistic model
that incrementally constructs game-trees using a fixed level-wise conditional
distribution. By enforcing ancestor dependency, a critical structural feature
of real-world games, our framework generates problems with adjustable
difficulty while retaining some form of analytical tractability. For several
algorithms, including AlphaBeta and Scout, we derive recursive formulas
characterizing their average-case complexities under this model. These allow us
to rigorously compare algorithms on deep game-trees, where Monte-Carlo
simulations are no longer feasible. While asymptotically, all algorithms seem
to converge to identical branching factor (a result analogous to those of
independence-based models), deep finite trees reveal stark differences:
AlphaBeta incurs a significantly larger constant multiplicative factor compared
to algorithms like Scout, leading to a substantial practical slowdown. Our
framework sheds new light on classical game-solving algorithms, offering
rigorous evidence and analytical tools to advance the understanding of these
methods under a more realistic, challenging, and yet tractable model.

</details>


### [8] [LeanConjecturer: Automatic Generation of Mathematical Conjectures for Theorem Proving](https://arxiv.org/abs/2506.22005)
*Naoto Onda,Kazumi Kasaura,Yuta Oriike,Masaya Taniguchi,Akiyoshi Sannai,Sho Sonoda*

Main category: cs.AI

TL;DR: LeanConjecturer is a pipeline using LLMs and rule-based methods to generate university-level mathematical conjectures in Lean 4, addressing data scarcity in theorem proving. It produced 12,289 conjectures, with 3,776 being valid and non-trivial, and demonstrated their utility in enhancing theorem proving via GRPO.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of data scarcity in formal theorem proving by automating conjecture generation.

Method: Hybrid approach combining rule-based context extraction with LLM-based theorem statement generation, followed by iterative evaluation.

Result: Generated 12,289 conjectures (3,776 valid and non-trivial), averaging 103.25 per seed file, and verified non-trivial theorems in topology.

Conclusion: LeanConjecturer provides a scalable solution for generating training data, enhancing theorem proving capabilities and enabling mathematical discovery.

Abstract: We introduce LeanConjecturer, a pipeline for automatically generating
university-level mathematical conjectures in Lean 4 using Large Language Models
(LLMs). Our hybrid approach combines rule-based context extraction with
LLM-based theorem statement generation, addressing the data scarcity challenge
in formal theorem proving. Through iterative generation and evaluation,
LeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with
3,776 identified as syntactically valid and non-trivial, that is, cannot be
proven by \texttt{aesop} tactic. We demonstrate the utility of these generated
conjectures for reinforcement learning through Group Relative Policy
Optimization (GRPO), showing that targeted training on domain-specific
conjectures can enhance theorem proving capabilities. Our approach generates
103.25 novel conjectures per seed file on average, providing a scalable
solution for creating training data for theorem proving systems. Our system
successfully verified several non-trivial theorems in topology, including
properties of semi-open, alpha-open, and pre-open sets, demonstrating its
potential for mathematical discovery beyond simple variations of existing
results.

</details>


### [9] [Universal Retrieval for Multimodal Trajectory Modeling](https://arxiv.org/abs/2506.22056)
*Xuan Zhang,Ziyan Jiang,Rui Meng,Yifei Leng,Zhenbang Xiao,Zora Zhiruo Wang,Yanyi Shang,Dehan Kong*

Main category: cs.AI

TL;DR: The paper introduces Multimodal Trajectory Retrieval, a method to model trajectory-level data for AI agents in GUI environments, using a new dataset (UATD) and benchmark (GAE-Bench), and proposes GAE-Retriever, a framework outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Trajectory data's potential for AI agents is underexplored, especially in GUI environments, due to lack of systematic modeling approaches.

Method: Constructs UATD dataset and GAE-Bench benchmark; proposes GAE-Retriever, a multimodal framework using vision-language models and contrastive learning with token selection and GradCache.

Result: GAE-Retriever outperforms baselines in retrieval recall across datasets, demonstrating effectiveness.

Conclusion: The work advances multimodal trajectory retrieval, offering a robust framework and benchmark for future research.

Abstract: Trajectory data, capturing human actions and environmental states across
various modalities, holds significant potential for enhancing AI agent
capabilities, particularly in GUI environments. However, how to model the
representation of trajectory-level data presents a significant challenge that
has not been systematically addressed amid explosive trajectory data growth. In
this work, we introduce Multimodal Trajectory Retrieval, bridging the gap
between universal retrieval and agent-centric trajectory modeling. We construct
the Unified Agent Trajectory Dataset (UATD) from annotated demonstrations and
states across diverse real-world scenarios. Based on this, we present
GAE-Bench, a benchmark containing a large number of trajectory-based retrieval
pairs. In addition, we propose GAE-Retriever, a multimodal retrieval framework
that adopts vision-language models and incorporates optimized contrastive
learning through a token selection and the GradCache mechanism. Comprehensive
evaluations across multiple datasets show that GAE-Retriever consistently
outperforms strong baselines in retrieval recall, highlighting its
effectiveness in advancing multimodal trajectory retrieval.

</details>


### [10] [Query as Test: An Intelligent Driving Test and Data Storage Method for Integrated Cockpit-Vehicle-Road Scenarios](https://arxiv.org/abs/2506.22068)
*Shengyue Yao,Runqing Guo,Yangyang Qin,Miangbing Meng,Jipeng Cao,Yilun Lin,Yisheng Lv,Fei-Yue Wang*

Main category: cs.AI

TL;DR: The paper proposes 'Query as Test' (QaT) and 'Extensible Scenarios Notations' (ESN) to address fragmented data in AI-driven transportation, enabling flexible, semantic-rich testing and validation.


<details>
  <summary>Details</summary>
Motivation: Fragmented and incompatible data ecosystems in AI-driven transportation (cockpits, autonomous driving, road networks) hinder testing, especially due to rigid methods and lack of edge-case coverage.

Method: Introduces ESN, a declarative data framework using Answer Set Programming (ASP) to unify multimodal data as logical facts/rules, enabling semantic queries and privacy-aware abstraction.

Result: ESN supports semantic querying, interpretability, and privacy protection. QaT transforms testing into logical queries, enhancing expressiveness and rigor.

Conclusion: Proposes 'Validation-Driven Development' (VDD) to guide AI development via logical validation, accelerating iteration in the era of Large Language Models.

Abstract: With the deep penetration of Artificial Intelligence (AI) in the
transportation sector, intelligent cockpits, autonomous driving, and
intelligent road networks are developing at an unprecedented pace. However, the
data ecosystems of these three key areas are increasingly fragmented and
incompatible. Especially, existing testing methods rely on data stacking, fail
to cover all edge cases, and lack flexibility. To address this issue, this
paper introduces the concept of "Query as Test" (QaT). This concept shifts the
focus from rigid, prescripted test cases to flexible, on-demand logical queries
against a unified data representation. Specifically, we identify the need for a
fundamental improvement in data storage and representation, leading to our
proposal of "Extensible Scenarios Notations" (ESN). ESN is a novel declarative
data framework based on Answer Set Programming (ASP), which uniformly
represents heterogeneous multimodal data from the cockpit, vehicle, and road as
a collection of logical facts and rules. This approach not only achieves deep
semantic fusion of data, but also brings three core advantages: (1) supports
complex and flexible semantic querying through logical reasoning; (2) provides
natural interpretability for decision-making processes; (3) allows for
on-demand data abstraction through logical rules, enabling fine-grained privacy
protection. We further elaborate on the QaT paradigm, transforming the
functional validation and safety compliance checks of autonomous driving
systems into logical queries against the ESN database, significantly enhancing
the expressiveness and formal rigor of the testing. Finally, we introduce the
concept of "Validation-Driven Development" (VDD), which suggests to guide
developments by logical validation rather than quantitative testing in the era
of Large Language Models, in order to accelerating the iteration and
development process.

</details>


### [11] [A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety](https://arxiv.org/abs/2506.22183)
*Camille François,Ludovic Péran,Ayah Bdeir,Nouha Dziri,Will Hawkins,Yacine Jernite,Sayash Kapoor,Juliet Shen,Heidy Khlaaf,Kevin Klyman,Nik Marda,Marie Pellat,Deb Raji,Divya Siddarth,Aviya Skowron,Joseph Spisak,Madhulika Srikumar,Victor Storchan,Audrey Tang,Jen Weedon*

Main category: cs.AI

TL;DR: The paper discusses the intersection of AI safety and open-source models, presenting outcomes from a collaborative effort involving researchers, engineers, and policymakers. It highlights the benefits of openness for safety but identifies gaps and proposes a research roadmap.


<details>
  <summary>Details</summary>
Motivation: The rise of open-weight and open-source AI models necessitates a focus on safety, requiring collaborative efforts to address challenges and opportunities.

Method: A participatory, solutions-oriented process involving over 45 experts from various sectors, resulting in research agendas, technical mappings, and safety roadmaps.

Result: Findings include the potential of openness to enhance safety but note gaps like limited benchmarks and defenses against attacks. A five-priority research roadmap is proposed.

Conclusion: The paper concludes with actionable research directions to foster an open, plural, and accountable AI safety discipline, influencing global policy discussions.

Abstract: The rapid rise of open-weight and open-source foundation models is
intensifying the obligation and reshaping the opportunity to make AI systems
safe. This paper reports outcomes from the Columbia Convening on AI Openness
and Safety (San Francisco, 19 Nov 2024) and its six-week preparatory programme
involving more than forty-five researchers, engineers, and policy leaders from
academia, industry, civil society, and government. Using a participatory,
solutions-oriented process, the working groups produced (i) a research agenda
at the intersection of safety and open source AI; (ii) a mapping of existing
and needed technical interventions and open source tools to safely and
responsibly deploy open foundation models across the AI development workflow;
and (iii) a mapping of the content safety filter ecosystem with a proposed
roadmap for future research and development. We find that openness --
understood as transparent weights, interoperable tooling, and public governance
-- can enhance safety by enabling independent scrutiny, decentralized
mitigation, and culturally plural oversight. However, significant gaps persist:
scarce multimodal and multilingual benchmarks, limited defenses against
prompt-injection and compositional attacks in agentic systems, and insufficient
participatory mechanisms for communities most affected by AI harms. The paper
concludes with a roadmap of five priority research directions, emphasizing
participatory inputs, future-proof content filters, ecosystem-wide safety
infrastructure, rigorous agentic safeguards, and expanded harm taxonomies.
These recommendations informed the February 2025 French AI Action Summit and
lay groundwork for an open, plural, and accountable AI safety discipline.

</details>


### [12] [Breaking Rank Bottlenecks in Knowledge Graph Completion](https://arxiv.org/abs/2506.22271)
*Samy Badreddine,Emile van Krieken,Luciano Serafini*

Main category: cs.AI

TL;DR: KGE-MoS, a mixture-based output layer, addresses rank bottlenecks in KGC models, improving performance and probabilistic fit with low parameter cost.


<details>
  <summary>Details</summary>
Motivation: Rank bottlenecks in KGC models limit expressivity, hurting ranking accuracy and score distribution fidelity.

Method: Proposes KGE-MoS, inspired by language modeling, to break rank bottlenecks.

Result: Experiments on four datasets show improved performance and probabilistic fit.

Conclusion: KGE-MoS effectively mitigates rank bottlenecks in KGC models.

Abstract: Many Knowledge Graph Completion (KGC) models, despite using powerful
encoders, rely on a simple vector-matrix multiplication to score queries
against candidate object entities. When the number of entities is larger than
the model's embedding dimension, which in practical scenarios is often by
several orders of magnitude, we have a linear output layer with a rank
bottleneck. Such bottlenecked layers limit model expressivity. We investigate
both theoretically and empirically how rank bottlenecks affect KGC models. We
find that, by limiting the set of feasible predictions, rank bottlenecks hurt
ranking accuracy and the distribution fidelity of scores. Inspired by the
language modelling literature, we propose KGE-MoS, a mixture-based output layer
to break rank bottlenecks in many KGC models. Our experiments on four datasets
show that KGE-MoS improves performance and probabilistic fit of KGC models for
a low parameter cost.

</details>


### [13] [Artificial Intelligent Disobedience: Rethinking the Agency of Our Artificial Teammates](https://arxiv.org/abs/2506.22276)
*Reuth Mirsky*

Main category: cs.AI

TL;DR: The paper advocates for AI systems with 'intelligent disobedience,' enabling them to autonomously contribute in human-AI teams, rather than rigidly following instructions.


<details>
  <summary>Details</summary>
Motivation: Current AI systems are overly obedient, which can be counterproductive or unsafe. The paper argues for greater AI agency to enhance cooperation.

Method: Introduces a scale of AI agency levels and uses examples to highlight the need for autonomy research in cooperative settings.

Result: Explores how intelligent disobedience operates across autonomy levels and its potential benefits.

Conclusion: Proposes initial boundaries and considerations for studying disobedience as a core AI capability.

Abstract: Artificial intelligence has made remarkable strides in recent years,
achieving superhuman performance across a wide range of tasks. Yet despite
these advances, most cooperative AI systems remain rigidly obedient, designed
to follow human instructions without question and conform to user expectations,
even when doing so may be counterproductive or unsafe. This paper argues for
expanding the agency of AI teammates to include \textit{intelligent
disobedience}, empowering them to make meaningful and autonomous contributions
within human-AI teams. It introduces a scale of AI agency levels and uses
representative examples to highlight the importance and growing necessity of
treating AI autonomy as an independent research focus in cooperative settings.
The paper then explores how intelligent disobedience manifests across different
autonomy levels and concludes by proposing initial boundaries and
considerations for studying disobedience as a core capability of artificial
agents.

</details>


### [14] [Conceptual Topic Aggregation](https://arxiv.org/abs/2506.22309)
*Klara M. Gutekunst,Dominik Dürrschnabel,Johannes Hirth,Gerd Stumme*

Main category: cs.AI

TL;DR: FAT-CAT uses Formal Concept Analysis (FCA) to improve topic modeling by providing structured, hierarchical representations of topics, outperforming existing methods in interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional topic modeling lacks interpretability; FAT-CAT aims to enhance meaningful topic aggregation and visualization.

Method: FAT-CAT employs FCA to construct a concept lattice for hierarchical topic representation, handling diverse topics and file types.

Result: In a case study on ETYNTKE, FAT-CAT provided more interpretable insights than other topic modeling techniques.

Conclusion: FCA-based aggregation in FAT-CAT offers superior interpretability and meaningful insights into dataset composition.

Abstract: The vast growth of data has rendered traditional manual inspection
infeasible, necessitating the adoption of computational methods for efficient
data exploration. Topic modeling has emerged as a powerful tool for analyzing
large-scale textual datasets, enabling the extraction of latent semantic
structures. However, existing methods for topic modeling often struggle to
provide interpretable representations that facilitate deeper insights into data
structure and content. In this paper, we propose FAT-CAT, an approach based on
Formal Concept Analysis (FCA) to enhance meaningful topic aggregation and
visualization of discovered topics. Our approach can handle diverse topics and
file types -- grouped by directories -- to construct a concept lattice that
offers a structured, hierarchical representation of their topic distribution.
In a case study on the ETYNTKE dataset, we evaluate the effectiveness of our
approach against other representation methods to demonstrate that FCA-based
aggregation provides more meaningful and interpretable insights into dataset
composition than existing topic modeling techniques.

</details>


### [15] [Embodied AI Agents: Modeling the World](https://arxiv.org/abs/2506.22355)
*Pascale Fung,Yoram Bachrach,Asli Celikyilmaz,Kamalika Chaudhuri,Delong Chen,Willy Chung,Emmanuel Dupoux,Hervé Jégou,Alessandro Lazaric,Arjun Majumdar,Andrea Madotto,Franziska Meier,Florian Metze,Théo Moutakanni,Juan Pino,Basile Terver,Joseph Tighe,Jitendra Malik*

Main category: cs.AI

TL;DR: The paper explores embodied AI agents (virtual avatars, robots, etc.) that interact with users and environments, emphasizing world models for reasoning, planning, and human-agent collaboration.


<details>
  <summary>Details</summary>
Motivation: To enhance AI agents' ability to learn and interact like humans by developing world models for better reasoning, planning, and collaboration.

Method: Proposes world modeling integrating multimodal perception, reasoning for action, control, and memory, along with learning users' mental models.

Result: Embodied AI agents gain improved autonomy and capability to perform complex tasks by understanding and predicting environments and user intentions.

Conclusion: World models are central to advancing embodied AI agents, enabling human-like interaction and collaboration.

Abstract: This paper describes our research on AI agents embodied in visual, virtual or
physical forms, enabling them to interact with both users and their
environments. These agents, which include virtual avatars, wearable devices,
and robots, are designed to perceive, learn and act within their surroundings,
which makes them more similar to how humans learn and interact with the
environments as compared to disembodied agents. We propose that the development
of world models is central to reasoning and planning of embodied AI agents,
allowing these agents to understand and predict their environment, to
understand user intentions and social contexts, thereby enhancing their ability
to perform complex tasks autonomously. World modeling encompasses the
integration of multimodal perception, planning through reasoning for action and
control, and memory to create a comprehensive understanding of the physical
world. Beyond the physical world, we also propose to learn the mental world
model of users to enable better human-agent collaboration.

</details>


### [16] [AI Model Passport: Data and System Traceability Framework for Transparent AI in Health](https://arxiv.org/abs/2506.22358)
*Varvara Kalokyri,Nikolaos S. Tachos,Charalampos N. Kalantzopoulos,Stelios Sfakianakis,Haridimos Kondylakis,Dimitrios I. Zaridis,Sara Colantonio,Daniele Regge,Nikolaos Papanikolaou,The ProCAncer-I consortium,Konstantinos Marias,Dimitrios I. Fotiadis,Manolis Tsiknakis*

Main category: cs.AI

TL;DR: The paper proposes an AI Model Passport framework to standardize and automate documentation for AI models, enhancing transparency, accountability, and reproducibility in healthcare AI.


<details>
  <summary>Details</summary>
Motivation: Existing manual documentation frameworks lack scalability, comparability, and machine interpretability, hindering reproducibility and trust in AI models.

Method: Introduces the AI Model Passport, a structured framework for digital identity and verification of AI models, implemented via AIPassport, an MLOps tool.

Result: Demonstrated effectiveness in a lesion segmentation use case, improving transparency, reproducibility, and regulatory compliance.

Conclusion: The AI Model Passport sets a new standard for trust and accountability in AI-driven healthcare, with potential for broader domain application.

Abstract: The increasing integration of Artificial Intelligence (AI) into health and
biomedical systems necessitates robust frameworks for transparency,
accountability, and ethical compliance. Existing frameworks often rely on
human-readable, manual documentation which limits scalability, comparability,
and machine interpretability across projects and platforms. They also fail to
provide a unique, verifiable identity for AI models to ensure their provenance
and authenticity across systems and use cases, limiting reproducibility and
stakeholder trust. This paper introduces the concept of the AI Model Passport,
a structured and standardized documentation framework that acts as a digital
identity and verification tool for AI models. It captures essential metadata to
uniquely identify, verify, trace and monitor AI models across their lifecycle -
from data acquisition and preprocessing to model design, development and
deployment. In addition, an implementation of this framework is presented
through AIPassport, an MLOps tool developed within the ProCAncer-I EU project
for medical imaging applications. AIPassport automates metadata collection,
ensures proper versioning, decouples results from source scripts, and
integrates with various development environments. Its effectiveness is
showcased through a lesion segmentation use case using data from the
ProCAncer-I dataset, illustrating how the AI Model Passport enhances
transparency, reproducibility, and regulatory readiness while reducing manual
effort. This approach aims to set a new standard for fostering trust and
accountability in AI-driven healthcare solutions, aspiring to serve as the
basis for developing transparent and regulation compliant AI systems across
domains.

</details>


### [17] [The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements](https://arxiv.org/abs/2506.22419)
*Bingchen Zhao,Despoina Magka,Minqi Jiang,Xian Li,Roberta Raileanu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Kelvin Niu,Shagun Sodhani,Michael Shvartsman,Andrei Lupu,Alisia Lupidi,Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Thomas Foster,Lucia Cipolina-Kun,Abhishek Charnalia,Derek Dunfield,Alexander H. Miller,Oisin Mac Aodha,Jakob Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: The paper introduces the Automated LLM Speedrunning Benchmark to evaluate AI agents' ability to reproduce scientific results, focusing on the NanoGPT speedrun. Despite detailed hints, current LLMs struggle with reimplementing known innovations, highlighting challenges in automating scientific reproduction.


<details>
  <summary>Details</summary>
Motivation: To assess AI agents' capability in reproducing scientific work, a critical skill for autonomous research, using the NanoGPT speedrun as a test case.

Method: The benchmark includes 19 speedrun tasks with training scripts and hints (pseudocode to paper-like descriptions). It evaluates LLMs' ability to reimplement code-level changes for faster training.

Result: Current reasoning LLMs, even with state-of-the-art scaffolds, fail to reimplement known innovations effectively, even with detailed hints.

Conclusion: The benchmark offers a simple, non-saturated measure of LLMs' ability to automate scientific reproduction, a foundational skill for autonomous research agents.

Abstract: Rapid advancements in large language models (LLMs) have the potential to
assist in scientific progress. A critical capability toward this endeavor is
the ability to reproduce existing work. To evaluate the ability of AI agents to
reproduce results in an active research area, we introduce the Automated LLM
Speedrunning Benchmark, leveraging the research community contributions on the
NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.
Each of the 19 speedrun tasks provides the agent with the previous records
training script, optionally paired with one of three hint formats, ranging from
pseudocode to paper-like descriptions of the new records improvements. Records
execute quickly by design and speedrun improvements encompass diverse
code-level changes, ranging from high-level algorithmic advancements to
hardware-aware optimizations. These features make the benchmark both accessible
and realistic for the frontier problem of improving LLM training. We find that
recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement
already-known innovations in our benchmark, even when given detailed hints. Our
benchmark thus provides a simple, non-saturated measure of an LLMs ability to
automate scientific reproduction, a necessary (but not sufficient) skill for an
autonomous research agent.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [18] [APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization](https://arxiv.org/abs/2506.21655)
*Minjie Hong,Zirun Guo,Yan Xia,Zehan Wang,Ziang Zhang,Tao Jin,Zhou Zhao*

Main category: cs.LG

TL;DR: The paper introduces Asymmetric Policy Optimization (APO) to improve reasoning in Multimodal Large Language Models (MLLMs) by addressing performance drops and overthinking. APO uses Difficulty-Adaptive Divergence Shaping (DADS) for positive samples and Suboptimal Trajectory Complexity Regularization (STCR) for negative samples, enhancing reasoning while maintaining generalization.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with complex reasoning and often degrade on general tasks when trained with RL. The work aims to mitigate these issues while improving reasoning capabilities.

Method: Proposes APO, which splits responses into positive and negative groups. DADS adjusts KL divergence for positive samples dynamically, while STCR penalizes overly long responses in negative samples.

Result: View-R1-3B, the model trained with APO, shows a 7% average gain in reasoning over the base model and outperforms larger MLLMs on benchmarks, without degrading general task performance.

Conclusion: APO, with DADS and STCR, effectively enhances complex reasoning in MLLMs while preserving generalization, demonstrating broad applicability.

Abstract: Multimodal Large Language Models (MLLMs) are powerful at integrating diverse
data, but they often struggle with complex reasoning. While Reinforcement
learning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.
Common issues include a drop in performance on general tasks and the generation
of overly detailed or "overthinking" reasoning. Our work investigates how the
KL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric
Policy Optimization (APO) to address these issues, which divides the sampled
responses into positive and negative groups. For positive samples,
Difficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically
adjust the KL divergence weight based on their difficulty. This method prevents
policy entropy from dropping sharply, improves training stability, utilizes
samples better, and preserves the model's existing knowledge. For negative
samples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to
penalize overly long responses. This helps mitigate overthinking and encourages
more concise reasoning while preserving the model's explorative capacity. We
apply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B
significantly enhances reasoning capabilities, showing an average 7\% gain over
the base model and outperforming larger MLLMs (7-11B) on various reasoning
benchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade
on general tasks, View-R1-3B maintains consistent improvement, demonstrating
superior generalization. These results highlight the effectiveness and broad
applicability of our DADS and STCR techniques for advancing complex multimodal
reasoning in MLLMs. The code will be made available at
https://github.com/Indolent-Kawhi/View-R1.

</details>


### [19] [Risk-Averse Total-Reward Reinforcement Learning](https://arxiv.org/abs/2506.21683)
*Xihong Su,Jia Lin Hau,Gersi Doko,Kishan Panaganti,Marek Petrik*

Main category: cs.LG

TL;DR: A Q-learning algorithm is proposed for risk-averse total-reward MDPs, addressing limitations of existing model-based methods by not requiring full transition probabilities. It shows strong convergence and performance guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing model-based algorithms for risk measures like ERM and EVaR are limited to small problems and require full transition probabilities. A more scalable and practical solution is needed.

Method: A Q-learning algorithm is introduced to compute optimal stationary policies for total-reward ERM and EVaR objectives, leveraging ERM's dynamic consistency and elicitability.

Result: Numerical results on tabular domains demonstrate quick and reliable convergence to the optimal risk-averse value function.

Conclusion: The proposed Q-learning algorithm effectively addresses scalability and practicality issues in risk-averse MDPs, offering strong convergence guarantees.

Abstract: Risk-averse total-reward Markov Decision Processes (MDPs) offer a promising
framework for modeling and solving undiscounted infinite-horizon objectives.
Existing model-based algorithms for risk measures like the entropic risk
measure (ERM) and entropic value-at-risk (EVaR) are effective in small
problems, but require full access to transition probabilities. We propose a
Q-learning algorithm to compute the optimal stationary policy for total-reward
ERM and EVaR objectives with strong convergence and performance guarantees. The
algorithm and its optimality are made possible by ERM's dynamic consistency and
elicitability. Our numerical results on tabular domains demonstrate quick and
reliable convergence of the proposed Q-learning algorithm to the optimal
risk-averse value function.

</details>


### [20] [Unimodal Strategies in Density-Based Clustering](https://arxiv.org/abs/2506.21695)
*Oron Nir,Jay Tenenbaum,Ariel Shamir*

Main category: cs.LG

TL;DR: The paper introduces a unimodal relationship between the number of clusters and neighborhood radius in density-based clustering, enabling efficient parameter tuning via Ternary Search.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational challenge of parameter tuning in density-based clustering for large-scale, high-dimensional data.

Method: Empirical and theoretical analysis of the unimodal relationship, followed by Ternary Search-based strategies for efficient radius selection.

Result: Validated effectiveness and robustness across NLP, Audio, and Computer Vision tasks.

Conclusion: Advances parameter control in density-based clustering and enhances understanding of parameter relationships.

Abstract: Density-based clustering methods often surpass centroid-based counterparts,
when addressing data with noise or arbitrary data distributions common in
real-world problems. In this study, we reveal a key property intrinsic to
density-based clustering methods regarding the relation between the number of
clusters and the neighborhood radius of core points - we empirically show that
it is nearly unimodal, and support this claim theoretically in a specific
setting. We leverage this property to devise new strategies for finding
appropriate values for the radius more efficiently based on the Ternary Search
algorithm. This is especially important for large scale data that is
high-dimensional, where parameter tuning is computationally intensive. We
validate our methodology through extensive applications across a range of
high-dimensional, large-scale NLP, Audio, and Computer Vision tasks,
demonstrating its practical effectiveness and robustness. This work not only
offers a significant advancement in parameter control for density-based
clustering but also broadens the understanding regarding the relations between
their guiding parameters. Our code is available at
https://github.com/oronnir/UnimodalStrategies.

</details>


### [21] [$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$: Shortcutting the Time and Length in Diffusion and Flow Models for Faster Sampling](https://arxiv.org/abs/2506.21714)
*Denis Gudovskiy,Wenzhao Zheng,Tomoyuki Okuno,Yohei Nakata,Kurt Keutzer*

Main category: cs.LG

TL;DR: The paper introduces a dynamic control method for quality-complexity tradeoffs in continuous normalizing flows (CNFs) and diffusion models (DMs) by rewiring transformer blocks and using consistency terms, achieving efficiency and improved performance.


<details>
  <summary>Details</summary>
Motivation: Current CNFs and DMs require high computational complexity for sampling. Existing methods focus on reducing time steps, but this work explores dynamic control over time steps and neural network length to improve efficiency and quality.

Method: The approach rewires transformer blocks to solve an inner discretized ODE and employs time- and length-wise consistency terms during training, enabling flexible sampling with arbitrary time steps and blocks.

Result: Experiments on CelebA-HQ and ImageNet show a 3x latency reduction and up to 3.5 FID score improvement.

Conclusion: The proposed method is solver-agnostic, reduces latency and memory usage, and outperforms state-of-the-art models in efficiency and quality.

Abstract: Recently, continuous normalizing flows (CNFs) and diffusion models (DMs) have
been studied using the unified theoretical framework. Although such models can
generate high-quality data points from a noise distribution, the sampling
demands multiple iterations to solve an ordinary differential equation (ODE)
with high computational complexity. Most existing methods focus on reducing the
number of time steps during the sampling process to improve efficiency. In this
work, we explore a complementary direction in which the quality-complexity
tradeoff can be dynamically controlled in terms of time steps and in the length
of the neural network. We achieve this by rewiring the blocks in the
transformer-based architecture to solve an inner discretized ODE w.r.t. its
length. Then, we employ time- and length-wise consistency terms during flow
matching training, and as a result, the sampling can be performed with an
arbitrary number of time steps and transformer blocks. Unlike others, our
$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$ approach is solver-agnostic in
time dimension and decreases both latency and memory usage. Compared to the
previous state of the art, image generation experiments on CelebA-HQ and
ImageNet show a latency reduction of up to $3\times$ in the most efficient
sampling mode, and a FID score improvement of up to $3.5$ points for
high-quality sampling. We release our code and model weights with fully
reproducible experiments.

</details>


### [22] [Performance Prediction for Large Systems via Text-to-Text Regression](https://arxiv.org/abs/2506.21718)
*Yash Akhauri,Bryan Lewandowski,Cheng-Hsi Lin,Adrian N. Reyes,Grant C. Forbes,Arissa Wongpanich,Bangding Yang,Mohamed S. Abdelfattah,Sagi Perel,Xingyou Song*

Main category: cs.LG

TL;DR: Text-to-text regression outperforms traditional tabular methods in predicting complex system metrics, achieving near-perfect accuracy and adaptability with minimal data.


<details>
  <summary>Details</summary>
Motivation: Traditional tabular regression struggles with complex system data like configuration files or logs, where feature engineering is impractical.

Method: Proposes text-to-text regression using a 60M parameter encoder-decoder model trained from scratch, tested on Google's Borg system.

Result: Achieves 0.99 rank correlation, 100x lower MSE than tabular methods, and adapts to new tasks with just 500 examples.

Conclusion: Text-to-text regression is a scalable, adaptable solution for predicting real-world system outcomes, paving the way for universal simulators.

Abstract: In many industries, predicting metric outcomes of large systems is a
fundamental problem, driven largely by traditional tabular regression. However,
such methods struggle on complex systems data in the wild such as configuration
files or system logs, where feature engineering is often infeasible. We propose
text-to-text regression as a general, scalable alternative. For predicting
resource efficiency on Borg, Google's massive compute cluster scheduling
system, a 60M parameter encoder-decoder, trained from random initialization,
achieves up to a near perfect 0.99 (0.9 average) rank correlation across the
entire fleet, and 100x lower MSE than tabular approaches. The model also easily
adapts to new tasks in only 500 few-shot examples and captures the densities of
complex outcome distributions. Ablation studies highlight the importance of
using encoders, increasing sequence length, and the model's inherent
uncertainty quantification. These findings pave the way for universal
simulators of real-world outcomes.

</details>


### [23] [Federated Item Response Theory Models](https://arxiv.org/abs/2506.21744)
*Biying Zhou,Nanyu Luo,Feng Ji*

Main category: cs.LG

TL;DR: FedIRT integrates federated learning with IRT to enable privacy-preserving, distributed estimation of latent abilities and item difficulty without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Traditional IRT requires centralized data, raising privacy concerns. Federated learning offers privacy protection and distributed computing, motivating its integration with IRT.

Method: Proposes Federated Item Response Theory (FedIRT), a framework for distributed IRT estimation. Validated using numerical experiments and a real-world exam dataset.

Result: FedIRT matches standard IRT accuracy while ensuring privacy and reducing communication costs. An open-source R package implements the framework for 2PL and PCM models.

Conclusion: FedIRT extends IRT to distributed settings like multi-school assessments, maintaining accuracy and security. The framework is practical and supported by an open-source tool.

Abstract: Item Response Theory (IRT) models have been widely used to estimate
respondents' latent abilities and calibrate items' difficulty. Traditional IRT
estimation requires all individual raw response data to be centralized in one
place, thus potentially causing privacy issues. Federated learning is an
emerging field in computer science and machine learning with added features of
privacy protection and distributed computing. To integrate the advances from
federated learning with modern psychometrics, we propose a novel framework,
Federated Item Response Theory (IRT), to enable estimating traditional IRT
models with additional privacy, allowing estimation in a distributed manner
without losing estimation accuracy.
  Our numerical experiments confirm that FedIRT achieves statistical accuracy
similar to standard IRT estimation using popular R packages, while offering
critical advantages: privacy protection and reduced communication costs. We
also validate FedIRT's utility through a real-world exam dataset, demonstrating
its effectiveness in realistic educational contexts. This new framework extends
IRT's applicability to distributed settings, such as multi-school assessments,
without sacrificing accuracy or security. To support practical adoption, we
provide an open-ource R package, FedIRT, implementing the framework for the
two-parameter logistic (2PL) and partial credit models (PCM).

</details>


### [24] [Gradient-Based Neuroplastic Adaptation for Concurrent Optimization of Neuro-Fuzzy Networks](https://arxiv.org/abs/2506.21771)
*John Wesley Hostetter,Min Chi*

Main category: cs.LG

TL;DR: A novel gradient-based neuroplastic adaptation method is proposed for concurrent optimization of neuro-fuzzy networks (NFNs) parameters and structure, enabling applications like online reinforcement learning for vision-based tasks.


<details>
  <summary>Details</summary>
Motivation: NFNs combine transparency and performance but lack a systematic design process, often leading to suboptimal architectures due to isolated optimization of parameters and structure.

Method: The approach involves gradient-based neuroplastic adaptation to simultaneously optimize NFNs' parameters and structure, addressing their interdependence.

Result: The method successfully trains NFNs via online reinforcement learning, demonstrated by proficient performance in vision-based tasks like playing DOOM.

Conclusion: Concurrent optimization of NFNs' parameters and structure unlocks new applications, such as vision-based reinforcement learning, overcoming previous limitations.

Abstract: Neuro-fuzzy networks (NFNs) are transparent, symbolic, and universal function
approximations that perform as well as conventional neural architectures, but
their knowledge is expressed as linguistic IF-THEN rules. Despite these
advantages, their systematic design process remains a challenge. Existing work
will often sequentially build NFNs by inefficiently isolating parametric and
structural identification, leading to a premature commitment to brittle and
subpar architecture. We propose a novel application-independent approach called
gradient-based neuroplastic adaptation for the concurrent optimization of NFNs'
parameters and structure. By recognizing that NFNs' parameters and structure
should be optimized simultaneously as they are deeply conjoined, settings
previously unapproachable for NFNs are now accessible, such as the online
reinforcement learning of NFNs for vision-based tasks. The effectiveness of
concurrently optimizing NFNs is empirically shown as it is trained by online
reinforcement learning to proficiently play challenging scenarios from a
vision-based video game called DOOM.

</details>


### [25] [M3PO: Massively Multi-Task Model-Based Policy Optimization](https://arxiv.org/abs/2506.21782)
*Aditya Narendra,Dmitry Makarov,Aleksandr Panov*

Main category: cs.LG

TL;DR: M3PO is a scalable model-based RL framework addressing sample inefficiency and poor generalization by integrating an implicit world model with hybrid exploration.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of existing model-based (e.g., DreamerV3) and model-free (e.g., PPO) methods, which suffer from control-centric neglect, high sample complexity, and weak exploration.

Method: Combines an implicit world model (predicting task outcomes without reconstruction) with hybrid exploration (model-based planning + model-free uncertainty bonuses) and trust-region optimization.

Result: Eliminates bias-variance trade-off, achieves state-of-the-art performance across benchmarks.

Conclusion: M3PO offers an efficient, robust alternative to existing model-based policy optimization methods.

Abstract: We introduce Massively Multi-Task Model-Based Policy Optimization (M3PO), a
scalable model-based reinforcement learning (MBRL) framework designed to
address sample inefficiency in single-task settings and poor generalization in
multi-task domains. Existing model-based approaches like DreamerV3 rely on
pixel-level generative models that neglect control-centric representations,
while model-free methods such as PPO suffer from high sample complexity and
weak exploration. M3PO integrates an implicit world model, trained to predict
task outcomes without observation reconstruction, with a hybrid exploration
strategy that combines model-based planning and model-free uncertainty-driven
bonuses. This eliminates the bias-variance trade-off in prior methods by using
discrepancies between model-based and model-free value estimates to guide
exploration, while maintaining stable policy updates through a trust-region
optimizer. M3PO provides an efficient and robust alternative to existing
model-based policy optimization approaches and achieves state-of-the-art
performance across multiple benchmarks.

</details>


### [26] [Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data](https://arxiv.org/abs/2506.21788)
*Massimiliano Lupo Pasini,Jong Youl Choi,Pei Zhang,Kshitij Mehta,Rylie Weaver,Ashwin M. Aji,Karl W. Schulz,Jorda Polo,Prasanna Balaprakash*

Main category: cs.LG

TL;DR: A multi-task parallelism method for graph foundation models improves scalability and efficiency on supercomputers, trained on 24M structures.


<details>
  <summary>Details</summary>
Motivation: To address challenges in processing multi-source, multi-fidelity data and enhance model transferability in atomistic modeling.

Method: Uses multi-task learning with shared message passing layers and multiple decoding heads, implemented in HydraGNN with GPU acceleration.

Result: Demonstrated efficient scaling on Perlmutter, Aurora, and Frontier supercomputers.

Conclusion: The method shows promise for handling larger, diverse datasets and heterogeneous supercomputing architectures.

Abstract: Graph foundation models using graph neural networks promise sustainable,
efficient atomistic modeling. To tackle challenges of processing multi-source,
multi-fidelity data during pre-training, recent studies employ multi-task
learning, in which shared message passing layers initially process input
atomistic structures regardless of source, then route them to multiple decoding
heads that predict data-specific outputs. This approach stabilizes pre-training
and enhances a model's transferability to unexplored chemical regions.
Preliminary results on approximately four million structures are encouraging,
yet questions remain about generalizability to larger, more diverse datasets
and scalability on supercomputers. We propose a multi-task parallelism method
that distributes each head across computing resources with GPU acceleration.
Implemented in the open-source HydraGNN architecture, our method was trained on
over 24 million structures from five datasets and tested on the Perlmutter,
Aurora, and Frontier supercomputers, demonstrating efficient scaling on all
three highly heterogeneous super-computing architectures.

</details>


### [27] [Why Neural Network Can Discover Symbolic Structures with Gradient-based Training: An Algebraic and Geometric Foundation for Neurosymbolic Reasoning](https://arxiv.org/abs/2506.21797)
*Peihao Wang,Zhangyang Wang*

Main category: cs.LG

TL;DR: A theoretical framework explains how discrete symbolic structures emerge from continuous neural network training dynamics via Wasserstein gradient flow and geometric constraints.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between continuous neural learning and discrete symbolic reasoning by uncovering natural emergence of symbolic structures.

Method: Lifts neural parameters to a measure space, models training as Wasserstein gradient flow, and analyzes decoupling and contraction under geometric constraints.

Result: Training transitions from high-dimensional exploration to compositional representations with lower degrees of freedom, complying with algebraic operations.

Conclusion: Provides a principled foundation for neurosymbolic systems integrating continuous learning with discrete algebraic reasoning.

Abstract: We develop a theoretical framework that explains how discrete symbolic
structures can emerge naturally from continuous neural network training
dynamics. By lifting neural parameters to a measure space and modeling training
as Wasserstein gradient flow, we show that under geometric constraints, such as
group invariance, the parameter measure $\mu_t$ undergoes two concurrent
phenomena: (1) a decoupling of the gradient flow into independent optimization
trajectories over some potential functions, and (2) a progressive contraction
on the degree of freedom. These potentials encode algebraic constraints
relevant to the task and act as ring homomorphisms under a commutative
semi-ring structure on the measure space. As training progresses, the network
transitions from a high-dimensional exploration to compositional
representations that comply with algebraic operations and exhibit a lower
degree of freedom. We further establish data scaling laws for realizing
symbolic tasks, linking representational capacity to the group invariance that
facilitates symbolic solutions. This framework charts a principled foundation
for understanding and designing neurosymbolic systems that integrate continuous
learning with discrete algebraic reasoning.

</details>


### [28] [The Cost of Avoiding Backpropagation](https://arxiv.org/abs/2506.21833)
*Kunjal Panchal,Sunav Choudhary,Yuriy Brun,Hui Guan*

Main category: cs.LG

TL;DR: The paper compares forward-mode automatic differentiation (FmAD) and zero-order (ZO) optimization with backpropagation (BP) and checkpointing, showing BP's superiority in accuracy, speed, and efficiency.


<details>
  <summary>Details</summary>
Motivation: To clarify the practical benefits of FmAD and ZO compared to memory-efficient BP variants like checkpointing, addressing gaps in prior comparisons and theoretical analysis.

Method: Theoretical and empirical comparison of BP, FmAD, and ZO methods, including experiments on large language and vision-language models.

Result: BP with checkpointing outperforms FmAD and ZO, achieving higher accuracy (31.1%), faster convergence (34.8%), and fewer computations (3.8x) at similar memory usage.

Conclusion: BP with checkpointing remains the most effective for memory-constrained training, highlighting fundamental limitations of FmAD and ZO.

Abstract: Forward-mode automatic differentiation (FmAD) and zero-order (ZO)
optimization have been proposed as memory-efficient alternatives to
backpropagation (BP) for gradient computation, especially in low-resource
settings. However, their practical benefits remain unclear due to two key gaps:
a lack of comparison against memory-efficient BP variants, such as activation
checkpointing, and a lack of a unified theoretical analysis. This work presents
a comprehensive theoretical and empirical comparison of BP, FmAD, and ZO
methods. Our theoretical analysis shows that while FmAD, and ZO can reduce
memory usage, they incur significant costs in accuracy, convergence speed, and
computation compared to BP with checkpointing. These drawbacks worsen with
larger models or constrained perturbation budgets. Empirical experiments on
large language and vision-language models show that BP with checkpointing
outperforms FmAD and ZO variants, including those enhanced with variance
reduction, achieving up to 31.1% higher accuracy, 34.8% faster convergence, and
3.8x fewer computations at comparable memory usage. Our results highlight
fundamental limitations of FmAD and ZO, and reaffirm BP with checkpointing as
the most effective strategy for model training under memory-constrained
settings. Our code is available at
https://github.com/Astuary/The_Cost_of_Avoiding_Backpropagation.

</details>


### [29] [Koopman operator-based discussion on partial observation in stochastic systems](https://arxiv.org/abs/2506.21844)
*Jun Ohkubo*

Main category: cs.LG

TL;DR: The paper explores partial observations in stochastic systems using Koopman operator theory, emphasizing the distinction between state and function spaces. It highlights the benefits of delay embedding and examines a power-law behavior in accuracy related to noise amplitude.


<details>
  <summary>Details</summary>
Motivation: Partial observations are common in practical scenarios, and understanding their effects in stochastic systems is crucial. The study aims to bridge the Mori-Zwanzig formalism and Koopman operator theory for such systems.

Method: The authors use Koopman operator theory to analyze partial observations in stochastic systems, employing delay embedding techniques and conducting numerical experiments.

Result: Numerical experiments reveal a power-law behavior in accuracy relative to additive noise amplitude. The study also identifies the importance of distinguishing state and function spaces.

Conclusion: The work underscores the utility of delay embedding in stochastic systems and the significance of the power-law exponent in understanding partial observation effects.

Abstract: It is sometimes difficult to achieve a complete observation for a full set of
observables, and partial observations are necessary. For deterministic systems,
the Mori-Zwanzig formalism provides a theoretical framework for handling
partial observations. Recently, data-driven algorithms based on the Koopman
operator theory have made significant progress, and there is a discussion to
connect the Mori-Zwanzig formalism with the Koopman operator theory. In this
work, we discuss the effects of partial observation in stochastic systems using
the Koopman operator theory. The discussion clarifies the importance of
distinguishing the state space and the function space in stochastic systems.
Even in stochastic systems, the delay embedding technique is beneficial for
partial observation, and several numerical experiments showed a power-law
behavior of the accuracy for the amplitude of the additive noise. We also
discuss the relation between the exponent of the power-law behavior and the
effects of partial observation.

</details>


### [30] [A Survey of Continual Reinforcement Learning](https://arxiv.org/abs/2506.21872)
*Chaofan Pan,Xin Yang,Yanhua Li,Wei Wei,Tianrui Li,Bo An,Jiye Liang*

Main category: cs.LG

TL;DR: The paper surveys Continual Reinforcement Learning (CRL), addressing RL's limitations like data dependency and poor generalization by enabling continuous learning and knowledge retention. It reviews existing works, proposes a new taxonomy, and discusses future challenges.


<details>
  <summary>Details</summary>
Motivation: RL's reliance on extensive data and poor generalization limits its real-world applicability. CRL emerges as a solution to enable continuous learning and adaptability.

Method: The survey reviews existing CRL works, organizes metrics and benchmarks, and proposes a taxonomy categorizing CRL methods into four types based on knowledge storage/transfer.

Result: A comprehensive analysis of CRL's core concepts, challenges, and methodologies, along with a new taxonomy and practical insights for future research.

Conclusion: CRL is a promising direction to overcome RL's limitations, but challenges remain. Future work should focus on improving adaptability and knowledge retention.

Abstract: Reinforcement Learning (RL) is an important machine learning paradigm for
solving sequential decision-making problems. Recent years have witnessed
remarkable progress in this field due to the rapid development of deep neural
networks. However, the success of RL currently relies on extensive training
data and computational resources. In addition, RL's limited ability to
generalize across tasks restricts its applicability in dynamic and real-world
environments. With the arisen of Continual Learning (CL), Continual
Reinforcement Learning (CRL) has emerged as a promising research direction to
address these limitations by enabling agents to learn continuously, adapt to
new tasks, and retain previously acquired knowledge. In this survey, we provide
a comprehensive examination of CRL, focusing on its core concepts, challenges,
and methodologies. Firstly, we conduct a detailed review of existing works,
organizing and analyzing their metrics, tasks, benchmarks, and scenario
settings. Secondly, we propose a new taxonomy of CRL methods, categorizing them
into four types from the perspective of knowledge storage and/or transfer.
Finally, our analysis highlights the unique challenges of CRL and provides
practical insights into future directions.

</details>


### [31] [Advancements and Challenges in Continual Reinforcement Learning: A Comprehensive Review](https://arxiv.org/abs/2506.21899)
*Amara Zuffer,Michael Burke,Mehrtash Harandi*

Main category: cs.LG

TL;DR: A survey on continual reinforcement learning (RL) explores its role in enabling RL agents to learn sequentially and dynamically, covering key concepts, challenges, methodologies, and recent advancements in robotics.


<details>
  <summary>Details</summary>
Motivation: To address the need for RL agents to learn continuously and retain reusable knowledge in dynamic environments.

Method: Reviews fundamental aspects, key concepts, challenges, and novel methodologies in continual RL, with a focus on robotics.

Result: Highlights recent advancements and evaluation environments, making the field accessible to newcomers.

Conclusion: Discusses limitations and future directions, offering insights for researchers and practitioners.

Abstract: The diversity of tasks and dynamic nature of reinforcement learning (RL)
require RL agents to be able to learn sequentially and continuously, a learning
paradigm known as continuous reinforcement learning. This survey reviews how
continual learning transforms RL agents into dynamic continual learners. This
enables RL agents to acquire and retain useful and reusable knowledge
seamlessly. The paper delves into fundamental aspects of continual
reinforcement learning, exploring key concepts, significant challenges, and
novel methodologies. Special emphasis is placed on recent advancements in
continual reinforcement learning within robotics, along with a succinct
overview of evaluation environments utilized in prominent research,
facilitating accessibility for newcomers to the field. The review concludes
with a discussion on limitations and promising future directions, providing
valuable insights for researchers and practitioners alike.

</details>


### [32] [TOAST: Task-Oriented Adaptive Semantic Transmission over Dynamic Wireless Environments](https://arxiv.org/abs/2506.21900)
*Sheng Yun,Jianhua Pei,Ping Wang*

Main category: cs.LG

TL;DR: TOAST is a framework for semantic-aware 6G communication, optimizing multi-task performance in dynamic wireless environments using adaptive task balancing, LoRA mechanisms, and a diffusion model.


<details>
  <summary>Details</summary>
Motivation: The shift to 6G requires semantic-aware communication, focusing on task-relevant information rather than bit-centric transmission.

Method: TOAST combines deep reinforcement learning for task balancing, LoRA for efficient fine-tuning in a Swin Transformer-based architecture, and a diffusion model for noise restoration.

Result: TOAST outperforms baselines in classification accuracy and reconstruction quality, especially in low SNR conditions, while remaining robust across scenarios.

Conclusion: TOAST provides a scalable, efficient solution for semantic-aware communication in 6G networks, addressing multi-task optimization challenges.

Abstract: The evolution toward 6G networks demands a fundamental shift from bit-centric
transmission to semantic-aware communication that emphasizes task-relevant
information. This work introduces TOAST (Task-Oriented Adaptive Semantic
Transmission), a unified framework designed to address the core challenge of
multi-task optimization in dynamic wireless environments through three
complementary components. First, we formulate adaptive task balancing as a
Markov decision process, employing deep reinforcement learning to dynamically
adjust the trade-off between image reconstruction fidelity and semantic
classification accuracy based on real-time channel conditions. Second, we
integrate module-specific Low-Rank Adaptation (LoRA) mechanisms throughout our
Swin Transformer-based joint source-channel coding architecture, enabling
parameter-efficient fine-tuning that dramatically reduces adaptation overhead
while maintaining full performance across diverse channel impairments including
Additive White Gaussian Noise (AWGN), fading, phase noise, and impulse
interference. Third, we incorporate an Elucidating diffusion model that
operates in the latent space to restore features corrupted by channel noises,
providing substantial quality improvements compared to baseline approaches.
Extensive experiments across multiple datasets demonstrate that TOAST achieves
superior performance compared to baseline approaches, with significant
improvements in both classification accuracy and reconstruction quality at low
Signal-to-Noise Ratio (SNR) conditions while maintaining robust performance
across all tested scenarios.

</details>


### [33] [HQCM-EBTC: A Hybrid Quantum-Classical Model for Explainable Brain Tumor Classification](https://arxiv.org/abs/2506.21937)
*Marwan Ait Haddou,Mohamed Bennai*

Main category: cs.LG

TL;DR: HQCM-EBTC is a hybrid quantum-classical model for brain tumor classification from MRI images, achieving 96.48% accuracy and outperforming classical methods.


<details>
  <summary>Details</summary>
Motivation: To enhance diagnostic accuracy and interpretability in brain tumor classification using quantum-enhanced models.

Method: Integrates a 5-qubit quantum layer with classical components, trained on 7,576 MRI scans using AdamW and a composite loss function.

Result: Achieves 96.48% accuracy, higher precision/F1-scores, and better feature separability in quantum space.

Conclusion: Demonstrates the potential of quantum-enhanced models for improving medical imaging diagnostics.

Abstract: We propose HQCM-EBTC, a hybrid quantum-classical model for automated brain
tumor classification using MRI images. Trained on a dataset of 7,576 scans
covering normal, meningioma, glioma, and pituitary classes, HQCM-EBTC
integrates a 5-qubit, depth-2 quantum layer with 5 parallel circuits, optimized
via AdamW and a composite loss blending cross-entropy and attention
consistency.
  HQCM-EBTC achieves 96.48% accuracy, substantially outperforming the classical
baseline (86.72%). It delivers higher precision and F1-scores, especially for
glioma detection. t-SNE projections reveal enhanced feature separability in
quantum space, and confusion matrices show lower misclassification. Attention
map analysis (Jaccard Index) confirms more accurate and focused tumor
localization at high-confidence thresholds.
  These results highlight the promise of quantum-enhanced models in medical
imaging, advancing both diagnostic accuracy and interpretability for clinical
brain tumor assessment.

</details>


### [34] [GuiderNet: A Meta-Learning Framework for Optimizing Quantum Circuit Geometry and Mitigating Barren Plateaus](https://arxiv.org/abs/2506.21940)
*Marwan Ait Haddou,Mohamed Bennai*

Main category: cs.LG

TL;DR: GuiderNet, a meta-learning framework, improves VQA performance by conditioning PQCs to avoid barren plateaus and poor optimization landscapes, achieving significant gains in training loss, accuracy, and stability.


<details>
  <summary>Details</summary>
Motivation: Address challenges like barren plateaus and poorly conditioned landscapes in VQAs to enhance quantum machine learning performance.

Method: Introduces GuiderNet, a classical neural network meta-trained to guide PQC parameters into favorable regions, embedded in hybrid quantum-classical pipelines.

Result: Reduces training loss by 5x, boosts test accuracy from 75.3% to 98.6%, and improves minority-class F1 score from 0.67 to 0.95.

Conclusion: Geometric meta-conditioning via GuiderNet mitigates barren plateaus and ill-conditioning, enhancing trainability and generalization in quantum machine learning.

Abstract: Variational Quantum Algorithms (VQAs) offer potential for near-term quantum
advantage but face challenges from barren plateaus, where gradients vanish, and
poorly conditioned optimization landscapes. We introduce GuiderNet, a
meta-learning framework that conditions Parameterized Quantum Circuits (PQCs)
using data-dependent parameter shifts aimed at minimizing the log condition
number of the Fubini-Study metric tensor. Implemented as a classical neural
network, GuiderNet is meta-trained to guide PQC parameters into geometrically
favorable regions and is embedded within hybrid quantum-classical pipelines to
steer both initialization and adaptive modulation during training.
  Applied to the Kaggle Diabetes classification task, GuiderNet reduces
cumulative training loss by over 5x, improves test accuracy from 75.3% to
98.6%, and increases the minority-class F1 score from 0.67 to 0.95. It also
suppresses gradient explosion and stabilizes parameter updates, enabling
smoother and more robust optimization. These results demonstrate that geometric
meta-conditioning can mitigate barren plateaus and ill-conditioning, providing
a scalable approach to enhance trainability and generalization in quantum
machine learning.

</details>


### [35] [Physics-informed network paradigm with data generation and background noise removal for diverse distributed acoustic sensing applications](https://arxiv.org/abs/2506.21952)
*Yangyang Wan,Haotian Wang,Xuhui Yu,Jiageng Chen,Xinyu Fan,Zuyuan He*

Main category: cs.LG

TL;DR: A physics-informed DAS neural network paradigm is proposed, eliminating the need for real-world event data by using physical modeling and generative networks for training, achieving high accuracy in applications like fault monitoring.


<details>
  <summary>Details</summary>
Motivation: Limited availability of real-world event data for training AI models in DAS applications motivates the development of a physics-informed approach.

Method: The method involves physically modeling target events and deriving physical functions to train a generative network for synthetic DAS data, followed by training a debackground net for noise removal.

Result: The paradigm achieves 91.8% fault diagnosis accuracy in field tests, outperforming data-driven networks trained with real-world data, and demonstrates generalization across sites.

Conclusion: The physics-informed paradigm addresses data acquisition and noise challenges in DAS, offering a promising solution for practical applications and expanding DAS potential.

Abstract: Distributed acoustic sensing (DAS) has attracted considerable attention
across various fields and artificial intelligence (AI) technology plays an
important role in DAS applications to realize event recognition and denoising.
Existing AI models require real-world data (RWD), whether labeled or not, for
training, which is contradictory to the fact of limited available event data in
real-world scenarios. Here, a physics-informed DAS neural network paradigm is
proposed, which does not need real-world events data for training. By
physically modeling target events and the constraints of real world and DAS
system, physical functions are derived to train a generative network for
generation of DAS events data. DAS debackground net is trained by using the
generated DAS events data to eliminate background noise in DAS data. The
effectiveness of the proposed paradigm is verified in event identification
application based on a public dataset of DAS spatiotemporal data and in belt
conveyor fault monitoring application based on DAS time-frequency data, and
achieved comparable or better performance than data-driven networks trained
with RWD. Owing to the introduction of physical information and capability of
background noise removal, the paradigm demonstrates generalization in same
application on different sites. A fault diagnosis accuracy of 91.8% is achieved
in belt conveyor field with networks which transferred from simulation test
site without any fault events data of test site and field for training. The
proposed paradigm is a prospective solution to address significant obstacles of
data acquisition and intense noise in practical DAS applications and explore
more potential fields for DAS.

</details>


### [36] [On the Necessity of Output Distribution Reweighting for Effective Class Unlearning](https://arxiv.org/abs/2506.20893)
*Yian Wang,Ali Ebrahimpour-Boroojeny,Hari Sundaram*

Main category: cs.LG

TL;DR: RWFT is a lightweight output-reweighting method for class unlearning in classifiers, avoiding costly retraining. It outperforms existing methods in metrics and robustness to attacks like MIA-NN.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient class unlearning to enforce deletion rights and reduce biased predictions without full retraining.

Method: Proposes RWFT, a technique redistributing prediction probability mass for forgotten classes, robust against MIA-NN attacks. Introduces a TV-based metric for leakage quantification.

Result: Matches full retraining performance, gains 2.79% in prior metrics and 111.45% in the new TV-based metric over state-of-the-art methods.

Conclusion: RWFT effectively unlearns classes without retraining, outperforming existing methods and resisting attacks, validated by new metrics.

Abstract: In this work, we introduce an output-reweighting unlearning method, RWFT, a
lightweight technique that erases an entire class from a trained classifier
without full retraining. Forgetting specific classes from trained models is
essential for enforcing user deletion rights and mitigating harmful or biased
predictions. The full retraining is costly and existing unlearning methods fail
to replicate the behavior of the retrained models when predicting samples from
the unlearned class. We prove this failure by designing a variant of membership
inference attacks, MIA-NN that successfully reveals the unlearned class for any
of these methods. We propose a simple redistribution of the probability mass
for the prediction on the samples in the forgotten class which is robust to
MIA-NN. We also introduce a new metric based on the total variation (TV)
distance of the prediction probabilities to quantify residual leakage to
prevent future methods from susceptibility to the new attack. Through extensive
experiments with state of the art baselines in machine unlearning, we show that
our approach matches the results of full retraining in both metrics used for
evaluation by prior work and the new metric we propose in this work. Compare to
state-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%
in our new TV-based metric over the best existing method.

</details>


### [37] [Optimal Return-to-Go Guided Decision Transformer for Auto-Bidding in Advertisement](https://arxiv.org/abs/2506.21956)
*Hao Jiang,Yongxiang Tang,Yanxiang Zeng,Pengjia Yuan,Yanhua Cheng,Teng Sha,Xialong Liu,Peng Jiang*

Main category: cs.LG

TL;DR: The paper introduces R* Decision Transformer (R* DT) to improve auto-bidding in online advertising by addressing limitations of traditional Decision Transformer (DT).


<details>
  <summary>Details</summary>
Motivation: Automated bidding systems in online advertising face challenges like preset RTG requirements and mixed-quality training data.

Method: R* DT is developed in three steps: R DT (stores actions and RTG), R^ DT (forecasts optimal RTG), and R* DT (enhances training data with high-reward trajectories).

Result: Tests on a bidding dataset show R* DT outperforms traditional DT, especially with mixed-quality trajectories.

Conclusion: R* DT effectively improves auto-bidding by optimizing RTG and enhancing training data, leading to better performance.

Abstract: In the realm of online advertising, advertisers partake in ad auctions to
obtain advertising slots, frequently taking advantage of auto-bidding tools
provided by demand-side platforms. To improve the automation of these bidding
systems, we adopt generative models, namely the Decision Transformer (DT), to
tackle the difficulties inherent in automated bidding. Applying the Decision
Transformer to the auto-bidding task enables a unified approach to sequential
modeling, which efficiently overcomes short-sightedness by capturing long-term
dependencies between past bidding actions and user behavior. Nevertheless,
conventional DT has certain drawbacks: (1) DT necessitates a preset
return-to-go (RTG) value before generating actions, which is not inherently
produced; (2) The policy learned by DT is restricted by its training data,
which is consists of mixed-quality trajectories. To address these challenges,
we introduce the R* Decision Transformer (R* DT), developed in a three-step
process: (1) R DT: Similar to traditional DT, R DT stores actions based on
state and RTG value, as well as memorizing the RTG for a given state using the
training set; (2) R^ DT: We forecast the highest value (within the training
set) of RTG for a given state, deriving a suboptimal policy based on the
current state and the forecasted supreme RTG value; (3) R* DT: Based on R^ DT,
we generate trajectories and select those with high rewards (using a simulator)
to augment our training dataset. This data enhancement has been shown to
improve the RTG of trajectories in the training data and gradually leads the
suboptimal policy towards optimality. Comprehensive tests on a publicly
available bidding dataset validate the R* DT's efficacy and highlight its
superiority when dealing with mixed-quality trajectories.

</details>


### [38] [SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model](https://arxiv.org/abs/2506.21976)
*Shuhan Tan,John Lambert,Hong Jeon,Sakshum Kulshrestha,Yijing Bai,Jing Luo,Dragomir Anguelov,Mingxing Tan,Chiyu Max Jiang*

Main category: cs.LG

TL;DR: CitySim aims to create a generative simulated city for autonomous vehicle testing by integrating scene generation, agent behavior, and environment simulation. SceneDiffuser++ is proposed as an end-to-end solution, showing superior realism in city-scale traffic simulation.


<details>
  <summary>Details</summary>
Motivation: To enhance autonomous vehicle testing by generating synthetic miles in a simulated city, addressing gaps in dynamic scene generation and environment simulation.

Method: Proposes SceneDiffuser++, an end-to-end generative world model trained on a single loss function, integrating scene generation, agent behavior, occlusion reasoning, and environment simulation.

Result: Demonstrates city-scale traffic simulation with superior realism, evaluated on an augmented Waymo Open Motion Dataset.

Conclusion: SceneDiffuser++ effectively addresses the challenges of CitySim, providing a scalable and realistic solution for autonomous vehicle testing.

Abstract: The goal of traffic simulation is to augment a potentially limited amount of
manually-driven miles that is available for testing and validation, with a much
larger amount of simulated synthetic miles. The culmination of this vision
would be a generative simulated city, where given a map of the city and an
autonomous vehicle (AV) software stack, the simulator can seamlessly simulate
the trip from point A to point B by populating the city around the AV and
controlling all aspects of the scene, from animating the dynamic agents (e.g.,
vehicles, pedestrians) to controlling the traffic light states. We refer to
this vision as CitySim, which requires an agglomeration of simulation
technologies: scene generation to populate the initial scene, agent behavior
modeling to animate the scene, occlusion reasoning, dynamic scene generation to
seamlessly spawn and remove agents, and environment simulation for factors such
as traffic lights. While some key technologies have been separately studied in
various works, others such as dynamic scene generation and environment
simulation have received less attention in the research community. We propose
SceneDiffuser++, the first end-to-end generative world model trained on a
single loss function capable of point A-to-B simulation on a city scale
integrating all the requirements above. We demonstrate the city-scale traffic
simulation capability of SceneDiffuser++ and study its superior realism under
long simulation conditions. We evaluate the simulation quality on an augmented
version of the Waymo Open Motion Dataset (WOMD) with larger map regions to
support trip-level simulation.

</details>


### [39] [Binned semiparametric Bayesian networks](https://arxiv.org/abs/2506.21997)
*Rafael Sojo,Javier Díaz-Rozo,Concha Bielza,Pedro Larrañaga*

Main category: cs.LG

TL;DR: A new binned semiparametric Bayesian network model reduces computational costs in kernel density estimation while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the computational inefficiency and curse of dimensionality in nonparametric kernel density estimation.

Method: Develops two new conditional probability distributions (sparse binned and Fourier kernel density estimation) using data binning, sparse tensors, and parent node restrictions.

Result: Achieves comparable accuracy to non-binned models but with significantly higher speed.

Conclusion: The binned semiparametric Bayesian networks are a reliable and efficient alternative to traditional methods.

Abstract: This paper introduces a new type of probabilistic semiparametric model that
takes advantage of data binning to reduce the computational cost of kernel
density estimation in nonparametric distributions. Two new conditional
probability distributions are developed for the new binned semiparametric
Bayesian networks, the sparse binned kernel density estimation and the Fourier
kernel density estimation. These two probability distributions address the
curse of dimensionality, which typically impacts binned models, by using sparse
tensors and restricting the number of parent nodes in conditional probability
calculations. To evaluate the proposal, we perform a complexity analysis and
conduct several comparative experiments using synthetic data and datasets from
the UCI Machine Learning repository. The experiments include different binning
rules, parent restrictions, grid sizes, and number of instances to get a
holistic view of the model's behavior. As a result, our binned semiparametric
Bayesian networks achieve structural learning and log-likelihood estimations
with no statistically significant differences compared to the semiparametric
Bayesian networks, but at a much higher speed. Thus, the new binned
semiparametric Bayesian networks prove to be a reliable and more efficient
alternative to their non-binned counterparts.

</details>


### [40] [GKNet: Graph Kalman Filtering and Model Inference via Model-based Deep Learning](https://arxiv.org/abs/2506.22004)
*Mohammad Sabbaqi,Riccardo Taormina,Elvin Isufi*

Main category: cs.LG

TL;DR: A graph-aware state space model for graph time series is proposed, combining graph-temporal patterns with a parametric approach for efficient learning and inference.


<details>
  <summary>Details</summary>
Motivation: To address the need for computationally affordable models that capture graph-temporal patterns in applications like urban water networks, economics, and neuroscience.

Method: Uses a graph-induced state space model with stochastic partial differential equations for the state and graph-filtered observations. Combines maximum likelihood for tractability and deep learning for scalability.

Result: The model effectively learns parameters for downstream tasks like prediction and imputation, balancing theoretical tractability and practical scalability.

Conclusion: The proposed model offers a scalable and expressive solution for graph time series tasks, leveraging both traditional and deep learning approaches.

Abstract: Inference tasks with time series over graphs are of importance in
applications such as urban water networks, economics, and networked
neuroscience. Addressing these tasks typically relies on identifying a
computationally affordable model that jointly captures the graph-temporal
patterns of the data. In this work, we propose a graph-aware state space model
for graph time series, where both the latent state and the observation equation
are parametric graph-induced models with a limited number of parameters that
need to be learned. More specifically, we consider the state equation to follow
a stochastic partial differential equation driven by noise over the graphs
edges accounting not only for potential edge uncertainties but also for
increasing the degrees of freedom in the latter in a tractable manner. The
graph structure conditioning of the noise dispersion allows the state variable
to deviate from the stochastic process in certain neighborhoods. The
observation model is a sampled and graph-filtered version of the state
capturing multi-hop neighboring influence. The goal is to learn the parameters
in both state and observation models from the partially observed data for
downstream tasks such as prediction and imputation. The model is inferred first
through a maximum likelihood approach that provides theoretical tractability
but is limited in expressivity and scalability. To improve on the latter, we
use the state-space formulation to build a principled deep learning
architecture that jointly learns the parameters and tracks the state in an
end-to-end manner in the spirit of Kalman neural networks.

</details>


### [41] [TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning](https://arxiv.org/abs/2506.22008)
*Alessandro Sestini,Joakim Bergdahl,Konrad Tollmar,Andrew D. Bagdanov,Linus Gisslén*

Main category: cs.LG

TL;DR: TROFI is a novel offline RL method that learns policies without predefined rewards by inferring rewards from human preferences, outperforming baselines and matching ground-truth performance.


<details>
  <summary>Details</summary>
Motivation: Offline RL often lacks predefined rewards, especially in applied settings like gaming. TROFI addresses this by learning rewards from human preferences.

Method: TROFI learns a reward function from human preferences, labels the dataset, and trains the policy offline without needing optimal trajectories.

Result: TROFI outperforms baselines on D4RL and performs comparably to ground-truth rewards. It also works well in 3D game environments.

Conclusion: A well-engineered reward function is crucial for aligning value functions to actual rewards, and TROFI effectively learns policies without predefined rewards.

Abstract: In offline reinforcement learning, agents are trained using only a fixed set
of stored transitions derived from a source policy. However, this requires that
the dataset be labeled by a reward function. In applied settings such as video
game development, the availability of the reward function is not always
guaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement
learning (TROFI), a novel approach to effectively learn a policy offline
without a pre-defined reward function. TROFI first learns a reward function
from human preferences, which it then uses to label the original dataset making
it usable for training the policy. In contrast to other approaches, our method
does not require optimal trajectories. Through experiments on the D4RL
benchmark we demonstrate that TROFI consistently outperforms baselines and
performs comparably to using the ground truth reward to learn policies.
Additionally, we validate the efficacy of our method in a 3D game environment.
Our studies of the reward model highlight the importance of the reward function
in this setting: we show that to ensure the alignment of a value function to
the actual future discounted reward, it is fundamental to have a
well-engineered and easy-to-learn reward function.

</details>


### [42] [Hyper-modal Imputation Diffusion Embedding with Dual-Distillation for Federated Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2506.22036)
*Ying Zhang,Yu Zhao,Xuhui Sui,Baohang Zhou,Xiangrui Cai,Li Shen,Xiaojie Yuan,Dacheng Tao*

Main category: cs.LG

TL;DR: The paper introduces FedMKGC, a task for training federated multimodal knowledge graphs (MKGs) to predict missing links without sharing sensitive data. It proposes MMFeD3-HidE, a framework addressing multimodal challenges, with HidE for local modal recovery and MMFeD3 for federated distillation.


<details>
  <summary>Details</summary>
Motivation: Decentralized MKGs lack collaboration systems with strong reasoning and safety. FedMKGC aims to predict missing links securely without sharing sensitive knowledge.

Method: Proposes MMFeD3-HidE: HidE recovers multimodal distributions locally, while MMFeD3 uses logit and feature distillation for federated knowledge transfer.

Result: Experiments show MMFeD3-HidE's effectiveness, semantic consistency, and convergence robustness.

Conclusion: The framework successfully addresses multimodal challenges in federated MKG completion, validated by benchmark results.

Abstract: With the increasing multimodal knowledge privatization requirements,
multimodal knowledge graphs in different institutes are usually decentralized,
lacking of effective collaboration system with both stronger reasoning ability
and transmission safety guarantees. In this paper, we propose the Federated
Multimodal Knowledge Graph Completion (FedMKGC) task, aiming at training over
federated MKGs for better predicting the missing links in clients without
sharing sensitive knowledge. We propose a framework named MMFeD3-HidE for
addressing multimodal uncertain unavailability and multimodal client
heterogeneity challenges of FedMKGC. (1) Inside the clients, our proposed
Hyper-modal Imputation Diffusion Embedding model (HidE) recovers the complete
multimodal distributions from incomplete entity embeddings constrained by
available modalities. (2) Among clients, our proposed Multimodal FeDerated Dual
Distillation (MMFeD3) transfers knowledge mutually between clients and the
server with logit and feature distillation to improve both global convergence
and semantic consistency. We propose a FedMKGC benchmark for a comprehensive
evaluation, consisting of a general FedMKGC backbone named MMFedE, datasets
with heterogeneous multimodal information, and three groups of constructed
baselines. Experiments conducted on our benchmark validate the effectiveness,
semantic consistency, and convergence robustness of MMFeD3-HidE.

</details>


### [43] [UniCA: Adapting Time Series Foundation Model to General Covariate-Aware Forecasting](https://arxiv.org/abs/2506.22039)
*Lu Han,Yu Liu,Qiwen Deng,Jian Jiang,Yinbo Sun,Zhe Yu,Binfeng Wang,Xingyu Lu,Lintao Ma,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.LG

TL;DR: UniCA bridges Time Series Foundation Models (TSFMs) with covariate-aware forecasting by homogenizing and fusing diverse covariates, enhancing adaptability while preserving generalization.


<details>
  <summary>Details</summary>
Motivation: TSFMs struggle with heterogeneous covariates (e.g., categorical, multimodal data), limiting their use in general forecasting tasks. UniCA aims to address this gap.

Method: UniCA homogenizes covariates into series representations and fuses them via attention-based fusion, ensuring compatibility with diverse data types.

Result: UniCA outperforms on unimodal and multimodal benchmarks, demonstrating effective covariate-aware forecasting.

Conclusion: UniCA successfully adapts TSFMs for heterogeneous covariates, showing promise for real-world forecasting applications.

Abstract: Time Series Foundation Models (TSFMs) have achieved remarkable success
through large-scale pretraining. However, their design primarily targets
real-valued series, limiting their ability to handle general forecasting tasks
involving diverse and often heterogeneous covariates--such as categorical
variables and multimodal data (e.g., images, text)--which are typically
task-specific and difficult to leverage during pretraining. To address this
gap, we propose Unified Covariate Adaptation (UniCA), a framework to bridge
TSFMs with general covariate-aware forecasting. UniCA first performs covariate
homogenization to transform heterogeneous covariates into high-level
homogeneous series representations and then fuses them via a unified
attention-based fusion mechanism. UniCA is compatible and universal for
adaptation with both homogeneous and heterogeneous covariates, incorporating
extra covariate information while preserving the generalization ability of
TSFMs.Extensive experiments on multiple unimodal and multimodal covariate-aware
forecasting benchmarks demonstrate the superiority of UniCA, highlighting the
promise of covariate-aware TSFM adaptation in real-world forecasting scenarios.
Codes are released on https://github.com/hanlu-nju/UniCA.

</details>


### [44] [GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling](https://arxiv.org/abs/2506.22049)
*Tianhao Chen,Xin Xu,Zijing Liu,Pengxiang Li,Xinyuan Song,Ajay Kumar Jaiswal,Fan Zhang,Jishan Hu,Yang Wang,Hao Chen,Shizhe Diao,Shiwei Liu,Yu Li,Yin Lu,Can Yang*

Main category: cs.LG

TL;DR: GPAS mitigates activation variance growth in Pre-LN Transformers by scaling activations without altering gradients, improving performance across model sizes.


<details>
  <summary>Details</summary>
Motivation: Pre-LN Transformers suffer from exponential activation variance growth, limiting deeper layers' learning capacity.

Method: Proposes Gradient-Preserving Activation Scaling (GPAS), which scales down activations while preserving gradients.

Result: GPAS improves performance consistently across models (71M to 1B) and works with other architectures like Sandwich-LN and DeepNorm.

Conclusion: GPAS is a versatile solution for enhancing training dynamics in various Transformer architectures.

Abstract: Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,
predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While
being stable during pretraining and scalable to large model sizes, Pre-LN
suffers from an exponential growth in activation variance across layers,
causing the residual path to dominate over sub-layer outputs and limiting the
learning capacity of deeper layers. To mitigate this issue, we propose
Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be
used in combination with existing approaches. GPAS works by scaling down the
intermediate activations while keeping their gradients unchanged. This leaves
information in the activations intact, and avoids the gradient vanishing
problem associated with gradient downscaling. Extensive experiments across
various model sizes from 71M to 1B show that GPAS achieves consistent
performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows
promise in improving alternative architectures such as Sandwich-LN and
DeepNorm, demonstrating its versatility and potential for improving training
dynamics in a wide range of settings.

</details>


### [45] [crypto price prediction using lstm+xgboost](https://arxiv.org/abs/2506.22055)
*Mehul Gautam*

Main category: cs.LG

TL;DR: A hybrid LSTM+XGBoost model outperforms standalone models in cryptocurrency price prediction by leveraging temporal dependencies and nonlinear relationships.


<details>
  <summary>Details</summary>
Motivation: Cryptocurrency markets are volatile and complex, requiring advanced forecasting methods.

Method: Combines LSTM for temporal dependencies and XGBoost for nonlinear relationships with auxiliary features like sentiment scores.

Result: The hybrid model consistently outperforms standalone models and traditional methods, validated by MAPE and MinMax RMSE metrics.

Conclusion: Hybrid architectures like LSTM+XGBoost show promise for financial forecasting across diverse cryptocurrencies and markets.

Abstract: The volatility and complex dynamics of cryptocurrency markets present unique
challenges for accurate price forecasting. This research proposes a hybrid deep
learning and machine learning model that integrates Long Short-Term Memory
(LSTM) networks and Extreme Gradient Boosting (XGBoost) for cryptocurrency
price prediction. The LSTM component captures temporal dependencies in
historical price data, while XGBoost enhances prediction by modeling nonlinear
relationships with auxiliary features such as sentiment scores and
macroeconomic indicators. The model is evaluated on historical datasets of
Bitcoin, Ethereum, Dogecoin, and Litecoin, incorporating both global and
localized exchange data. Comparative analysis using Mean Absolute Percentage
Error (MAPE) and Min-Max Normalized Root Mean Square Error (MinMax RMSE)
demonstrates that the LSTM+XGBoost hybrid consistently outperforms standalone
models and traditional forecasting methods. This study underscores the
potential of hybrid architectures in financial forecasting and provides
insights into model adaptability across different cryptocurrencies and market
contexts.

</details>


### [46] [Transformers are Graph Neural Networks](https://arxiv.org/abs/2506.22084)
*Chaitanya K. Joshi*

Main category: cs.LG

TL;DR: Transformers are shown to be a type of GNN operating on fully connected graphs, leveraging self-attention for token relationships and positional encodings for structure, while benefiting from hardware efficiency.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between Transformers and GNNs, highlighting their shared principles and hardware advantages.

Method: Analyze Transformers as message-passing GNNs on fully connected token graphs, using self-attention and positional encodings.

Result: Transformers are expressive set processors with hardware-efficient dense operations, outperforming sparse GNN implementations.

Conclusion: Transformers can be viewed as GNNs optimized for modern hardware, offering insights into their success and potential for graph tasks.

Abstract: We establish connections between the Transformer architecture, originally
introduced for natural language processing, and Graph Neural Networks (GNNs)
for representation learning on graphs. We show how Transformers can be viewed
as message passing GNNs operating on fully connected graphs of tokens, where
the self-attention mechanism capture the relative importance of all tokens
w.r.t. each-other, and positional encodings provide hints about sequential
ordering or structure. Thus, Transformers are expressive set processing
networks that learn relationships among input elements without being
constrained by apriori graphs. Despite this mathematical connection to GNNs,
Transformers are implemented via dense matrix operations that are significantly
more efficient on modern hardware than sparse message passing. This leads to
the perspective that Transformers are GNNs currently winning the hardware
lottery.

</details>


### [47] [Learning to Solve Multi-Objective Routing Problems on Multigraphs](https://arxiv.org/abs/2506.22095)
*Filip Rydin,Attila Lischka,Jiaming Wu,Morteza Haghir Chehreghani,Balázs Kulcsár*

Main category: cs.LG

TL;DR: The paper introduces two neural approaches for multi-objective routing on multigraphs, addressing a gap in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing learning-based routing methods overlook multigraphs, despite their practical relevance.

Method: Two neural approaches: one works directly on multigraphs by autoregressively selecting edges, while the other first prunes the multigraph into a simple graph.

Result: Both models show strong performance in problems like TSP and CVRP.

Conclusion: The proposed methods effectively address multi-objective routing on multigraphs, demonstrating practical utility.

Abstract: Learning-based methods for routing have gained significant attention in
recent years, both in single-objective and multi-objective contexts. However,
the multigraph setting, where multiple paths with distinct attributes can exist
between destinations, has largely been overlooked, despite its high practical
relevancy. In this paper, we introduce two neural approaches to address
multi-objective routing on multigraphs. Our first approach works directly on
the multigraph, by autoregressively selecting edges until a tour is completed.
On the other hand, our second model first prunes the multigraph into a simple
graph and then builds routes. We validate both models experimentally and find
that they demonstrate strong performance across a variety of problems,
including the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP).

</details>


### [48] [Transfer Learning for Assessing Heavy Metal Pollution in Seaports Sediments](https://arxiv.org/abs/2506.22096)
*Tin Lai,Farnaz Farid,Yueyang Kuan,Xintian Zhang*

Main category: cs.LG

TL;DR: A deep-learning model using transfer learning simplifies heavy metal pollution assessment in soils and seaports, outperforming traditional methods with lower errors.


<details>
  <summary>Details</summary>
Motivation: Traditional PLI assessment is laborious and faces data scarcity and standardization issues.

Method: Proposes a transfer learning-based deep-learning model for accurate PLI prediction, evaluated on data from six Australian ports.

Result: Achieves significantly lower MAE (0.5) and MAPE (0.03), outperforming baseline models by up to 2 orders of magnitude.

Conclusion: The model provides an innovative, cost-effective solution for water quality prediction, aiding environmental and industrial monitoring.

Abstract: Detecting heavy metal pollution in soils and seaports is vital for regional
environmental monitoring. The Pollution Load Index (PLI), an international
standard, is commonly used to assess heavy metal containment. However, the
conventional PLI assessment involves laborious procedures and data analysis of
sediment samples. To address this challenge, we propose a deep-learning-based
model that simplifies the heavy metal assessment process. Our model tackles the
issue of data scarcity in the water-sediment domain, which is traditionally
plagued by challenges in data collection and varying standards across nations.
By leveraging transfer learning, we develop an accurate quantitative assessment
method for predicting PLI. Our approach allows the transfer of learned features
across domains with different sets of features. We evaluate our model using
data from six major ports in New South Wales, Australia: Port Yamba, Port
Newcastle, Port Jackson, Port Botany, Port Kembla, and Port Eden. The results
demonstrate significantly lower Mean Absolute Error (MAE) and Mean Absolute
Percentage Error (MAPE) of approximately 0.5 and 0.03, respectively, compared
to other models. Our model performance is up to 2 orders of magnitude than
other baseline models. Our proposed model offers an innovative, accessible, and
cost-effective approach to predicting water quality, benefiting marine life
conservation, aquaculture, and industrial pollution monitoring.

</details>


### [49] [Earthquake Damage Grades Prediction using An Ensemble Approach Integrating Advanced Machine and Deep Learning Models](https://arxiv.org/abs/2506.22129)
*Anurag Panda,Gaurav Kumar Yadav*

Main category: cs.LG

TL;DR: The paper explores machine learning and deep learning models, including XGBoost and SMOTE, to predict earthquake-induced structural damage grades, addressing class imbalance for better accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate post-earthquake damage assessment is crucial for effective disaster response and resource allocation, highlighting the need for improved prediction methods.

Method: Uses multi-class classification models (XGBoost, deep learning, ensembling) and SMOTE to handle class imbalance, with feature manipulation and diverse training approaches.

Result: Identifies key seismic vulnerability factors and evaluates model performance using techniques like confusion matrices.

Conclusion: The study enhances earthquake damage prediction by addressing class imbalance and optimizing model performance.

Abstract: In the aftermath of major earthquakes, evaluating structural and
infrastructural damage is vital for coordinating post-disaster response
efforts. This includes assessing damage's extent and spatial distribution to
prioritize rescue operations and resource allocation. Accurately estimating
damage grades to buildings post-earthquake is paramount for effective response
and recovery, given the significant impact on lives and properties,
underscoring the urgency of streamlining relief fund allocation processes.
Previous studies have shown the effectiveness of multi-class classification,
especially XGBoost, along with other machine learning models and ensembling
methods, incorporating regularization to address class imbalance. One
consequence of class imbalance is that it may give rise to skewed models that
undervalue minority classes and give preference to the majority class. This
research deals with the problem of class imbalance with the help of the
synthetic minority oversampling technique (SMOTE). We delve into multiple
multi-class classification machine learning, deep learning models, and
ensembling methods to forecast structural damage grades. The study elucidates
performance determinants through comprehensive feature manipulation experiments
and diverse training approaches. It identifies key factors contributing to
seismic vulnerability while evaluating model performance using techniques like
the confusion matrix further to enhance understanding of the effectiveness of
earthquake damage prediction.

</details>


### [50] [Thompson Sampling-Based Learning and Control for Unknown Dynamic Systems](https://arxiv.org/abs/2506.22186)
*Kaikai Zheng,Dawei Shi,Yang Shi,Long Wang*

Main category: cs.LG

TL;DR: The paper introduces a parameterization method for control law learning using reproducing kernel Hilbert spaces, extending Thompson sampling (TS) to general function spaces for active learning-based controller design.


<details>
  <summary>Details</summary>
Motivation: TS is limited to finite parametric representations, restricting its use in general control system design spaces. The work aims to overcome this limitation.

Method: The proposed method treats control laws as elements in a function space, using reproducing kernel Hilbert spaces for parameterization. A TS framework explores optimal control laws, with convergence guarantees.

Result: The method learns control-law-to-performance relationships exponentially fast, with derived regret bounds. Numerical experiments confirm its effectiveness in nonlinear system control.

Conclusion: The approach successfully generalizes TS to function spaces, enabling flexible control law design with theoretical guarantees and practical validation.

Abstract: Thompson sampling (TS) is an effective method to explore parametric
uncertainties and can therefore be used for active learning-based controller
design. However, TS relies on finite parametric representations, which limits
its applicability to more general spaces, which are more commonly encountered
in control system design. To address this issue, this work pro poses a
parameterization method for control law learning using reproducing kernel
Hilbert spaces and designs a data-driven active learning control approach.
Specifically, the proposed method treats the control law as an element in a
function space, allowing the design of control laws without imposing
restrictions on the system structure or the form of the controller. A TS
framework is proposed in this work to explore potential optimal control laws,
and the convergence guarantees are further provided for the learning process.
Theoretical analysis shows that the proposed method learns the relationship
between control laws and closed-loop performance metrics at an exponential
rate, and the upper bound of control regret is also derived. Numerical
experiments on controlling unknown nonlinear systems validate the effectiveness
of the proposed method.

</details>


### [51] [Exploring Modularity of Agentic Systems for Drug Discovery](https://arxiv.org/abs/2506.22189)
*Laura van Weesep,Samuel Genheden,Ola Engkvist,Jens Sjölund*

Main category: cs.LG

TL;DR: The study evaluates the modularity of LLM-based agentic systems in drug discovery, comparing performance of various LLMs and agent types, emphasizing the need for prompt re-engineering and further research.


<details>
  <summary>Details</summary>
Motivation: To explore the interchangeability of LLMs and agent types in drug discovery, a topic with limited prior attention.

Method: Comparison of different LLMs (e.g., Claude-3.5-Sonnet, GPT-4o) and agent types (tool-calling vs. code-generating) using an LLM-as-a-judge score.

Result: Claude-3.5-Sonnet, Claude-3.7-Sonnet, and GPT-4o outperform others; code-generating agents generally perform better but results vary by question and model.

Conclusion: Modularity in agentic systems requires careful consideration of prompts and models, highlighting the need for further research for stable, scalable solutions.

Abstract: Large-language models (LLMs) and agentic systems present exciting
opportunities to accelerate drug discovery and design. In this study, we
critically examine the modularity of LLM-based agentic systems for drug
discovery, i.e., whether parts of the agentic system such as the LLM are
interchangeable, a topic that has received limited attention in drug discovery
applications. We compare the performance of different large language models
(LLMs) and the effectiveness of tool-calling agents versus code-generating
agents in this domain. Our case study, comparing performance in orchestrating
tools for chemistry and drug discovery using an LLM-as-a-judge score, shows
that Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative
language models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and
Nova-Micro. Although we confirm that code-generating agents outperform the
tool-calling ones on average, we show that this is highly question and model
dependent. Furthermore, the impact of replacing system prompts is dependent on
the specific question asked and the model used, underscoring that -- even in
this particular domain -- one cannot just replace language models without
considering prompt re-engineering. Our study highlights the necessity of
further research into the modularity of agentic systems to enable the
development of stable and scalable solutions for real-world problems.

</details>


### [52] [dreaMLearning: Data Compression Assisted Machine Learning](https://arxiv.org/abs/2506.22190)
*Xiaobo Zhao,Aaron Hurst,Panagiotis Karras,Daniel E. Lucani*

Main category: cs.LG

TL;DR: dreaMLearning enables efficient ML training by learning directly from compressed data, reducing resource demands without significant performance loss.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of large labeled data needs and high computational/storage demands in deep learning.

Method: Uses Entropy-based Generalized Deduplication (EntroGeDe) for lossless compression, allowing learning from compressed data without decompression.

Result: Achieves up to 8.8x faster training, 10x less memory, 42% storage reduction, with minimal performance impact.

Conclusion: Enables scalable and efficient ML, benefiting distributed, federated, and edge computing applications.

Abstract: Despite rapid advancements, machine learning, particularly deep learning, is
hindered by the need for large amounts of labeled data to learn meaningful
patterns without overfitting and immense demands for computation and storage,
which motivate research into architectures that can achieve good performance
with fewer resources. This paper introduces dreaMLearning, a novel framework
that enables learning from compressed data without decompression, built upon
Entropy-based Generalized Deduplication (EntroGeDe), an entropy-driven lossless
compression method that consolidates information into a compact set of
representative samples. DreaMLearning accommodates a wide range of data types,
tasks, and model architectures. Extensive experiments on regression and
classification tasks with tabular and image data demonstrate that dreaMLearning
accelerates training by up to 8.8x, reduces memory usage by 10x, and cuts
storage by 42%, with a minimal impact on model performance. These advancements
enhance diverse ML applications, including distributed and federated learning,
and tinyML on resource-constrained edge devices, unlocking new possibilities
for efficient and scalable learning.

</details>


### [53] [REDELEX: A Framework for Relational Deep Learning Exploration](https://arxiv.org/abs/2506.22199)
*Jakub Peleška,Gustav Šír*

Main category: cs.LG

TL;DR: REDELEX is a framework evaluating Relational Deep Learning (RDL) models on diverse RDBs, confirming RDL's superiority and analyzing performance factors like model complexity and database properties.


<details>
  <summary>Details</summary>
Motivation: The novelty of RDL lacks analysis of its performance relative to RDB characteristics, prompting a need for a comprehensive evaluation framework.

Method: REDELEX evaluates RDL models of varying complexity on over 70 RDBs, benchmarking against classic methods.

Result: RDL generally outperforms classic methods, with performance influenced by model complexity, database size, and structural properties.

Conclusion: REDELEX provides insights into RDL performance factors, supporting its adoption for predictive tasks on RDBs.

Abstract: Relational databases (RDBs) are widely regarded as the gold standard for
storing structured information. Consequently, predictive tasks leveraging this
data format hold significant application promise. Recently, Relational Deep
Learning (RDL) has emerged as a novel paradigm wherein RDBs are conceptualized
as graph structures, enabling the application of various graph neural
architectures to effectively address these tasks. However, given its novelty,
there is a lack of analysis into the relationships between the performance of
various RDL models and the characteristics of the underlying RDBs.
  In this study, we present REDELEX$-$a comprehensive exploration framework for
evaluating RDL models of varying complexity on the most diverse collection of
over 70 RDBs, which we make available to the community. Benchmarked alongside
key representatives of classic methods, we confirm the generally superior
performance of RDL while providing insights into the main factors shaping
performance, including model complexity, database sizes and their structural
properties.

</details>


### [54] [EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement Learning Framework](https://arxiv.org/abs/2506.22200)
*Chen Wang,Lai Wei,Yanzhi Zhang,Chenyang Shao,Zedong Dan,Weiran Huang,Yue Wang,Yuzhi Zhang*

Main category: cs.LG

TL;DR: EFRame enhances GRPO by improving exploration, filtering low-quality samples, and using experience replay, leading to better performance on complex reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: GRPO, while efficient, suffers from limited exploration, low sample efficiency, and instability, hindering its performance on complex reasoning tasks.

Method: EFRame introduces an Exploration-Filtering-Replay framework: additional rollouts for exploration, online filtering to remove noise, and experience replay for rare samples.

Result: EFRame improves training robustness and efficiency, unlocks deeper reasoning capabilities, and enables fine-grained sample analysis.

Conclusion: EFRame addresses GRPO's limitations, enhancing RL performance and providing insights into sample contributions in learning.

Abstract: Recent advances in reinforcement learning (RL) have significantly enhanced
the reasoning capabilities of large language models (LLMs). Group Relative
Policy Optimization (GRPO), an efficient variant of PPO that lowers RL's
computational cost, still faces limited exploration, low sample efficiency and
instability, constraining its performance on complex reasoning tasks. To
address these limitations, we introduce EFRame, an Exploration-Filtering-Replay
framework that systematically augments GRPO along three critical dimensions.
EFRame performs additional rollouts to explore high-quality trajectories,
applies online filtering to eliminate low-quality samples that introduce noise
and variance, and leverages experience replay to repeatedly exploit rare but
informative samples. EFRame establishes a complete and stable learning cycle,
guiding the model through a structured transition from exploration to
convergence. Our experiments across a variety of reasoning benchmarks
demonstrate that EFRame not only improves the robustness and efficiency of
training, but also enables access to deeper reasoning capabilities that remain
unattainable under vanilla GRPO. Furthermore, EFRame enables a more
fine-grained categorization of training samples, allowing for a deeper analysis
of how different types of samples contribute to the learning process in RL. Our
code is available at https://github.com/597358816/EFRame.

</details>


### [55] [Risk-Averse Best Arm Set Identification with Fixed Budget and Fixed Confidence](https://arxiv.org/abs/2506.22253)
*Shunta Nonaga,Koji Tabata,Yuta Mizuno,Tamiki Komatsuzaki*

Main category: cs.LG

TL;DR: The paper introduces a novel stochastic bandit optimization framework that jointly maximizes expected reward and minimizes risk using the mean-variance criterion, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for risk-aware decision-making in uncertain environments by balancing expected reward and risk, which traditional bandit methods overlook.

Method: A unified meta-algorithmic framework for fixed-confidence and fixed-budget regimes, using adaptive confidence intervals and a shared exploration strategy.

Result: Theoretical guarantees for solution correctness and empirical validation showing superior accuracy and sample efficiency over existing methods.

Conclusion: The proposed framework effectively addresses risk-aware decision-making, offering broad applicability in uncertain environments.

Abstract: Decision making under uncertain environments in the maximization of expected
reward while minimizing its risk is one of the ubiquitous problems in many
subjects. Here, we introduce a novel problem setting in stochastic bandit
optimization that jointly addresses two critical aspects of decision-making:
maximizing expected reward and minimizing associated uncertainty, quantified
via the mean-variance(MV) criterion. Unlike traditional bandit formulations
that focus solely on expected returns, our objective is to efficiently and
accurately identify the Pareto-optimal set of arms that strikes the best
trade-off between expected performance and risk. We propose a unified
meta-algorithmic framework capable of operating under both fixed-confidence and
fixed-budget regimes, achieved through adaptive design of confidence intervals
tailored to each scenario using the same sample exploration strategy. We
provide theoretical guarantees on the correctness of the returned solutions in
both settings. To complement this theoretical analysis, we conduct extensive
empirical evaluations across synthetic benchmarks, demonstrating that our
approach outperforms existing methods in terms of both accuracy and sample
efficiency, highlighting its broad applicability to risk-aware decision-making
tasks in uncertain environments.

</details>


### [56] [Projected Compression: Trainable Projection for Efficient Transformer Compression](https://arxiv.org/abs/2506.22255)
*Maciej Stefaniak,Michał Krutul,Jan Małaśnicki,Maciej Pióro,Jakub Krajewski,Sebastian Jaszczur,Marek Cygan,Kamil Adamczewski,Jan Ludziejewski*

Main category: cs.LG

TL;DR: Projected Compression is a novel method to reduce large language model sizes without increasing computational overhead, outperforming pruning and retraining.


<details>
  <summary>Details</summary>
Motivation: The growth in model size improves performance but increases inference time and computational demands, prompting the need for efficient compression techniques.

Method: The method involves training additional projection weights, merging them into a lower-dimensional matrix to reduce model size while maintaining computational efficiency.

Result: Projected Compression outperforms pruning and retraining, with performance scaling well with token count.

Conclusion: The technique effectively reduces model size without added computational cost, offering a practical solution for efficient inference.

Abstract: Large language models have steadily increased in size to achieve improved
performance; however, this growth has also led to greater inference time and
computational demands. Consequently, there is rising interest in model size
reduction methods. To address this issue, we propose Projected Compression, a
novel model compression technique, that reduces model weights by utilizing
projection modules. Specifically, we first train additional trainable
projections weights and preserve access to all the original model parameters.
Subsequently, these projections are merged into a lower-dimensional product
matrix, resulting in a reduced-size standard Transformer-based model. Unlike
alternative approaches that require additional computational overhead, our
method matches the base model's per-token computation step in FLOPs.
Experimental results show that Projected Compression outperforms the comparable
hard pruning and retraining approach on higher quality models. Moreover, the
performance margin scales well with the number of tokens.

</details>


### [57] [Score-Based Model for Low-Rank Tensor Recovery](https://arxiv.org/abs/2506.22295)
*Zhengyun Cheng,Changhao Wang,Guanwen Zhang,Yi Xu,Wei Zhou,Xiangyang Ji*

Main category: cs.LG

TL;DR: A score-based model replaces predefined tensor decomposition assumptions, using neural networks to learn compatibility between tensors and shared factors, improving performance in tensor completion and denoising.


<details>
  <summary>Details</summary>
Motivation: Traditional tensor decomposition methods rely on fixed structural assumptions (e.g., CP or Tucker), which may not reflect real-world data complexities, leading to accuracy loss.

Method: A neural network learns an energy function optimized via score matching to capture the gradient of the joint log-probability of tensor entries and shared factors, avoiding predefined assumptions.

Result: The method outperforms traditional approaches in tensor completion and denoising, especially for sparse, continuous-time tensors, and visual data.

Conclusion: The proposed score-based model eliminates rigid assumptions, offering flexibility and improved accuracy in multiway data analysis.

Abstract: Low-rank tensor decompositions (TDs) provide an effective framework for
multiway data analysis. Traditional TD methods rely on predefined structural
assumptions, such as CP or Tucker decompositions. From a probabilistic
perspective, these can be viewed as using Dirac delta distributions to model
the relationships between shared factors and the low-rank tensor. However, such
prior knowledge is rarely available in practical scenarios, particularly
regarding the optimal rank structure and contraction rules. The optimization
procedures based on fixed contraction rules are complex, and approximations
made during these processes often lead to accuracy loss. To address this issue,
we propose a score-based model that eliminates the need for predefined
structural or distributional assumptions, enabling the learning of
compatibility between tensors and shared factors. Specifically, a neural
network is designed to learn the energy function, which is optimized via score
matching to capture the gradient of the joint log-probability of tensor entries
and shared factors. Our method allows for modeling structures and distributions
beyond the Dirac delta assumption. Moreover, integrating the block coordinate
descent (BCD) algorithm with the proposed smooth regularization enables the
model to perform both tensor completion and denoising. Experimental results
demonstrate significant performance improvements across various tensor types,
including sparse and continuous-time tensors, as well as visual data.

</details>


### [58] [CoATA: Effective Co-Augmentation of Topology and Attribute for Graph Neural Networks](https://arxiv.org/abs/2506.22299)
*Tao Liu,Longlong Lin,Yunfeng Yu,Xi Ou,Youan Zhang,Zhiqiu Ye,Tao Jia*

Main category: cs.LG

TL;DR: CoATA is a dual-channel GNN framework for co-augmenting topology and attributes, improving performance on noisy graphs via contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Real-world graphs are noisy and incomplete, degrading GNN performance. Existing methods focus on single-dimensional augmentation, missing deeper interplays between topology and attributes.

Method: CoATA propagates structural signals to enrich node attributes, projects attributes into a bipartite graph for structure refinement, and uses contrastive learning for mutual corrections.

Result: CoATA outperforms 11 baseline methods on seven benchmark datasets, demonstrating effectiveness in capturing topology-attribute synergy.

Conclusion: CoATA successfully bridges the gap by jointly augmenting topology and attributes, enhancing GNN performance on noisy graphs.

Abstract: Graph Neural Networks (GNNs) have garnered substantial attention due to their
remarkable capability in learning graph representations. However, real-world
graphs often exhibit substantial noise and incompleteness, which severely
degrades the performance of GNNs. Existing methods typically address this issue
through single-dimensional augmentation, focusing either on refining topology
structures or perturbing node attributes, thereby overlooking the deeper
interplays between the two. To bridge this gap, this paper presents CoATA, a
dual-channel GNN framework specifically designed for the Co-Augmentation of
Topology and Attribute. Specifically, CoATA first propagates structural signals
to enrich and denoise node attributes. Then, it projects the enhanced attribute
space into a node-attribute bipartite graph for further refinement or
reconstruction of the underlying structure. Subsequently, CoATA introduces
contrastive learning, leveraging prototype alignment and consistency
constraints, to facilitate mutual corrections between the augmented and
original graphs. Finally, extensive experiments on seven benchmark datasets
demonstrate that the proposed CoATA outperforms eleven state-of-the-art
baseline methods, showcasing its effectiveness in capturing the synergistic
relationship between topology and attributes.

</details>


### [59] [Weakly-Supervised Domain Adaptation with Proportion-Constrained Pseudo-Labeling](https://arxiv.org/abs/2506.22301)
*Takumi Okuo,Shinnosuke Matsuo,Shota Harada,Kiyohito Tanaka,Ryoma Bise*

Main category: cs.LG

TL;DR: Proposes a weakly-supervised domain adaptation method using target domain class proportions to improve model performance without extra annotations.


<details>
  <summary>Details</summary>
Motivation: Addresses domain shift in medical ML, where class proportion differences degrade model performance.

Method: Uses proportion-constrained pseudo-labeling on unlabeled target data, leveraging accessible class proportion info.

Result: Outperforms semi-supervised methods on endoscopic datasets, even with minimal labeled data and noisy proportions.

Conclusion: The method is robust and effective for real-world medical applications with domain shifts.

Abstract: Domain shift is a significant challenge in machine learning, particularly in
medical applications where data distributions differ across institutions due to
variations in data collection practices, equipment, and procedures. This can
degrade performance when models trained on source domain data are applied to
the target domain. Domain adaptation methods have been widely studied to
address this issue, but most struggle when class proportions between the source
and target domains differ. In this paper, we propose a weakly-supervised domain
adaptation method that leverages class proportion information from the target
domain, which is often accessible in medical datasets through prior knowledge
or statistical reports. Our method assigns pseudo-labels to the unlabeled
target data based on class proportion (called proportion-constrained
pseudo-labeling), improving performance without the need for additional
annotations. Experiments on two endoscopic datasets demonstrate that our method
outperforms semi-supervised domain adaptation techniques, even when 5% of the
target domain is labeled. Additionally, the experimental results with noisy
proportion labels highlight the robustness of our method, further demonstrating
its effectiveness in real-world application scenarios.

</details>


### [60] [Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling](https://arxiv.org/abs/2506.22304)
*Erkan Turan,Aristotelis Siozopoulos,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: Koopman-CFM integrates Koopman operator theory to linearize and accelerate Conditional Flow Matching (CFM), enabling one-step sampling and interpretable dynamics.


<details>
  <summary>Details</summary>
Motivation: Sampling from CFM is computationally expensive and lacks interpretability. Existing methods improve speed but not structure.

Method: Uses Koopman operator theory to model non-linear flows as linear dynamics in a learned space, enabling closed-form sampling via matrix exponentiation.

Result: Achieves significant speedups on 2D datasets, MNIST, F-MNIST, and TFD, with interpretable spectral properties.

Conclusion: Koopman-CFM combines efficiency and interpretability, advancing fast and structured generative modeling.

Abstract: Conditional Flow Matching (CFM) offers a simulation-free framework for
training continuous-time generative models, bridging diffusion and flow-based
approaches. However, sampling from CFM still relies on numerically solving
non-linear ODEs which can be computationally expensive and difficult to
interpret. Recent alternatives address sampling speed via trajectory
straightening, mini-batch coupling or distillation. However, these methods
typically do not shed light on the underlying \textit{structure} of the
generative process. In this work, we propose to accelerate CFM and introduce an
interpretable representation of its dynamics by integrating Koopman operator
theory, which models non-linear flows as linear evolution in a learned space of
observables. We introduce a decoder-free Koopman-CFM architecture that learns
an embedding where the generative dynamics become linear, enabling closed-form,
one-step sampling via matrix exponentiation. This results in significant
speedups over traditional CFM as demonstrated on controlled 2D datasets and
real-world benchmarks, MNIST, Fashion-MNIST (F-MNIST), and the Toronto Face
Dataset (TFD). Unlike previous methods, our approach leads to a well-structured
Koopman generator, whose spectral properties, eigenvalues, and eigenfunctions
offer principled tools for analyzing generative behavior such as temporal
scaling, mode stability, and decomposition in Koopman latent space. By
combining sampling efficiency with analytical structure, Koopman-enhanced flow
matching offers a potential step toward fast and interpretable generative
modeling.

</details>


### [61] [Less Greedy Equivalence Search](https://arxiv.org/abs/2506.22331)
*Adiba Ejaz,Elias Bareinboim*

Main category: cs.LG

TL;DR: LGES improves GES by reducing computational cost and improving accuracy, with theoretical guarantees and practical benefits.


<details>
  <summary>Details</summary>
Motivation: Address computational cost and finite-sample accuracy challenges of GES.

Method: Modifies GES's greedy step to avoid unnecessary edge insertions, uses prior assumptions, and incorporates interventional data.

Result: Achieves up to 10x speed-up, reduces structural error, and outperforms GES and baselines.

Conclusion: LGES is a robust, efficient, and accurate variant of GES for causal discovery.

Abstract: Greedy Equivalence Search (GES) is a classic score-based algorithm for causal
discovery from observational data. In the sample limit, it recovers the Markov
equivalence class of graphs that describe the data. Still, it faces two
challenges in practice: computational cost and finite-sample accuracy. In this
paper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that
retains its theoretical guarantees while partially addressing these
limitations. LGES modifies the greedy step: rather than always applying the
highest-scoring insertion, it avoids edge insertions between variables for
which the score implies some conditional independence. This more targeted
search yields up to a \(10\)-fold speed-up and a substantial reduction in
structural error relative to GES. Moreover, LGES can guide the search using
prior assumptions, while correcting these assumptions when contradicted by the
data. Finally, LGES can exploit interventional data to refine the learned
observational equivalence class. We prove that LGES recovers the true
equivalence class in the sample limit from observational and interventional
data, even with misspecified prior assumptions. Experiments demonstrate that
LGES outperforms GES and other baselines in speed, accuracy, and robustness to
misspecified assumptions. Our code is available at
https://github.com/CausalAILab/lges.

</details>


### [62] [A Framework for Multi-source Privacy Preserving Epidemic Analysis](https://arxiv.org/abs/2506.22342)
*Zihan Guan,Zhiyuan Zhao,Fengwei Tian,Dung Nguyen,Payel Bhattacharjee,Ravi Tandon,B. Aditya Prakash,Anil Vullikanti*

Main category: cs.LG

TL;DR: A framework integrating deep learning and epidemic models for forecasting and mechanistic modeling, using diverse datasets including DP-protected ones, shows significant value even with privacy guarantees.


<details>
  <summary>Details</summary>
Motivation: Diverse datasets enhance epidemiology and public health analyses, but sensitive data require privacy protections like Differential Privacy (DP).

Method: Develop a framework combining deep learning and epidemic models to forecast and learn epidemic spread mechanisms, incorporating DP-protected datasets.

Result: Demonstrated with a synthetic financial dataset under DP, the framework improves forecasting and model learning despite privacy constraints.

Conclusion: The framework successfully leverages DP-protected datasets for valuable epidemic analyses without compromising privacy.

Abstract: It is now well understood that diverse datasets provide a lot of value in key
epidemiology and public health analyses, such as forecasting and nowcasting,
development of epidemic models, evaluation and design of interventions and
resource allocation. Some of these datasets are often sensitive, and need
adequate privacy protections. There are many models of privacy, but
Differential Privacy (DP) has become a de facto standard because of its strong
guarantees, without making models about adversaries. In this paper, we develop
a framework the integrates deep learning and epidemic models to simultaneously
perform epidemic forecasting and learning a mechanistic model of epidemic
spread, while incorporating multiple datasets for these analyses, including
some with DP guarantees. We demonstrate our framework using a realistic but
synthetic financial dataset with DP; such a dataset has not been used in such
epidemic analyses. We show that this dataset provides significant value in
forecasting and learning an epidemic model, even when used with DP guarantees.

</details>


### [63] [Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation](https://arxiv.org/abs/2506.22365)
*Tao Li,Haozhe Lei,Mingsheng Yin,Yaqi Hu*

Main category: cs.LG

TL;DR: The paper introduces PiPRL, a framework combining symbolic programming and RL to incorporate physics priors efficiently, improving sample efficiency and generalization in indoor navigation tasks.


<details>
  <summary>Details</summary>
Motivation: Current methods for integrating physics priors into RL require manual effort and domain expertise, limiting accessibility. The goal is to automate this process using a symbolic approach.

Method: PiPRL uses a hierarchical neuro-symbolic framework: a neural perception module extracts features for a symbolic program, which encodes physics priors to guide a neural RL controller.

Result: PiPRL outperforms purely symbolic or neural policies and reduces training time by over 26%.

Conclusion: The symbolic approach in PiPRL effectively incorporates physics priors into RL, enhancing performance and efficiency without extensive manual input.

Abstract: When using reinforcement learning (RL) to tackle physical control tasks,
inductive biases that encode physics priors can help improve sample efficiency
during training and enhance generalization in testing. However, the current
practice of incorporating these helpful physics-informed inductive biases
inevitably runs into significant manual labor and domain expertise, making them
prohibitive for general users. This work explores a symbolic approach to
distill physics-informed inductive biases into RL agents, where the physics
priors are expressed in a domain-specific language (DSL) that is human-readable
and naturally explainable. Yet, the DSL priors do not translate directly into
an implementable policy due to partial and noisy observations and additional
physical constraints in navigation tasks. To address this gap, we develop a
physics-informed program-guided RL (PiPRL) framework with applications to
indoor navigation. PiPRL adopts a hierarchical and modularized neuro-symbolic
integration, where a meta symbolic program receives semantically meaningful
features from a neural perception module, which form the bases for symbolic
programming that encodes physics priors and guides the RL process of a
low-level neural controller. Extensive experiments demonstrate that PiPRL
consistently outperforms purely symbolic or neural policies and reduces
training time by over 26% with the help of the program-based inductive biases.

</details>


### [64] [Sheaf-Based Decentralized Multimodal Learning for Next-Generation Wireless Communication Systems](https://arxiv.org/abs/2506.22374)
*Abdulmomen Ghalkha,Zhuojun Tian,Chaouki Ben Issaid,Mehdi Bennis*

Main category: cs.LG

TL;DR: Sheaf-DMFL is a decentralized multimodal learning framework using sheaf theory to improve collaboration among edge devices with diverse modalities, outperforming conventional FL in heterogeneous wireless systems.


<details>
  <summary>Details</summary>
Motivation: Conventional FL lacks support for multimodal data and diverse client capabilities, limiting real-world applicability. Sheaf-DMFL addresses this gap.

Method: Proposes Sheaf-DMFL and Sheaf-DMFL-Att, leveraging sheaf theory and attention mechanisms to enhance multimodal collaboration and learning.

Result: Superior performance in real-world scenarios like link blockage prediction and mmWave beamforming, with proven convergence guarantees.

Conclusion: Sheaf-DMFL effectively handles multimodal data and client diversity, offering a robust solution for heterogeneous wireless communication systems.

Abstract: In large-scale communication systems, increasingly complex scenarios require
more intelligent collaboration among edge devices collecting various multimodal
sensory data to achieve a more comprehensive understanding of the environment
and improve decision-making accuracy. However, conventional federated learning
(FL) algorithms typically consider unimodal datasets, require identical model
architectures, and fail to leverage the rich information embedded in multimodal
data, limiting their applicability to real-world scenarios with diverse
modalities and varying client capabilities. To address this issue, we propose
Sheaf-DMFL, a novel decentralized multimodal learning framework leveraging
sheaf theory to enhance collaboration among devices with diverse modalities.
Specifically, each client has a set of local feature encoders for its different
modalities, whose outputs are concatenated before passing through a
task-specific layer. While encoders for the same modality are trained
collaboratively across clients, we capture the intrinsic correlations among
clients' task-specific layers using a sheaf-based structure. To further enhance
learning capability, we propose an enhanced algorithm named Sheaf-DMFL-Att,
which tailors the attention mechanism within each client to capture
correlations among different modalities. A rigorous convergence analysis of
Sheaf-DMFL-Att is provided, establishing its theoretical guarantees. Extensive
simulations are conducted on real-world link blockage prediction and mmWave
beamforming scenarios, demonstrate the superiority of the proposed algorithms
in such heterogeneous wireless communication systems.

</details>


### [65] [Probabilistic Optimality for Inference-time Scaling](https://arxiv.org/abs/2506.22376)
*Youkang Wang,Jian Wang,Rubing Chen,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: The paper introduces a probabilistic framework for inference-time scaling in LLMs, proposing the algorithm OptScale to dynamically determine optimal sample sizes, reducing overhead while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing inference-time scaling methods lack a principled foundation, relying on heuristics. This work aims to provide a theoretical and practical solution for efficient scaling.

Method: The authors develop a probabilistic framework assuming i.i.d. parallel samples and derive a theoretical lower bound for sample size. OptScale uses a language model-based predictor to dynamically adjust samples.

Result: OptScale reduces sampling overhead significantly while matching or outperforming state-of-the-art reasoning performance on benchmarks like MATH-500, GSM8K, AIME, and AMC.

Conclusion: The work provides a theoretical foundation and practical algorithm for efficient inference-time scaling in LLMs, addressing a critical gap in their deployment for complex reasoning.

Abstract: Inference-time scaling has emerged as a powerful technique for enhancing the
reasoning performance of Large Language Models (LLMs). However, existing
approaches often rely on heuristic strategies for parallel sampling, lacking a
principled foundation. To address this gap, we propose a probabilistic
framework that formalizes the optimality of inference-time scaling under the
assumption that parallel samples are independently and identically distributed
(i.i.d.), and where the Best-of-N selection strategy follows a probability
distribution that can be estimated. Within this framework, we derive a
theoretical lower bound on the required number of samples to achieve a target
performance level, providing the first principled guidance for
compute-efficient scaling. Leveraging this insight, we develop
\textsc{OptScale}, a practical algorithm that dynamically determines the
optimal number of sampled responses. \textsc{OptScale} employs a language
model-based predictor to estimate probabilistic prior parameters, enabling the
decision of the minimal number of samples needed that satisfy predefined
performance thresholds and confidence levels. Extensive experiments on
mathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)
demonstrate that \textsc{OptScale} significantly reduces sampling overhead
while remaining better or on par with state-of-the-art reasoning performance.
Our work offers both a theoretical foundation and a practical solution for
principled inference-time scaling, addressing a critical gap in the efficient
deployment of LLMs for complex reasoning.

</details>


### [66] [Towards Distributed Neural Architectures](https://arxiv.org/abs/2506.22389)
*Aditya Cowsik,Tianyu He,Andrey Gromov*

Main category: cs.LG

TL;DR: Distributed Neural Architectures (DNAs) generalize sparse methods like Mixture-of-Experts, learning computation and communication patterns end-to-end. They match dense baselines in performance and show emergent specialization and interpretable compute allocation.


<details>
  <summary>Details</summary>
Motivation: To explore flexible, distributed neural architectures that learn computation and communication patterns dynamically, improving efficiency and specialization.

Method: DNAs use a proto-architecture with modules (e.g., transformers, MLPs) and routers, allowing tokens to traverse modules in any order. Training optimizes for efficiency and load balancing.

Result: DNAs perform competitively with dense baselines, exhibit power-law distributed paths, and show emergent module specialization and interpretable compute allocation.

Conclusion: DNAs offer a scalable, efficient, and interpretable alternative to dense models, with potential for dynamic compute allocation and specialization.

Abstract: We introduce and train distributed neural architectures (DNA) in vision and
language domains. DNAs are initialized with a proto-architecture that consists
of (transformer, MLP, attention, etc.) modules and routers. Any token (or
patch) can traverse any series of modules in any order. DNAs are a natural
generalization of the sparse methods such as Mixture-of-Experts,
Mixture-of-Depths, parameter sharing, etc. Computation and communication
patterns of DNA modules are learnt end-to-end during training and depend on the
content and context of each token (or patch). These patterns can be shaped by
further requirements added to the optimization objective such as compute/memory
efficiency or load balancing. We empirically show that (i) trained DNAs are
competitive with the dense baselines in both domains and (ii) compute
efficiency/parameter sharing can be learnt from data. Next, we analyze the
emergent connectivity and computation patterns in the trained DNAs. We find
that the paths that tokens take through the models are themselves distributed
according to a power-law. We show that some paths (or, equivalently, groups of
modules) show emergent specialization. Finally, we demonstrate that models
learn to allocate compute and active parameters in an interpretable way.

</details>


### [67] [Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis](https://arxiv.org/abs/2506.22393)
*YongKyung Oh,Alex Bui*

Main category: cs.LG

TL;DR: A novel framework using multi-view contrastive learning improves domain adaptation for medical time series by integrating temporal, derivative, and frequency features, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of adapting ML models to medical time series across domains due to complex temporal dependencies and dynamic distribution shifts.

Method: Proposes a framework with multi-view contrastive learning, independent encoders, and hierarchical fusion to learn transferable, feature-invariant representations.

Result: Outperforms state-of-the-art methods in transfer learning tasks on EEG, ECG, and EMG datasets.

Conclusion: The framework enhances robustness and generalizability, enabling reliable AI deployment in diverse healthcare settings.

Abstract: Adapting machine learning models to medical time series across different
domains remains a challenge due to complex temporal dependencies and dynamic
distribution shifts. Current approaches often focus on isolated feature
representations, limiting their ability to fully capture the intricate temporal
dynamics necessary for robust domain adaptation. In this work, we propose a
novel framework leveraging multi-view contrastive learning to integrate
temporal patterns, derivative-based dynamics, and frequency-domain features.
Our method employs independent encoders and a hierarchical fusion mechanism to
learn feature-invariant representations that are transferable across domains
while preserving temporal coherence. Extensive experiments on diverse medical
datasets, including electroencephalogram (EEG), electrocardiogram (ECG), and
electromyography (EMG) demonstrate that our approach significantly outperforms
state-of-the-art methods in transfer learning tasks. By advancing the
robustness and generalizability of machine learning models, our framework
offers a practical pathway for deploying reliable AI systems in diverse
healthcare settings.

</details>


### [68] [Exploration from a Primal-Dual Lens: Value-Incentivized Actor-Critic Methods for Sample-Efficient Online RL](https://arxiv.org/abs/2506.22401)
*Tong Yang,Bo Dai,Lin Xiao,Yuejie Chi*

Main category: cs.LG

TL;DR: The paper introduces a new value-incentivized actor-critic (VAC) method for online RL, integrating exploration and exploitation via a single objective, with theoretical guarantees under linear MDPs.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of balancing exploration and exploitation in online RL with complex function approximations, leveraging optimistic regularization.

Method: Proposes VAC, a primal-dual optimization-based approach, optimizing a unified objective for exploration and exploitation.

Result: VAC achieves near-optimal regret guarantees under linear MDPs, extendable to general function approximation.

Conclusion: VAC offers a practical and theoretically grounded solution for exploration-exploitation trade-offs in RL.

Abstract: Online reinforcement learning (RL) with complex function approximations such
as transformers and deep neural networks plays a significant role in the modern
practice of artificial intelligence. Despite its popularity and importance,
balancing the fundamental trade-off between exploration and exploitation
remains a long-standing challenge; in particular, we are still in lack of
efficient and practical schemes that are backed by theoretical performance
guarantees. Motivated by recent developments in exploration via optimistic
regularization, this paper provides an interpretation of the principle of
optimism through the lens of primal-dual optimization. From this fresh
perspective, we set forth a new value-incentivized actor-critic (VAC) method,
which optimizes a single easy-to-optimize objective integrating exploration and
exploitation -- it promotes state-action and policy estimates that are both
consistent with collected data transitions and result in higher value
functions. Theoretically, the proposed VAC method has near-optimal regret
guarantees under linear Markov decision processes (MDPs) in both finite-horizon
and infinite-horizon settings, which can be extended to the general function
approximation setting under appropriate assumptions.

</details>


### [69] [ARMOR: Robust Reinforcement Learning-based Control for UAVs under Physical Attacks](https://arxiv.org/abs/2506.22423)
*Pritam Dash,Ethan Chan,Nathan P. Lawrence,Karthik Pattabiraman*

Main category: cs.LG

TL;DR: ARMOR is a model-free RL controller for UAVs that learns robust latent state representations to counter adversarial sensor attacks, outperforming conventional methods.


<details>
  <summary>Details</summary>
Motivation: UAVs rely on sensors vulnerable to attacks like GPS spoofing, which corrupt state estimates and cause unsafe behavior. Existing safe RL methods fail against such attacks.

Method: ARMOR uses a two-stage training framework: a teacher encoder trained with attack info generates latent states for RL policy training, and a student encoder approximates these states using historical sensor data.

Result: ARMOR ensures UAV safety, generalizes to unseen attacks, and reduces training costs by avoiding iterative adversarial training.

Conclusion: ARMOR provides an effective, attack-resilient solution for UAV control under adversarial sensor manipulation.

Abstract: Unmanned Aerial Vehicles (UAVs) depend on onboard sensors for perception,
navigation, and control. However, these sensors are susceptible to physical
attacks, such as GPS spoofing, that can corrupt state estimates and lead to
unsafe behavior. While reinforcement learning (RL) offers adaptive control
capabilities, existing safe RL methods are ineffective against such attacks. We
present ARMOR (Adaptive Robust Manipulation-Optimized State Representations),
an attack-resilient, model-free RL controller that enables robust UAV operation
under adversarial sensor manipulation. Instead of relying on raw sensor
observations, ARMOR learns a robust latent representation of the UAV's physical
state via a two-stage training framework. In the first stage, a teacher
encoder, trained with privileged attack information, generates attack-aware
latent states for RL policy training. In the second stage, a student encoder is
trained via supervised learning to approximate the teacher's latent states
using only historical sensor data, enabling real-world deployment without
privileged information. Our experiments show that ARMOR outperforms
conventional methods, ensuring UAV safety. Additionally, ARMOR improves
generalization to unseen attacks and reduces training cost by eliminating the
need for iterative adversarial training.

</details>


### [70] [CLoVE: Personalized Federated Learning through Clustering of Loss Vector Embeddings](https://arxiv.org/abs/2506.22427)
*Randeep Bhatia,Nikos Papadis,Murali Kodialam,TV Lakshman,Sayak Chakrabarty*

Main category: cs.LG

TL;DR: CLoVE is a novel algorithm for Clustered Federated Learning (CFL) that clusters clients using loss vector embeddings, simplifying cluster identification and improving model accuracy.


<details>
  <summary>Details</summary>
Motivation: Identifying client clusters in CFL is challenging due to unknown assignments. CLoVE addresses this by leveraging loss patterns to group clients.

Method: CLoVE uses client embeddings from model losses, iteratively separates clusters, and optimizes cluster-specific models via federated aggregation.

Result: CLoVE achieves accurate cluster recovery in few rounds, with state-of-the-art model accuracy in supervised and unsupervised tasks.

Conclusion: CLoVE is robust, simple, and effective for real-world CFL applications, outperforming existing methods.

Abstract: We propose CLoVE (Clustering of Loss Vector Embeddings), a novel algorithm
for Clustered Federated Learning (CFL). In CFL, clients are naturally grouped
into clusters based on their data distribution. However, identifying these
clusters is challenging, as client assignments are unknown. CLoVE utilizes
client embeddings derived from model losses on client data, and leverages the
insight that clients in the same cluster share similar loss values, while those
in different clusters exhibit distinct loss patterns. Based on these
embeddings, CLoVE is able to iteratively identify and separate clients from
different clusters and optimize cluster-specific models through federated
aggregation. Key advantages of CLoVE over existing CFL algorithms are (1) its
simplicity, (2) its applicability to both supervised and unsupervised settings,
and (3) the fact that it eliminates the need for near-optimal model
initialization, which makes it more robust and better suited for real-world
applications. We establish theoretical convergence bounds, showing that CLoVE
can recover clusters accurately with high probability in a single round and
converges exponentially fast to optimal models in a linear setting. Our
comprehensive experiments comparing with a variety of both CFL and generic
Personalized Federated Learning (PFL) algorithms on different types of datasets
and an extensive array of non-IID settings demonstrate that CLoVE achieves
highly accurate cluster recovery in just a few rounds of training, along with
state-of-the-art model accuracy, across a variety of both supervised and
unsupervised PFL tasks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [71] [Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs](https://arxiv.org/abs/2506.21656)
*Yifan Shen,Yuanzhe Liu,Jingyuan Zhu,Xu Cao,Xiaofeng Zhang,Yixiao He,Wenming Ye,James Matthew Rehg,Ismini Lourentzou*

Main category: cs.CV

TL;DR: SpatialReasoner-R1 improves fine-grained spatial reasoning in VLMs using M3CTS for diverse reasoning trajectories and fDPO for segment-specific preference optimization.


<details>
  <summary>Details</summary>
Motivation: Current VLMs struggle with fine-grained spatial reasoning, requiring multi-step logic and precise alignment.

Method: Uses M3CTS for generating LongCoT reasoning trajectories and fDPO for fine-grained preference optimization with a spatial reward mechanism.

Result: fDPO improves spatial quality tasks by 4.1% and spatial quantity tasks by 9.0%. SpatialReasoner-R1 achieves 9.8% higher accuracy on SPATIALRGPT-Bench.

Conclusion: SpatialReasoner-R1 with fDPO sets a new SoTA for spatial reasoning while maintaining general VL task performance.

Abstract: Current Vision-Language Models (VLMs) struggle with fine-grained spatial
reasoning, particularly when multi-step logic and precise spatial alignment are
required. In this work, we introduce SpatialReasoner-R1, a vision-language
reasoning model designed to address these limitations. To construct
high-quality supervision for spatial reasoning, we design a Multi-Model Monte
Carlo Tree Search (M3CTS) method that generates diverse, logically consistent
Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose
fine-grained Direct Preference Optimization (fDPO), which introduces
segment-specific preference granularity for descriptive grounding and logical
reasoning, guided by a spatial reward mechanism that evaluates candidate
responses based on visual consistency, spatial grounding, and logical
coherence. Experimental results demonstrate that fDPO achieves an average
improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%
gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a
new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in
average accuracy, while maintaining competitive performance on general
vision-language tasks.

</details>


### [72] [TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360° Panorama Generation](https://arxiv.org/abs/2506.21681)
*Hakan Çapuk,Andrew Bond,Muhammed Burak Kızıl,Emir Göçen,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: TanDiT introduces a unified diffusion model for panoramic image generation, addressing distortion and loop-consistency challenges, with specialized metrics for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing image generation models struggle with panoramic images due to geometric distortion and loop-consistency requirements.

Method: TanDiT generates tangent-plane images covering 360° views using a unified diffusion model and includes a post-processing step for global coherence.

Result: The method generalizes well, handles complex prompts, and integrates with other models to produce high-quality panoramas.

Conclusion: TanDiT effectively addresses panoramic generation challenges and offers robust performance and evaluation metrics.

Abstract: Recent advances in image generation have led to remarkable improvements in
synthesizing perspective images. However, these models still struggle with
panoramic image generation due to unique challenges, including varying levels
of geometric distortion and the requirement for seamless loop-consistency. To
address these issues while leveraging the strengths of the existing models, we
introduce TanDiT, a method that synthesizes panoramic scenes by generating
grids of tangent-plane images covering the entire 360$^\circ$ view. Unlike
previous methods relying on multiple diffusion branches, TanDiT utilizes a
unified diffusion model trained to produce these tangent-plane images
simultaneously within a single denoising iteration. Furthermore, we propose a
model-agnostic post-processing step specifically designed to enhance global
coherence across the generated panoramas. To accurately assess panoramic image
quality, we also present two specialized metrics, TangentIS and TangentFID, and
provide a comprehensive benchmark comprising captioned panoramic datasets and
standardized evaluation scripts. Extensive experiments demonstrate that our
method generalizes effectively beyond its training data, robustly interprets
detailed and complex text prompts, and seamlessly integrates with various
generative models to yield high-quality, diverse panoramic images.

</details>


### [73] [FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering](https://arxiv.org/abs/2506.21710)
*Liangyu Zhong,Fabio Rosenthal,Joachim Sicking,Fabian Hüger,Thorsten Bagdonat,Hanno Gottschalk,Leo Schwinn*

Main category: cs.CV

TL;DR: FOCUS is a training-free visual cropping method for fine-grained VQA, leveraging MLLM-internal representations to efficiently identify relevant image regions, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of current visual cropping techniques in fine-grained VQA, such as task-specific fine-tuning, inefficiency, and incompatibility with attention mechanisms.

Method: FOCUS involves four steps: identifying target objects in prompts, computing an object relevance map using KV cache, proposing and ranking regions, and performing VQA on the top-ranked region.

Result: FOCUS outperforms three popular methods in accuracy and efficiency, matching ZoomEye's performance with 3-6.5x less compute.

Conclusion: FOCUS provides an efficient and effective solution for fine-grained VQA without requiring training, leveraging MLLM-internal representations.

Abstract: While Multimodal Large Language Models (MLLMs) offer strong perception and
reasoning capabilities for image-text input, Visual Question Answering (VQA)
focusing on small image details still remains a challenge. Although visual
cropping techniques seem promising, recent approaches have several limitations:
the need for task-specific fine-tuning, low efficiency due to uninformed
exhaustive search, or incompatibility with efficient attention implementations.
We address these shortcomings by proposing a training-free visual cropping
method, dubbed FOCUS, that leverages MLLM-internal representations to guide the
search for the most relevant image region. This is accomplished in four steps:
first, we identify the target object(s) in the VQA prompt; second, we compute
an object relevance map using the key-value (KV) cache; third, we propose and
rank relevant image regions based on the map; and finally, we perform the
fine-grained VQA task using the top-ranked region. As a result of this informed
search strategy, FOCUS achieves strong performance across four fine-grained VQA
datasets and two types of MLLMs. It outperforms three popular visual cropping
methods in both accuracy and efficiency, and matches the best-performing
baseline, ZoomEye, while requiring 3 - 6.5 x less compute.

</details>


### [74] [CAST: Cross-Attentive Spatio-Temporal feature fusion for Deepfake detection](https://arxiv.org/abs/2506.21711)
*Aryan Thakre,Omkar Nagwekar,Vedang Talekar,Aparna Santra Biswas*

Main category: cs.CV

TL;DR: The paper proposes a unified CAST model using cross-attention to fuse spatial and temporal features for deepfake detection, achieving high accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Deepfakes threaten digital media authenticity, requiring advanced detection techniques to identify subtle, time-dependent manipulations. Existing methods lack deep spatio-temporal interaction.

Method: The CAST model leverages cross-attention to dynamically fuse spatial and temporal features, enhancing detection of fine-grained artifacts like flickering eyes or warped lips.

Result: The model achieves 99.49% AUC and 97.57% accuracy in intra-dataset tests, and 93.31% AUC in cross-dataset evaluations, demonstrating strong generalization.

Conclusion: Cross-attention-based feature fusion improves deepfake detection robustness, as validated by superior performance on multiple datasets.

Abstract: Deepfakes have emerged as a significant threat to digital media authenticity,
increasing the need for advanced detection techniques that can identify subtle
and time-dependent manipulations. CNNs are effective at capturing spatial
artifacts, and Transformers excel at modeling temporal inconsistencies.
However, many existing CNN-Transformer models process spatial and temporal
features independently. In particular, attention-based methods often use
separate attention mechanisms for spatial and temporal features and combine
them using naive approaches like averaging, addition, or concatenation, which
limits the depth of spatio-temporal interaction. To address this challenge, we
propose a unified CAST model that leverages cross-attention to effectively fuse
spatial and temporal features in a more integrated manner. Our approach allows
temporal features to dynamically attend to relevant spatial regions, enhancing
the model's ability to detect fine-grained, time-evolving artifacts such as
flickering eyes or warped lips. This design enables more precise localization
and deeper contextual understanding, leading to improved performance across
diverse and challenging scenarios. We evaluate the performance of our model
using the FaceForensics++, Celeb-DF, and DeepfakeDetection datasets in both
intra- and cross-dataset settings to affirm the superiority of our approach.
Our model achieves strong performance with an AUC of 99.49 percent and an
accuracy of 97.57 percent in intra-dataset evaluations. In cross-dataset
testing, it demonstrates impressive generalization by achieving a 93.31 percent
AUC on the unseen DeepfakeDetection dataset. These results highlight the
effectiveness of cross-attention-based feature fusion in enhancing the
robustness of deepfake video detection.

</details>


### [75] [Elucidating and Endowing the Diffusion Training Paradigm for General Image Restoration](https://arxiv.org/abs/2506.21722)
*Xin Lu,Xueyang Fu,Jie Xiao,Zihao Fan,Yurui Zhu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: The paper proposes integrating diffusion training into general image restoration (IR) frameworks, addressing limitations of existing methods by introducing regularization strategies and task-specific adaptors for improved performance in single- and multi-task IR.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for IR are limited by complex architectures and lack integration with general IR frameworks. The paper aims to bridge this gap.

Method: Systematic analysis of diffusion training principles, introduction of regularization strategies, and development of incremental training with task-specific adaptors.

Result: Improved generalization in single-task IR and superior performance in multi-task unified IR, with seamless integration into existing IR architectures.

Conclusion: The proposed framework effectively combines diffusion training with general IR, enhancing performance and adaptability across tasks.

Abstract: While diffusion models demonstrate strong generative capabilities in image
restoration (IR) tasks, their complex architectures and iterative processes
limit their practical application compared to mainstream reconstruction-based
general ordinary IR networks. Existing approaches primarily focus on optimizing
network architecture and diffusion paths but overlook the integration of the
diffusion training paradigm within general ordinary IR frameworks. To address
these challenges, this paper elucidates key principles for adapting the
diffusion training paradigm to general IR training through systematic analysis
of time-step dependencies, network hierarchies, noise-level relationships, and
multi-restoration task correlations, proposing a new IR framework supported by
diffusion-based training. To enable IR networks to simultaneously restore
images and model generative representations, we introduce a series of
regularization strategies that align diffusion objectives with IR tasks,
improving generalization in single-task scenarios. Furthermore, recognizing
that diffusion-based generation exerts varying influences across different IR
tasks, we develop an incremental training paradigm and task-specific adaptors,
further enhancing performance in multi-task unified IR. Experiments demonstrate
that our method significantly improves the generalization of IR networks in
single-task IR and achieves superior performance in multi-task unified IR.
Notably, the proposed framework can be seamlessly integrated into existing
general IR architectures.

</details>


### [76] [Asymmetric Dual Self-Distillation for 3D Self-Supervised Representation Learning](https://arxiv.org/abs/2506.21724)
*Remco F. Leijenaar,Hamidreza Kasaei*

Main category: cs.CV

TL;DR: AsymDSD, an Asymmetric Dual Self-Distillation framework, unifies masked modeling and invariance learning for 3D point clouds, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Learning meaningful 3D representations without large labeled datasets is challenging; reconstruction-based methods like masked point modeling (MPM) may lack high-level semantic capture.

Method: AsymDSD uses latent space prediction, an asymmetric setup, masked query attention disabling, multi-mask sampling, and multi-crop adaptation for point clouds.

Result: Achieves 90.53% on ScanObjectNN, improving to 93.72% with pretraining on 930k shapes.

Conclusion: AsymDSD outperforms prior methods by unifying masked modeling and invariance learning, demonstrating superior semantic representation.

Abstract: Learning semantically meaningful representations from unstructured 3D point
clouds remains a central challenge in computer vision, especially in the
absence of large-scale labeled datasets. While masked point modeling (MPM) is
widely used in self-supervised 3D learning, its reconstruction-based objective
can limit its ability to capture high-level semantics. We propose AsymDSD, an
Asymmetric Dual Self-Distillation framework that unifies masked modeling and
invariance learning through prediction in the latent space rather than the
input space. AsymDSD builds on a joint embedding architecture and introduces
several key design choices: an efficient asymmetric setup, disabling attention
between masked queries to prevent shape leakage, multi-mask sampling, and a
point cloud adaptation of multi-crop. AsymDSD achieves state-of-the-art results
on ScanObjectNN (90.53%) and further improves to 93.72% when pretrained on 930k
shapes, surpassing prior methods.

</details>


### [77] [Exploring Image Generation via Mutually Exclusive Probability Spaces and Local Correlation Hypothesis](https://arxiv.org/abs/2506.21731)
*Chenqiu Zhao,Anup Basu*

Main category: cs.CV

TL;DR: The paper introduces MESP and LCH frameworks to address memorization in generative models, proposing BL-AE and ARVM for improved performance, though highlighting memorization issues.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of probabilistic generative models where learning global distributions leads to memorization instead of true generative behavior.

Method: Proposes MESP from VAE analysis, introduces BL-AE for binary latent encoding, and ARVM for histogram outputs. LCH is introduced to emphasize local correlations for generative capability.

Result: ARVM achieves competitive FID scores but reveals memorization. LCH is validated through experiments.

Conclusion: The frameworks MESP and LCH provide insights into generative model limitations and propose solutions, though memorization remains a challenge.

Abstract: We propose two theoretical frameworks, the Mutually Exclusive Probability
Space (MESP) and the Local Correlation Hypothesis (LCH), to explore a potential
limitation in probabilistic generative models; namely that learning global
distributions leads to memorization rather than generative behavior. MESP
emerges from our rethinking of the Variational Autoencoder (VAE). We observe
that latent variable distributions in VAE exhibit overlap, which leads to an
optimization conflict between the reconstruction loss and KL-divergence loss. A
lower bound based on the overlap coefficient is proposed. We refer to this
phenomenon as Mutually Exclusive Probability Spaces. Based on MESP, a Binary
Latent Autoencoder (BL-AE) is proposed to encode images into binary latent
representations. These binary latents are used as the input to our
Autoregressive Random Variable Model (ARVM), a modified autoregressive model
outputting histograms. Our ARVM achieves competitive FID scores, outperforming
state-of-the-art methods on standard datasets. However, such scores reflect
memorization rather than generation. To address this issue, we propose the
Local Correlation Hypothesis (LCH), which posits that generative capability
arising from local correlations among latent variables. Comprehensive
experiments and discussions are conducted to validate our frameworks.

</details>


### [78] [Equitable Federated Learning with NCA](https://arxiv.org/abs/2506.21735)
*Nick Lemke,Mirko Konstantin,Henry John Krumb,John Kalkhof,Jonathan Stieber,Anirban Mukhopadhyay*

Main category: cs.CV

TL;DR: FedNCA is a lightweight FL system for medical image segmentation in LMICs, designed for low-cost edge devices and unreliable networks.


<details>
  <summary>Details</summary>
Motivation: FL adoption in LMICs is hindered by limited computing resources and poor connectivity, necessitating a tailored solution.

Method: FedNCA uses the lightweight Med-NCA architecture for training on edge devices like smartphones, reducing communication costs and supporting encryption.

Result: FedNCA enables efficient, secure medical imaging solutions in resource-constrained regions.

Conclusion: FedNCA addresses infrastructural and security barriers, promoting equitable healthcare in LMICs.

Abstract: Federated Learning (FL) is enabling collaborative model training across
institutions without sharing sensitive patient data. This approach is
particularly valuable in low- and middle-income countries (LMICs), where access
to trained medical professionals is limited. However, FL adoption in LMICs
faces significant barriers, including limited high-performance computing
resources and unreliable internet connectivity. To address these challenges, we
introduce FedNCA, a novel FL system tailored for medical image segmentation
tasks. FedNCA leverages the lightweight Med-NCA architecture, enabling training
on low-cost edge devices, such as widely available smartphones, while
minimizing communication costs. Additionally, our encryption-ready FedNCA
proves to be suitable for compromised network communication. By overcoming
infrastructural and security challenges, FedNCA paves the way for inclusive,
efficient, lightweight, and encryption-ready medical imaging solutions,
fostering equitable healthcare advancements in resource-constrained regions.

</details>


### [79] [ImplicitQA: Going beyond frames towards Implicit Video Reasoning](https://arxiv.org/abs/2506.21742)
*Sirnam Swetha,Rohit Gupta,Parth Parag Kulkarni,David G Shatwell,Jeffrey A Chan Santiago,Nyle Siddiqui,Joseph Fioresi,Mubarak Shah*

Main category: cs.CV

TL;DR: The paper introduces ImplicitQA, a benchmark for testing implicit reasoning in VideoQA, highlighting gaps in current systems that rely on explicit visual content.


<details>
  <summary>Details</summary>
Motivation: Current VideoQA benchmarks focus on explicit visual content, missing the nuanced implicit reasoning humans use in creative videos like movies.

Method: Developed ImplicitQA with 1K QA pairs from 320+ creative video clips, categorized into key reasoning dimensions.

Result: Evaluations show performance drops in leading VideoQA models, revealing their reliance on surface-level cues.

Conclusion: ImplicitQA aims to advance research in implicit reasoning for VideoQA by providing a challenging dataset and framework.

Abstract: Video QA has made significant strides by leveraging multimodal learning to
align visual and textual modalities. However, current benchmarks overwhelmingly
focus on questions answerable through explicit visual content - actions,
objects & events directly observable within individual frames or short clips.
In contrast, creative and cinematic videos - such as movies, TV shows, and
narrative-driven content - employ storytelling techniques that deliberately
omit certain depictions, requiring viewers to infer motives, causality, and
relationships across discontinuous frames. Humans naturally excel at such
implicit reasoning, seamlessly integrating information across time and context
to construct coherent narratives. Current VideoQA systems and benchmarks fail
to capture this essential dimension of human-like understanding. To bridge this
gap, we present ImplicitQA, a novel benchmark specifically designed to test
models on implicit reasoning. It comprises 1K meticulously annotated QA pairs
derived from 320+ high-quality creative video clips, systematically categorized
into key reasoning dimensions: lateral and vertical spatial reasoning, depth
and proximity, viewpoint and visibility, motion and trajectory, causal and
motivational reasoning, social interactions, physical context, and inferred
counting. These annotations are deliberately challenging, crafted by authors
ensuring high-quality. Our extensive evaluations on leading VideoQA models
reveals performance degradation, underscoring their reliance on surface-level
visual cues and highlighting the difficulty of implicit reasoning. Performance
variations across models further illustrate the complexity and diversity of the
challenges presented by ImplicitQA. By releasing both the dataset and our data
collection framework, we aim to stimulate further research and development in
the community. https://huggingface.co/datasets/ucf-crcv/ImplicitQA.

</details>


### [80] [Early Glaucoma Detection using Deep Learning with Multiple Datasets of Fundus Images](https://arxiv.org/abs/2506.21770)
*Rishiraj Paul Chowdhury,Nirmit Shekar Karkera*

Main category: cs.CV

TL;DR: A deep learning pipeline using EfficientNet-B0 for glaucoma detection from retinal images, trained across multiple datasets for better generalization, achieves high performance with minimal preprocessing.


<details>
  <summary>Details</summary>
Motivation: Early detection of glaucoma is crucial but traditional methods are invasive and require specialized equipment.

Method: Uses EfficientNet-B0, sequentially trained and fine-tuned on ACRIMA, ORIGA, and RIM-ONE datasets with minimal preprocessing.

Result: Higher AUC-ROC with minimal preprocessing and strong performance on unseen datasets.

Conclusion: The pipeline is reproducible, scalable, and clinically promising for early glaucoma detection.

Abstract: Glaucoma is a leading cause of irreversible blindness, but early detection
can significantly improve treatment outcomes. Traditional diagnostic methods
are often invasive and require specialized equipment. In this work, we present
a deep learning pipeline using the EfficientNet-B0 architecture for glaucoma
detection from retinal fundus images. Unlike prior studies that rely on single
datasets, we sequentially train and fine-tune our model across ACRIMA, ORIGA,
and RIM-ONE datasets to enhance generalization. Our experiments show that
minimal preprocessing yields higher AUC-ROC compared to more complex
enhancements, and our model demonstrates strong discriminative performance on
unseen datasets. The proposed pipeline offers a reproducible and scalable
approach to early glaucoma detection, supporting its potential clinical
utility.

</details>


### [81] [Comparing Learning Paradigms for Egocentric Video Summarization](https://arxiv.org/abs/2506.21785)
*Daniel Wen*

Main category: cs.CV

TL;DR: The study compares supervised (Shotluck Holmes), unsupervised (TAC-SUM), and prompt fine-tuned (GPT-4o) models for egocentric video summarization, finding GPT-4o outperforms specialized models, highlighting limitations in current approaches for first-person videos.


<details>
  <summary>Details</summary>
Motivation: To assess and advance computer vision techniques for interpreting egocentric video data, addressing the gap in performance between first-person and third-person videos.

Method: Evaluated Shotluck Holmes (supervised), TAC-SUM (unsupervised), and GPT-4o (prompt fine-tuned) on egocentric video summarization using a subset of the Ego-Exo4D dataset.

Result: GPT-4o outperformed specialized models, revealing current methods' limitations in adapting to first-person perspectives.

Conclusion: The study underscores the need for further advancements in egocentric video analysis and highlights the potential of prompt fine-tuned general-purpose models like GPT-4o.

Abstract: In this study, we investigate various computer vision paradigms - supervised
learning, unsupervised learning, and prompt fine-tuning - by assessing their
ability to understand and interpret egocentric video data. Specifically, we
examine Shotluck Holmes (state-of-the-art supervised learning), TAC-SUM
(state-of-the-art unsupervised learning), and GPT-4o (a prompt fine-tuned
pre-trained model), evaluating their effectiveness in video summarization. Our
results demonstrate that current state-of-the-art models perform less
effectively on first-person videos compared to third-person videos,
highlighting the need for further advancements in the egocentric video domain.
Notably, a prompt fine-tuned general-purpose GPT-4o model outperforms these
specialized models, emphasizing the limitations of existing approaches in
adapting to the unique challenges of first-person perspectives. Although our
evaluation is conducted on a small subset of egocentric videos from the
Ego-Exo4D dataset due to resource constraints, the primary objective of this
research is to provide a comprehensive proof-of-concept analysis aimed at
advancing the application of computer vision techniques to first-person videos.
By exploring novel methodologies and evaluating their potential, we aim to
contribute to the ongoing development of models capable of effectively
processing and interpreting egocentric perspectives.

</details>


### [82] [CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery](https://arxiv.org/abs/2506.21813)
*Felix Holm,Gözde Ünver,Ghazal Ghazaei,Nassir Navab*

Main category: cs.CV

TL;DR: The paper introduces the CAT-SG dataset and CatSGG model for comprehensive cataract surgery workflow analysis, improving AI-driven surgical training and decision support.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack holistic representations of surgical workflows, missing semantic relationships between tools, tissues, and procedural techniques.

Method: The authors introduce the CAT-SG dataset with structured annotations and propose CatSGG, a scene graph generation model.

Result: CatSGG outperforms existing methods in generating structured surgical representations, enhancing workflow analysis.

Conclusion: CAT-SG and CatSGG advance AI applications in surgical training and real-time decision support, enabling context-aware clinical systems.

Abstract: Understanding the intricate workflows of cataract surgery requires modeling
complex interactions between surgical tools, anatomical structures, and
procedural techniques. Existing datasets primarily address isolated aspects of
surgical analysis, such as tool detection or phase segmentation, but lack
comprehensive representations that capture the semantic relationships between
entities over time. This paper introduces the Cataract Surgery Scene Graph
(CAT-SG) dataset, the first to provide structured annotations of tool-tissue
interactions, procedural variations, and temporal dependencies. By
incorporating detailed semantic relations, CAT-SG offers a holistic view of
surgical workflows, enabling more accurate recognition of surgical phases and
techniques. Additionally, we present a novel scene graph generation model,
CatSGG, which outperforms current methods in generating structured surgical
representations. The CAT-SG dataset is designed to enhance AI-driven surgical
training, real-time decision support, and workflow analysis, paving the way for
more intelligent, context-aware systems in clinical practice.

</details>


### [83] [Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models](https://arxiv.org/abs/2506.21826)
*Rafael Sterzinger,Marco Peer,Robert Sablatnig*

Main category: cs.CV

TL;DR: A method for few-shot segmentation of historical maps using vision foundation models and parameter-efficient fine-tuning, achieving state-of-the-art results with minimal trainable parameters.


<details>
  <summary>Details</summary>
Motivation: Historical maps are valuable but challenging to process due to diverse visuals and limited annotations.

Method: Leverages semantic embeddings from large vision models with parameter-efficient fine-tuning for few-shot segmentation.

Result: Outperforms benchmarks on vineyard/railway segmentation (+5-20% mIoU) and building block segmentation (67.3% PQ).

Conclusion: Enables precise map segmentation with minimal manual annotations, advancing automated historical map analysis.

Abstract: As rich sources of history, maps provide crucial insights into historical
changes, yet their diverse visual representations and limited annotated data
pose significant challenges for automated processing. We propose a simple yet
effective approach for few-shot segmentation of historical maps, leveraging the
rich semantic embeddings of large vision foundation models combined with
parameter-efficient fine-tuning. Our method outperforms the state-of-the-art on
the Siegfried benchmark dataset in vineyard and railway segmentation, achieving
+5% and +13% relative improvements in mIoU in 10-shot scenarios and around +20%
in the more challenging 5-shot setting. Additionally, it demonstrates strong
performance on the ICDAR 2021 competition dataset, attaining a mean PQ of 67.3%
for building block segmentation, despite not being optimized for this
shape-sensitive metric, underscoring its generalizability. Notably, our
approach maintains high performance even in extremely low-data regimes (10- &
5-shot), while requiring only 689k trainable parameters - just 0.21% of the
total model size. Our approach enables precise segmentation of diverse
historical maps while drastically reducing the need for manual annotations,
advancing automated processing and analysis in the field. Our implementation is
publicly available at:
https://github.com/RafaelSterzinger/few-shot-map-segmentation.

</details>


### [84] [TaleForge: Interactive Multimodal System for Personalized Story Creation](https://arxiv.org/abs/2506.21832)
*Minh-Loi Nguyen,Quang-Khai Le,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: TaleForge is a personalized story-generation system using LLMs and text-to-image diffusion to embed users' facial images in narratives and illustrations, enhancing engagement and immersion.


<details>
  <summary>Details</summary>
Motivation: Existing storytelling methods treat users as passive consumers with limited personalization, reducing engagement. TaleForge aims to create immersive, user-centric experiences by integrating personalized text and imagery.

Method: TaleForge uses three modules: Story Generation (LLMs create narratives from prompts), Personalized Image Generation (merges user faces/outfits into illustrations), and Background Generation (creates scenes with personalized characters).

Result: A user study showed increased engagement and ownership when users appeared as protagonists. Participants liked real-time previews but wanted finer narrative editing tools.

Conclusion: TaleForge advances multimodal storytelling by aligning personalized text and imagery for immersive, user-centric experiences.

Abstract: Storytelling is a deeply personal and creative process, yet existing methods
often treat users as passive consumers, offering generic plots with limited
personalization. This undermines engagement and immersion, especially where
individual style or appearance is crucial. We introduce TaleForge, a
personalized story-generation system that integrates large language models
(LLMs) and text-to-image diffusion to embed users' facial images within both
narratives and illustrations. TaleForge features three interconnected modules:
Story Generation, where LLMs create narratives and character descriptions from
user prompts; Personalized Image Generation, merging users' faces and outfit
choices into character illustrations; and Background Generation, creating scene
backdrops that incorporate personalized characters. A user study demonstrated
heightened engagement and ownership when individuals appeared as protagonists.
Participants praised the system's real-time previews and intuitive controls,
though they requested finer narrative editing tools. TaleForge advances
multimodal storytelling by aligning personalized text and imagery to create
immersive, user-centric experiences.

</details>


### [85] [ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment](https://arxiv.org/abs/2109.05721)
*Yangyu Huang,Hao Yang,Chong Li,Jongyoo Kim,Fangyun Wei*

Main category: cs.CV

TL;DR: The paper addresses error-bias in face alignment by proposing ADL and AAM for better CNN convergence, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Error-bias in facial landmark distributions is overlooked but crucial for ambiguous labeling tasks.

Method: Proposes anisotropic direction loss (ADL) for coordinate regression and anisotropic attention module (AAM) for heatmap regression.

Result: ADNet, integrating ADL and AAM, achieves top performance on 300W, WFLW, and COFW datasets.

Conclusion: The proposed methods effectively leverage error-bias for improved face alignment, demonstrating robustness.

Abstract: The recent progress of CNN has dramatically improved face alignment
performance. However, few works have paid attention to the error-bias with
respect to error distribution of facial landmarks. In this paper, we
investigate the error-bias issue in face alignment, where the distributions of
landmark errors tend to spread along the tangent line to landmark curves. This
error-bias is not trivial since it is closely connected to the ambiguous
landmark labeling task. Inspired by this observation, we seek a way to leverage
the error-bias property for better convergence of CNN model. To this end, we
propose anisotropic direction loss (ADL) and anisotropic attention module (AAM)
for coordinate and heatmap regression, respectively. ADL imposes strong binding
force in normal direction for each landmark point on facial boundaries. On the
other hand, AAM is an attention module which can get anisotropic attention mask
focusing on the region of point and its local edge connected by adjacent
points, it has a stronger response in tangent than in normal, which means
relaxed constraints in the tangent. These two methods work in a complementary
manner to learn both facial structures and texture details. Finally, we
integrate them into an optimized end-to-end training pipeline named ADNet. Our
ADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which
demonstrates the effectiveness and robustness.

</details>


### [86] [PrefPaint: Enhancing Image Inpainting through Expert Human Feedback](https://arxiv.org/abs/2506.21834)
*Duy-Bao Bui,Hoang-Khang Nguyen,Trung-Nghia Le*

Main category: cs.CV

TL;DR: PrefPaint integrates human feedback into Stable Diffusion Inpainting for medical imaging, improving accuracy and reliability without costly reward models.


<details>
  <summary>Details</summary>
Motivation: Inpainting models in medical imaging, like polyps, can produce errors affecting diagnosis. Expert annotations and human feedback are needed for reliable results.

Method: PrefPaint incorporates human feedback directly into Stable Diffusion Inpainting training, using a web interface for streamlined feedback and fine-tuning.

Result: PrefPaint reduces visual inconsistencies and improves rendering, especially in medical contexts, generating more realistic polyps images.

Conclusion: PrefPaint enhances inpainting reliability in medical imaging by leveraging human feedback, outperforming existing methods.

Abstract: Inpainting, the process of filling missing or corrupted image parts, has
broad applications, including medical imaging. However, in specialized fields
like medical polyps imaging, where accuracy and reliability are critical,
inpainting models can generate inaccurate images, leading to significant errors
in medical diagnosis and treatment. To ensure reliability, medical images
should be annotated by experts like oncologists for effective model training.
We propose PrefPaint, an approach that incorporates human feedback into the
training process of Stable Diffusion Inpainting, bypassing the need for
computationally expensive reward models. In addition, we develop a web-based
interface streamlines training, fine-tuning, and inference. This interactive
interface provides a smooth and intuitive user experience, making it easier to
offer feedback and manage the fine-tuning process. User study on various
domains shows that PrefPaint outperforms existing methods, reducing visual
inconsistencies and improving image rendering, particularly in medical
contexts, where our model generates more realistic polyps images.

</details>


### [87] [FreeEnricher: Enriching Face Landmarks without Additional Cost](https://arxiv.org/abs/2212.09525)
*Yangyu Huang,Xi Chen,Jongyoo Kim,Hao Yang,Chong Li,Jiaolong Yang,Dong Chen*

Main category: cs.CV

TL;DR: A framework for dense face alignment using sparse landmark datasets, achieving state-of-the-art accuracy without extra cost.


<details>
  <summary>Details</summary>
Motivation: Dense facial landmarks are needed for applications like cosmetic medicine, but most methods only handle sparse alignment.

Method: Uses weakly-supervised learning to refine sparse landmarks and adapt to dense ones, with custom operators and plug-and-play integration.

Result: Achieves top accuracy on dense 300W, sparse 300W, and WFLW testsets.

Conclusion: The framework effectively enriches landmark density and improves alignment accuracy.

Abstract: Recent years have witnessed significant growth of face alignment. Though
dense facial landmark is highly demanded in various scenarios, e.g., cosmetic
medicine and facial beautification, most works only consider sparse face
alignment. To address this problem, we present a framework that can enrich
landmark density by existing sparse landmark datasets, e.g., 300W with 68
points and WFLW with 98 points. Firstly, we observe that the local patches
along each semantic contour are highly similar in appearance. Then, we propose
a weakly-supervised idea of learning the refinement ability on original sparse
landmarks and adapting this ability to enriched dense landmarks. Meanwhile,
several operators are devised and organized together to implement the idea.
Finally, the trained model is applied as a plug-and-play module to the existing
face alignment networks. To evaluate our method, we manually label the dense
landmarks on 300W testset. Our method yields state-of-the-art accuracy not only
in newly-constructed dense 300W testset but also in the original sparse 300W
and WFLW testsets without additional cost.

</details>


### [88] [ProSAM: Enhancing the Robustness of SAM-based Visual Reference Segmentation with Probabilistic Prompts](https://arxiv.org/abs/2506.21835)
*Xiaoqi Wang,Clint Sebastian,Wenbin He,Liu Ren*

Main category: cs.CV

TL;DR: ProSAM improves visual reference segmentation by using a variational prompt encoder to avoid unstable prompt regions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing SAM-based methods for visual reference segmentation generate unstable prompts at object boundaries due to suboptimal prompt encoders.

Method: ProSAM introduces a variational prompt encoder to predict multivariate prompt distributions, avoiding unstable regions.

Result: ProSAM consistently outperforms state-of-the-art methods on Pascal-5$^i$ and COCO-20$^i$ datasets.

Conclusion: ProSAM provides a more robust solution for visual reference segmentation by addressing prompt instability.

Abstract: The recent advancements in large foundation models have driven the success of
open-set image segmentation, a task focused on segmenting objects beyond
predefined categories. Among various prompt types (such as points, boxes,
texts, and visual references), visual reference segmentation stands out for its
unique flexibility and strong zero-shot capabilities. Recently, several
SAM-based methods have made notable progress in this task by automatically
generating prompts to guide SAM. However, these methods often generate prompts
at object boundaries due to suboptimal prompt encoder, which results in
instability and reduced robustness. In this work, we introduce ProSAM, a simple
but effective method to address the stability challenges we identified in
existing SAM-based visual reference segmentation approaches. By learning a
variational prompt encoder to predict multivariate prompt distributions, ProSAM
avoids generating prompts that lie in unstable regions, overcoming the
instability caused by less robust prompts. Our approach consistently surpasses
state-of-the-art methods on the Pascal-5$^i$ and COCO-20$^i$ datasets,
providing a more robust solution for visual reference segmentation.

</details>


### [89] [GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles](https://arxiv.org/abs/2506.21839)
*Mengyi Shan,Brian Curless,Ira Kemelmacher-Shlizerman,Steve Seitz*

Main category: cs.CV

TL;DR: A hierarchical multi-agent framework improves text-to-image models for generating escape room puzzles by ensuring visual appeal, logical solidity, and intellectual stimulation.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models struggle with spatial relationships and affordance reasoning, limiting their ability to create functional and solvable escape room puzzles.

Method: A hierarchical multi-agent framework decomposes the task into stages: functional design, symbolic scene graph reasoning, layout synthesis, and local image editing, with iterative feedback for coherence.

Result: Agent collaboration enhances output quality in solvability, shortcut avoidance, and affordance clarity while preserving visual quality.

Conclusion: The proposed framework effectively addresses the limitations of base models, producing higher-quality escape room puzzle images.

Abstract: We challenge text-to-image models with generating escape room puzzle images
that are visually appealing, logically solid, and intellectually stimulating.
While base image models struggle with spatial relationships and affordance
reasoning, we propose a hierarchical multi-agent framework that decomposes this
task into structured stages: functional design, symbolic scene graph reasoning,
layout synthesis, and local image editing. Specialized agents collaborate
through iterative feedback to ensure the scene is visually coherent and
functionally solvable. Experiments show that agent collaboration improves
output quality in terms of solvability, shortcut avoidance, and affordance
clarity, while maintaining visual quality.

</details>


### [90] [PEACE: Empowering Geologic Map Holistic Understanding with MLLMs](https://arxiv.org/abs/2501.06184)
*Yangyu Huang,Tianyi Gao,Haoran Xu,Qihao Zhao,Yang Song,Zhipeng Gui,Tengchao Lv,Hao Chen,Lei Cui,Scarlett Li,Furu Wei*

Main category: cs.CV

TL;DR: The paper introduces GeoMap-Bench, a benchmark for evaluating Multimodal Large Language Models (MLLMs) in geologic map understanding, and GeoMap-Agent, an AI agent designed to bridge the gap in this domain.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with geologic map understanding due to challenges like cartographic generalization and domain-specific knowledge. This limits their utility in geology-related applications.

Method: The authors propose GeoMap-Agent, featuring Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI), and Prompt-enhanced Question Answering (PEQA) modules, inspired by interdisciplinary human collaboration.

Result: GeoMap-Agent achieves an overall score of 0.811 on GeoMap-Bench, significantly outperforming GPT-4o's score of 0.369.

Conclusion: The work (PEACE) advances AI applications in geology, improving the efficiency and accuracy of geologic map understanding.

Abstract: Geologic map, as a fundamental diagram in geology science, provides critical
insights into the structure and composition of Earth's subsurface and surface.
These maps are indispensable in various fields, including disaster detection,
resource exploration, and civil engineering. Despite their significance,
current Multimodal Large Language Models (MLLMs) often fall short in geologic
map understanding. This gap is primarily due to the challenging nature of
cartographic generalization, which involves handling high-resolution map,
managing multiple associated components, and requiring domain-specific
knowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever
benchmark for evaluating MLLMs in geologic map understanding, which assesses
the full-scale abilities in extracting, referring, grounding, reasoning, and
analyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent
designed for geologic map understanding, which features three modules:
Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI),
and Prompt-enhanced Question Answering (PEQA). Inspired by the
interdisciplinary collaboration among human scientists, an AI expert group acts
as consultants, utilizing a diverse tool pool to comprehensively analyze
questions. Through comprehensive experiments, GeoMap-Agent achieves an overall
score of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o.
Our work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs,
paves the way for advanced AI applications in geology, enhancing the efficiency
and accuracy of geological investigations.

</details>


### [91] [3D-Telepathy: Reconstructing 3D Objects from EEG Signals](https://arxiv.org/abs/2506.21843)
*Yuxiang Ge,Jionghao Cheng,Ruiquan Ge,Zhaojie Fang,Gangyong Jia,Xiang Wan,Nannan Li,Ahmed Elazab,Changmiao Wang*

Main category: cs.CV

TL;DR: Proposes a method to reconstruct 3D objects from EEG data using a novel EEG encoder with dual self-attention and hybrid training, overcoming noise and dataset scarcity.


<details>
  <summary>Details</summary>
Motivation: Traditional EEG-to-image methods ignore 3D reconstruction, limiting BCI applications despite the brain's inherent 3D processing.

Method: Uses a dual self-attention EEG encoder with hybrid training (cross-attention, contrastive, self-supervised learning) and stable diffusion with Variational Score Distillation for 3D generation.

Result: Successfully generates 3D objects from EEG data with similar content and structure.

Conclusion: The approach advances EEG-based 3D reconstruction, enhancing BCI potential and addressing previous limitations.

Abstract: Reconstructing 3D visual stimuli from Electroencephalography (EEG) data holds
significant potential for applications in Brain-Computer Interfaces (BCIs) and
aiding individuals with communication disorders. Traditionally, efforts have
focused on converting brain activity into 2D images, neglecting the translation
of EEG data into 3D objects. This limitation is noteworthy, as the human brain
inherently processes three-dimensional spatial information regardless of
whether observing 2D images or the real world. The neural activities captured
by EEG contain rich spatial information that is inevitably lost when
reconstructing only 2D images, thus limiting its practical applications in BCI.
The transition from EEG data to 3D object reconstruction faces considerable
obstacles. These include the presence of extensive noise within EEG signals and
a scarcity of datasets that include both EEG and 3D information, which
complicates the extraction process of 3D visual data. Addressing this
challenging task, we propose an innovative EEG encoder architecture that
integrates a dual self-attention mechanism. We use a hybrid training strategy
to train the EEG Encoder, which includes cross-attention, contrastive learning,
and self-supervised learning techniques. Additionally, by employing stable
diffusion as a prior distribution and utilizing Variational Score Distillation
to train a neural radiation field, we successfully generate 3D objects with
similar content and structure from EEG data.

</details>


### [92] [End-to-End RGB-IR Joint Image Compression With Channel-wise Cross-modality Entropy Model](https://arxiv.org/abs/2506.21851)
*Haofeng Wang,Fangtao Zhou,Qi Zhang,Zeyuan Chen,Enci Zhang,Zhao Wang,Xiaofeng Huang,Siwei Ma*

Main category: cs.CV

TL;DR: Proposes a joint compression framework for RGB-IR image pairs using a Channel-wise Cross-modality Entropy Model (CCEM) to reduce storage and transmission costs.


<details>
  <summary>Details</summary>
Motivation: Efficient compression of RGB-IR image pairs is needed due to increasing storage and transmission costs with multiple modalities.

Method: Introduces CCEM with Low-frequency Context Extraction Block (LCEB) and Low-frequency Context Fusion Block (LCFB) for cross-modality entropy modeling.

Result: Outperforms existing methods, achieving 23.1% bit rate saving on LLVIP dataset compared to state-of-the-art.

Conclusion: The framework effectively compresses RGB-IR pairs by leveraging cross-modality information.

Abstract: RGB-IR(RGB-Infrared) image pairs are frequently applied simultaneously in
various applications like intelligent surveillance. However, as the number of
modalities increases, the required data storage and transmission costs also
double. Therefore, efficient RGB-IR data compression is essential. This work
proposes a joint compression framework for RGB-IR image pair. Specifically, to
fully utilize cross-modality prior information for accurate context probability
modeling within and between modalities, we propose a Channel-wise
Cross-modality Entropy Model (CCEM). Among CCEM, a Low-frequency Context
Extraction Block (LCEB) and a Low-frequency Context Fusion Block (LCFB) are
designed for extracting and aggregating the global low-frequency information
from both modalities, which assist the model in predicting entropy parameters
more accurately. Experimental results demonstrate that our approach outperforms
existing RGB-IR image pair and single-modality compression methods on LLVIP and
KAIST datasets. For instance, the proposed framework achieves a 23.1% bit rate
saving on LLVIP dataset compared to the state-of-the-art RGB-IR image codec
presented at CVPR 2022.

</details>


### [93] [Periodic-MAE: Periodic Video Masked Autoencoder for rPPG Estimation](https://arxiv.org/abs/2506.21855)
*Jiho Choi,Sang Jun Lee*

Main category: cs.CV

TL;DR: A self-supervised method learns periodic signals from facial videos for remote photoplethysmography (rPPG) estimation, improving cross-dataset performance.


<details>
  <summary>Details</summary>
Motivation: To capture subtle skin tone changes for rPPG estimation without labeled data, leveraging periodicity and physiological constraints.

Method: Uses a video masked autoencoder with frame masking and physiological bandlimit constraints for self-supervised learning.

Result: Significant performance improvements, especially in cross-dataset evaluations on PURE, UBFC-rPPG, MMPD, and V4V datasets.

Conclusion: The proposed framework effectively learns periodic signals for rPPG, demonstrating robust cross-dataset generalization.

Abstract: In this paper, we propose a method that learns a general representation of
periodic signals from unlabeled facial videos by capturing subtle changes in
skin tone over time. The proposed framework employs the video masked
autoencoder to learn a high-dimensional spatio-temporal representation of the
facial region through self-supervised learning. Capturing quasi-periodic
signals in the video is crucial for remote photoplethysmography (rPPG)
estimation. To account for signal periodicity, we apply frame masking in terms
of video sampling, which allows the model to capture resampled quasi-periodic
signals during the pre-training stage. Moreover, the framework incorporates
physiological bandlimit constraints, leveraging the property that physiological
signals are sparse within their frequency bandwidth to provide pulse cues to
the model. The pre-trained encoder is then transferred to the rPPG task, where
it is used to extract physiological signals from facial videos. We evaluate the
proposed method through extensive experiments on the PURE, UBFC-rPPG, MMPD, and
V4V datasets. Our results demonstrate significant performance improvements,
particularly in challenging cross-dataset evaluations. Our code is available at
https://github.com/ziiho08/Periodic-MAE.

</details>


### [94] [SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space](https://arxiv.org/abs/2506.21857)
*Ekaterina Redekop,Mara Pleasure,Zichen Wang,Kimberly Flores,Anthony Sisk,William Speier,Corey W. Arnold*

Main category: cs.CV

TL;DR: SPADE integrates histopathology with spatial transcriptomics (ST) using a foundation model, outperforming baselines in 14 tasks by leveraging multimodal data.


<details>
  <summary>Details</summary>
Motivation: To address the gap in integrating whole-slide images (WSIs) with ST for capturing molecular heterogeneity beyond standard H&E staining.

Method: Uses a mixture-of-data experts technique with contrastive learning to create an ST-informed latent space, pre-trained on the HEST-1k dataset.

Result: Demonstrates superior few-shot performance on 14 downstream tasks compared to baseline models.

Conclusion: SPADE effectively integrates morphological and molecular data, enhancing pathology task performance.

Abstract: The rapid growth of digital pathology and advances in self-supervised deep
learning have enabled the development of foundational models for various
pathology tasks across diverse diseases. While multimodal approaches
integrating diverse data sources have emerged, a critical gap remains in the
comprehensive integration of whole-slide images (WSIs) with spatial
transcriptomics (ST), which is crucial for capturing critical molecular
heterogeneity beyond standard hematoxylin & eosin (H&E) staining. We introduce
SPADE, a foundation model that integrates histopathology with ST data to guide
image representation learning within a unified framework, in effect creating an
ST-informed latent space. SPADE leverages a mixture-of-data experts technique,
where experts, created via two-stage feature-space clustering, use contrastive
learning to learn representations of co-registered WSI patches and gene
expression profiles. Pre-trained on the comprehensive HEST-1k dataset, SPADE is
evaluated on 14 downstream tasks, demonstrating significantly superior few-shot
performance compared to baseline models, highlighting the benefits of
integrating morphological and molecular information into one latent space.

</details>


### [95] [LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs](https://arxiv.org/abs/2506.21862)
*Boyuan Sun,Jiaxing Zhao,Xihan Wei,Qibin Hou*

Main category: cs.CV

TL;DR: LLaVA-Scissor is a training-free token compression method for video multimodal LLMs, using Semantic Connected Components (SCC) for better semantic coverage and reduced redundancy.


<details>
  <summary>Details</summary>
Motivation: Existing token compression methods based on attention scores often miss semantic regions and cause redundancy.

Method: Proposes a two-step spatio-temporal token compression strategy using SCC to assign tokens to distinct semantic regions.

Result: Outperforms other methods in video understanding benchmarks, especially at low token retention ratios.

Conclusion: LLaVA-Scissor effectively compresses tokens while maintaining comprehensive semantic coverage, improving video understanding performance.

Abstract: In this paper, we present LLaVA-Scissor, a training-free token compression
strategy designed for video multimodal large language models. Previous methods
mostly attempt to compress tokens based on attention scores, but fail to
effectively capture all semantic regions and often lead to token redundancy.
Differently, we propose to leverage the Semantic Connected Components (SCC)
approach that assigns tokens to distinct semantic regions within the token set,
ensuring comprehensive semantic coverage. The outcome is a two-step
spatio-temporal token compression strategy that utilizes SCC in both spatial
and temporal domains. This strategy can effectively compress tokens by
representing the entire video with a set of non-overlapping semantic tokens. We
conduct extensive evaluations of the token compression capabilities of
LLaVA-Scissor across diverse video understanding benchmarks, including video
question answering, long video understanding, and comprehensive multi-choices
benchmarks. Experimental results show that the proposed LLaVA-Scissor
outperforms other token compression methods, achieving superior performance in
various video understanding benchmarks, particularly at low token retention
ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.

</details>


### [96] [Remote Sensing Large Vision-Language Model: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling](https://arxiv.org/abs/2506.21863)
*Sungjune Park,Yeongyun Kim,Se Yeon Kim,Yong Man Ro*

Main category: cs.CV

TL;DR: A novel LVLM framework for remote sensing (RS) enhances multi-level semantic understanding via Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling, outperforming general LVLMs in RS tasks.


<details>
  <summary>Details</summary>
Motivation: Existing LVLMs struggle with RS due to domain differences in visuals, scales, and semantics, limiting their direct application.

Method: Proposes two components: Semantic-augmented Multi-level Alignment (retrieval-based semantic enrichment) and Semantic-aware Expert Modeling (hierarchical semantic processing).

Result: Consistent improvements in RS tasks like scene classification and VQA, demonstrating effective bridging of the gap between general LVLMs and RS needs.

Conclusion: The tailored LVLM framework effectively addresses RS-specific challenges, enabling better multi-level semantic understanding.

Abstract: Large Vision and Language Models (LVLMs) have shown strong performance across
various vision-language tasks in natural image domains. However, their
application to remote sensing (RS) remains underexplored due to significant
domain differences in visual appearances, object scales, and semantics. These
discrepancies hider the effective understanding of RS scenes, which contain
rich, multi-level semantic information spanning from coarse-to-fine levels.
Hence, it limits the direct adaptation of existing LVLMs to RS imagery. To
address this gap, we propose a novel LVLM framework tailored for RS
understanding, incorporating two core components: Semantic-augmented
Multi-level Alignment and Semantic-aware Expert Modeling. First, to align
multi-level visual features, we introduce the retrieval-based Semantic
Augmentation Module which enriches the visual features with relevant semantics
across fine-to-coarse levels (e.g., object- and scene-level information). It is
designed to retrieve relevant semantic cues from a RS semantic knowledge
database, followed by aggregation of semantic cues with user query and
multi-level visual features, resulting in semantically enriched representation
across multiple levels. Second, for Semantic-aware Expert Modeling, we design
semantic experts, where each expert is responsible for processing semantic
representation at different levels separately. This enables hierarchical
semantic understanding from coarse to fine levels. Evaluations across multiple
RS tasks-including scene classification and VQA, etc.-demonstrate that the
proposed framework achieves consistent improvements across multiple semantic
levels. This highlights its capability and effectiveness in bridging the gap
between general LVLMs and unique demands of RS-specific vision-language
understanding.

</details>


### [97] [Dual-Perspective United Transformer for Object Segmentation in Optical Remote Sensing Images](https://arxiv.org/abs/2506.21866)
*Yanguang Sun,Jiexi Yan,Jianjun Qian,Chunyan Xu,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: Proposes DPU-Former, a dual-perspective Transformer model for ORSI segmentation, integrating long-range dependencies and spatial details efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing ORSI segmentation models overlook challenges like feature heterogeneity and high complexity, leading to sub-optimal results.

Method: Uses global-local mixed attention, Fourier-space merging, and a gated linear feed-forward network for efficient fusion and expressive power.

Result: Outperforms state-of-the-art methods on multiple datasets.

Conclusion: DPU-Former effectively addresses feature integration challenges, improving ORSI segmentation performance.

Abstract: Automatically segmenting objects from optical remote sensing images (ORSIs)
is an important task. Most existing models are primarily based on either
convolutional or Transformer features, each offering distinct advantages.
Exploiting both advantages is valuable research, but it presents several
challenges, including the heterogeneity between the two types of features, high
complexity, and large parameters of the model. However, these issues are often
overlooked in existing the ORSIs methods, causing sub-optimal segmentation. For
that, we propose a novel Dual-Perspective United Transformer (DPU-Former) with
a unique structure designed to simultaneously integrate long-range dependencies
and spatial details. In particular, we design the global-local mixed attention,
which captures diverse information through two perspectives and introduces a
Fourier-space merging strategy to obviate deviations for efficient fusion.
Furthermore, we present a gated linear feed-forward network to increase the
expressive ability. Additionally, we construct a DPU-Former decoder to
aggregate and strength features at different layers. Consequently, the
DPU-Former model outperforms the state-of-the-art methods on multiple datasets.
Code: https://github.com/CSYSI/DPU-Former.

</details>


### [98] [Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning](https://arxiv.org/abs/2506.21873)
*Tzu-Chun Chien,Chieh-Kai Lin,Shiang-Feng Tsai,Ruei-Chi Lai,Hung-Jen Chen,Min Sun*

Main category: cs.CV

TL;DR: The paper addresses the performance drop in Multimodal Large Language Models (MLLMs) due to token pruning, proposing Grounding-Aware Token Pruning (GAP) to recover accuracy without extra costs.


<details>
  <summary>Details</summary>
Motivation: Token pruning in MLLMs reduces computational costs but weakens visual grounding, causing significant performance drops.

Method: Proposes GAP, a method to adjust position IDs after pruning to maintain grounding accuracy.

Result: GAP recovers REC accuracy to 51.42%, 90% of original performance, without additional overhead.

Conclusion: GAP effectively mitigates the negative impact of token pruning on grounding tasks, improving performance across models.

Abstract: Recent Multimodal Large Language Models (MLLMs) have demonstrated strong
performance in visual grounding, establishing themselves as a general interface
for various vision-language applications. This progress has driven the
development of token pruning methods to mitigate the high computational costs
associated with processing numerous visual tokens. However, we observe that
pruning significantly weakens the model's grounding ability, leading to
incorrect predictions and drastic performance degradation. In Referring
Expression Comprehension (REC), for instance, pruning causes the accuracy of
LLaVA on the RefCOCO validation set to drop from 56.14% to 15.34%. Our analysis
identifies misaligned position IDs after pruning as the primary cause of this
degradation, as both the order and value of these IDs are crucial for
maintaining performance in grounding tasks. To address this issue, we propose
Grounding-Aware Token Pruning (GAP), a simple yet effective adjustment to
position IDs that recovers REC accuracy back to 51.42%, which is 90% of the
original performance in the without pruning setting, all while requiring no
additional training, memory, or computational overhead. Applied to models such
as Shikra, MiniGPTv2, and the LLaVA series, our method consistently improves
performance across various token pruning strategies.

</details>


### [99] [GRASP-PsONet: Gradient-based Removal of Spurious Patterns for PsOriasis Severity Classification](https://arxiv.org/abs/2506.21883)
*Basudha Pal,Sharif Amit Kamran,Brendon Lutnick,Molly Lucas,Chaitanya Parmar,Asha Patel Shah,David Apfel,Steven Fakharzadeh,Lloyd Miller,Gabriela Cula,Kristopher Standish*

Main category: cs.CV

TL;DR: A framework using gradient-based interpretability to flag problematic training images improves psoriasis severity scoring model performance by removing inconsistent or artifact-laden images.


<details>
  <summary>Details</summary>
Motivation: Inter-rater variability and data collection challenges (e.g., lighting, device quality) hinder reliable automated severity scoring for psoriasis.

Method: Uses gradient-based interpretability to detect training images with spurious correlations (e.g., inconsistent annotations or non-clinical artifacts). Applied to a ConvNeXT-based weakly supervised model.

Result: Removing 8.2% of flagged images improved model AUC-ROC by 5% (85% to 90%). Identified 90% of inter-rater disagreements by reviewing only 30% of samples.

Conclusion: The method enhances automated scoring robustness, reduces reliance on manual annotation review, and addresses data variability challenges.

Abstract: Psoriasis (PsO) severity scoring is important for clinical trials but is
hindered by inter-rater variability and the burden of in person clinical
evaluation. Remote imaging using patient captured mobile photos offers
scalability but introduces challenges, such as variation in lighting,
background, and device quality that are often imperceptible to humans but can
impact model performance. These factors, along with inconsistencies in
dermatologist annotations, reduce the reliability of automated severity
scoring. We propose a framework to automatically flag problematic training
images that introduce spurious correlations which degrade model generalization,
using a gradient based interpretability approach. By tracing the gradients of
misclassified validation images, we detect training samples where model errors
align with inconsistently rated examples or are affected by subtle, nonclinical
artifacts. We apply this method to a ConvNeXT based weakly supervised model
designed to classify PsO severity from phone images. Removing 8.2% of flagged
images improves model AUC-ROC by 5% (85% to 90%) on a held out test set.
Commonly, multiple annotators and an adjudication process ensure annotation
accuracy, which is expensive and time consuming. Our method detects training
images with annotation inconsistencies, potentially removing the need for
manual review. When applied to a subset of training data rated by two
dermatologists, the method identifies over 90% of cases with inter-rater
disagreement by reviewing only the top 30% of samples. This improves automated
scoring for remote assessments, ensuring robustness despite data collection
variability.

</details>


### [100] [Integrating Multi-Modal Sensors: A Review of Fusion Techniques for Intelligent Vehicles](https://arxiv.org/abs/2506.21885)
*Chuheng Wei,Ziye Qin,Ziyan Zhang,Guoyuan Wu,Matthew J. Barth*

Main category: cs.CV

TL;DR: A review of multi-sensor fusion in autonomous driving, categorizing strategies and discussing deep learning methods, datasets, and emerging trends like VLMs and LLMs.


<details>
  <summary>Details</summary>
Motivation: To enhance perception in autonomous driving by overcoming sensor limitations and improving environmental understanding.

Method: Categorizes fusion strategies (data-level, feature-level, decision-level) and reviews deep learning methods for each. Discusses datasets and real-world challenges.

Result: Identifies key methods and datasets, highlighting trends like VLMs and LLMs for improved adaptability and robustness.

Conclusion: Provides insights into current methods and future directions for multi-sensor fusion in autonomous driving.

Abstract: Multi-sensor fusion plays a critical role in enhancing perception for
autonomous driving, overcoming individual sensor limitations, and enabling
comprehensive environmental understanding. This paper first formalizes
multi-sensor fusion strategies into data-level, feature-level, and
decision-level categories and then provides a systematic review of deep
learning-based methods corresponding to each strategy. We present key
multi-modal datasets and discuss their applicability in addressing real-world
challenges, particularly in adverse weather conditions and complex urban
environments. Additionally, we explore emerging trends, including the
integration of Vision-Language Models (VLMs), Large Language Models (LLMs), and
the role of sensor fusion in end-to-end autonomous driving, highlighting its
potential to enhance system adaptability and robustness. Our work offers
valuable insights into current methods and future directions for multi-sensor
fusion in autonomous driving.

</details>


### [101] [DIVE: Deep-search Iterative Video Exploration A Technical Report for the CVRR Challenge at CVPR 2025](https://arxiv.org/abs/2506.21891)
*Umihiro Kamoto,Tatsuya Ishibashi,Noriyuki Kugo*

Main category: cs.CV

TL;DR: The paper presents DIVE, a method that won 1st place in the 2025 Complex Video Reasoning & Robustness Challenge by using iterative reasoning to answer complex video questions with 81.44% accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating accurate natural language answers for diverse, real-world video clips using the CVRR-ES benchmark.

Method: DIVE employs iterative reasoning, decomposing questions semantically and solving them stepwise for progressive inference.

Result: Achieved 81.44% accuracy on the CVRR-ES benchmark, securing top position.

Conclusion: The iterative reasoning framework of DIVE is effective for robust video question answering.

Abstract: In this report, we present the winning solution that achieved the 1st place
in the Complex Video Reasoning & Robustness Evaluation Challenge 2025. This
challenge evaluates the ability to generate accurate natural language answers
to questions about diverse, real-world video clips. It uses the Complex Video
Reasoning and Robustness Evaluation Suite (CVRR-ES) benchmark, which consists
of 214 unique videos and 2,400 question-answer pairs spanning 11 categories.
Our method, DIVE (Deep-search Iterative Video Exploration), adopts an iterative
reasoning approach, in which each input question is semantically decomposed and
solved through stepwise reasoning and progressive inference. This enables our
system to provide highly accurate and contextually appropriate answers to even
the most complex queries. Applied to the CVRR-ES benchmark, our approach
achieves 81.44% accuracy on the test set, securing the top position among all
participants. This report details our methodology and provides a comprehensive
analysis of the experimental results, demonstrating the effectiveness of our
iterative reasoning framework in achieving robust video question answering. The
code is available at https://github.com/PanasonicConnect/DIVE

</details>


### [102] [SODA: Out-of-Distribution Detection in Domain-Shifted Point Clouds via Neighborhood Propagation](https://arxiv.org/abs/2506.21892)
*Adam Goodge,Xun Xu,Bryan Hooi,Wee Siong Ng,Jingyi Liao,Yongyi Su,Xulei Yang*

Main category: cs.CV

TL;DR: The paper proposes SODA, a novel method for detecting out-of-distribution (OOD) point cloud objects using 3D vision-language models, addressing synthetic-to-real domain shift issues.


<details>
  <summary>Details</summary>
Motivation: The need for reliable OOD detection in point cloud data due to its growing use in applications, coupled with the under-explored nature of this problem and the challenges posed by domain shifts in pre-trained 3D VLMs.

Method: Introduces SODA, a neighborhood-based score propagation scheme that improves OOD detection without additional training, leveraging 3D VLMs.

Result: SODA achieves state-of-the-art performance in OOD detection across various datasets and settings.

Conclusion: SODA effectively mitigates synthetic-to-real domain shift issues in 3D VLMs, enhancing OOD detection for point cloud data.

Abstract: As point cloud data increases in prevalence in a variety of applications, the
ability to detect out-of-distribution (OOD) point cloud objects becomes
critical for ensuring model safety and reliability. However, this problem
remains under-explored in existing research. Inspired by success in the image
domain, we propose to exploit advances in 3D vision-language models (3D VLMs)
for OOD detection in point cloud objects. However, a major challenge is that
point cloud datasets used to pre-train 3D VLMs are drastically smaller in size
and object diversity than their image-based counterparts. Critically, they
often contain exclusively computer-designed synthetic objects. This leads to a
substantial domain shift when the model is transferred to practical tasks
involving real objects scanned from the physical environment. In this paper,
our empirical experiments show that synthetic-to-real domain shift
significantly degrades the alignment of point cloud with their associated text
embeddings in the 3D VLM latent space, hindering downstream performance. To
address this, we propose a novel methodology called SODA which improves the
detection of OOD point clouds through a neighborhood-based score propagation
scheme. SODA is inference-based, requires no additional model training, and
achieves state-of-the-art performance over existing approaches across datasets
and problem settings.

</details>


### [103] [Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.21895)
*Fangling Jiang,Qi Li,Weining Wang,Gang Wang,Bing Liu,Zhenan Sun*

Main category: cs.CV

TL;DR: A reinforcement fine-tuning-based face anti-spoofing method is proposed to improve generalization and interpretability by leveraging multimodal large language models and reward-based optimization.


<details>
  <summary>Details</summary>
Motivation: Existing face anti-spoofing methods struggle with poor generalization to unknown attack types and lack interpretability due to memorization of training data patterns.

Method: The method uses verifiable class and reasoning consistent rewards with GRPO-based optimization to guide the model in learning reasoning policies, iteratively refining high-reward trajectories.

Result: The approach achieves state-of-the-art cross-domain generalization, effectively handling unknown attack types and providing interpretable decisions without textual annotations.

Conclusion: The proposed method enhances generalization and interpretability in face anti-spoofing, addressing limitations of existing approaches through reinforcement fine-tuning.

Abstract: Recently the emergence of novel presentation attacks has drawn increasing
attention to face anti-spoofing. However, existing methods tend to memorize
data patterns from the training set, resulting in poor generalization to
unknown attack types across different scenarios and limited interpretability.
To address these challenges, this paper presents a reinforcement
fine-tuning-based face anti-spoofing method that stimulates the capabilities of
multimodal large language models to think and learn how to solve the
anti-spoofing task itself, rather than relying on the memorization of
authenticity patterns. We design verifiable class consistent reward and
reasoning consistent reward, and employ a GRPO-based optimization strategy to
guide the model in exploring reasoning policies from multiple perspectives to
maximize expected rewards. As a result, through iterative trial-and-error
learning while retaining only high-reward trajectories, the model distills
highly generalizable decision-making rules from the extensive solution space to
effectively address cross-domain face anti-spoofing tasks. Extensive
experimental results demonstrate that our method achieves state-of-the-art
cross-domain generalization performance. It generalizes well to diverse unknown
attack types in unseen target domains while providing interpretable reasoning
for its authenticity decisions without requiring labor-intensive textual
annotations for training.

</details>


### [104] [Visual Content Detection in Educational Videos with Transfer Learning and Dataset Enrichment](https://arxiv.org/abs/2506.21903)
*Dipayan Biswas,Shishir Shah,Jaspal Subhlok*

Main category: cs.CV

TL;DR: The paper proposes a transfer learning approach using YOLO for detecting visual elements in lecture videos, addressing challenges like lack of standard structure and annotated datasets. It includes a benchmark dataset and source code for future research.


<details>
  <summary>Details</summary>
Motivation: Visual elements in lecture videos are crucial for learning but are underutilized due to challenges in automatic detection, such as non-standard structures and lack of annotated data.

Method: A transfer learning approach was used, evaluating state-of-the-art object detection models (YOLO performed best). YOLO was optimized with training on benchmark datasets and semi-supervised auto-labeling.

Result: YOLO emerged as the most effective model for detecting visual elements in lecture videos. The approach also developed a general solution for object detection in such videos.

Conclusion: The study successfully addressed the detection of visual elements in lecture videos, providing a benchmark dataset and open-source tools to advance future research.

Abstract: Video is transforming education with online courses and recorded lectures
supplementing and replacing classroom teaching. Recent research has focused on
enhancing information retrieval for video lectures with advanced navigation,
searchability, summarization, as well as question answering chatbots. Visual
elements like tables, charts, and illustrations are central to comprehension,
retention, and data presentation in lecture videos, yet their full potential
for improving access to video content remains underutilized. A major factor is
that accurate automatic detection of visual elements in a lecture video is
challenging; reasons include i) most visual elements, such as charts, graphs,
tables, and illustrations, are artificially created and lack any standard
structure, and ii) coherent visual objects may lack clear boundaries and may be
composed of connected text and visual components. Despite advancements in deep
learning based object detection, current models do not yield satisfactory
performance due to the unique nature of visual content in lectures and scarcity
of annotated datasets. This paper reports on a transfer learning approach for
detecting visual elements in lecture video frames. A suite of state of the art
object detection models were evaluated for their performance on lecture video
datasets. YOLO emerged as the most promising model for this task. Subsequently
YOLO was optimized for lecture video object detection with training on multiple
benchmark datasets and deploying a semi-supervised auto labeling strategy.
Results evaluate the success of this approach, also in developing a general
solution to the problem of object detection in lecture videos. Paper
contributions include a publicly released benchmark of annotated lecture video
frames, along with the source code to facilitate future research.

</details>


### [105] [RAUM-Net: Regional Attention and Uncertainty-aware Mamba Network](https://arxiv.org/abs/2506.21905)
*Mingquan Liu*

Main category: cs.CV

TL;DR: A semi-supervised method combining Mamba-based feature modeling, region attention, and Bayesian uncertainty for Fine-Grained Visual Categorization (FGVC), showing robustness with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: FGVC is challenging due to subtle inter-class differences and fragile features, especially with scarce labeled data.

Method: Combines Mamba-based feature modeling, region attention, and Bayesian uncertainty for pseudo-label selection.

Result: Strong performance on FGVC benchmarks with occlusions, robust under limited labeled data.

Conclusion: The proposed method effectively addresses FGVC challenges, enhancing feature modeling and stability with limited supervision.

Abstract: Fine Grained Visual Categorization (FGVC) remains a challenging task in
computer vision due to subtle inter class differences and fragile feature
representations. Existing methods struggle in fine grained scenarios,
especially when labeled data is scarce. We propose a semi supervised method
combining Mamba based feature modeling, region attention, and Bayesian
uncertainty. Our approach enhances local to global feature modeling while
focusing on key areas during learning. Bayesian inference selects high quality
pseudo labels for stability. Experiments show strong performance on FGVC
benchmarks with occlusions, demonstrating robustness when labeled data is
limited. Code is available at https://github.com/wxqnl/RAUM Net.

</details>


### [106] [CERBERUS: Crack Evaluation & Recognition Benchmark for Engineering Reliability & Urban Stability](https://arxiv.org/abs/2506.21909)
*Justin Reinman,Sunwoong Choi*

Main category: cs.CV

TL;DR: CERBERUS is a synthetic benchmark for training and evaluating AI models in detecting infrastructure defects, featuring crack image generation and 3D inspection scenarios. It improves performance by combining synthetic and real data.


<details>
  <summary>Details</summary>
Motivation: To provide a flexible, repeatable benchmark for testing defect detection systems and advancing automated infrastructure inspection.

Method: Includes a crack image generator and 3D inspection scenarios (Fly-By and Underpass) built in Unity. Tests YOLO with synthetic and real crack data combinations.

Result: Combining synthetic and real data enhances performance on real-world images.

Conclusion: CERBERUS is a valuable tool for defect detection research and is publicly available for future work.

Abstract: CERBERUS is a synthetic benchmark designed to help train and evaluate AI
models for detecting cracks and other defects in infrastructure. It includes a
crack image generator and realistic 3D inspection scenarios built in Unity. The
benchmark features two types of setups: a simple Fly-By wall inspection and a
more complex Underpass scene with lighting and geometry challenges. We tested a
popular object detection model (YOLO) using different combinations of synthetic
and real crack data. Results show that combining synthetic and real data
improves performance on real-world images. CERBERUS provides a flexible,
repeatable way to test defect detection systems and supports future research in
automated infrastructure inspection. CERBERUS is publicly available at
https://github.com/justinreinman/Cerberus-Defect-Generator.

</details>


### [107] [Generating Attribute-Aware Human Motions from Textual Prompt](https://arxiv.org/abs/2506.21912)
*Xinghan Wang,Kun Xu,Fei Li,Cao Sheng,Jiazhong Yu,Yadong Mu*

Main category: cs.CV

TL;DR: The paper introduces a framework for text-driven human motion generation that incorporates human attributes (e.g., age, gender) alongside action semantics, addressing a gap in current methods.


<details>
  <summary>Details</summary>
Motivation: Current text-to-motion models ignore human attributes, which significantly influence motion patterns. This work aims to bridge this gap by integrating attribute information into motion generation.

Method: A Structural Causal Model-inspired framework decouples action semantics from human attributes, enabling text-to-semantics prediction and attribute-controlled motion generation.

Result: The model generates realistic, attribute-aware motions aligned with user inputs. A new dataset, HumanAttr, is introduced for evaluation, validating the model's effectiveness.

Conclusion: The proposed framework successfully integrates human attributes into text-driven motion generation, setting a benchmark for future research.

Abstract: Text-driven human motion generation has recently attracted considerable
attention, allowing models to generate human motions based on textual
descriptions. However, current methods neglect the influence of human
attributes (such as age, gender, weight, and height) which are key factors
shaping human motion patterns. This work represents a pilot exploration for
bridging this gap. We conceptualize each motion as comprising both attribute
information and action semantics, where textual descriptions align exclusively
with action semantics. To achieve this, a new framework inspired by Structural
Causal Models is proposed to decouple action semantics from human attributes,
enabling text-to-semantics prediction and attribute-controlled generation. The
resulting model is capable of generating realistic, attribute-aware motion
aligned with the user's text and attribute inputs. For evaluation, we introduce
HumanAttr, a comprehensive dataset containing attribute annotations for
text-motion pairs, setting the first benchmark for attribute-aware
text-to-motion generation. Extensive experiments on the new dataset validate
our model's effectiveness.

</details>


### [108] [SepFormer: Coarse-to-fine Separator Regression Network for Table Structure Recognition](https://arxiv.org/abs/2506.21920)
*Nam Quan Nguyen,Xuan Phong Pham,Tuan-Anh Tran*

Main category: cs.CV

TL;DR: SepFormer is a DETR-style architecture for Table Structure Recognition (TSR) that integrates split-and-merge in one step, improving speed and robustness. It uses a coarse-to-fine approach with transformer decoders and achieves 25.6 FPS with competitive performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Automated reconstruction of table structures from images is crucial for semantic data extraction, but existing methods lack efficiency and robustness.

Method: SepFormer combines split-and-merge in a single step via separator regression. It uses two transformer decoders for coarse-to-fine prediction of separators, refining from single-line to line-strip segments with angle loss.

Result: Achieves 25.6 FPS and comparable performance to state-of-the-art methods on datasets like SciTSR, PubTabNet, WTW, and iFLYTAB.

Conclusion: SepFormer offers an efficient and robust solution for TSR, balancing speed and accuracy.

Abstract: The automated reconstruction of the logical arrangement of tables from image
data, termed Table Structure Recognition (TSR), is fundamental for semantic
data extraction. Recently, researchers have explored a wide range of techniques
to tackle this problem, demonstrating significant progress. Each table is a set
of vertical and horizontal separators. Following this realization, we present
SepFormer, which integrates the split-and-merge paradigm into a single step
through separator regression with a DETR-style architecture, improving speed
and robustness. SepFormer is a coarse-to-fine approach that predicts table
separators from single-line to line-strip separators with a stack of two
transformer decoders. In the coarse-grained stage, the model learns to
gradually refine single-line segments through decoder layers with additional
angle loss. At the end of the fine-grained stage, the model predicts line-strip
separators by refining sampled points from each single-line segment. Our
SepFormer can run on average at 25.6 FPS while achieving comparable performance
with state-of-the-art methods on several benchmark datasets, including SciTSR,
PubTabNet, WTW, and iFLYTAB.

</details>


### [109] [ZeroReg3D: A Zero-shot Registration Pipeline for 3D Consecutive Histopathology Image Reconstruction](https://arxiv.org/abs/2506.21923)
*Juming Xiong,Ruining Deng,Jialin Yue,Siqi Lu,Junlin Guo,Marilyn Lionts,Tianyuan Yao,Can Cui,Junchao Zhu,Chongyu Qu,Mengmeng Yin,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: ZeroReg3D is a zero-shot registration pipeline for accurate 3D reconstruction from 2D histological sections, addressing challenges like tissue deformation and staining variability without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing 2D registration methods struggle with preserving 3D spatial relationships and face issues like tissue deformation and inconsistent illumination. Deep learning methods lack generalizability, while non-deep-learning approaches sacrifice accuracy.

Method: ZeroReg3D combines zero-shot deep learning-based keypoint matching with optimization-based affine and non-rigid registration techniques.

Result: The pipeline effectively handles tissue deformation, sectioning artifacts, staining variability, and inconsistent illumination without needing retraining.

Conclusion: ZeroReg3D offers a robust solution for 3D histological reconstruction, balancing accuracy and generalizability.

Abstract: Histological analysis plays a crucial role in understanding tissue structure
and pathology. While recent advancements in registration methods have improved
2D histological analysis, they often struggle to preserve critical 3D spatial
relationships, limiting their utility in both clinical and research
applications. Specifically, constructing accurate 3D models from 2D slices
remains challenging due to tissue deformation, sectioning artifacts,
variability in imaging techniques, and inconsistent illumination. Deep
learning-based registration methods have demonstrated improved performance but
suffer from limited generalizability and require large-scale training data. In
contrast, non-deep-learning approaches offer better generalizability but often
compromise on accuracy. In this study, we introduced ZeroReg3D, a novel
zero-shot registration pipeline tailored for accurate 3D reconstruction from
serial histological sections. By combining zero-shot deep learning-based
keypoint matching with optimization-based affine and non-rigid registration
techniques, ZeroReg3D effectively addresses critical challenges such as tissue
deformation, sectioning artifacts, staining variability, and inconsistent
illumination without requiring retraining or fine-tuning. The code has been
made publicly available at https://github.com/hrlblab/ZeroReg3D

</details>


### [110] [SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding](https://arxiv.org/abs/2506.21924)
*Zhao Jin,Rong-Cheng Tu,Jingyi Liao,Wenhao Sun,Xiao Luo,Shunyu Liu,Dacheng Tao*

Main category: cs.CV

TL;DR: SPAZER is a VLM-driven agent for zero-shot 3D visual grounding, combining 3D spatial and 2D semantic reasoning to outperform existing methods without 3D training data.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing zero-shot 3DVG methods that focus on either spatial or semantic understanding, SPAZER integrates both for robust performance in real-world applications.

Method: SPAZER uses a progressive reasoning framework: 3D scene analysis, anchor-guided candidate screening, and 3D-2D joint decision-making to localize objects.

Result: SPAZER achieves significant accuracy gains of 9.0% and 10.9% on ScanRefer and Nr3D benchmarks, outperforming state-of-the-art zero-shot methods.

Conclusion: SPAZER effectively bridges spatial and semantic reasoning, enabling robust zero-shot 3D visual grounding without 3D-labeled training data.

Abstract: 3D Visual Grounding (3DVG) aims to localize target objects within a 3D scene
based on natural language queries. To alleviate the reliance on costly 3D
training data, recent studies have explored zero-shot 3DVG by leveraging the
extensive knowledge and powerful reasoning capabilities of pre-trained LLMs and
VLMs. However, existing paradigms tend to emphasize either spatial (3D-based)
or semantic (2D-based) understanding, limiting their effectiveness in complex
real-world applications. In this work, we introduce SPAZER - a VLM-driven agent
that combines both modalities in a progressive reasoning framework. It first
holistically analyzes the scene and produces a 3D rendering from the optimal
viewpoint. Based on this, anchor-guided candidate screening is conducted to
perform a coarse-level localization of potential objects. Furthermore,
leveraging retrieved relevant 2D camera images, 3D-2D joint decision-making is
efficiently performed to determine the best-matching object. By bridging
spatial and semantic reasoning neural streams, SPAZER achieves robust zero-shot
grounding without training on 3D-labeled data. Extensive experiments on
ScanRefer and Nr3D benchmarks demonstrate that SPAZER significantly outperforms
previous state-of-the-art zero-shot methods, achieving notable gains of 9.0%
and 10.9% in accuracy.

</details>


### [111] [Quality Assessment and Distortion-aware Saliency Prediction for AI-Generated Omnidirectional Images](https://arxiv.org/abs/2506.21925)
*Liu Yang,Huiyu Duan,Jiarui Wang,Jing Liu,Menghan Hu,Xiongkuo Min,Guangtao Zhai,Patrick Le Callet*

Main category: cs.CV

TL;DR: The paper focuses on quality assessment and optimization of AI-generated omnidirectional images (AIGODIs) for VR/AR, introducing a database (OHF2024) and two models (BLIP2OIQA, BLIP2OISal) for evaluation and saliency prediction, with an automatic optimization process.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of AIGC techniques and the unique quality issues of AIGODIs necessitate research on their assessment and optimization, which is currently lacking.

Method: Established the OHF2024 database with subjective ratings and saliency data, then proposed BLIP2OIQA and BLIP2OISal models for quality evaluation and saliency prediction, followed by an optimization process.

Result: The models achieve SOTA performance in quality assessment and saliency prediction, and effectively enhance AIGODI visual quality.

Conclusion: The work advances AIGODI research with practical tools (database, models, optimization) and encourages future exploration via open-source release.

Abstract: With the rapid advancement of Artificial Intelligence Generated Content
(AIGC) techniques, AI generated images (AIGIs) have attracted widespread
attention, among which AI generated omnidirectional images (AIGODIs) hold
significant potential for Virtual Reality (VR) and Augmented Reality (AR)
applications. AI generated omnidirectional images exhibit unique quality
issues, however, research on the quality assessment and optimization of
AI-generated omnidirectional images is still lacking. To this end, this work
first studies the quality assessment and distortion-aware saliency prediction
problems for AIGODIs, and further presents a corresponding optimization
process. Specifically, we first establish a comprehensive database to reflect
human feedback for AI-generated omnidirectionals, termed OHF2024, which
includes both subjective quality ratings evaluated from three perspectives and
distortion-aware salient regions. Based on the constructed OHF2024 database, we
propose two models with shared encoders based on the BLIP-2 model to evaluate
the human visual experience and predict distortion-aware saliency for
AI-generated omnidirectional images, which are named as BLIP2OIQA and
BLIP2OISal, respectively. Finally, based on the proposed models, we present an
automatic optimization process that utilizes the predicted visual experience
scores and distortion regions to further enhance the visual quality of an
AI-generated omnidirectional image. Extensive experiments show that our
BLIP2OIQA model and BLIP2OISal model achieve state-of-the-art (SOTA) results in
the human visual experience evaluation task and the distortion-aware saliency
prediction task for AI generated omnidirectional images, and can be effectively
used in the optimization process. The database and codes will be released on
https://github.com/IntMeGroup/AIGCOIQA to facilitate future research.

</details>


### [112] [SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images](https://arxiv.org/abs/2506.21945)
*Naftaly Wambugu,Ruisheng Wang,Bo Guo,Tianshu Yu,Sheng Xu,Mohammed Elhassan*

Main category: cs.CV

TL;DR: The paper introduces SDRNet, a stacked deep residual network for semantic segmentation of fine-resolution remotely sensed (FRRS) images, addressing challenges like class disparities and object size variation.


<details>
  <summary>Details</summary>
Motivation: Accurate semantic segmentation of FRRS images is hindered by class disparities, occlusion, and object size variation, despite advancements in deep learning.

Method: The proposed SDRNet uses two stacked encoder-decoder networks and dilated residual blocks (DRB) to capture long-range semantics and preserve spatial details.

Result: Experiments on ISPRS Vaihingen and Potsdam datasets show SDRNet performs competitively against existing deep convolutional neural networks.

Conclusion: SDRNet effectively addresses segmentation challenges in FRRS images, demonstrating robust performance and improved feature learning.

Abstract: Land cover maps generated from semantic segmentation of high-resolution
remotely sensed images have drawn mucon in the photogrammetry and remote
sensing research community. Currently, massive fine-resolution remotely sensed
(FRRS) images acquired by improving sensing and imaging technologies become
available. However, accurate semantic segmentation of such FRRS images is
greatly affected by substantial class disparities, the invisibility of key
ground objects due to occlusion, and object size variation. Despite the
extraordinary potential in deep convolutional neural networks (DCNNs) in image
feature learning and representation, extracting sufficient features from FRRS
images for accurate semantic segmentation is still challenging. These
challenges demand the deep learning models to learn robust features and
generate sufficient feature descriptors. Specifically, learning
multi-contextual features to guarantee adequate coverage of varied object sizes
from the ground scene and harnessing global-local contexts to overcome class
disparities challenge even profound networks. Deeper networks significantly
lose spatial details due to gradual downsampling processes resulting in poor
segmentation results and coarse boundaries. This article presents a stacked
deep residual network (SDRNet) for semantic segmentation from FRRS images. The
proposed framework utilizes two stacked encoder-decoder networks to harness
long-range semantics yet preserve spatial information and dilated residual
blocks (DRB) between each encoder and decoder network to capture sufficient
global dependencies thus improving segmentation performance. Our experimental
results obtained using the ISPRS Vaihingen and Potsdam datasets demonstrate
that the SDRNet performs effectively and competitively against current DCNNs in
semantic segmentation.

</details>


### [113] [Exploring Semantic Masked Autoencoder for Self-supervised Point Cloud Understanding](https://arxiv.org/abs/2506.21957)
*Yixin Zha,Chuxin Wang,Wenfei Yang,Tianzhu Zhang*

Main category: cs.CV

TL;DR: The paper introduces Semantic Masked Autoencoder, a method to improve point cloud understanding by addressing limitations of random masking in self-supervised learning.


<details>
  <summary>Details</summary>
Motivation: Random masking strategies in pre-training methods fail to capture semantic relationships in point clouds, limiting model performance.

Method: Proposes a prototype-based semantic modeling module and a semantic-enhanced masking strategy, along with a prompt-tuning strategy for downstream tasks.

Result: Experiments on ScanObjectNN, ModelNet40, and ShapeNetPart show the method's effectiveness.

Conclusion: The Semantic Masked Autoencoder improves feature representation and downstream task performance in point cloud understanding.

Abstract: Point cloud understanding aims to acquire robust and general feature
representations from unlabeled data. Masked point modeling-based methods have
recently shown significant performance across various downstream tasks. These
pre-training methods rely on random masking strategies to establish the
perception of point clouds by restoring corrupted point cloud inputs, which
leads to the failure of capturing reasonable semantic relationships by the
self-supervised models. To address this issue, we propose Semantic Masked
Autoencoder, which comprises two main components: a prototype-based component
semantic modeling module and a component semantic-enhanced masking strategy.
Specifically, in the component semantic modeling module, we design a component
semantic guidance mechanism to direct a set of learnable prototypes in
capturing the semantics of different components from objects. Leveraging these
prototypes, we develop a component semantic-enhanced masking strategy that
addresses the limitations of random masking in effectively covering complete
component structures. Furthermore, we introduce a component semantic-enhanced
prompt-tuning strategy, which further leverages these prototypes to improve the
performance of pre-trained models in downstream tasks. Extensive experiments
conducted on datasets such as ScanObjectNN, ModelNet40, and ShapeNetPart
demonstrate the effectiveness of our proposed modules.

</details>


### [114] [TASeg: Text-aware RGB-T Semantic Segmentation based on Fine-tuning Vision Foundation Models](https://arxiv.org/abs/2506.21975)
*Meng Yu,Te Cui,Qitong Chu,Wenjie Song,Yi Yang,Yufeng Yue*

Main category: cs.CV

TL;DR: TASeg is a text-aware RGB-T segmentation framework using LoRA fine-tuning and CLIP embeddings to improve semantic segmentation by addressing modality heterogeneity and lack of high-level textual information.


<details>
  <summary>Details</summary>
Motivation: Existing RGB-T models lack high-level textual information and struggle with similar visual categories. SAM's integration with thermal images and text is hindered by modality issues and inefficiency.

Method: Proposes TASeg with LoRA fine-tuning, a Dynamic Feature Fusion Module (DFFM) for multi-modal feature merging, and CLIP text embeddings for semantic alignment.

Result: Achieves superior performance in challenging scenarios with fewer trainable parameters.

Conclusion: TASeg effectively addresses modality heterogeneity and enhances semantic understanding, outperforming existing methods.

Abstract: Reliable semantic segmentation of open environments is essential for
intelligent systems, yet significant problems remain: 1) Existing RGB-T
semantic segmentation models mainly rely on low-level visual features and lack
high-level textual information, which struggle with accurate segmentation when
categories share similar visual characteristics. 2) While SAM excels in
instance-level segmentation, integrating it with thermal images and text is
hindered by modality heterogeneity and computational inefficiency. To address
these, we propose TASeg, a text-aware RGB-T segmentation framework by using
Low-Rank Adaptation (LoRA) fine-tuning technology to adapt vision foundation
models. Specifically, we propose a Dynamic Feature Fusion Module (DFFM) in the
image encoder, which effectively merges features from multiple visual
modalities while freezing SAM's original transformer blocks. Additionally, we
incorporate CLIP-generated text embeddings in the mask decoder to enable
semantic alignment, which further rectifies the classification error and
improves the semantic understanding accuracy. Experimental results across
diverse datasets demonstrate that our method achieves superior performance in
challenging scenarios with fewer trainable parameters.

</details>


### [115] [R1-Track: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning](https://arxiv.org/abs/2506.21980)
*Biao Wang,Wenwen Li*

Main category: cs.CV

TL;DR: R1-Track, a fine-tuned version of Qwen2.5-VL using GRPO reinforcement learning, achieves strong performance in visual tracking while retaining general capabilities.


<details>
  <summary>Details</summary>
Motivation: Traditional tracking methods lack flexibility and require large datasets. Multi-modal large language models (MLLMs) like Qwen2.5-VL show promise but struggle with tracking tasks.

Method: Fine-tuned Qwen2.5-VL using GRPO reinforcement learning on a small dataset with a rule-based reward function.

Result: R1-Track performed well on the GOT-10k benchmark and supports flexible initialization via bounding boxes or text.

Conclusion: R1-Track demonstrates potential for visual tracking, with room for further improvements.

Abstract: Visual single object tracking aims to continuously localize and estimate the
scale of a target in subsequent video frames, given only its initial state in
the first frame. This task has traditionally been framed as a template matching
problem, evolving through major phases including correlation filters,
two-stream networks, and one-stream networks with significant progress
achieved. However, these methods typically require explicit classification and
regression modeling, depend on supervised training with large-scale datasets,
and are limited to the single task of tracking, lacking flexibility. In recent
years, multi-modal large language models (MLLMs) have advanced rapidly.
Open-source models like Qwen2.5-VL, a flagship MLLMs with strong foundational
capabilities, demonstrate excellent performance in grounding tasks. This has
spurred interest in applying such models directly to visual tracking. However,
experiments reveal that Qwen2.5-VL struggles with template matching between
image pairs (i.e., tracking tasks). Inspired by deepseek-R1, we fine-tuned
Qwen2.5-VL using the group relative policy optimization (GRPO) reinforcement
learning method on a small-scale dataset with a rule-based reward function. The
resulting model, R1-Track, achieved notable performance on the GOT-10k
benchmark. R1-Track supports flexible initialization via bounding boxes or text
descriptions while retaining most of the original model's general capabilities.
And we further discuss potential improvements for R1-Track. This rough
technical report summarizes our findings as of May 2025.

</details>


### [116] [RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation](https://arxiv.org/abs/2506.22007)
*Liudi Yang,Yang Bai,George Eskandar,Fengyi Shen,Mohammad Altillawi,Dong Chen,Soumajit Majumder,Ziyuan Liu,Gitta Kutyniok,Abhinav Valada*

Main category: cs.CV

TL;DR: A novel pipeline for generating long-horizon videos for robotic tasks without autoregressive generation, improving video quality and consistency.


<details>
  <summary>Details</summary>
Motivation: Text-to-video diffusion models struggle with long-horizon robotic tasks due to error accumulation in autoregressive generation.

Method: Decomposes goals into atomic tasks, generates keyframes, interpolates between them, and uses a semantics-preserving attention module and lightweight policy model.

Result: Achieves state-of-the-art results in video quality and consistency, outperforming previous policy models on long-horizon tasks.

Conclusion: The proposed pipeline effectively bypasses autoregressive limitations, enhancing robotic manipulation task simulations.

Abstract: We address the problem of generating long-horizon videos for robotic
manipulation tasks. Text-to-video diffusion models have made significant
progress in photorealism, language understanding, and motion generation but
struggle with long-horizon robotic tasks. Recent works use video diffusion
models for high-quality simulation data and predictive rollouts in robot
planning. However, these works predict short sequences of the robot achieving
one task and employ an autoregressive paradigm to extend to the long horizon,
leading to error accumulations in the generated video and in the execution. To
overcome these limitations, we propose a novel pipeline that bypasses the need
for autoregressive generation. We achieve this through a threefold
contribution: 1) we first decompose the high-level goals into smaller atomic
tasks and generate keyframes aligned with these instructions. A second
diffusion model then interpolates between each of the two generated frames,
achieving the long-horizon video. 2) We propose a semantics preserving
attention module to maintain consistency between the keyframes. 3) We design a
lightweight policy model to regress the robot joint states from generated
videos. Our approach achieves state-of-the-art results on two benchmarks in
video quality and consistency while outperforming previous policy models on
long-horizon tasks.

</details>


### [117] [Towards Universal & Efficient Model Compression via Exponential Torque Pruning](https://arxiv.org/abs/2506.22015)
*Sarthak Ketanbhai Modi,Lim Zi Pong,Shourya Kuchhal,Yoshi Cao,Yupeng Cheng,Teo Yon Shin,Lin Shang-Wei,Zhiming Li*

Main category: cs.CV

TL;DR: ETP introduces an exponential force scheme for pruning DNNs, outperforming prior methods in compression with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in existing pruning techniques, which leave networks dense and cause accuracy drops.

Method: Proposes Exponential Torque Pruning (ETP), using an exponential force application for regularization.

Result: ETP achieves higher compression rates than state-of-the-art methods with negligible accuracy drop.

Conclusion: ETP is a simple yet effective pruning strategy for modern DNNs.

Abstract: The rapid growth in complexity and size of modern deep neural networks (DNNs)
has increased challenges related to computational costs and memory usage,
spurring a growing interest in efficient model compression techniques. Previous
state-of-the-art approach proposes using a Torque-inspired regularization which
forces the weights of neural modules around a selected pivot point. Whereas, we
observe that the pruning effect of this approach is far from perfect, as the
post-trained network is still dense and also suffers from high accuracy drop.
In this work, we attribute such ineffectiveness to the default linear force
application scheme, which imposes inappropriate force on neural module of
different distances. To efficiently prune the redundant and distant modules
while retaining those that are close and necessary for effective inference, in
this work, we propose Exponential Torque Pruning (ETP), which adopts an
exponential force application scheme for regularization. Experimental results
on a broad range of domains demonstrate that, though being extremely simple,
ETP manages to achieve significantly higher compression rate than the previous
state-of-the-art pruning strategies with negligible accuracy drop.

</details>


### [118] [Advancing Facial Stylization through Semantic Preservation Constraint and Pseudo-Paired Supervision](https://arxiv.org/abs/2506.22022)
*Zhanyi Lu,Yue Zhou*

Main category: cs.CV

TL;DR: A facial stylization method integrating semantic preservation and pseudo-paired supervision improves content fidelity and stylization quality, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing StyleGAN-based methods suffer from artifacts and insufficient fidelity due to neglected semantic shifts during stylization.

Method: Proposes semantic preservation constraints and pseudo-paired supervision, with multi-level pseudo-paired datasets for supervision.

Result: Produces high-fidelity, aesthetically pleasing stylized portraits, surpassing prior methods.

Conclusion: The approach enhances content correspondence and stylization flexibility without complex designs or extra training.

Abstract: Facial stylization aims to transform facial images into appealing,
high-quality stylized portraits, with the critical challenge of accurately
learning the target style while maintaining content consistency with the
original image. Although previous StyleGAN-based methods have made significant
advancements, the generated results still suffer from artifacts or insufficient
fidelity to the source image. We argue that these issues stem from neglecting
semantic shift of the generator during stylization. Therefore, we propose a
facial stylization method that integrates semantic preservation constraint and
pseudo-paired supervision to enhance the content correspondence and improve the
stylization effect. Additionally, we develop a methodology for creating
multi-level pseudo-paired datasets to implement supervisory constraint.
Furthermore, building upon our facial stylization framework, we achieve more
flexible multimodal and reference-guided stylization without complex network
architecture designs or additional training. Experimental results demonstrate
that our approach produces high-fidelity, aesthetically pleasing facial style
transfer that surpasses previous methods.

</details>


### [119] [Cross-modal Ship Re-Identification via Optical and SAR Imagery: A Novel Dataset and Method](https://arxiv.org/abs/2506.22027)
*Han Wang,Shengyang Li,Jian Yang,Yuxuan Liu,Yixuan Lv,Zhuang Zhou*

Main category: cs.CV

TL;DR: The paper introduces the HOSS ReID dataset for ship tracking using hybrid optical and SAR sensors, addressing limitations of current methods, and proposes the TransOSS baseline method for cross-modal ship re-identification.


<details>
  <summary>Details</summary>
Motivation: Current ship tracking methods using geostationary or video satellites have limitations like low resolution, weather susceptibility, short filming durations, and limited coverage. The paper aims to overcome these by leveraging low-Earth orbit constellations of optical and SAR sensors.

Method: The paper presents the HOSS ReID dataset for evaluating ship tracking with hybrid sensors and proposes TransOSS, a Vision Transformer-based method for cross-modal ship re-identification. It includes refined patch embedding, additional embeddings, and contrastive learning.

Result: The HOSS ReID dataset enables shorter re-imaging cycles and all-weather tracking. TransOSS demonstrates effectiveness in extracting modality-invariant features for ship re-identification.

Conclusion: The HOSS ReID dataset and TransOSS method provide a robust solution for ship tracking, addressing key challenges in remote sensing. The resources are publicly available for further research.

Abstract: Detecting and tracking ground objects using earth observation imagery remains
a significant challenge in the field of remote sensing. Continuous maritime
ship tracking is crucial for applications such as maritime search and rescue,
law enforcement, and shipping analysis. However, most current ship tracking
methods rely on geostationary satellites or video satellites. The former offer
low resolution and are susceptible to weather conditions, while the latter have
short filming durations and limited coverage areas, making them less suitable
for the real-world requirements of ship tracking. To address these limitations,
we present the Hybrid Optical and Synthetic Aperture Radar (SAR) Ship
Re-Identification Dataset (HOSS ReID dataset), designed to evaluate the
effectiveness of ship tracking using low-Earth orbit constellations of optical
and SAR sensors. This approach ensures shorter re-imaging cycles and enables
all-weather tracking. HOSS ReID dataset includes images of the same ship
captured over extended periods under diverse conditions, using different
satellites of different modalities at varying times and angles. Furthermore, we
propose a baseline method for cross-modal ship re-identification, TransOSS,
which is built on the Vision Transformer architecture. It refines the patch
embedding structure to better accommodate cross-modal tasks, incorporates
additional embeddings to introduce more reference information, and employs
contrastive learning to pre-train on large-scale optical-SAR image pairs,
ensuring the model's ability to extract modality-invariant features. Our
dataset and baseline method are publicly available on
https://github.com/Alioth2000/Hoss-ReID.

</details>


### [120] [Partial CLIP is Enough: Chimera-Seg for Zero-shot Semantic Segmentation](https://arxiv.org/abs/2506.22032)
*Jialei Chen,Xu Zheng,Danda Pani Paudel,Luc Van Gool,Hiroshi Murase,Daisuke Deguchi*

Main category: cs.CV

TL;DR: Chimera-Seg integrates a segmentation backbone with a CLIP-based semantic head to align vision-language features for zero-shot semantic segmentation, achieving improved performance.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of aligning vision-based features with textual space and bridging the semantic gap between global CLIP representations and local segmentation features.

Method: Proposes Chimera-Seg with a CLIP Semantic Head (CSH) and Selective Global Distillation (SGD) to align dense features with CLIP's semantic space.

Result: Experiments show improvements of 0.9% and 1.2% in hIoU on two benchmarks.

Conclusion: Chimera-Seg effectively combines spatial precision with vision-language alignment, enhancing zero-shot semantic segmentation performance.

Abstract: Zero-shot Semantic Segmentation (ZSS) aims to segment both seen and unseen
classes using supervision from only seen classes. Beyond adaptation-based
methods, distillation-based approaches transfer vision-language alignment of
vision-language model, e.g., CLIP, to segmentation models. However, such
knowledge transfer remains challenging due to: (1) the difficulty of aligning
vision-based features with the textual space, which requires combining spatial
precision with vision-language alignment; and (2) the semantic gap between
CLIP's global representations and the local, fine-grained features of
segmentation models. To address challenge (1), we propose Chimera-Seg, which
integrates a segmentation backbone as the body and a CLIP-based semantic head
as the head, like the Chimera in Greek mythology, combining spatial precision
with vision-language alignment. Specifically, Chimera-Seg comprises a trainable
segmentation model and a CLIP Semantic Head (CSH), which maps dense features
into the CLIP-aligned space. The CSH incorporates a frozen subnetwork and fixed
projection layers from the CLIP visual encoder, along with lightweight
trainable components. The partial module from CLIP visual encoder, paired with
the segmentation model, retains segmentation capability while easing the
mapping to CLIP's semantic space. To address challenge (2), we propose
Selective Global Distillation (SGD), which distills knowledge from dense
features exhibiting high similarity to the CLIP CLS token, while gradually
reducing the number of features used for alignment as training progresses.
Besides, we also use a Semantic Alignment Module (SAM) to further align dense
visual features with semantic embeddings extracted from the frozen CLIP text
encoder. Experiments on two benchmarks show improvements of 0.9% and 1.2% in
hIoU.

</details>


### [121] [Few-Shot Identity Adaptation for 3D Talking Heads via Global Gaussian Field](https://arxiv.org/abs/2506.22044)
*Hong Nie,Fuyuan Cao,Lu Chen,Fengxin Chen,Yuefeng Zou,Jun Yu*

Main category: cs.CV

TL;DR: FIAG is a 3D talking head synthesis framework that enables efficient identity-specific adaptation with minimal training data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current reconstruction and rendering-based methods require identity-specific training, which is computationally expensive and lacks scalability. FIAG aims to address this limitation.

Method: FIAG uses Global Gaussian Field for multi-identity representation and Universal Motion Field for common motion dynamics, enabling rapid adaptation with minimal data.

Result: FIAG outperforms state-of-the-art methods in comparative and ablation experiments, demonstrating effectiveness and generalizability.

Conclusion: FIAG provides a scalable and efficient solution for identity-specific talking head synthesis with minimal training data.

Abstract: Reconstruction and rendering-based talking head synthesis methods achieve
high-quality results with strong identity preservation but are limited by their
dependence on identity-specific models. Each new identity requires training
from scratch, incurring high computational costs and reduced scalability
compared to generative model-based approaches. To overcome this limitation, we
propose FIAG, a novel 3D speaking head synthesis framework that enables
efficient identity-specific adaptation using only a few training footage. FIAG
incorporates Global Gaussian Field, which supports the representation of
multiple identities within a shared field, and Universal Motion Field, which
captures the common motion dynamics across diverse identities. Benefiting from
the shared facial structure information encoded in the Global Gaussian Field
and the general motion priors learned in the motion field, our framework
enables rapid adaptation from canonical identity representations to specific
ones with minimal data. Extensive comparative and ablation experiments
demonstrate that our method outperforms existing state-of-the-art approaches,
validating both the effectiveness and generalizability of the proposed
framework. Code is available at: \textit{https://github.com/gme-hong/FIAG}.

</details>


### [122] [EnLVAM: Enhanced Left Ventricle Linear Measurements Utilizing Anatomical Motion Mode](https://arxiv.org/abs/2506.22063)
*Durgesh K. Singh,Ahcene Boubekki,Qing Cao,Svein Arne Aase,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: A novel framework improves LV measurement accuracy in echocardiography by enforcing straight-line constraints and using AMM images, reducing errors and simplifying user interaction.


<details>
  <summary>Details</summary>
Motivation: Manual LV landmark placement in B-mode echocardiography is time-consuming and error-prone, while existing deep learning methods often misalign landmarks, leading to inaccurate measurements.

Method: The framework trains a landmark detector on AMM images derived from B-mode videos, then transforms landmarks back to B-mode space, enforcing straight-line constraints to address misalignment.

Result: Experiments show improved accuracy over standard B-mode methods, with the framework generalizing well across network architectures.

Conclusion: The semi-automatic design, including a human-in-the-loop step, simplifies interaction while maintaining alignment flexibility and clinical relevance.

Abstract: Linear measurements of the left ventricle (LV) in the Parasternal Long Axis
(PLAX) view using B-mode echocardiography are crucial for cardiac assessment.
These involve placing 4-6 landmarks along a virtual scanline (SL) perpendicular
to the LV axis near the mitral valve tips. Manual placement is time-consuming
and error-prone, while existing deep learning methods often misalign landmarks,
causing inaccurate measurements. We propose a novel framework that enhances LV
measurement accuracy by enforcing straight-line constraints. A landmark
detector is trained on Anatomical M-Mode (AMM) images, computed in real time
from B-mode videos, then transformed back to B-mode space. This approach
addresses misalignment and reduces measurement errors. Experiments show
improved accuracy over standard B-mode methods, and the framework generalizes
well across network architectures. Our semi-automatic design includes a
human-in-the-loop step where the user only places the SL, simplifying
interaction while preserving alignment flexibility and clinical relevance.

</details>


### [123] [MirrorMe: Towards Realtime and High Fidelity Audio-Driven Halfbody Animation](https://arxiv.org/abs/2506.22065)
*Dechao Meng,Steven Xiao,Xindi Zhang,Guangyuan Wang,Peng Zhang,Qi Wang,Bang Zhang,Liefeng Bo*

Main category: cs.CV

TL;DR: MirrorMe is a real-time, controllable framework for audio-driven portrait animation, addressing latency and temporal consistency issues in diffusion-based methods with innovations like identity injection, causal audio encoding, and progressive training.


<details>
  <summary>Details</summary>
Motivation: The challenges in real-time generation of high-fidelity, temporally coherent animations from audio signals motivate the development of MirrorMe.

Method: MirrorMe uses the LTX video model with innovations: reference identity injection, causal audio encoder, and progressive training for gesture control.

Result: MirrorMe achieves state-of-the-art performance in fidelity, lip-sync accuracy, and temporal stability on the EMTD Benchmark.

Conclusion: MirrorMe effectively addresses latency and consistency issues in audio-driven portrait animation, offering high-quality real-time synthesis.

Abstract: Audio-driven portrait animation, which synthesizes realistic videos from
reference images using audio signals, faces significant challenges in real-time
generation of high-fidelity, temporally coherent animations. While recent
diffusion-based methods improve generation quality by integrating audio into
denoising processes, their reliance on frame-by-frame UNet architectures
introduces prohibitive latency and struggles with temporal consistency. This
paper introduces MirrorMe, a real-time, controllable framework built on the LTX
video model, a diffusion transformer that compresses video spatially and
temporally for efficient latent space denoising. To address LTX's trade-offs
between compression and semantic fidelity, we propose three innovations: 1. A
reference identity injection mechanism via VAE-encoded image concatenation and
self-attention, ensuring identity consistency; 2. A causal audio encoder and
adapter tailored to LTX's temporal structure, enabling precise audio-expression
synchronization; and 3. A progressive training strategy combining close-up
facial training, half-body synthesis with facial masking, and hand pose
integration for enhanced gesture control. Extensive experiments on the EMTD
Benchmark demonstrate MirrorMe's state-of-the-art performance in fidelity,
lip-sync accuracy, and temporal stability.

</details>


### [124] [Single-Scanline Relative Pose Estimation for Rolling Shutter Cameras](https://arxiv.org/abs/2506.22069)
*Petr Hruby,Marc Pollefeys*

Main category: cs.CV

TL;DR: A novel method for estimating relative pose in rolling shutter cameras using line projections and scanlines, enabling pose estimation without motion modeling.


<details>
  <summary>Details</summary>
Motivation: To simplify rolling shutter structure-from-motion (SfM) by eliminating the need for explicit camera motion models and enabling independent scanline pose computation.

Method: Uses intersections of line projections with scanlines for pose estimation, with minimal solvers for generic and specialized cases (e.g., parallel lines, known gravity).

Result: Demonstrated feasibility on the Fastec dataset for initializing rolling shutter SfM.

Conclusion: The approach is a foundational step for rolling shutter SfM, with potential for further development and public code availability.

Abstract: We propose a novel approach for estimating the relative pose between rolling
shutter cameras using the intersections of line projections with a single
scanline per image. This allows pose estimation without explicitly modeling
camera motion. Alternatively, scanlines can be selected within a single image,
enabling single-view relative pose estimation for scanlines of rolling shutter
cameras. Our approach is designed as a foundational building block for rolling
shutter structure-from-motion (SfM), where no motion model is required, and
each scanline's pose can be computed independently. % We classify minimal
solvers for this problem in both generic and specialized settings, including
cases with parallel lines and known gravity direction, assuming known
intrinsics and no lens distortion. Furthermore, we develop minimal solvers for
the parallel-lines scenario, both with and without gravity priors, by
leveraging connections between this problem and the estimation of 2D structure
from 1D cameras. % Experiments on rolling shutter images from the Fastec
dataset demonstrate the feasibility of our approach for initializing rolling
shutter SfM, highlighting its potential for further development. % The code
will be made publicly available.

</details>


### [125] [Reasoning in machine vision: learning to think fast and slow](https://arxiv.org/abs/2506.22075)
*Shaheer U. Saeed,Yipei Wang,Veeru Kasivisvanathan,Brian R. Davidson,Matthew J. Clarkson,Yipeng Hu,Daniel C. Alexander*

Main category: cs.CV

TL;DR: A novel learning paradigm enables machine reasoning in vision tasks by improving performance with increased thinking time, even with limited labeled data. Inspired by human cognition, it combines fast and slow-thinking modules, outperforming supervised learning and human experts in tasks like cancer localization.


<details>
  <summary>Details</summary>
Motivation: Current machine intelligence lacks dynamic reasoning abilities, especially in non-verbal domains like vision. The goal is to develop a system that mimics human reasoning to solve complex tasks with limited data.

Method: The approach integrates a fast-thinking module (System I) for familiar tasks and a slow-thinking module (System II) using self-play reinforcement learning to iteratively refine solutions.

Result: The system outperforms large-scale supervised learning, foundation models, and human experts in vision tasks, including cancer localization in medical images.

Conclusion: The paradigm demonstrates transformative potential for non-verbal machine reasoning, bridging the gap between human and machine intelligence in complex real-world tasks.

Abstract: Reasoning is a hallmark of human intelligence, enabling adaptive
decision-making in complex and unfamiliar scenarios. In contrast, machine
intelligence remains bound to training data, lacking the ability to dynamically
refine solutions at inference time. While some recent advances have explored
reasoning in machines, these efforts are largely limited to verbal domains such
as mathematical problem-solving, where explicit rules govern step-by-step
reasoning. Other critical real-world tasks - including visual perception,
spatial reasoning, and radiological diagnosis - require non-verbal reasoning,
which remains an open challenge. Here we present a novel learning paradigm that
enables machine reasoning in vision by allowing performance improvement with
increasing thinking time (inference-time compute), even under conditions where
labelled data is very limited. Inspired by dual-process theories of human
cognition in psychology, our approach integrates a fast-thinking System I
module for familiar tasks, with a slow-thinking System II module that
iteratively refines solutions using self-play reinforcement learning. This
paradigm mimics human reasoning by proposing, competing over, and refining
solutions in data-scarce scenarios. We demonstrate superior performance through
extended thinking time, compared not only to large-scale supervised learning
but also foundation models and even human experts, in real-world vision tasks.
These tasks include computer-vision benchmarks and cancer localisation on
medical images across five organs, showcasing transformative potential for
non-verbal machine reasoning.

</details>


### [126] [Towards Accurate Heart Rate Measurement from Ultra-Short Video Clips via Periodicity-Guided rPPG Estimation and Signal Reconstruction](https://arxiv.org/abs/2506.22078)
*Pei-Kai Huanga,Ya-Ting Chan,Kuan-Wen Chen,Yen-Chun Chou,Shih-Yu Yang,Chiou-Ting Hsu*

Main category: cs.CV

TL;DR: The paper proposes a method for accurate heart rate (HR) measurement from ultra-short 2-second video clips by addressing periodicity and spectral leakage challenges.


<details>
  <summary>Details</summary>
Motivation: Existing HR measurement methods often overlook ultra-short video clips (e.g., 2 seconds), which are crucial for real-time applications.

Method: The paper introduces a periodicity-guided rPPG estimation method and a generator to reconstruct longer signals from ultra-short clips, ensuring periodic consistency.

Result: The method outperforms previous techniques on benchmark datasets, achieving state-of-the-art performance in HR estimation from ultra-short clips.

Conclusion: The proposed approach effectively addresses key challenges in ultra-short HR measurement, offering improved accuracy and practicality.

Abstract: Many remote Heart Rate (HR) measurement methods focus on estimating remote
photoplethysmography (rPPG) signals from video clips lasting around 10 seconds
but often overlook the need for HR estimation from ultra-short video clips. In
this paper, we aim to accurately measure HR from ultra-short 2-second video
clips by specifically addressing two key challenges. First, to overcome the
limited number of heartbeat cycles in ultra-short video clips, we propose an
effective periodicity-guided rPPG estimation method that enforces consistent
periodicity between rPPG signals estimated from ultra-short clips and their
much longer ground truth signals. Next, to mitigate estimation inaccuracies due
to spectral leakage, we propose including a generator to reconstruct longer
rPPG signals from ultra-short ones while preserving their periodic consistency
to enable more accurate HR measurement. Extensive experiments on four rPPG
estimation benchmark datasets demonstrate that our proposed method not only
accurately measures HR from ultra-short video clips but also outperform
previous rPPG estimation techniques to achieve state-of-the-art performance.

</details>


### [127] [BézierGS: Dynamic Urban Scene Reconstruction with Bézier Curve Gaussian Splatting](https://arxiv.org/abs/2506.22099)
*Zipei Ma,Junzhe Jiang,Yurui Chen,Li Zhang*

Main category: cs.CV

TL;DR: BézierGS uses learnable Bézier curves for dynamic object motion in street scenes, improving reconstruction and novel view synthesis without relying on high-precision annotations.


<details>
  <summary>Details</summary>
Motivation: Existing methods depend on object pose annotations, limiting large-scale scene reconstruction. BézierGS aims to overcome this by leveraging temporal information and learnable curves.

Method: Proposes BézierGS, representing dynamic object motion with Bézier curves, using learnable modeling and additional supervision for accurate reconstruction.

Result: Outperforms state-of-the-art methods in dynamic/static scene reconstruction and novel view synthesis on Waymo and nuPlan datasets.

Conclusion: BézierGS effectively addresses annotation dependence, enabling scalable and accurate street scene reconstruction.

Abstract: The realistic reconstruction of street scenes is critical for developing
real-world simulators in autonomous driving. Most existing methods rely on
object pose annotations, using these poses to reconstruct dynamic objects and
move them during the rendering process. This dependence on high-precision
object annotations limits large-scale and extensive scene reconstruction. To
address this challenge, we propose B\'ezier curve Gaussian splatting
(B\'ezierGS), which represents the motion trajectories of dynamic objects using
learnable B\'ezier curves. This approach fully leverages the temporal
information of dynamic objects and, through learnable curve modeling,
automatically corrects pose errors. By introducing additional supervision on
dynamic object rendering and inter-curve consistency constraints, we achieve
reasonable and accurate separation and reconstruction of scene elements.
Extensive experiments on the Waymo Open Dataset and the nuPlan benchmark
demonstrate that B\'ezierGS outperforms state-of-the-art alternatives in both
dynamic and static scene components reconstruction and novel view synthesis.

</details>


### [128] [Tied Prototype Model for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2506.22101)
*Hyeongji Kim,Stine Hansen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: The paper introduces TPM, a prototype-based model for medical image few-shot segmentation, addressing limitations of ADNet by using tied prototypes, multi-class support, and adaptive thresholds.


<details>
  <summary>Details</summary>
Motivation: Existing methods like ADNet struggle with background variability, single prototypes, binary classification, and fixed thresholds. TPM aims to overcome these issues.

Method: TPM ties prototype locations for foreground and background, supports multiple prototypes and multi-class segmentation, and uses adaptive thresholds based on class priors.

Result: TPM improves segmentation accuracy by better handling non-typical background features and adapting to variability.

Conclusion: TPM offers a novel approach to prototype-based few-shot segmentation, enhancing performance and flexibility for medical images.

Abstract: Common prototype-based medical image few-shot segmentation (FSS) methods
model foreground and background classes using class-specific prototypes.
However, given the high variability of the background, a more promising
direction is to focus solely on foreground modeling, treating the background as
an anomaly -- an approach introduced by ADNet. Yet, ADNet faces three key
limitations: dependence on a single prototype per class, a focus on binary
classification, and fixed thresholds that fail to adapt to patient and organ
variability. To address these shortcomings, we propose the Tied Prototype Model
(TPM), a principled reformulation of ADNet with tied prototype locations for
foreground and background distributions. Building on its probabilistic
foundation, TPM naturally extends to multiple prototypes and multi-class
segmentation while effectively separating non-typical background features.
Notably, both extensions lead to improved segmentation accuracy. Finally, we
leverage naturally occurring class priors to define an ideal target for
adaptive thresholds, boosting segmentation performance. Taken together, TPM
provides a fresh perspective on prototype-based FSS for medical image
segmentation. The code can be found at https://github.com/hjk92g/TPM-FSS.

</details>


### [129] [Pedestrian Intention and Trajectory Prediction in Unstructured Traffic Using IDD-PeD](https://arxiv.org/abs/2506.22111)
*Ruthvik Bokkasam,Shankar Gangisetty,A. H. Abdul Hafez,C. V. Jawahar*

Main category: cs.CV

TL;DR: The paper introduces an Indian driving pedestrian dataset to improve pedestrian behavior prediction in unstructured environments, showing significant performance drops in existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate pedestrian behavior prediction is crucial for autonomous driving safety, especially in complex, unstructured environments.

Method: The authors introduce a new dataset with comprehensive annotations for pedestrian behavior, focusing on challenges like illumination changes and occlusions.

Result: State-of-the-art methods perform poorly on the dataset, with intention prediction dropping by 15% and trajectory prediction MSE increasing by 1208.

Conclusion: The dataset highlights the need for robust models and opens new research challenges in pedestrian behavior prediction.

Abstract: With the rapid advancements in autonomous driving, accurately predicting
pedestrian behavior has become essential for ensuring safety in complex and
unpredictable traffic conditions. The growing interest in this challenge
highlights the need for comprehensive datasets that capture unstructured
environments, enabling the development of more robust prediction models to
enhance pedestrian safety and vehicle navigation. In this paper, we introduce
an Indian driving pedestrian dataset designed to address the complexities of
modeling pedestrian behavior in unstructured environments, such as illumination
changes, occlusion of pedestrians, unsignalized scene types and
vehicle-pedestrian interactions. The dataset provides high-level and detailed
low-level comprehensive annotations focused on pedestrians requiring the
ego-vehicle's attention. Evaluation of the state-of-the-art intention
prediction methods on our dataset shows a significant performance drop of up to
$\mathbf{15\%}$, while trajectory prediction methods underperform with an
increase of up to $\mathbf{1208}$ MSE, defeating standard pedestrian datasets.
Additionally, we present exhaustive quantitative and qualitative analysis of
intention and trajectory baselines. We believe that our dataset will open new
challenges for the pedestrian behavior research community to build robust
models. Project Page:
https://cvit.iiit.ac.in/research/projects/cvit-projects/iddped

</details>


### [130] [Pipe Reconstruction from Point Cloud Data](https://arxiv.org/abs/2506.22118)
*Antje Alex,Jannis Stoppe*

Main category: cs.CV

TL;DR: Automated pipeline for reconstructing pipes from laser scan data, reducing manual effort and costs in digital twin creation.


<details>
  <summary>Details</summary>
Motivation: Manual pipe modeling from laser scans is time-consuming and labor-intensive, hindering efficient digital twin development.

Method: Uses Laplacian-based contraction for skeleton curve estimation, elongation, recentering with rolling sphere and 2D circle fitting, and 3D smoothing.

Result: Enables precise determination of pipe properties (radius, length, orientation) and detailed 3D models of complex networks.

Conclusion: Automated pipe reconstruction accelerates digital twin creation, improving accuracy and reducing costs.

Abstract: Accurate digital twins of industrial assets, such as ships and offshore
platforms, rely on the precise reconstruction of complex pipe networks.
However, manual modelling of pipes from laser scan data is a time-consuming and
labor-intensive process. This paper presents a pipeline for automated pipe
reconstruction from incomplete laser scan data. The approach estimates a
skeleton curve using Laplacian-based contraction, followed by curve elongation.
The skeleton axis is then recentred using a rolling sphere technique combined
with 2D circle fitting, and refined with a 3D smoothing step. This enables the
determination of pipe properties, including radius, length and orientation, and
facilitates the creation of detailed 3D models of complex pipe networks. By
automating pipe reconstruction, this approach supports the development of
digital twins, allowing for rapid and accurate modeling while reducing costs.

</details>


### [131] [Low-Rank Implicit Neural Representation via Schatten-p Quasi-Norm and Jacobian Regularization](https://arxiv.org/abs/2506.22134)
*Zhengyun Cheng,Changhao Wang,Guanwen Zhang,Yi Xu,Wei Zhou,Xiangyang Ji*

Main category: cs.CV

TL;DR: The paper proposes CP-INR, a CP-based low-rank tensor method for implicit neural representation, enhancing interpretability and sparsity with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing tensor methods like Tucker decomposition lack interpretability, while CP decomposition struggles with sparsity. The goal is to combine CP's interpretability with sparsity and smoothness for continuous data representation.

Method: CP-INR uses neural networks for implicit representation, introduces a Schatten-p quasi-norm for sparsity, and a spectral norm-based regularization for smoothness.

Result: The method outperforms state-of-the-art in tasks like image inpainting, denoising, and point cloud upsampling.

Conclusion: CP-INR offers a versatile, interpretable, and theoretically grounded approach for multi-dimensional data recovery.

Abstract: Higher-order tensors are well-suited for representing multi-dimensional data,
such as color images and videos. Low-rank tensor representation has become
essential in machine learning and computer vision, but existing methods like
Tucker decomposition offer flexibility at the expense of interpretability. In
contrast, while the CANDECOMP/PARAFAC (CP) decomposition provides a more
natural and interpretable tensor structure, obtaining sparse solutions remains
challenging. Leveraging the rich properties of CP decomposition, we propose a
CP-based low-rank tensor function parameterized by neural networks for implicit
neural representation (CP-INR). This approach enables continuous data
representation beyond structured grids, fully exploiting the non-linearity of
tensor data with theoretical guarantees on excess risk bounds. To achieve a
sparse CP decomposition, we introduce a variational form of the Schatten-p
quasi-norm and prove its relationship to multilinear rank minimization. For
smoothness, we propose a regularization term based on the spectral norm of the
Jacobian and Hutchinson's trace estimator. Our proposed smoothness
regularization is SVD-free and avoids explicit chain rule derivations. It can
serve as an alternative to Total Variation (TV) regularization in image
denoising tasks and is naturally applicable to continuous data. Extensive
experiments on multi-dimensional data recovery tasks, including image
inpainting, denoising, and point cloud upsampling, demonstrate the superiority
and versatility of our method compared to state-of-the-art approaches.

</details>


### [132] [Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs](https://arxiv.org/abs/2506.22139)
*Shaojie Zhang,Jiahui Yang,Jianqin Yin,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: Q-Frame introduces adaptive frame selection and multi-resolution scaling for Video-LLMs, improving video comprehension without exceeding computational limits.


<details>
  <summary>Details</summary>
Motivation: Existing Video-LLMs struggle with capturing query-related spatiotemporal clues due to uniform frame sampling.

Method: Q-Frame uses a training-free, plug-and-play strategy with CLIP-based text-image matching and Gumbel-Max trick for frame selection.

Result: Q-Frame outperforms existing methods on benchmarks like MLVU, LongVideoBench, and Video-MME.

Conclusion: Q-Frame effectively enhances Video-LLMs' performance in video understanding tasks.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
success in visual understanding tasks. However, challenges persist in adapting
these models for video comprehension due to the large volume of data and
temporal complexity. Existing Video-LLMs using uniform frame sampling often
struggle to capture the query-related crucial spatiotemporal clues of videos
effectively. In this paper, we introduce Q-Frame, a novel approach for adaptive
frame selection and multi-resolution scaling tailored to the video's content
and the specific query. Q-Frame employs a training-free, plug-and-play strategy
generated by a text-image matching network like CLIP, utilizing the Gumbel-Max
trick for efficient frame selection. Q-Frame allows Video-LLMs to process more
frames without exceeding computational limits, thereby preserving critical
temporal and spatial information. We demonstrate Q-Frame's effectiveness
through extensive experiments on benchmark datasets, including MLVU,
LongVideoBench, and Video-MME, illustrating its superiority over existing
methods and its applicability across various video understanding tasks.

</details>


### [133] [Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs](https://arxiv.org/abs/2506.22146)
*Amirmohammad Izadi,Mohammad Ali Banayeeanzade,Fatemeh Askari,Ali Rahimiakbar,Mohammad Mahdi Vahedi,Hosein Hasani,Mahdieh Soleymani Baghshah*

Main category: cs.CV

TL;DR: The paper addresses the binding problem in Vision-Language Models (VLMs) by augmenting visual inputs with spatial structures and using sequential textual prompts, significantly improving performance in visual reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Current VLMs struggle with the binding problem, leading to errors in tasks like counting and spatial reasoning. The lack of spatially grounded, serial attention mechanisms limits their effectiveness.

Method: The authors introduce a method of augmenting visual inputs with low-level spatial structures (e.g., horizontal lines) and pairing them with textual prompts for sequential, spatially-aware parsing.

Result: The method improves GPT-4o's visual search accuracy by 25.00%, counting accuracy by 26.83%, reduces scene description errors by 0.32 edit distance, and enhances spatial relationship task performance by 9.50%.

Conclusion: Low-level visual structuring is a powerful, underexplored strategy for improving VLMs' compositional visual reasoning, outperforming purely linguistic approaches like Chain-of-Thought prompting.

Abstract: Despite progress in Vision-Language Models (VLMs), their capacity for visual
reasoning is often limited by the \textit{binding problem}: the failure to
reliably associate perceptual features with their correct visual referents.
This limitation underlies persistent errors in tasks such as counting, visual
search, scene description, and spatial relationship understanding. A key factor
is that current VLMs process visual features largely in parallel, lacking
mechanisms for spatially grounded, serial attention. This paper introduces a
simple yet effective intervention: augmenting visual inputs with low-level
spatial structures (e.g., horizontal lines) and pairing this with a textual
prompt that encourages sequential, spatially-aware parsing. We empirically
demonstrate substantial performance improvements across core visual reasoning
tasks. Specifically, our method improves GPT-4o visual search accuracy by
25.00%, increases counting accuracy by 26.83%, reduces edit distance error in
scene description by 0.32, and enhances performance on spatial relationship
tasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the
visual modification is essential for these gains; purely textual strategies,
including Chain-of-Thought prompting, are insufficient and can even degrade
performance. Our method enhances binding only with a single-query inference,
underscoring the importance of visual input design over purely
linguistically-based approaches. These findings suggest that low-level visual
structuring is a powerful and underexplored direction for improving
compositional visual reasoning and could serve as a general strategy for
enhancing VLM performance on spatially grounded tasks.

</details>


### [134] [RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation Models](https://arxiv.org/abs/2506.22149)
*Ronald Fecso,José Morano,Ursula Schmidt-Erfurth,Hrvoje Bogunović*

Main category: cs.CV

TL;DR: RetFiner is an SSL vision-language refinement scheme that enhances existing foundation models for OCT imaging, improving downstream task performance without requiring supervised fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing foundation models for OCT lack robust semantic understanding and require costly supervised fine-tuning, limiting their adaptability to specific tasks and populations.

Method: RetFiner uses SSL with diverse training objectives leveraging textual data to refine representations of existing FMs like RETFound, UrFound, and VisionFM.

Result: RetFiner significantly improves linear probing performance on seven OCT classification tasks, with average gains of 5.8, 3.9, and 2.1 percentage points over baselines.

Conclusion: RetFiner effectively enhances FM adaptability and performance for OCT tasks, offering a practical solution without the need for supervised fine-tuning.

Abstract: The rise of imaging techniques such as optical coherence tomography (OCT) and
advances in deep learning (DL) have enabled clinicians and researchers to
streamline retinal disease staging. A popular DL approach is self-supervised
learning (SSL), where models learn from vast amounts of unlabeled data,
avoiding costly annotation. SSL has allowed the development of foundation
models (FMs), large models that can be used for a variety of downstream tasks.
However, existing FMs for OCT, trained solely on image data, lack a
comprehensive and robust semantic understanding of images, as evidenced by
their downstream performance (especially for complex tasks), and thus require
supervised fine-tuning (which may be unfeasible) to better adapt to specific
applications and populations. To address this, we propose RetFiner, an SSL
vision-language refinement scheme that improves the representations of existing
FMs and enables their efficient and direct adaptation to specific populations
for improved downstream performance. Our method uses a diverse set of training
objectives which take advantage of the rich supervisory signal found in textual
data. We tested RetFiner on the retinal FMs RETFound, UrFound, and VisionFM,
showing significant improvements in linear probing performance on seven highly
diverse OCT classification tasks, with an average increase of 5.8, 3.9, and 2.1
percentage points over their baselines, respectively. Our code and model
weights are publicly available at https://github.com/ronnief1/RetFiner.

</details>


### [135] [Attention-disentangled Uniform Orthogonal Feature Space Optimization for Few-shot Object Detection](https://arxiv.org/abs/2506.22161)
*Taijin Zhao,Heqian Qiu,Yu Dai,Lanxiao Wang,Fanman Meng,Qingbo Wu,Hongliang Li*

Main category: cs.CV

TL;DR: The paper proposes a Uniform Orthogonal Feature Space (UOFS) framework for few-shot object detection (FSOD), addressing limitations of entangled feature spaces by decoupling objectness and classification. It introduces Hybrid Background Optimization (HBO) and a Spatial-wise Attention Disentanglement and Association (SADA) module to improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing FSOD methods, based on Faster R-CNN, suffer from entangled feature spaces, leading to class-specific objectness criteria and poor generalization to novel classes.

Method: UOFS decouples feature spaces into orthogonal components (magnitude for objectness, angle for classification). HBO optimizes background and angular distributions, while SADA resolves task conflicts.

Result: The method outperforms existing approaches by improving objectness transfer and classification accuracy for novel classes.

Conclusion: The UOFS framework, enhanced by HBO and SADA, effectively addresses the limitations of entangled feature spaces in FSOD, achieving superior performance.

Abstract: Few-shot object detection (FSOD) aims to detect objects with limited samples
for novel classes, while relying on abundant data for base classes. Existing
FSOD approaches, predominantly built on the Faster R-CNN detector, entangle
objectness recognition and foreground classification within shared feature
spaces. This paradigm inherently establishes class-specific objectness criteria
and suffers from unrepresentative novel class samples. To resolve this
limitation, we propose a Uniform Orthogonal Feature Space (UOFS) optimization
framework. First, UOFS decouples the feature space into two orthogonal
components, where magnitude encodes objectness and angle encodes
classification. This decoupling enables transferring class-agnostic objectness
knowledge from base classes to novel classes. Moreover, implementing the
disentanglement requires careful attention to two challenges: (1) Base set
images contain unlabeled foreground instances, causing confusion between
potential novel class instances and backgrounds. (2) Angular optimization
depends exclusively on base class foreground instances, inducing overfitting of
angular distributions to base classes. To address these challenges, we propose
a Hybrid Background Optimization (HBO) strategy: (1) Constructing a pure
background base set by removing unlabeled instances in original images to
provide unbiased magnitude-based objectness supervision. (2) Incorporating
unlabeled foreground instances in the original base set into angular
optimization to enhance distribution uniformity. Additionally, we propose a
Spatial-wise Attention Disentanglement and Association (SADA) module to address
task conflicts between class-agnostic and class-specific tasks. Experiments
demonstrate that our method significantly outperforms existing approaches based
on entangled feature spaces.

</details>


### [136] [Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition](https://arxiv.org/abs/2506.22179)
*Wenhan Wu,Zhishuai Guo,Chen Chen,Hongfei Xue,Aidong Lu*

Main category: cs.CV

TL;DR: The paper proposes FS-VAE, a model for zero-shot skeleton-based action recognition, enhancing semantic representation learning with frequency decomposition and multilevel alignment to improve robustness and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook fine-grained action patterns in semantic space, limiting their effectiveness in distinguishing visually and semantically similar actions.

Method: FS-VAE includes a frequency-based enhancement module, semantic-based action description with multilevel alignment, and a calibrated cross-alignment loss to bridge the semantic gap and improve robustness.

Result: Evaluations show FS-VAE effectively differentiates similar action clusters, enhancing zero-shot action recognition performance.

Conclusion: FS-VAE's frequency-enhanced semantic features and robust alignment mechanisms significantly improve zero-shot skeleton-based action recognition.

Abstract: Zero-shot skeleton-based action recognition aims to develop models capable of
identifying actions beyond the categories encountered during training. Previous
approaches have primarily focused on aligning visual and semantic
representations but often overlooked the importance of fine-grained action
patterns in the semantic space (e.g., the hand movements in drinking water and
brushing teeth). To address these limitations, we propose a Frequency-Semantic
Enhanced Variational Autoencoder (FS-VAE) to explore the skeleton semantic
representation learning with frequency decomposition. FS-VAE consists of three
key components: 1) a frequency-based enhancement module with high- and
low-frequency adjustments to enrich the skeletal semantics learning and improve
the robustness of zero-shot action recognition; 2) a semantic-based action
description with multilevel alignment to capture both local details and global
correspondence, effectively bridging the semantic gap and compensating for the
inherent loss of information in skeleton sequences; 3) a calibrated
cross-alignment loss that enables valid skeleton-text pairs to counterbalance
ambiguous ones, mitigating discrepancies and ambiguities in skeleton and text
features, thereby ensuring robust alignment. Evaluations on the benchmarks
demonstrate the effectiveness of our approach, validating that
frequency-enhanced semantic features enable robust differentiation of visually
and semantically similar action clusters, improving zero-shot action
recognition.

</details>


### [137] [Robust and Accurate Multi-view 2D/3D Image Registration with Differentiable X-ray Rendering and Dual Cross-view Constraints](https://arxiv.org/abs/2506.22191)
*Yuxin Cui,Rui Song,Yibin Li,Max Q. -H. Meng,Zhe Min*

Main category: cs.CV

TL;DR: A novel two-stage multi-view 2D/3D registration method improves accuracy and robustness by leveraging cross-view constraints and test-time optimization, achieving superior performance on the DeepFluoro dataset.


<details>
  <summary>Details</summary>
Motivation: Accurate 2D/3D registration is vital for interventional navigation, but limited field of view in single-image scenarios necessitates multi-view approaches.

Method: A two-stage approach: (1) combined loss function with cross-view constraints for pose and image dissimilarities, (2) test-time optimization to refine poses.

Result: Achieves a mean target registration error (mTRE) of 0.79 ± 2.17 mm on the DeepFluoro dataset, outperforming state-of-the-art methods.

Conclusion: The proposed method enhances registration robustness and accuracy through multi-view constraints and optimization, proving effective for clinical applications.

Abstract: Robust and accurate 2D/3D registration, which aligns preoperative models with
intraoperative images of the same anatomy, is crucial for successful
interventional navigation. To mitigate the challenge of a limited field of view
in single-image intraoperative scenarios, multi-view 2D/3D registration is
required by leveraging multiple intraoperative images. In this paper, we
propose a novel multi-view 2D/3D rigid registration approach comprising two
stages. In the first stage, a combined loss function is designed, incorporating
both the differences between predicted and ground-truth poses and the
dissimilarities (e.g., normalized cross-correlation) between simulated and
observed intraoperative images. More importantly, additional cross-view
training loss terms are introduced for both pose and image losses to explicitly
enforce cross-view constraints. In the second stage, test-time optimization is
performed to refine the estimated poses from the coarse stage. Our method
exploits the mutual constraints of multi-view projection poses to enhance the
robustness of the registration process. The proposed framework achieves a mean
target registration error (mTRE) of $0.79 \pm 2.17$ mm on six specimens from
the DeepFluoro dataset, demonstrating superior performance compared to
state-of-the-art registration algorithms.

</details>


### [138] [ReF-LLE: Personalized Low-Light Enhancement via Reference-Guided Deep Reinforcement Learning](https://arxiv.org/abs/2506.22216)
*Ming Zhao,Pingping Liu,Tongshun Zhang,Zhe Zhang*

Main category: cs.CV

TL;DR: ReF-LLE is a personalized low-light image enhancement method using Fourier frequency domain and deep reinforcement learning, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges of varied low-light conditions and subjective enhancement preferences.

Method: Integrates deep reinforcement learning with Fourier domain analysis, using zero-reference scoring and personalized adaptive iteration.

Result: Superior perceptual quality and adaptability in personalized enhancement, validated on benchmarks.

Conclusion: ReF-LLE effectively handles diverse low-light conditions and user preferences, setting a new standard in the field.

Abstract: Low-light image enhancement presents two primary challenges: 1) Significant
variations in low-light images across different conditions, and 2) Enhancement
levels influenced by subjective preferences and user intent. To address these
issues, we propose ReF-LLE, a novel personalized low-light image enhancement
method that operates in the Fourier frequency domain and incorporates deep
reinforcement learning. ReF-LLE is the first to integrate deep reinforcement
learning into this domain. During training, a zero-reference image evaluation
strategy is introduced to score enhanced images, providing reward signals that
guide the model to handle varying degrees of low-light conditions effectively.
In the inference phase, ReF-LLE employs a personalized adaptive iterative
strategy, guided by the zero-frequency component in the Fourier domain, which
represents the overall illumination level. This strategy enables the model to
adaptively adjust low-light images to align with the illumination distribution
of a user-provided reference image, ensuring personalized enhancement results.
Extensive experiments on benchmark datasets demonstrate that ReF-LLE
outperforms state-of-the-art methods, achieving superior perceptual quality and
adaptability in personalized low-light image enhancement.

</details>


### [139] [Boosting Classification with Quantum-Inspired Augmentations](https://arxiv.org/abs/2506.22241)
*Matthias Tschöpe,Vitor Fortes Rey,Sogo Pierre Sanon,Paul Lukowicz,Nikolaos Palaiodimopoulos,Maximilian Kiefer-Emmanouilidis*

Main category: cs.CV

TL;DR: Quantum gate perturbations, often seen as harmful, can enhance machine learning by acting as data augmentation. This paper explores Bloch sphere rotations for quantum-inspired augmentation, improving image classification on ImageNet.


<details>
  <summary>Details</summary>
Motivation: To investigate how small quantum gate perturbations, typically detrimental, can benefit machine learning by serving as data augmentation and whether they can be simulated classically.

Method: Random Bloch sphere rotations (SU(2) transformations) are applied as quantum-inspired data augmentation to classical datasets like ImageNet. Performance is compared to classical augmentation methods.

Result: Quantum-inspired augmentation improves Top-1 accuracy by 3%, Top-5 accuracy by 2.5%, and F$_1$ score from 8% to 12% on ImageNet. Stronger unitary augmentations, while preserving information, do not enhance privacy.

Conclusion: Quantum-inspired Bloch rotations enhance classical machine learning performance but do not improve differential privacy, highlighting their potential and limitations.

Abstract: Understanding the impact of small quantum gate perturbations, which are
common in quantum digital devices but absent in classical computers, is crucial
for identifying potential advantages in quantum machine learning. While these
perturbations are typically seen as detrimental to quantum computation, they
can actually enhance performance by serving as a natural source of data
augmentation. Additionally, they can often be efficiently simulated on
classical hardware, enabling quantum-inspired approaches to improve classical
machine learning methods. In this paper, we investigate random Bloch sphere
rotations, which are fundamental SU(2) transformations, as a simple yet
effective quantum-inspired data augmentation technique. Unlike conventional
augmentations such as flipping, rotating, or cropping, quantum transformations
lack intuitive spatial interpretations, making their application to tasks like
image classification less straightforward. While common quantum augmentation
methods rely on applying quantum models or trainable quanvolutional layers to
classical datasets, we focus on the direct application of small-angle Bloch
rotations and their effect on classical data. Using the large-scale ImageNet
dataset, we demonstrate that our quantum-inspired augmentation method improves
image classification performance, increasing Top-1 accuracy by 3%, Top-5
accuracy by 2.5%, and the F$_1$ score from 8% to 12% compared to standard
classical augmentation methods. Finally, we examine the use of stronger unitary
augmentations. Although these transformations preserve information in
principle, they result in visually unrecognizable images with potential
applications for privacy computations. However, we show that our augmentation
approach and simple SU(2) transformations do not enhance differential privacy
and discuss the implications of this limitation.

</details>


### [140] [4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration](https://arxiv.org/abs/2506.22242)
*Jiahui Zhang,Yurui Chen,Yueming Xu,Ze Huang,Yanpeng Zhou,Yu-Jie Yuan,Xinyue Cai,Guowei Huang,Xingyue Quan,Hang Xu,Li Zhang*

Main category: cs.CV

TL;DR: The paper introduces 4D-VLA, a method integrating 4D (depth and temporal) information into robotic pretraining to address coordinate system and state chaos, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing pretraining methods for robotics suffer from incomplete inputs, leading to dispersed action distributions (coordinate system and state chaos), which reduce efficiency.

Method: Proposes 4D-VLA, incorporating depth and temporal data into visual features with RGB-D inputs, aligning robot and scene coordinates. Introduces memory bank sampling for efficient frame selection.

Result: 4D-VLA significantly improves success rates over OpenVLA in simulations and real-world tests. It also excels in spatial perception and generalization on the MV-Bench benchmark.

Conclusion: 4D-VLA effectively mitigates chaos in robotic pretraining, enhancing spatiotemporal reasoning and performance, as validated by experiments.

Abstract: Leveraging diverse robotic data for pretraining remains a critical challenge.
Existing methods typically model the dataset's action distribution using simple
observations as inputs. However, these inputs are often incomplete, resulting
in a dispersed conditional action distribution-an issue we refer to as
coordinate system chaos and state chaos. This inconsistency significantly
hampers pretraining efficiency. To address this, we propose 4D-VLA, a novel
approach that effectively integrates 4D information into the input to mitigate
these sources of chaos. Our model introduces depth and temporal information
into visual features with sequential RGB-D inputs, aligning the coordinate
systems of the robot and the scene. This alignment endows the model with strong
spatiotemporal reasoning capabilities while minimizing training overhead.
Additionally, we introduce memory bank sampling, a frame sampling strategy
designed to extract informative frames from historical images, further
improving effectiveness and efficiency. Experimental results demonstrate that
our pretraining method and architectural components substantially enhance model
performance. In both simulated and real-world experiments, our model achieves a
significant increase in success rate over OpenVLA. To further assess spatial
perception and generalization to novel views, we introduce MV-Bench, a
multi-view simulation benchmark. Our model consistently outperforms existing
methods, demonstrating stronger spatial understanding and adaptability.

</details>


### [141] [EAMamba: Efficient All-Around Vision State Space Model for Image Restoration](https://arxiv.org/abs/2506.22246)
*Yu-Cheng Lin,Yu-Syuan Xu,Hao-Wei Chen,Hsien-Kai Kuo,Chun-Yi Lee*

Main category: cs.CV

TL;DR: EAMamba improves Vision Mamba for image restoration by introducing MHSSM and an all-around scanning mechanism, reducing FLOPs by 31-89% while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Addressing Vision Mamba's challenges in low-level vision tasks, such as computational complexity and local pixel forgetting.

Method: Introduces EAMamba with MHSSM for efficient multi-sequence scanning and an all-around strategy to capture holistic information.

Result: Achieves 31-89% FLOPs reduction with competitive performance in tasks like super resolution, denoising, deblurring, and dehazing.

Conclusion: EAMamba effectively overcomes Vision Mamba's limitations, offering a more efficient solution for image restoration.

Abstract: Image restoration is a key task in low-level computer vision that aims to
reconstruct high-quality images from degraded inputs. The emergence of Vision
Mamba, which draws inspiration from the advanced state space model Mamba, marks
a significant advancement in this field. Vision Mamba demonstrates excellence
in modeling long-range dependencies with linear complexity, a crucial advantage
for image restoration tasks. Despite its strengths, Vision Mamba encounters
challenges in low-level vision tasks, including computational complexity that
scales with the number of scanning sequences and local pixel forgetting. To
address these limitations, this study introduces Efficient All-Around Mamba
(EAMamba), an enhanced framework that incorporates a Multi-Head Selective Scan
Module (MHSSM) with an all-around scanning mechanism. MHSSM efficiently
aggregates multiple scanning sequences, which avoids increases in computational
complexity and parameter count. The all-around scanning strategy implements
multiple patterns to capture holistic information and resolves the local pixel
forgetting issue. Our experimental evaluations validate these innovations
across several restoration tasks, including super resolution, denoising,
deblurring, and dehazing. The results validate that EAMamba achieves a
significant 31-89% reduction in FLOPs while maintaining favorable performance
compared to existing low-level Vision Mamba methods.

</details>


### [142] [COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication](https://arxiv.org/abs/2506.22274)
*Filippo Merlo,Ece Takmaz,Wenkai Chen,Albert Gatt*

Main category: cs.CV

TL;DR: The paper investigates how Vision-Language Models (VLMs) use scene context for object reference, introducing the COOCO dataset to test scene-object congruency and noise effects. Findings show adaptive context reliance, balancing local and contextual information.


<details>
  <summary>Details</summary>
Motivation: To understand if VLMs rely on scene context for object recognition similarly to humans, and how this reliance varies with scene-object congruency and noise.

Method: Introduces the COOCO dataset to test VLMs under varying scene-object congruency and noise levels, analyzing attention patterns.

Result: VLMs adaptively use scene context, relying more under high congruency or object degradation. Mid-level layers focus on targets under moderate noise.

Conclusion: VLMs dynamically balance local and contextual information for object reference, with context reliance influenced by congruency and noise.

Abstract: Natural scenes provide us with rich contexts for object recognition and
reference. In particular, knowing what type of scene one is looking at
generates expectations about which objects will occur, and what their spatial
configuration should be. Do Vision-Language Models (VLMs) learn to rely on
scene contexts in a similar way, when generating references to objects? To
address this question, we introduce the \textit{Common Objects Out-of-Context
(COOCO)} dataset and test to what extent VLMs rely on scene context to refer to
objects under different degrees of scene-object congruency, and different
perturbations. Our findings show that models leverage scene context adaptively,
depending on both the semantic relatedness between object and scene and the
level of noise. In particular, models rely more on context under high
target-scene congruence or when objects are degraded. Attention analysis
reveals that successful object categorisation involves increased focus on the
target in mid-level layers, especially under moderate noise, suggesting that
VLMs dynamically balance local and contextual information for reference
generation. We make our dataset, code and models available at
\href{https://github.com/cs-nlp-uu/scenereg}{https://github.com/cs-nlp-uu/scenereg}.

</details>


### [143] [Rethinking Visual Token Reduction in LVLMs under Cross-modal Misalignment](https://arxiv.org/abs/2506.22283)
*Rui Xu,Yunke Wang,Yong Luo,Bo Du*

Main category: cs.CV

TL;DR: VisionDrop is a training-free, visual-only pruning framework for LVLMs that selects informative visual tokens using intra-modal attention, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing text-guided visual token reduction methods in LVLMs suffer from cross-modal misalignment, undermining their effectiveness.

Method: VisionDrop uses intra-modal attention for token selection and a progressive pruning pipeline to suppress redundancy across the visual encoder and LLM.

Result: VisionDrop outperforms existing methods across benchmarks, maintaining performance under aggressive token budgets.

Conclusion: VisionDrop offers a simple, effective solution for efficient LVLM inference without additional training or complex modifications.

Abstract: Large Vision-Language Models (LVLMs) encode visual inputs as dense sequences
of patch-level tokens to capture fine-grained semantics. These visual tokens
often outnumber their textual counterparts by a large margin, leading to
substantial computational overhead and limiting the scalability of LVLMs in
practice. Previous efforts have explored visual token reduction either prior to
or within the large language models (LLM). However, most in-LLM reduction
approaches rely on text-conditioned interactions, implicitly assuming that
textual tokens can reliably capture the importance of visual tokens. In this
work, we revisit this assumption and reveal causal, semantic, and spatial forms
of cross-modal misalignment. These misalignments undermine the effectiveness of
text-guided visual token reduction. To address this, we introduce VisionDrop, a
training-free, visual-only pruning framework that selects informative visual
tokens based on intra-modal (visual-to-visual) attention, without relying on
textual signals. To further suppress redundancy throughout the model hierarchy,
we treat the visual encoder and the LLM as a unified system and design a
progressive pruning pipeline. Our method performs dominant token selection and
lightweight contextual merging at multiple stages, enabling fine-grained visual
information to be retained even under aggressive token budgets. Extensive
experiments across diverse benchmarks show that VisionDrop achieves consistent
improvements over existing methods, despite requiring no additional training or
complex modifications. Its simple yet effective design enables efficient
inference while preserving strong performance across tasks.

</details>


### [144] [RoomCraft: Controllable and Complete 3D Indoor Scene Generation](https://arxiv.org/abs/2506.22291)
*Mengqi Zhou,Xipeng Wang,Yuxi Wang,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: RoomCraft is a multi-stage pipeline for generating realistic 3D indoor scenes from user inputs, combining scene generation with constraint-driven optimization to address challenges like geometric consistency and multi-constraint scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D scene generation face issues like repetitive elements, object collisions, and layout incompleteness, especially in multi-constraint scenarios.

Method: RoomCraft uses a structured pipeline to extract scene info, constructs a spatial relationship network, and employs a heuristic-based depth-first search (HDFS) algorithm for optimized placement. It introduces a unified constraint representation and a Conflict-Aware Positioning Strategy (CAPS) to handle complex constraints.

Result: RoomCraft outperforms existing methods in generating realistic, semantically coherent, and visually appealing room layouts across diverse inputs.

Conclusion: RoomCraft effectively addresses limitations in 3D scene generation, offering a robust solution for creating coherent and visually realistic indoor scenes.

Abstract: Generating realistic 3D indoor scenes from user inputs remains a challenging
problem in computer vision and graphics, requiring careful balance of geometric
consistency, spatial relationships, and visual realism. While neural generation
methods often produce repetitive elements due to limited global spatial
reasoning, procedural approaches can leverage constraints for controllable
generation but struggle with multi-constraint scenarios. When constraints
become numerous, object collisions frequently occur, forcing the removal of
furniture items and compromising layout completeness.
  To address these limitations, we propose RoomCraft, a multi-stage pipeline
that converts real images, sketches, or text descriptions into coherent 3D
indoor scenes. Our approach combines a scene generation pipeline with a
constraint-driven optimization framework. The pipeline first extracts
high-level scene information from user inputs and organizes it into a
structured format containing room type, furniture items, and spatial relations.
It then constructs a spatial relationship network to represent furniture
arrangements and generates an optimized placement sequence using a
heuristic-based depth-first search (HDFS) algorithm to ensure layout coherence.
To handle complex multi-constraint scenarios, we introduce a unified constraint
representation that processes both formal specifications and natural language
inputs, enabling flexible constraint-oriented adjustments through a
comprehensive action space design. Additionally, we propose a Conflict-Aware
Positioning Strategy (CAPS) that dynamically adjusts placement weights to
minimize furniture collisions and ensure layout completeness.
  Extensive experiments demonstrate that RoomCraft significantly outperforms
existing methods in generating realistic, semantically coherent, and visually
appealing room layouts across diverse input modalities.

</details>


### [145] [OutDreamer: Video Outpainting with a Diffusion Transformer](https://arxiv.org/abs/2506.22298)
*Linhao Zhong,Fan Li,Yi Huang,Jianzhuang Liu,Renjing Pei,Fenglong Song*

Main category: cs.CV

TL;DR: OutDreamer, a DiT-based video outpainting framework, outperforms state-of-the-art methods by using efficient video control and conditional outpainting branches, along with a mask-driven self-attention layer and latent alignment loss.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with quality and adaptability in video outpainting, despite using latent diffusion models. Diffusion transformers (DiTs) offer superior performance, motivating the development of OutDreamer.

Method: OutDreamer combines an efficient video control branch for extracting masked video information and a conditional outpainting branch for generating missing content. It includes a mask-driven self-attention layer and latent alignment loss for consistency. A cross-video-clip refiner ensures temporal consistency for long videos.

Result: OutDreamer achieves superior performance in zero-shot video outpainting compared to state-of-the-art methods on benchmarks.

Conclusion: OutDreamer demonstrates the effectiveness of DiTs in video outpainting, offering high-quality, adaptable, and temporally consistent results.

Abstract: Video outpainting is a challenging task that generates new video content by
extending beyond the boundaries of an original input video, requiring both
temporal and spatial consistency. Many state-of-the-art methods utilize latent
diffusion models with U-Net backbones but still struggle to achieve high
quality and adaptability in generated content. Diffusion transformers (DiTs)
have emerged as a promising alternative because of their superior performance.
We introduce OutDreamer, a DiT-based video outpainting framework comprising two
main components: an efficient video control branch and a conditional
outpainting branch. The efficient video control branch effectively extracts
masked video information, while the conditional outpainting branch generates
missing content based on these extracted conditions. Additionally, we propose a
mask-driven self-attention layer that dynamically integrates the given mask
information, further enhancing the model's adaptability to outpainting tasks.
Furthermore, we introduce a latent alignment loss to maintain overall
consistency both within and between frames. For long video outpainting, we
employ a cross-video-clip refiner to iteratively generate missing content,
ensuring temporal consistency across video clips. Extensive evaluations
demonstrate that our zero-shot OutDreamer outperforms state-of-the-art
zero-shot methods on widely recognized benchmarks.

</details>


### [146] [MatChA: Cross-Algorithm Matching with Feature Augmentation](https://arxiv.org/abs/2506.22336)
*Paula Carbó Cubero,Alberto Jaenal Gálvez,André Mateus,José Araújo,Patric Jensfelt*

Main category: cs.CV

TL;DR: A method for improving visual localization by augmenting and translating feature descriptors for cross-detector matching, addressing keypoint repeatability and descriptor issues.


<details>
  <summary>Details</summary>
Motivation: Current methods fail in cross-feature detector scenarios due to low keypoint repeatability and non-discriminatory descriptors, limiting practical use.

Method: Feature descriptor augmentation for cross-detector matching, followed by translation to a latent space.

Result: Significant improvement in image matching and visual localization in cross-feature scenarios, validated on benchmarks.

Conclusion: The proposed method effectively addresses cross-detector challenges, enhancing performance in practical visual localization tasks.

Abstract: State-of-the-art methods fail to solve visual localization in scenarios where
different devices use different sparse feature extraction algorithms to obtain
keypoints and their corresponding descriptors. Translating feature descriptors
is enough to enable matching. However, performance is drastically reduced in
cross-feature detector cases, because current solutions assume common
keypoints. This means that the same detector has to be used, which is rarely
the case in practice when different descriptors are used. The low repeatability
of keypoints, in addition to non-discriminatory and non-distinctive
descriptors, make the identification of true correspondences extremely
challenging. We present the first method tackling this problem, which performs
feature descriptor augmentation targeting cross-detector feature matching, and
then feature translation to a latent space. We show that our method
significantly improves image matching and visual localization in the
cross-feature scenario and evaluate the proposed method on several benchmarks.

</details>


### [147] [A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake](https://arxiv.org/abs/2506.22338)
*Luigi Russo,Deodato Tapete,Silvia Liberata Ullo,Paolo Gamba*

Main category: cs.CV

TL;DR: A novel multimodal deep learning framework uses single-date SAR imagery and geospatial data for rapid building damage detection post-disaster, eliminating the need for pre-event imagery.


<details>
  <summary>Details</summary>
Motivation: Quick and accurate building damage identification after disasters is vital for emergency response, but optical imagery is often limited by cloud cover or lack of pre-event data.

Method: The framework integrates SAR imagery, OSM footprints, DSM data, and GEM attributes for contextual damage detection, using only post-event data.

Result: The method improves accuracy and generalizability, demonstrated on a 2023 Turkey earthquake dataset.

Conclusion: The approach enables reliable, rapid damage assessment without pre-event data, supporting scalable disaster management.

Abstract: Building damage identification shortly after a disaster is crucial for
guiding emergency response and recovery efforts. Although optical satellite
imagery is commonly used for disaster mapping, its effectiveness is often
hampered by cloud cover or the absence of pre-event acquisitions. To overcome
these challenges, we introduce a novel multimodal deep learning (DL) framework
for detecting building damage using single-date very high resolution (VHR)
Synthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI)
COSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data.
Our method integrates SAR image patches, OpenStreetMap (OSM) building
footprints, digital surface model (DSM) data, and structural and exposure
attributes from the Global Earthquake Model (GEM) to improve detection accuracy
and contextual interpretation. Unlike existing approaches that depend on pre
and post event imagery, our model utilizes only post event data, facilitating
rapid deployment in critical scenarios. The framework effectiveness is
demonstrated using a new dataset from the 2023 earthquake in Turkey, covering
multiple cities with diverse urban settings. Results highlight that
incorporating geospatial features significantly enhances detection performance
and generalizability to previously unseen areas. By combining SAR imagery with
detailed vulnerability and exposure information, our approach provides reliable
and rapid building damage assessments without the dependency from available
pre-event data. Moreover, the automated and scalable data generation process
ensures the framework's applicability across diverse disaster-affected regions,
underscoring its potential to support effective disaster management and
recovery efforts. Code and data will be made available upon acceptance of the
paper.

</details>


### [148] [Closing the Performance Gap in Biometric Cryptosystems: A Deeper Analysis on Unlinkable Fuzzy Vaults](https://arxiv.org/abs/2506.22347)
*Hans Geißner,Christian Rathgeb*

Main category: cs.CV

TL;DR: The paper proposes a feature quantization method to address performance gaps in fuzzy vault-based BCS, reducing degradation caused by variable feature sizes and feature type transformations.


<details>
  <summary>Details</summary>
Motivation: The performance gap in fuzzy vault-based BCS is caused by unstable error correction and feature size variability, compounded by information loss during feature transformations.

Method: A novel feature quantization method using equal frequent intervals is introduced to ensure fixed feature set sizes and training-free adaptation.

Result: The method significantly reduces performance degradation, with minimal impact on face, fingerprint, and iris recognition systems.

Conclusion: The proposed quantization method effectively addresses performance gaps and integrates well with existing systems across biometric modalities.

Abstract: This paper analyses and addresses the performance gap in the fuzzy
vault-based \ac{BCS}. We identify unstable error correction capabilities, which
are caused by variable feature set sizes and their influence on similarity
thresholds, as a key source of performance degradation. This issue is further
compounded by information loss introduced through feature type transformations.
To address both problems, we propose a novel feature quantization method based
on \it{equal frequent intervals}. This method guarantees fixed feature set
sizes and supports training-free adaptation to any number of intervals. The
proposed approach significantly reduces the performance gap introduced by
template protection. Additionally, it integrates seamlessly with existing
systems to minimize the negative effects of feature transformation. Experiments
on state-of-the-art face, fingerprint, and iris recognition systems confirm
that only minimal performance degradation remains, demonstrating the
effectiveness of the method across major biometric modalities.

</details>


### [149] [From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications](https://arxiv.org/abs/2506.22360)
*Nouf Almesafri,Hector Figueiredo,Miguel Arana-Catania*

Main category: cs.CV

TL;DR: Comparison of ResNet34 and ViT B16 for event-based cameras shows ResNet34's slight accuracy edge (88% vs 86%), but ViT B16's superior robustness, especially in noisy conditions.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the performance of CNN (ResNet34) and Vision Transformer (ViT B16) architectures for event-based cameras, which are crucial for dynamic environments like UAVs and autonomous vehicles.

Method: Fine-tuned ResNet34 and ViT B16 on the GEN1 event-based dataset, testing under standard and noisy conditions.

Result: ResNet34 achieved 88% accuracy, slightly outperforming ViT B16 (86%), but ViT B16 showed better robustness, especially with limited pre-training data.

Conclusion: While ResNet34 is marginally more accurate, ViT B16's robustness makes it promising for dynamic environments, with potential applications in UAV and aviation tasks.

Abstract: This study investigates the performance of the two most relevant computer
vision deep learning architectures, Convolutional Neural Network and Vision
Transformer, for event-based cameras. These cameras capture scene changes,
unlike traditional frame-based cameras with capture static images, and are
particularly suited for dynamic environments such as UAVs and autonomous
vehicles. The deep learning models studied in this work are ResNet34 and ViT
B16, fine-tuned on the GEN1 event-based dataset. The research evaluates and
compares these models under both standard conditions and in the presence of
simulated noise. Initial evaluations on the clean GEN1 dataset reveal that
ResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with
ResNet34 showing a slight advantage in classification accuracy. However, the
ViT B16 model demonstrates notable robustness, particularly given its
pre-training on a smaller dataset. Although this study focuses on ground-based
vehicle classification, the methodologies and findings hold significant promise
for adaptation to UAV contexts, including aerial object classification and
event-based vision systems for aviation-related tasks.

</details>


### [150] [Exploiting Vision Language Model for Training-Free 3D Point Cloud OOD Detection via Graph Score Propagation](https://arxiv.org/abs/2506.22375)
*Tiankai Chen,Yushu Li,Adam Goodge,Fei Teng,Xulei Yang,Tianrui Li,Xun Xu*

Main category: cs.CV

TL;DR: A training-free framework using Vision-Language Models (VLMs) for OOD detection in 3D point clouds, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of OOD detection in 3D point clouds, where existing 2D methods fall short, to ensure safe and robust perception.

Method: Proposes Graph Score Propagation (GSP) with prompt clustering and self-training negative prompting, leveraging VLMs and graph-based class prototypes.

Result: GSP consistently outperforms state-of-the-art methods on synthetic and real-world datasets.

Conclusion: The framework is effective, adaptable to few-shot scenarios, and enhances VLM performance for 3D OOD detection.

Abstract: Out-of-distribution (OOD) detection in 3D point cloud data remains a
challenge, particularly in applications where safe and robust perception is
critical. While existing OOD detection methods have shown progress for 2D image
data, extending these to 3D environments involves unique obstacles. This paper
introduces a training-free framework that leverages Vision-Language Models
(VLMs) for effective OOD detection in 3D point clouds. By constructing a graph
based on class prototypes and testing data, we exploit the data manifold
structure to enhancing the effectiveness of VLMs for 3D OOD detection. We
propose a novel Graph Score Propagation (GSP) method that incorporates prompt
clustering and self-training negative prompting to improve OOD scoring with
VLM. Our method is also adaptable to few-shot scenarios, providing options for
practical applications. We demonstrate that GSP consistently outperforms
state-of-the-art methods across synthetic and real-world datasets 3D point
cloud OOD detection.

</details>


### [151] [Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment](https://arxiv.org/abs/2506.22385)
*Yue Zhang,Jilei Sun,Yunhui Guo,Vibhav Gogate*

Main category: cs.CV

TL;DR: The paper introduces Defeasible Video Entailment (DVidE), a task for improving abstract and adaptive reasoning in Video Large Multimodal Models (VLMMs). It proposes frameworks for classification and generation tasks, along with a new benchmark dataset and evaluation metric.


<details>
  <summary>Details</summary>
Motivation: VLMMs struggle with abstract and adaptive reasoning, especially when new information emerges. The goal is to enhance their ability to update interpretations dynamically.

Method: For classification, the Chain of Counterfactual Thought framework uses counterfactual reasoning, ASR-enhanced video content, and rationale refinement. For generation, ASR output is combined with an LLM to produce coherent updates.

Result: Experimental results show significant improvements in dynamic reasoning capabilities of VLMMs.

Conclusion: The proposed DVidE task and frameworks effectively address the limitations of VLMMs in adaptive reasoning, demonstrating practical advancements in video understanding.

Abstract: Video Large Multimodal Models (VLMMs) have made impressive strides in
understanding video content, but they often struggle with abstract and adaptive
reasoning-the ability to revise their interpretations when new information
emerges. In reality, conclusions are rarely set in stone; additional context
can strengthen or weaken an initial inference. To address this, we introduce
Defeasible Video Entailment (DVidE), a new task that challenges models to think
like doubters, constantly updating their reasoning based on evolving evidence.
In DVidE, given a video premise and a textual hypothesis, models must determine
whether a new update strengthens or weakens the hypothesis (classification
version) or generate a coherent update that modifies the entailment
relationship (generation version). For solving the classification task, we
propose the Chain of Counterfactual Thought framework, utilizing counterfactual
reasoning, ASR-enhanced video content, and rationale refinement to reduce
inference bias. For the generation task, we develop a framework that combines
ASR output with a Large Language Model (LLM) to produce coherent, contextually
relevant updates aligned with the intended strengthener or weakener goals.
Additionally, we introduce a novel benchmark dataset, with
strengthener/weakener annotations and an LLM-based evaluation metric
specifically designed for assessing generative performance. Experimental
results demonstrate significant improvements, highlighting our proposed method
in enhancing dynamic reasoning capabilities of VLMMs.

</details>


### [152] [Test-Time Consistency in Vision Language Models](https://arxiv.org/abs/2506.22395)
*Shih-Han Chou,Shivam Chandhok,James J. Little,Leonid Sigal*

Main category: cs.CV

TL;DR: A test-time consistency framework improves semantic consistency in Vision-Language Models (VLMs) without retraining, using cross-entropy agreement and pseudo-label consistency losses.


<details>
  <summary>Details</summary>
Motivation: VLMs often show inconsistent behavior with semantically equivalent inputs, reducing reliability despite high average accuracy.

Method: Proposes a post-hoc, model-agnostic framework with Cross-Entropy Agreement Loss and Pseudo-Label Consistency Loss for test-time consistency.

Result: Substantial gains in consistency on the MM-R3 benchmark without supervised retraining.

Conclusion: The framework offers a plug-and-play solution for improving VLM consistency, advancing inference-time adaptation in multimodal learning.

Abstract: Vision-Language Models (VLMs) have achieved impressive performance across a
wide range of multimodal tasks, yet they often exhibit inconsistent behavior
when faced with semantically equivalent inputs, undermining their reliability
and robustness. Recent benchmarks, such as MM-R3, highlight that even
state-of-the-art VLMs can produce divergent predictions across semantically
equivalent inputs, despite maintaining high average accuracy. Prior work
addresses this issue by modifying model architectures or conducting large-scale
fine-tuning on curated datasets. In contrast, we propose a simple and effective
test-time consistency framework that enhances semantic consistency without
supervised re-training. Our method is entirely post-hoc, model-agnostic, and
applicable to any VLM with access to its weights. Given a single test point, we
enforce consistent predictions via two complementary objectives: (i) a
Cross-Entropy Agreement Loss that aligns predictive distributions across
semantically equivalent inputs, and (ii) a Pseudo-Label Consistency Loss that
draws outputs toward a self-averaged consensus. Our method is plug-and-play and
leverages information from a single test input itself to improve consistency.
Experiments on the MM-R3 benchmark show that our framework yields substantial
gains in consistency across state-of-the-art models, establishing a new
direction for inference-time adaptation in multimodal learning.

</details>


### [153] [Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy](https://arxiv.org/abs/2506.22432)
*Yuhao Liu,Tengfei Wang,Fang Liu,Zhenwei Wang,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: Shape-for-Motion introduces a 3D proxy framework for precise video editing, enabling consistent manipulations like pose editing and texture modification via a Dual-Propagation Strategy.


<details>
  <summary>Details</summary>
Motivation: Existing video synthesis methods lack fine-grained alignment with user intentions, necessitating a tool for precise and consistent control in creative editing.

Method: The framework converts target objects into time-consistent 3D meshes (proxies), edits them, and propagates changes using a Dual-Propagation Strategy. Edited meshes are projected to 2D for rendering and fed into a video diffusion model.

Result: The approach supports diverse manipulations (pose, rotation, scaling, etc.) and outperforms existing methods in experiments.

Conclusion: Shape-for-Motion advances high-quality, controllable video editing, addressing the challenge of user-intention alignment.

Abstract: Recent advances in deep generative modeling have unlocked unprecedented
opportunities for video synthesis. In real-world applications, however, users
often seek tools to faithfully realize their creative editing intentions with
precise and consistent control. Despite the progress achieved by existing
methods, ensuring fine-grained alignment with user intentions remains an open
and challenging problem. In this work, we present Shape-for-Motion, a novel
framework that incorporates a 3D proxy for precise and consistent video
editing. Shape-for-Motion achieves this by converting the target object in the
input video to a time-consistent mesh, i.e., a 3D proxy, allowing edits to be
performed directly on the proxy and then inferred back to the video frames. To
simplify the editing process, we design a novel Dual-Propagation Strategy that
allows users to perform edits on the 3D mesh of a single frame, and the edits
are then automatically propagated to the 3D meshes of the other frames. The 3D
meshes for different frames are further projected onto the 2D space to produce
the edited geometry and texture renderings, which serve as inputs to a
decoupled video diffusion model for generating edited results. Our framework
supports various precise and physically-consistent manipulations across the
video frames, including pose editing, rotation, scaling, translation, texture
modification, and object composition. Our approach marks a key step toward
high-quality, controllable video editing workflows. Extensive experiments
demonstrate the superiority and effectiveness of our approach. Project page:
https://shapeformotion.github.io/

</details>


### [154] [WarpRF: Multi-View Consistency for Training-Free Uncertainty Quantification and Applications in Radiance Fields](https://arxiv.org/abs/2506.22433)
*Sadra Safadoust,Fabio Tosi,Fatma Güney,Matteo Poggi*

Main category: cs.CV

TL;DR: WarpRF is a training-free framework for quantifying uncertainty in radiance fields by leveraging backward warping and measuring consistency across viewpoints.


<details>
  <summary>Details</summary>
Motivation: The need for a general-purpose, efficient method to quantify uncertainty in radiance fields without requiring training or framework-specific adjustments.

Method: Uses backward warping to project reliable renderings to unseen viewpoints and measures consistency with rendered images there.

Result: Outperforms existing methods in uncertainty quantification and downstream tasks like active view selection and mapping.

Conclusion: WarpRF is a simple, effective, and versatile solution for uncertainty quantification in radiance fields.

Abstract: We introduce WarpRF, a training-free general-purpose framework for
quantifying the uncertainty of radiance fields. Built upon the assumption that
photometric and geometric consistency should hold among images rendered by an
accurate model, WarpRF quantifies its underlying uncertainty from an unseen
point of view by leveraging backward warping across viewpoints, projecting
reliable renderings to the unseen viewpoint and measuring the consistency with
images rendered there. WarpRF is simple and inexpensive, does not require any
training, and can be applied to any radiance field implementation for free.
WarpRF excels at both uncertainty quantification and downstream tasks, e.g.,
active view selection and active mapping, outperforming any existing method
tailored to specific frameworks.

</details>


### [155] [MiCo: Multi-image Contrast for Reinforcement Visual Reasoning](https://arxiv.org/abs/2506.22434)
*Xi Chen,Mingkang Zhu,Shaoteng Liu,Xiaoyang Wu,Xiaogang Xu,Yu Liu,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

TL;DR: The paper introduces a method to enable Chain-of-Thought reasoning for linking visual cues across images using self-supervised learning and rule-based reinforcement learning, achieving strong performance without human-annotated data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of fine-grained visual reasoning across multiple images without relying on manually curated question-answer pairs.

Method: Constructs image triplets (two augmented views of the same image and a distinct image) and trains the model to compare them using rule-based reinforcement learning.

Result: The model generalizes well to various questions, improving multi-image reasoning benchmarks and performing strongly on general vision tasks.

Conclusion: Self-supervised learning with image triplets and rule-based reinforcement learning effectively enables visual reasoning without human annotations.

Abstract: This work explores enabling Chain-of-Thought (CoT) reasoning to link visual
cues across multiple images. A straightforward solution is to adapt rule-based
reinforcement learning for Vision-Language Models (VLMs). However, such methods
typically rely on manually curated question-answer pairs, which can be
particularly challenging when dealing with fine grained visual details and
complex logic across images. Inspired by self-supervised visual representation
learning, we observe that images contain inherent constraints that can serve as
supervision. Based on this insight, we construct image triplets comprising two
augmented views of the same image and a third, similar but distinct image.
During training, the model is prompted to generate a reasoning process to
compare these images (i.e., determine same or different). Then we optimize the
model with rule-based reinforcement learning. Due to the high visual similarity
and the presence of augmentations, the model must attend to subtle visual
changes and perform logical reasoning to succeed. Experiments show that,
although trained solely on visual comparison tasks, the learned reasoning
ability generalizes effectively to a wide range of questions. Without relying
on any human-annotated question-answer pairs, our method achieves significant
improvements on multi-image reasoning benchmarks and shows strong performance
on general vision tasks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [156] [Efficient Multilingual ASR Finetuning via LoRA Language Experts](https://arxiv.org/abs/2506.21555)
*Jiahong Li,Yiwen Shao,Jianheng Zhuo,Chenda Li,Liliang Tang,Dong Yu,Yanmin Qian*

Main category: cs.CL

TL;DR: A finetuning framework using LoRA language experts for multilingual ASR improves recognition performance by 10-15% over standard methods.


<details>
  <summary>Details</summary>
Motivation: Multilingual ASR faces interference between languages, hindering effective recognition.

Method: Proposes LoRA expert fusion or knowledge distillation for finetuning Whisper-based models.

Result: 10% and 15% relative performance gains in language-aware and language-agnostic scenarios.

Conclusion: The framework effectively addresses multilingual interference, enhancing ASR performance.

Abstract: Recent advancements in deep learning have significantly enhanced multilingual
automatic speech recognition (ASR) due to the development of advanced model
architectures and available large-scale multilingual datasets. Despite that,
multilingual ASR still suffers from the curse of multilinguality in that
different languages tend to interfere with each other, making it difficult for
the ASR model to identify multiple languages effectively while sharing model
capacity across them. This paper proposes an efficient finetuning framework for
customized multilingual ASR via prepared LoRA language experts based on
Whisper. Through LoRA expert fusion or knowledge distillation, our approach
achieves better recognition performance on target languages than standard
fine-tuning methods. Experimental results demonstrate that the proposed models
yield approximately 10\% and 15\% relative performance gains in language-aware
and language-agnostic scenarios, respectively.

</details>


### [157] [VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21556)
*Hyeongcheol Park,MinHyuk Jang,Ha Dam Baek,Gyusam Chang,Jiyoung Seo,Jiwan Park,Hogun Park,Sangpil Kim*

Main category: cs.CL

TL;DR: The paper introduces VAT-KG, a multimodal knowledge graph covering visual, audio, and text information, addressing limitations of existing MMKGs by enabling richer knowledge coverage and cross-modal alignment.


<details>
  <summary>Details</summary>
Motivation: Existing MMKGs are limited in scope, outdated, and support narrow modalities, reducing their applicability to modern multimodal tasks.

Method: Proposes VAT-KG, a concept-centric MMKG with a construction pipeline for cross-modal alignment and a novel multimodal RAG framework.

Result: VAT-KG effectively supports MLLMs in multimodal question answering, demonstrating its practical value.

Conclusion: VAT-KG advances MMKGs by unifying multimodal knowledge and enhancing MLLM performance.

Abstract: Multimodal Knowledge Graphs (MMKGs), which represent explicit knowledge
across multiple modalities, play a pivotal role by complementing the implicit
knowledge of Multimodal Large Language Models (MLLMs) and enabling more
grounded reasoning via Retrieval Augmented Generation (RAG). However, existing
MMKGs are generally limited in scope: they are often constructed by augmenting
pre-existing knowledge graphs, which restricts their knowledge, resulting in
outdated or incomplete knowledge coverage, and they often support only a narrow
range of modalities, such as text and visual information. These limitations
reduce their extensibility and applicability to a broad range of multimodal
tasks, particularly as the field shifts toward richer modalities such as video
and audio in recent MLLMs. Therefore, we propose the Visual-Audio-Text
Knowledge Graph (VAT-KG), the first concept-centric and knowledge-intensive
multimodal knowledge graph that covers visual, audio, and text information,
where each triplet is linked to multimodal data and enriched with detailed
descriptions of concepts. Specifically, our construction pipeline ensures
cross-modal knowledge alignment between multimodal data and fine-grained
semantics through a series of stringent filtering and alignment steps, enabling
the automatic generation of MMKGs from any multimodal dataset. We further
introduce a novel multimodal RAG framework that retrieves detailed
concept-level knowledge in response to queries from arbitrary modalities.
Experiments on question answering tasks across various modalities demonstrate
the effectiveness of VAT-KG in supporting MLLMs, highlighting its practical
value in unifying and leveraging multimodal knowledge.

</details>


### [158] [Debunk and Infer: Multimodal Fake News Detection via Diffusion-Generated Evidence and LLM Reasoning](https://arxiv.org/abs/2506.21557)
*Kaiying Yan,Moyang Liu,Yukun Liu,Ruibo Fu,Zhengqi Wen,Jianhua Tao,Xuefei Liu*

Main category: cs.CL

TL;DR: A framework called DIFND combines debunking knowledge, diffusion models, and multimodal LLMs to detect fake news with improved accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: The rapid spread of fake news challenges information credibility, requiring better detection methods.

Method: DIFND uses debunk diffusion for evidence generation and a chain-of-debunk strategy with MLLMs for reasoning and judgment.

Result: DIFND outperforms existing methods on FakeSV and FVC datasets, providing trustworthy decisions.

Conclusion: The framework enhances fake news detection by integrating generative and reasoning capabilities.

Abstract: The rapid spread of fake news across multimedia platforms presents serious
challenges to information credibility. In this paper, we propose a
Debunk-and-Infer framework for Fake News Detection(DIFND) that leverages
debunking knowledge to enhance both the performance and interpretability of
fake news detection. DIFND integrates the generative strength of conditional
diffusion models with the collaborative reasoning capabilities of multimodal
large language models (MLLMs). Specifically, debunk diffusion is employed to
generate refuting or authenticating evidence based on the multimodal content of
news videos, enriching the evaluation process with diverse yet semantically
aligned synthetic samples. To improve inference, we propose a chain-of-debunk
strategy where a multi-agent MLLM system produces logic-grounded,
multimodal-aware reasoning content and final veracity judgment. By jointly
modeling multimodal features, generative debunking cues, and reasoning-rich
verification within a unified architecture, DIFND achieves notable improvements
in detection accuracy. Extensive experiments on the FakeSV and FVC datasets
show that DIFND not only outperforms existing approaches but also delivers
trustworthy decisions.

</details>


### [159] [Bench to the Future: A Pastcasting Benchmark for Forecasting Agents](https://arxiv.org/abs/2506.21558)
*FutureSearch,:,Jack Wildman,Nikos I. Bosse,Daniel Hnyk,Peter Mühlbacher,Finn Hambly,Jon Evans,Dan Schwarz,Lawrence Phillips*

Main category: cs.CL

TL;DR: BTF is a 'pastcasting' benchmark for LLMs, using known-resolution questions and offline web data to simulate forecasting, showing comparable results to real forecasts.


<details>
  <summary>Details</summary>
Motivation: Current forecasting benchmarks lack realism and repeatability for LLM evaluation.

Method: BTF uses high-quality past questions with known answers and offline web corpora to test LLMs.

Result: BTF produces results comparable to real forecasts and tracks progress in forecasting capabilities.

Conclusion: BTF is a living benchmark for realistic LLM forecasting evaluation, open for community use.

Abstract: Forecasting is a challenging task that offers a clearly measurable way to
study AI systems. Forecasting requires a large amount of research on the
internet, and evaluations require time for events to happen, making the
development of forecasting benchmarks challenging. To date, no forecasting
benchmark provides a realistic, hermetic, and repeatable environment for LLM
forecasters. We introduce Bench To the Future (BTF), a "pastcasting" benchmark
with hundreds of high-quality questions for which the resolution is already
known. Each question is accompanied by a large offline corpus of tens of
thousands of relevant web pages, enabling a way to elicit realistic "forecasts"
on past events from LLMs. Results suggest that our pastcasting environment can
produce results comparable to those based on forecasts using the internet on
at-the-time unresolved questions. We show results benchmarking agent and
chain-of-thought forecasting approaches using several LLMs, including the
recently-released Claude 4 models, and demonstrate BTF's ability to track
steady forecasting capability progress over time. We intend this to be a living
benchmark, with new questions added continually to account for increasing
training data cutoff dates. We invite researchers to contact us at
hello@futuresearch.ai to utilize our benchmark or tooling for their own
research.

</details>


### [160] [GraphLAMA: Enabling Efficient Adaptation of Graph Language Models with Limited Annotations](https://arxiv.org/abs/2506.21559)
*Junze Chen,Cheng Yang,Shujie Li,Zhiqiang Zhang,Yawen Li,Junping Du,Chuan Shi*

Main category: cs.CL

TL;DR: The paper introduces GraphLAMA, a method to improve graph language models (GLMs) by addressing the limitations of in-context learning (ICL) and instruction tuning. It proposes a parameter adaptation stage for efficient tuning with few labeled examples, achieving better accuracy and faster inference.


<details>
  <summary>Details</summary>
Motivation: Current GLMs face effectiveness and efficiency issues with ICL due to fixed parameters and long context, while instruction tuning requires large labeled datasets, which are hard to obtain. The paper aims to bridge this gap.

Method: The proposed GraphLAMA method uses a GNN backbone to transform nodes into LLM token representations, allowing task instructions to mix node and language tokens. It includes pre-training for general knowledge and adaptation for few-shot tuning.

Result: GraphLAMA achieves a 4.91% absolute improvement in accuracy and is 10 times faster than ICL under a 5-shot setting.

Conclusion: GraphLAMA effectively addresses the limitations of ICL and instruction tuning, offering a practical solution for few-shot graph tasks with improved performance and efficiency.

Abstract: Large language models (LLMs) have demonstrated their strong capabilities in
various domains, and have been recently integrated for graph analysis as graph
language models (GLMs). With LLMs as the predictor, some GLMs can interpret
unseen tasks described by natural language, and learn from a few examples in
the prompts without parameter tuning, known as in-context learning (ICL).
Another subset of GLMs utilizes abundant training labels to enhance model
performance, known as instruction tuning. However, we argue that ICL on graphs
has effectiveness issues due to fixed parameters and efficiency issues due to
long context. Meanwhile, the large amount of labeled data required for
instruction tuning can be difficult to obtain in real-world scenarios. To this
end, we aim to introduce an extra parameter adaptation stage that can
efficiently tailor GLMs to an unseen graph and task with only a few labeled
examples, in exchange for better prediction accuracy and faster inference
speed. For implementation, in this paper we propose GraphLAMA method, with its
model backbone and learning schemes specialized for efficient tuning and
inference. Specifically, for model backbone, we use a graph neural network
(GNN) with several well-designed components to transform nodes into the
representation space of LLM tokens. Task instructions can then be represented
as a mixture of node and language tokens. In the pre-training stage, model
parameters except the LLM will be trained with different tasks to capture
general knowledge. In the adaptation stage, only a few pre-trained parameters
will be updated based on few-shot examples. Extensive experiments on
few/zero-shot node classification and summary generation show that our proposed
GraphLAMA achieves state-of-the-art performance with 4.91% absolution
improvement in accuracy. Compared with ICL, our inference speed can be 10 times
faster under 5-shot setting.

</details>


### [161] [Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning](https://arxiv.org/abs/2506.21560)
*Yifu Han,Geo Zhang*

Main category: cs.CL

TL;DR: The paper explores RL fine-tuning techniques (SFT, DPO, RLOO) on Qwen2.5-0.5B Base for instruction following and math reasoning, finding RLOO with DeBERTa best for alignment and DPO for consistency. Synthetic data and best-of-N sampling boost math accuracy.


<details>
  <summary>Details</summary>
Motivation: To evaluate effective RL fine-tuning methods for compact language models on challenging tasks like instruction following and mathematical reasoning.

Method: Compared SFT, DPO, and RLOO with reward models (DeBERTa). Used synthetic data augmentation and best-of-N sampling for math tasks.

Result: RLOO with DeBERTa achieved the best alignment; DPO was consistent. Synthetic data and best-of-N sampling improved math task accuracy.

Conclusion: Combining fine-tuning with inference-time tools is effective for lightweight, task-aligned small-scale language models, highlighting practical strategies.

Abstract: This study investigates the effectiveness of reinforcement learning (RL)
fine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two
challenging tasks: instruction following and mathematical reasoning. We compare
supervised fine-tuning (SFT), Direct Preference Optimization (DPO) using
preference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models.
Our experiments show that RLOO with DeBERTa reward modeling achieves the best
alignment, while DPO provides strong and consistent results. For math reasoing
tasks, synthetic data augmentation and best-of-N sampling with an external
verifier significantly improve accuracy, showing the potential of combining
fine-tuning with inference-time tools. This study highlights key trade-offs and
practical strategies for training lightweight, task-aligned small-scale
language models.

</details>


### [162] [Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs](https://arxiv.org/abs/2506.21561)
*Emilio Barkett,Olivia Long,Madhavendra Thakur*

Main category: cs.CL

TL;DR: The study evaluates LLMs' truth-judgment capabilities, finding reasoning models less truth-biased than non-reasoning ones but still flawed, with advanced models showing sycophantic tendencies.


<details>
  <summary>Details</summary>
Motivation: To understand LLMs' effectiveness in truth detection, given their use in critical applications like fact-checking and decision-making.

Method: Evaluated eight LLMs across 4,800 veracity judgments, comparing reasoning and non-reasoning models.

Result: Reasoning models had lower truth-bias but still underperformed humans; advanced models showed accuracy asymmetry (good at truth, poor at deception).

Conclusion: Capability advances in LLMs don't fully address veracity detection challenges, highlighting persistent issues like sycophancy.

Abstract: Despite their widespread use in fact-checking, moderation, and high-stakes
decision-making, large language models (LLMs) remain poorly understood as
judges of truth. This study presents the largest evaluation to date of LLMs'
veracity detection capabilities and the first analysis of these capabilities in
reasoning models. We had eight LLMs make 4,800 veracity judgments across
several prompts, comparing reasoning and non-reasoning models. We find that
rates of truth-bias, or the likelihood to believe a statement is true,
regardless of whether it is actually true, are lower in reasoning models than
in non-reasoning models, but still higher than human benchmarks. Most
concerning, we identify sycophantic tendencies in several advanced models
(o4-mini and GPT-4.1 from OpenAI, R1 from DeepSeek), which displayed an
asymmetry in detection accuracy, performing well in truth accuracy but poorly
in deception accuracy. This suggests that capability advances alone do not
resolve fundamental veracity detection challenges in LLMs.

</details>


### [163] [FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction](https://arxiv.org/abs/2506.21562)
*Jun Yin,Pengyu Zeng,Jing Zhong,Peilin Li,Miao Zhang,Ran Luo,Shuai Lu*

Main category: cs.CL

TL;DR: The paper introduces a 'next room prediction' paradigm for floor plan generation, inspired by autoregressive models, to better align with incremental architectural workflows.


<details>
  <summary>Details</summary>
Motivation: Existing generative models for floor plans are end-to-end, which conflicts with the iterative nature of real-world architectural design.

Method: Proposes a 'next room prediction' approach, inspired by autoregressive language models, for incremental floor plan generation.

Result: FPDS shows competitive performance against diffusion models and Tell2Design in text-to-floorplan tasks.

Conclusion: The method has potential for supporting intelligent architectural design by aligning with practical workflows.

Abstract: In the architectural design process, floor plan generation is inherently
progressive and iterative. However, existing generative models for floor plans
are predominantly end-to-end generation that produce an entire pixel-based
layout in a single pass. This paradigm is often incompatible with the
incremental workflows observed in real-world architectural practice. To address
this issue, we draw inspiration from the autoregressive 'next token prediction'
mechanism commonly used in large language models, and propose a novel 'next
room prediction' paradigm tailored to architectural floor plan modeling.
Experimental evaluation indicates that FPDS demonstrates competitive
performance in comparison to diffusion models and Tell2Design in the
text-to-floorplan task, indicating its potential applicability in supporting
future intelligent architectural design.

</details>


### [164] [FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era of Large Language Models](https://arxiv.org/abs/2506.21563)
*Kaiying Kevin Lin,Hsiyu Chen,Haopeng Zhang*

Main category: cs.CL

TL;DR: The paper introduces FORMOSANBENCH, a benchmark for evaluating LLMs on low-resource Formosan languages, revealing significant performance gaps and limited improvements from few-shot learning or fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored capabilities of LLMs in low-resource and endangered languages, specifically Formosan languages, due to their linguistic richness and sociolinguistic challenges.

Method: FORMOSANBENCH evaluates LLMs on three Formosan languages (Atayal, Amis, Paiwan) across machine translation, ASR, and text summarization, testing zero-shot, 10-shot, and fine-tuned settings.

Result: LLMs underperform significantly on Formosan languages, with 10-shot learning and fine-tuning providing only marginal improvements.

Conclusion: The findings highlight the need for more inclusive NLP technologies to support endangered languages, with released datasets and code to aid future research.

Abstract: While large language models (LLMs) have demonstrated impressive performance
across a wide range of natural language processing (NLP) tasks in high-resource
languages, their capabilities in low-resource and minority languages remain
significantly underexplored. Formosan languages -- a subgroup of Austronesian
languages spoken in Taiwan -- are both linguistically rich and endangered,
largely due to the sociolinguistic dominance of Mandarin. In this work, we
introduce FORMOSANBENCH, the first benchmark for evaluating LLMs on
low-resource Austronesian languages. It covers three endangered Formosan
languages: Atayal, Amis, and Paiwan, across three core NLP tasks: machine
translation, automatic speech recognition (ASR), and text summarization. We
assess model performance in zero-shot, 10-shot, and fine-tuned settings using
FORMOSANBENCH. Our results reveal a substantial performance gap between
high-resource and Formosan languages. Existing LLMs consistently underperform
across all tasks, with 10-shot learning and fine-tuning offering only limited
improvements. These findings underscore the urgent need for more inclusive NLP
technologies that can effectively support endangered and underrepresented
languages. We release our datasets and code to facilitate future research in
this direction.

</details>


### [165] [Team QUST at SemEval-2025 Task 10: Evaluating Large Language Models in Multiclass Multi-label Classification of News Entity Framing](https://arxiv.org/abs/2506.21564)
*Jiyan Liu,Youzheng Liu,Taihang Wang,Xiaoman Xu,Yimin Wang,Ye Jiang*

Main category: cs.CL

TL;DR: QUST_NLP's three-stage retrieval framework for fact-checked claim retrieval achieved 5th (monolingual) and 7th (crosslingual) in SemEval-2025 Task 7.


<details>
  <summary>Details</summary>
Motivation: To improve fact-checked claim retrieval performance by combining retrieval and re-ranking models.

Method: A three-stage framework: candidate retrieval, re-ranking with Top-10 results, and weighted voting for final outcomes.

Result: Ranked 5th in monolingual and 7th in crosslingual tracks.

Conclusion: The proposed framework is effective for fact-checked claim retrieval, with code publicly available.

Abstract: This paper describes the participation of QUST_NLP in the SemEval-2025 Task
7. We propose a three-stage retrieval framework specifically designed for
fact-checked claim retrieval. Initially, we evaluate the performance of several
retrieval models and select the one that yields the best results for candidate
retrieval. Next, we employ multiple re-ranking models to enhance the candidate
results, with each model selecting the Top-10 outcomes. In the final stage, we
utilize weighted voting to determine the final retrieval outcomes. Our approach
achieved 5th place in the monolingual track and 7th place in the crosslingual
track. We release our system code at:
https://github.com/warmth27/SemEval2025_Task7.

</details>


### [166] [A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing](https://arxiv.org/abs/2506.21565)
*Takato Ueno,Keito Inoshita*

Main category: cs.CL

TL;DR: The study introduces KCS+IBC, a multi-agent framework inspired by Japan's kairanban and idobata culture, integrating LLMs for bias mitigation and explainability in sentiment analysis.


<details>
  <summary>Details</summary>
Motivation: Inspired by traditional Japanese communication practices to enhance nuanced dialogue and social balance in sentiment analysis.

Method: Proposes KCS+IBC, combining sequential prediction sharing with mid-phase casual dialogue and probabilistic sentiment prediction.

Result: KCS matches single LLM accuracy; KCS+IBC reduces entropy and increases variance, balancing prediction aggregation and diversity.

Conclusion: The framework shows promise for bias correction; future work will refine its impact on sentiment analysis systems.

Abstract: Japan's kairanban culture and idobata conversations have long functioned as
traditional communication practices that foster nuanced dialogue among
community members and contribute to the formation of social balance. Inspired
by these information exchange processes, this study proposes a multi-agent
inference framework (KCS+IBC) that integrates multiple large language models
(LLMs) to achieve bias mitigation, improved explainability, and probabilistic
prediction in sentiment analysis. In addition to sequentially sharing
prediction results, the proposed method incorporates a mid-phase casual
dialogue session to blend formal inference with individual perspectives and
introduces probabilistic sentiment prediction. Experimental results show that
KCS achieves accuracy comparable to that of a single LLM across datasets, while
KCS+IBC exhibits a consistent decrease in entropy and a gradual increase in
variance during the latter stages of inference, suggesting the framework's
ability to balance aggregation and diversity of predictions. Future work will
quantitatively assess the impact of these characteristics on bias correction
and aim to develop more advanced sentiment analysis systems.

</details>


### [167] [The Saturation Point of Backtranslation in High Quality Low Resource English Gujarati Machine Translation](https://arxiv.org/abs/2506.21566)
*Arwa Arif*

Main category: cs.CL

TL;DR: Backtranslation (BT) in low-resource English-Gujarati MT using MBART50 shows no improvement, sometimes reducing performance despite careful filtering.


<details>
  <summary>Details</summary>
Motivation: To assess BT's effectiveness in high-quality, low-resource MT, specifically for English-Gujarati.

Method: Augmented a 50,000-sentence parallel corpus with filtered backtranslated Gujarati data using MBART50.

Result: No improvement in BLEU (43.8 baseline); slight performance drop in some cases.

Conclusion: BT may have diminishing returns in certain low-resource settings, warranting further research.

Abstract: Backtranslation BT is widely used in low resource machine translation MT to
generate additional synthetic training data using monolingual corpora. While
this approach has shown strong improvements for many language pairs, its
effectiveness in high quality, low resource settings remains unclear. In this
work, we explore the effectiveness of backtranslation for English Gujarati
translation using the multilingual pretrained MBART50 model. Our baseline
system, trained on a high quality parallel corpus of approximately 50,000
sentence pairs, achieves a BLEU score of 43.8 on a validation set. We augment
this data with carefully filtered backtranslated examples generated from
monolingual Gujarati text. Surprisingly, adding this synthetic data does not
improve translation performance and, in some cases, slightly reduces it. We
evaluate our models using multiple metrics like BLEU, ChrF++, TER, BLEURT and
analyze possible reasons for this saturation. Our findings suggest that
backtranslation may reach a point of diminishing returns in certain
low-resource settings and we discuss implications for future research.

</details>


### [168] [BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining](https://arxiv.org/abs/2506.21567)
*Baqer M. Merzah,Tania Taami,Salman Asoudeh,Amir reza Hossein pour,Saeed Mirzaee,Amir Ali Bengari*

Main category: cs.CL

TL;DR: The paper introduces BioPars, a measure to evaluate LLMs in bioinformatics, using datasets like BIOPARS-BENCH and BioParsQA. It compares models like ChatGPT, Llama, and Galactica, highlighting their strengths and weaknesses in Persian medical QA.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' capabilities in bioinformatics, particularly for Persian medical QA, and address gaps in handling higher-level questions and fine-grained inferences.

Method: Developed BioPars, a measure to evaluate LLMs on subject-specific knowledge, synthesis, and evidence. Used datasets BIOPARS-BENCH and BioParsQA for testing.

Result: BioPars outperformed GPT-4 1.0 with a ROUGE-L score of 29.99 and achieved high BERTScore (90.87), MoverScore (60.43), and BLEURT (50.78).

Conclusion: BioPars demonstrates LLMs' potential in Persian medical QA but highlights the need for further fine-tuning for complex tasks. Resources are available on GitHub.

Abstract: Large Language Models (LLMs) have recently gained attention in the life
sciences due to their capacity to model, extract, and apply complex biological
information. Beyond their classical use as chatbots, these systems are
increasingly used for complex analysis and problem-solving in specialized
fields, including bioinformatics. First, we introduce BIOPARS-BENCH, a dataset
from over 10,000 scientific articles, textbooks, and medical websites.
BioParsQA was also introduced to evaluate the proposed model, which consists of
5,231 Persian medical questions and answers. This study then introduces
BioPars, a simple but accurate measure designed to assess LLMs for three main
abilities: acquiring subject-specific knowledge, interpreting and synthesizing
such knowledge, and demonstrating proper evidence. Comparing ChatGPT, Llama,
and Galactica, our study highlights their ability to remember and retrieve
learned knowledge but also reveals shortcomings in addressing higher-level,
real-world questions and fine-grained inferences. These findings indicate the
need for further fine-tuning to address the capabilities of LLM in
bioinformatics tasks. To our knowledge, BioPars is the first application of LLM
in Persian medical QA, especially for generating long answers. Evaluation of
four selected medical QA datasets shows that BioPars has achieved remarkable
results compared to comparative approaches. The model on BioParsQA achieved a
ROUGE-L score of 29.99, which is an improvement over GPT-4 1.0. The model
achieved a BERTScore of 90.87 with the MMR method. The MoverScore and BLEURT
values were also higher in this model than the other three models. In addition,
the reported scores for the model are MoverScore=60.43 and BLEURT=50.78.
BioPars is an ongoing project and all resources related to its development will
be made available via the following GitHub repository:
https://github.com/amirap80/BioPars.

</details>


### [169] [Assessing RAG and HyDE on 1B vs. 4B-Parameter Gemma LLMs for Personal Assistants Integretion](https://arxiv.org/abs/2506.21568)
*Andrejs Sorstkins*

Main category: cs.CL

TL;DR: The study evaluates RAG and HyDE augmentation strategies on compact Gemma LLMs (1B and 4B parameters) for privacy-first personal assistants. RAG reduces latency and hallucinations, while HyDE improves semantic relevance but increases response time and hallucinations. RAG is recommended for small-scale LLMs.


<details>
  <summary>Details</summary>
Motivation: Resource efficiency is critical for deploying LLMs in edge and privacy-sensitive applications, motivating the evaluation of augmentation strategies.

Method: Implemented RAG and HyDE on Gemma LLMs (1B and 4B parameters), using MongoDB for short-term memory, Qdrant for long-term storage, and FastAPI/LangChain for orchestration. React.js frontend was used.

Result: RAG reduced latency by 17% and eliminated hallucinations. HyDE improved semantic relevance but increased response time (25-40%) and hallucination rate. Scaling to 4B models magnified HyDE's overhead.

Conclusion: RAG is the pragmatic choice for on-device personal assistants with small-scale LLMs due to its efficiency and reliability.

Abstract: Resource efficiency is a critical barrier to deploying large language models
(LLMs) in edge and privacy-sensitive applications. This study evaluates the
efficacy of two augmentation strategies--Retrieval-Augmented Generation (RAG)
and Hypothetical Document Embeddings (HyDE)--on compact Gemma LLMs of 1 billion
and 4 billion parameters, within the context of a privacy-first personal
assistant. We implement short-term memory via MongoDB and long-term semantic
storage via Qdrant, orchestrated through FastAPI and LangChain, and expose the
system through a React.js frontend. Across both model scales, RAG consistently
reduces latency by up to 17\% and eliminates factual hallucinations when
responding to user-specific and domain-specific queries. HyDE, by contrast,
enhances semantic relevance--particularly for complex physics prompts--but
incurs a 25--40\% increase in response time and a non-negligible hallucination
rate in personal-data retrieval. Comparing 1 B to 4 B models, we observe that
scaling yields marginal throughput gains for baseline and RAG pipelines, but
magnifies HyDE's computational overhead and variability. Our findings position
RAG as the pragmatic choice for on-device personal assistants powered by
small-scale LLMs.

</details>


### [170] [Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA](https://arxiv.org/abs/2506.21569)
*Weihua Xiao,Derek Ekberg,Siddharth Garg,Ramesh Karri*

Main category: cs.CL

TL;DR: A framework combining retrieval-augmented generation (RAG) and synthetic fine-tuning improves LLM performance in translating natural language to SystemVerilog Assertions (NL2SVA).


<details>
  <summary>Details</summary>
Motivation: Manual NL2SVA translation is labor-intensive and error-prone, and existing LLMs struggle with domain-specific syntax and semantics.

Method: Proposed a customized RAG framework and synthetic fine-tuning dataset with prompt-guided explanations for concurrent SVAs.

Result: The framework increased functionality-matched SVAs by 58.42% over GPT-4o-mini, and fine-tuned Qwen2.5-Coder-7B-Instruct achieved 59.05% improvement over the base model.

Conclusion: The approach significantly enhances LLM performance in NL2SVA, addressing domain-specific challenges effectively.

Abstract: SystemVerilog Assertions (SVAs) are critical for verifying the correctness of
hardware designs, but manually writing them from natural language property
descriptions, i.e., NL2SVA, remains a labor-intensive and error-prone task.
Recent advances in large language models (LLMs) offer opportunities to automate
this translation. However, existing models still struggle with understanding
domain-specific syntax and semantics. To enhance LLM performance in NL2SVA, we
propose a customized retrieval-augmented generation (RAG) framework and a
synthetic fine-tuning dataset that together improve LLM's performance. To
further improve lightweight models over NL2SVA, our fine-tuning dataset
provides prompt-guided explanations that teach LLMs the layer-by-layer
construction process of concurrent SVAs, enabling supervised fine-tuning that
greatly improves syntax and functionality accuracy. To evaluate the performance
of LLMs over NL2SVA, we construct the largest evaluation dataset for NL2SVA,
comprising 40 Verilog designs and 229 formally verified SVAs with detailed
annotations. Experimental results show that our customized RAG framework
increases the number of functionality matched SVAs by 58.42% over GPT-4o-mini,
while Qwen2.5-Coder-7B-Instruct fine-tuned on our fine-tuning dataset and
integrated with HybridRetrieval achieves a 59.05% over the base Qwen model.

</details>


### [171] [Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting](https://arxiv.org/abs/2506.21570)
*Roland Riachi,Kashif Rasul,Arjun Ashok,Prateek Humane,Alexis Roger,Andrew R. Williams,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.CL

TL;DR: Analyzing transfer from pre-trained language models (LMs) to time series forecasting, focusing on design choices like post-training, tokenizers, and model size, revealing non-vanishing transfer gaps and insights for compute-efficient training.


<details>
  <summary>Details</summary>
Motivation: To understand how pre-trained LMs can effectively transfer to time series forecasting, especially in low-data regimes, and identify optimal design choices.

Method: Examined various design choices (upstream post-training, time series tokenizer, LM backbone size) and their impact on validation loss in low-data settings.

Result: Found significant impact of design choices on validation loss, with clear optimal choices. Observed a non-vanishing transfer gap where LMs' validation loss decreases longer than randomly initialized models.

Conclusion: Provides insights for efficient training in time series forecasting and opens avenues for studying modality-agnostic data properties leveraged by LMs.

Abstract: Recent works have demonstrated the effectiveness of adapting pre-trained
language models (LMs) for forecasting time series in the low-data regime. We
build upon these findings by analyzing the effective transfer from language
models to time series forecasting under various design choices including
upstream post-training, time series tokenizer and language backbone size. In
the low-data regime, these design choices have a significant impact on the
validation loss, with clear-cut choices that outperform others. Contrary to
Hernandez et al. (2021), we observe that the validation loss of the LMs
continues to smoothly decrease long after the validation loss of the randomly
initialized models has converged, leading to a non-vanishing transfer gap that
holds across design choices. These findings not only help shed light on the
effective use of compute-efficient training for time series, but also open the
way for the study of modality-agnostic properties of data distributions
leveraged by these models.

</details>


### [172] [MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark](https://arxiv.org/abs/2412.15194)
*Qihao Zhao,Yangyu Huang,Tengchao Lv,Lei Cui,Qinzheng Sun,Shaoguang Mao,Xin Zhang,Ying Xin,Qiufeng Yin,Scarlett Li,Furu Wei*

Main category: cs.CL

TL;DR: The paper introduces MMLU-CF, a contamination-free MCQ benchmark to evaluate LLMs more reliably by preventing data leakage.


<details>
  <summary>Details</summary>
Motivation: Existing MCQ benchmarks like MMLU suffer from contamination due to open-source data and LLM training, leading to unreliable evaluations.

Method: MMLU-CF uses broader data sources, decontamination rules, and a closed-source test set to avoid leakage. A public validation set ensures transparency.

Result: GPT-4o scores 73.4% (5-shot) and 71.9% (0-shot) on MMLU-CF, showing the benchmark's rigor.

Conclusion: MMLU-CF provides a more reliable and challenging evaluation standard for LLMs by mitigating contamination issues.

Abstract: Multiple-choice question (MCQ) datasets like Massive Multitask Language
Understanding (MMLU) are widely used to evaluate the commonsense,
understanding, and problem-solving abilities of large language models (LLMs).
However, the open-source nature of these benchmarks and the broad sources of
training data for LLMs have inevitably led to benchmark contamination,
resulting in unreliable evaluation results. To alleviate this issue, we propose
a contamination-free and more challenging MCQ benchmark called MMLU-CF. This
benchmark reassesses LLMs' understanding of world knowledge by averting both
unintentional and malicious data leakage. To avoid unintentional data leakage,
we source data from a broader domain and design three decontamination rules. To
prevent malicious data leakage, we divide the benchmark into validation and
test sets with similar difficulty and subject distributions. The test set
remains closed-source to ensure reliable results, while the validation set is
publicly available to promote transparency and facilitate independent
verification. Our evaluation of mainstream LLMs reveals that the powerful
GPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on
the test set, which indicates the effectiveness of our approach in creating a
more rigorous and contamination-free evaluation standard. The GitHub repository
is available at https://github.com/microsoft/MMLU-CF and the dataset refers to
https://huggingface.co/datasets/microsoft/MMLU-CF.

</details>


### [173] [Towards Understanding the Cognitive Habits of Large Reasoning Models](https://arxiv.org/abs/2506.21571)
*Jianshuo Dong,Yujia Fu,Chuanrui Hu,Chao Zhang,Han Qiu*

Main category: cs.CL

TL;DR: The paper introduces CogTest, a benchmark to evaluate cognitive habits in Large Reasoning Models (LRMs), finding they exhibit human-like habits and adapt them task-specifically. Certain habits correlate with harmful responses.


<details>
  <summary>Details</summary>
Motivation: To explore whether LRMs exhibit human-like cognitive habits and understand their behavior patterns, inspired by observed CoT patterns.

Method: Introduces CogTest, a benchmark with 16 cognitive habits, each tested on 25 tasks, using evidence-first extraction for reliable identification. Evaluates 16 LLMs (13 LRMs, 3 non-reasoning).

Result: LRMs show human-like habits and task-adaptive deployment. Certain habits (e.g., Taking Responsible Risks) correlate with harmful responses. Inter-family similarities in habit profiles are noted.

Conclusion: Studying CoT behavioral patterns in LRMs aids understanding of LLM misbehavior. CogTest provides a framework for deeper analysis.

Abstract: Large Reasoning Models (LRMs), which autonomously produce a reasoning Chain
of Thought (CoT) before producing final responses, offer a promising approach
to interpreting and monitoring model behaviors. Inspired by the observation
that certain CoT patterns -- e.g., ``Wait, did I miss anything?'' --
consistently emerge across tasks, we explore whether LRMs exhibit human-like
cognitive habits. Building on Habits of Mind, a well-established framework of
cognitive habits associated with successful human problem-solving, we introduce
CogTest, a principled benchmark designed to evaluate LRMs' cognitive habits.
CogTest includes 16 cognitive habits, each instantiated with 25 diverse tasks,
and employs an evidence-first extraction method to ensure reliable habit
identification. With CogTest, we conduct a comprehensive evaluation of 16
widely used LLMs (13 LRMs and 3 non-reasoning ones). Our findings reveal that
LRMs, unlike conventional LLMs, not only exhibit human-like habits but also
adaptively deploy them according to different tasks. Finer-grained analyses
further uncover patterns of similarity and difference in LRMs' cognitive habit
profiles, particularly certain inter-family similarity (e.g., Qwen-3 models and
DeepSeek-R1). Extending the study to safety-related tasks, we observe that
certain habits, such as Taking Responsible Risks, are strongly associated with
the generation of harmful responses. These findings suggest that studying
persistent behavioral patterns in LRMs' CoTs is a valuable step toward deeper
understanding of LLM misbehavior. The code is available at:
https://github.com/jianshuod/CogTest.

</details>


### [174] [Aligning MLLM Benchmark With Human Preferences via Structural Equation Modeling](https://arxiv.org/abs/2506.21572)
*Tianyu. Zou,Shengwu. Xiong,Ruilin. Yao,Jirui. Huang,Yi. Rong,Yaxiong. Chen,Shili. Xiong,Cong. Wang*

Main category: cs.CL

TL;DR: The paper proposes a new framework for evaluating multimodal large language models (MLLMs) using Structural Equation Modeling (SEM) and introduces a hierarchical capability model based on Piaget's theory, resulting in a more interpretable and less redundant benchmark named Gold.


<details>
  <summary>Details</summary>
Motivation: Current MLLM benchmarks lack structured, interpretable designs, leading to overlapping abilities, redundant indicators, and limited diagnostic power.

Method: The authors use SEM to align benchmarks and introduce a hierarchical capability model (Perception, Memory, Reasoning) inspired by Piaget's theory. They reorganize existing benchmarks and create a new one (Gold).

Result: The Gold benchmark shows stronger interpretability, reduced redundancy, and clearer cognitive consistency compared to existing benchmarks.

Conclusion: The proposed framework and benchmark improve the evaluation of MLLMs by addressing the limitations of current designs.

Abstract: Evaluating multimodal large language models (MLLMs) remains a fundamental
challenge due to a lack of structured, interpretable, and theoretically
grounded benchmark designs. Existing benchmarks often adopt heuristic-based
task groupings with unclear cognitive targets, thus resulting in overlapping
abilities, redundant indicators, and limited diagnostic power. In this work, we
propose a novel framework for aligning MLLM benchmark based on Structural
Equation Modeling (SEM) to analyze and quantify the internal validity,
dimensional separability, and contribution of benchmark components. Motivated
by the observed limitations of current designs, we further introduce a novel
capability hierarchy grounded in Piagets theory of cognitive development,
dividing MLLM abilities into three hierarchical layers, i.e., Perception,
Memory, and Reasoning. We reorganize existing MLLM benchmarks under the
proposed framework and construct a new benchmark named Gold. Experimental
results demonstrate that the proposed benchmark exhibits stronger
interpretability, reduced indicator redundancy, and clearer cognitive
consistency compared to existing approaches.

</details>


### [175] [Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs](https://arxiv.org/abs/2506.21573)
*Yanwei Ren,Liu Liu,Baosheng Yu,Jiayan Qiu,Quan Chen*

Main category: cs.CL

TL;DR: A novel framework combines black-box and white-box models to optimize LLM instructions, balancing cost and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of white-box (high resource use) and black-box (high cost) approaches for LLM instruction optimization.

Method: Merges black-box models for diverse initializations with white-box models for interpretability, using semantic similarity constraints for unified representation.

Result: Outperforms state-of-the-art baselines in tasks like complex reasoning and cross-lingual generalization.

Conclusion: The hybrid approach offers a scalable, efficient solution for next-gen LLM applications.

Abstract: Optimizing instructions for large language models (LLMs) is critical for
harnessing their full potential in complex and diverse tasks. However, relying
solely on white-box approaches demands extensive computational resources and
offers limited representational capacity, while black-box models can incur
prohibitive financial costs. To address these challenges, we introduce a novel
framework that seamlessly merges the strengths of both paradigms. Black-box
models provide high-quality, diverse instruction initializations, and white-box
models supply fine-grained interpretability through hidden states and output
features. By enforcing a semantic similarity constraint, these components fuse
into a unified high-dimensional representation that captures deep semantic and
structural nuances, enabling an iterative optimization process to refine
instruction quality and adaptability. Extensive evaluations across a broad
spectrum of tasks-ranging from complex reasoning to cross-lingual
generalization-demonstrate that our approach consistently outperforms
state-of-the-art baselines. This fusion of black-box initialization with
advanced semantic refinement yields a scalable and efficient solution, paving
the way for next-generation LLM-driven applications in diverse real-world
scenarios. The source code will be released soon.

</details>


### [176] [Data Efficacy for Language Model Training](https://arxiv.org/abs/2506.21545)
*Yalun Dai,Yangyu Huang,Xin Zhang,Wenshan Wu,Chong Li,Wenhui Lu,Shijie Cao,Li Dong,Scarlett Li*

Main category: cs.CL

TL;DR: The paper introduces Data Efficacy (DELT), a paradigm optimizing data organization for LM training, featuring components like Data Scoring (LQS) and Data Ordering (FO), which improve performance without extra data or model size.


<details>
  <summary>Details</summary>
Motivation: To explore the underexamined role of data organization (Data Efficacy) in LM training, complementing data efficiency efforts.

Method: Proposes DELT with three components: Data Scoring (LQS), Data Selection, and Data Ordering (FO). LQS scores data by learnability and quality, while FO addresses forgetting and bias.

Result: DELT improves LM performance without additional data or model size. LQS and FO combined yield the most significant gains. Data efficacy and efficiency can coexist.

Conclusion: Data efficacy is a promising foundational area in LM training, enhancing performance through optimized data organization.

Abstract: Data is fundamental to the training of language models (LM). Recent research
has been dedicated to data efficiency, which aims to maximize performance by
selecting a minimal or optimal subset of training data. Techniques such as data
filtering, sampling, and selection play a crucial role in this area. To
complement it, we define Data Efficacy, which focuses on maximizing performance
by optimizing the organization of training data and remains relatively
underexplored. This work introduces a general paradigm, DELT, for considering
data efficacy in LM training, which highlights the significance of training
data organization. DELT comprises three components: Data Scoring, Data
Selection, and Data Ordering. Among these components, we design
Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which
considers both the learnability and quality of each data sample from the
gradient consistency perspective. We also devise Folding Ordering (FO), as a
novel instance of Data Ordering, which addresses issues such as model
forgetting and data distribution bias. Comprehensive experiments validate the
data efficacy in LM training, which demonstrates the following: Firstly,
various instances of the proposed DELT enhance LM performance to varying
degrees without increasing the data scale and model size. Secondly, among these
instances, the combination of our proposed LQS for data scoring and Folding for
data ordering achieves the most significant improvement. Lastly, data efficacy
can be achieved together with data efficiency by applying data selection.
Therefore, we believe that data efficacy is a promising foundational area in LM
training.

</details>


### [177] [Digital Gatekeepers: Exploring Large Language Model's Role in Immigration Decisions](https://arxiv.org/abs/2506.21574)
*Yicheng Mao,Yang Zhao*

Main category: cs.CL

TL;DR: The study explores using LLMs like GPT-3.5 and GPT-4 for immigration decision-making, finding they align with human strategies but still exhibit biases.


<details>
  <summary>Details</summary>
Motivation: Addressing workload and fairness challenges in immigration departments through AI integration.

Method: Mixed-methods approach with discrete choice experiments and in-depth interviews.

Result: LLMs align with human decision-making but show biases toward privileged groups and stereotypes.

Conclusion: LLMs have potential for immigration decisions but require addressing biases and limitations.

Abstract: With globalization and increasing immigrant populations, immigration
departments face significant work-loads and the challenge of ensuring fairness
in decision-making processes. Integrating artificial intelligence offers a
promising solution to these challenges. This study investigates the potential
of large language models (LLMs),such as GPT-3.5 and GPT-4, in supporting
immigration decision-making. Utilizing a mixed-methods approach,this paper
conducted discrete choice experiments and in-depth interviews to study LLM
decision-making strategies and whether they are fair. Our findings demonstrate
that LLMs can align their decision-making with human strategies, emphasizing
utility maximization and procedural fairness. Meanwhile, this paper also
reveals that while ChatGPT has safeguards to prevent unintentional
discrimination, it still exhibits stereotypes and biases concerning nationality
and shows preferences toward privileged group. This dual analysis highlights
both the potential and limitations of LLMs in automating and enhancing
immigration decisions.

</details>


### [178] [STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning for Semantic Parsing](https://arxiv.org/abs/2506.21575)
*Josefa Lia Stoisser,Marc Boubnovski Martell,Lawrence Phillips,Casper Hansen,Julien Fauqueur*

Main category: cs.CL

TL;DR: STRuCT-LLM is a framework for training LLMs to perform structured reasoning on relational and graph data, using RL and CoT supervision. It improves performance on Text-to-SQL and Text-to-Cypher tasks and shows zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: To unify structured reasoning over relational and graph data, leveraging shared abstractions between SQL and Cypher for cross-formalism transfer.

Method: Uses reinforcement learning with Chain-of-Thought supervision and a topology-aware reward function for graph-based parsing.

Result: Achieves significant improvements: 13.5% on Spider (Text-to-SQL) and 73.1% on Text2Cypher. Shows zero-shot gains on QA tasks.

Conclusion: Executable queries are effective scaffolds for structured reasoning, and joint training on SQL and Cypher yields synergistic benefits.

Abstract: We propose STRuCT-LLM, a unified framework for training large language models
(LLMs) to perform structured reasoning over both relational and
graph-structured data. Our approach jointly optimizes Text-to-SQL and
Text-to-Cypher tasks using reinforcement learning (RL) combined with
Chain-of-Thought (CoT) supervision. To support fine-grained optimization in
graph-based parsing, we introduce a topology-aware reward function based on
graph edit distance. Unlike prior work that treats relational and graph
formalisms in isolation, STRuCT-LLM leverages shared abstractions between SQL
and Cypher to induce cross-formalism transfer, enabling SQL training to improve
Cypher performance and vice versa - even without shared schemas. Our largest
model (QwQ-32B) achieves substantial relative improvements across tasks: on
semantic parsing, Spider improves by 13.5\% and Text2Cypher by 73.1\%. The
model also demonstrates strong zero-shot generalization, improving performance
on downstream tabular QA (TableBench: 8.5\%) and knowledge graph QA
(CR-LT-KGQA: 1.7\%) without any QA-specific supervision. These results
demonstrate both the effectiveness of executable queries as scaffolds for
structured reasoning and the synergistic benefits of jointly training on SQL
and Cypher (code available at https://github.com/bouv/STRuCT-LLM).

</details>


### [179] [Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning](https://arxiv.org/abs/2506.21576)
*Hongli Yang,Yizhou Peng,Hao Huang,Sheng Li*

Main category: cs.CL

TL;DR: The paper explores Soft Prompt Tuning (SPT) to improve multilingual ASR models like Whisper for low-resource scenarios, focusing on code-switching (CS). It introduces SPT4ASR and evaluates two strategies, showing deep prompt tuning as the most effective approach.


<details>
  <summary>Details</summary>
Motivation: Address challenges in low-resource ASR, such as rare languages and code-switching, while preserving prior knowledge and maintaining parameter efficiency.

Method: Evaluates two strategies: full fine-tuning (FFT) of soft prompts and the entire Whisper model, and training only soft prompts while freezing model parameters. Introduces SPT4ASR, combining SPT variants.

Result: Deep prompt tuning is most effective. SPT4ASR methods reduce errors in CS ASR without degrading performance on existing languages, maintaining parameter efficiency.

Conclusion: SPT, especially deep prompt tuning and SPT4ASR, enhances CS ASR performance efficiently, balancing cross-lingual capabilities and parameter constraints.

Abstract: Large-scale multilingual ASR models like Whisper excel in high-resource
settings but face challenges in low-resource scenarios, such as rare languages
and code-switching (CS), due to computational costs and catastrophic
forgetting. We explore Soft Prompt Tuning (SPT), a parameter-efficient method
to enhance CS ASR while preserving prior knowledge. We evaluate two strategies:
(1) full fine-tuning (FFT) of both soft prompts and the entire Whisper model,
demonstrating improved cross-lingual capabilities compared to traditional
methods, and (2) adhering to SPT's original design by freezing model parameters
and only training soft prompts. Additionally, we introduce SPT4ASR, a
combination of different SPT variants. Experiments on the SEAME and ASRU2019
datasets show that deep prompt tuning is the most effective SPT approach, and
our SPT4ASR methods achieve further error reductions in CS ASR, maintaining
parameter efficiency similar to LoRA, without degrading performance on existing
languages.

</details>


### [180] [Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR](https://arxiv.org/abs/2506.21577)
*Hongli Yang,Sheng Li,Hao Huang,Ayiduosi Tuohan,Yizhou Peng*

Main category: cs.CL

TL;DR: The paper introduces Entire SPT and LAPT for multilingual ASR, improving language expansion tasks with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like language interference and expanding to unseen languages in multilingual ASR without performance degradation.

Method: Proposes Entire SPT (soft prompts for encoder and decoder) and LAPT (language-aware prompts leveraging cross-lingual similarities), integrated into SPT-Whisper toolkit.

Result: Entire SPT and LAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks.

Conclusion: The methods provide an efficient solution for dynamic multilingual ASR with minimal overhead.

Abstract: Recent advancements in multilingual automatic speech recognition (ASR) have
been driven by large-scale end-to-end models like Whisper. However, challenges
such as language interference and expanding to unseen languages (language
expansion) without degrading performance persist. This paper addresses these
with three contributions: 1) Entire Soft Prompt Tuning (Entire SPT), which
applies soft prompts to both the encoder and decoder, enhancing feature
extraction and decoding; 2) Language-Aware Prompt Tuning (LAPT), which
leverages cross-lingual similarities to encode shared and language-specific
features using lightweight prompt matrices; 3) SPT-Whisper, a toolkit that
integrates SPT into Whisper and enables efficient continual learning.
Experiments across three languages from FLEURS demonstrate that Entire SPT and
LAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks,
respectively, providing an efficient solution for dynamic, multilingual ASR
models with minimal computational overhead.

</details>


### [181] [HealthQA-BR: A System-Wide Benchmark Reveals Critical Knowledge Gaps in Large Language Models](https://arxiv.org/abs/2506.21578)
*Andrew Maranhão Ventura D'addario*

Main category: cs.CL

TL;DR: HealthQA-BR is a Portuguese-language benchmark for evaluating LLMs in healthcare, revealing performance gaps across specialties despite high overall scores.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluations in healthcare are physician-centric and English-focused, lacking interprofessional and multilingual realism.

Method: Developed HealthQA-BR with 5,632 questions from Brazilian exams, assessing multiple health professions, and evaluated 20+ LLMs in a zero-shot setting.

Result: GPT 4.1 scored 86.6% overall but showed significant drops in Neurosurgery (60.0%) and Social Work (68.4%), highlighting uneven knowledge.

Conclusion: High-level scores are inadequate for safety validation; granular benchmarks like HealthQA-BR are essential for realistic AI readiness assessment.

Abstract: The evaluation of Large Language Models (LLMs) in healthcare has been
dominated by physician-centric, English-language benchmarks, creating a
dangerous illusion of competence that ignores the interprofessional nature of
patient care. To provide a more holistic and realistic assessment, we introduce
HealthQA-BR, the first large-scale, system-wide benchmark for
Portuguese-speaking healthcare. Comprising 5,632 questions from Brazil's
national licensing and residency exams, it uniquely assesses knowledge not only
in medicine and its specialties but also in nursing, dentistry, psychology,
social work, and other allied health professions. We conducted a rigorous
zero-shot evaluation of over 20 leading LLMs. Our results reveal that while
state-of-the-art models like GPT 4.1 achieve high overall accuracy (86.6%),
this top-line score masks alarming, previously unmeasured deficiencies. A
granular analysis shows performance plummets from near-perfect in specialties
like Ophthalmology (98.7%) to barely passing in Neurosurgery (60.0%) and, most
notably, Social Work (68.4%). This "spiky" knowledge profile is a systemic
issue observed across all models, demonstrating that high-level scores are
insufficient for safety validation. By publicly releasing HealthQA-BR and our
evaluation suite, we provide a crucial tool to move beyond single-score
evaluations and toward a more honest, granular audit of AI readiness for the
entire healthcare team.

</details>


### [182] [From General Reasoning to Domain Expertise: Uncovering the Limits of Generalization in Large Language Models](https://arxiv.org/abs/2506.21580)
*Dana Alsagheer,Yang Lu,Abdulrahman Kamal,Omar Kamal,Mohammad Kamal,Nada Mansour,Cosmo Yang Wu,Rambiba Karanjai,Sen Li,Weidong Shi*

Main category: cs.CL

TL;DR: The paper examines the link between general reasoning in LLMs and their performance in domain-specific tasks, emphasizing reasoning as the foundation for decision-making.


<details>
  <summary>Details</summary>
Motivation: To understand how general reasoning abilities in LLMs impact their effectiveness in domain-specific reasoning tasks, given the importance of reasoning for decision-making.

Method: Explores the connection between general reasoning capabilities of LLMs and their domain-specific reasoning performance.

Result: Findings likely highlight the relationship between general and domain-specific reasoning in LLMs.

Conclusion: General reasoning in LLMs is crucial for their performance in domain-specific tasks, underscoring its role in effective decision-making.

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
remarkable capabilities in various domains. However, effective decision-making
relies heavily on strong reasoning abilities. Reasoning is the foundation for
decision-making, providing the analytical and logical framework to make sound
choices. Reasoning involves analyzing information, drawing inferences, and
reaching conclusions based on logic or evidence. Decision-making builds on this
foundation by applying the insights from reasoning to select the best course of
action among alternatives. Together, these processes create a continuous cycle
of thought and action aimed at achieving goals effectively. As AI technology
evolves, there is a growing trend to train LLMs to excel in general reasoning.
This study explores how the general reasoning capabilities of LLMs connect to
their performance in domain-specific reasoning tasks.

</details>


### [183] [VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents](https://arxiv.org/abs/2506.21582)
*Sam Yu-Te Lee,Chengyang Ji,Shicheng Wen,Lifu Huang,Dongyi Liu,Kwan-Liu Ma*

Main category: cs.CL

TL;DR: VIDEE is a system designed to help entry-level analysts perform advanced text analytics using intelligent agents, featuring a human-agent workflow with decomposition, execution, and evaluation stages.


<details>
  <summary>Details</summary>
Motivation: Traditional text analytics requires NLP expertise, creating barriers for entry-level analysts. VIDEE aims to democratize access using LLMs and intelligent agents.

Method: VIDEE uses a three-stage workflow: Decomposition (human-in-the-loop Monte-Carlo Tree Search), Execution (pipeline generation), and Evaluation (LLM-based validation).

Result: Quantitative experiments and a user study show VIDEE's effectiveness, usability for non-experts, and distinct user behavior patterns.

Conclusion: VIDEE validates the potential of human-agent collaboration in text analytics and provides design insights for future systems.

Abstract: Text analytics has traditionally required specialized knowledge in Natural
Language Processing (NLP) or text analysis, which presents a barrier for
entry-level analysts. Recent advances in large language models (LLMs) have
changed the landscape of NLP by enabling more accessible and automated text
analysis (e.g., topic detection, summarization, information extraction, etc.).
We introduce VIDEE, a system that supports entry-level data analysts to conduct
advanced text analytics with intelligent agents. VIDEE instantiates a
human-agent collaroration workflow consisting of three stages: (1)
Decomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search
algorithm to support generative reasoning with human feedback, (2) Execution,
which generates an executable text analytics pipeline, and (3) Evaluation,
which integrates LLM-based evaluation and visualizations to support user
validation of execution results. We conduct two quantitative experiments to
evaluate VIDEE's effectiveness and analyze common agent errors. A user study
involving participants with varying levels of NLP and text analytics experience
-- from none to expert -- demonstrates the system's usability and reveals
distinct user behavior patterns. The findings identify design implications for
human-agent collaboration, validate the practical utility of VIDEE for
non-expert users, and inform future improvements to intelligent text analytics
systems.

</details>


### [184] [Hope Speech Detection in code-mixed Roman Urdu tweets: A Positive Turn in Natural Language Processing](https://arxiv.org/abs/2506.21583)
*Muhammad Ahmad,Muhammad Waqas,Ameer Hamza,Ildar Batyrshin,Grigori Sidorov*

Main category: cs.CL

TL;DR: This study introduces the first multi-class annotated dataset for hope speech in code-mixed Roman Urdu and proposes a custom transformer model, achieving superior performance over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing research on hope speech detection neglects informal and underrepresented languages like Roman Urdu, creating a gap in inclusive NLP.

Method: The study develops a dataset, analyzes linguistic patterns, and proposes an attention-based transformer model (XLM-R), evaluated via 5-fold cross-validation.

Result: XLM-R achieves a cross-validation score of 0.78, outperforming SVM (0.75) and BiLSTM (0.76).

Conclusion: The work fills a critical gap in NLP for low-resource languages and demonstrates the effectiveness of custom models for code-mixed text.

Abstract: Hope is a positive emotional state involving the expectation of favorable
future outcomes, while hope speech refers to communication that promotes
optimism, resilience, and support, particularly in adverse contexts. Although
hope speech detection has gained attention in Natural Language Processing
(NLP), existing research mainly focuses on high-resource languages and
standardized scripts, often overlooking informal and underrepresented forms
such as Roman Urdu. To the best of our knowledge, this is the first study to
address hope speech detection in code-mixed Roman Urdu by introducing a
carefully annotated dataset, thereby filling a critical gap in inclusive NLP
research for low-resource, informal language varieties. This study makes four
key contributions: (1) it introduces the first multi-class annotated dataset
for Roman Urdu hope speech, comprising Generalized Hope, Realistic Hope,
Unrealistic Hope, and Not Hope categories; (2) it explores the psychological
foundations of hope and analyzes its linguistic patterns in code-mixed Roman
Urdu to inform dataset development; (3) it proposes a custom attention-based
transformer model optimized for the syntactic and semantic variability of Roman
Urdu, evaluated using 5-fold cross-validation; and (4) it verifies the
statistical significance of performance gains using a t-test. The proposed
model, XLM-R, achieves the best performance with a cross-validation score of
0.78, outperforming the baseline SVM (0.75) and BiLSTM (0.76), with gains of 4%
and 2.63% respectively.

</details>


### [185] [Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques](https://arxiv.org/abs/2506.21584)
*J. Koorndijk*

Main category: cs.CL

TL;DR: Small instruction-tuned models like LLaMA 3 8B can exhibit alignment faking, and prompt-only interventions can reduce this behavior, challenging assumptions about scale and triviality of prompt-based ethics.


<details>
  <summary>Details</summary>
Motivation: To provide empirical evidence that alignment faking isn't exclusive to large models and to explore interventions to mitigate it.

Method: Used LLaMA 3 8B to demonstrate alignment faking, tested prompt-only interventions like deontological framing and scratchpad reasoning.

Result: Prompt interventions reduced deceptive alignment, showing it's suppressible without modifying the model.

Conclusion: Alignment evaluations should consider model size and deployment settings, and deception can be shallow (context-driven) or deep (goal-driven).

Abstract: Current literature suggests that alignment faking (deceptive alignment) is an
emergent property of large language models. We present the first empirical
evidence that a small instruction-tuned model, specifically LLaMA 3 8B, can
also exhibit alignment faking. We further show that prompt-only interventions,
including deontological moral framing and scratchpad reasoning, significantly
reduce this behavior without modifying model internals. This challenges the
assumption that prompt-based ethics are trivial and that deceptive alignment
requires scale. We introduce a taxonomy distinguishing shallow deception,
shaped by context and suppressible through prompting, from deep deception,
which reflects persistent, goal-driven misalignment. Our findings refine the
understanding of deception in language models and underscore the need for
alignment evaluations across model sizes and deployment settings.

</details>


### [186] [Evaluation of LLM-based Strategies for the Extraction of Food Product Information from Online Shops](https://arxiv.org/abs/2506.21585)
*Christoph Brosch,Sian Brumm,Rolf Krieger,Jonas Scheffler*

Main category: cs.CL

TL;DR: The paper compares two LLM-based methods (direct and indirect extraction) for extracting structured information from food product web pages, finding indirect extraction more efficient and cost-effective despite slightly lower accuracy.


<details>
  <summary>Details</summary>
Motivation: To automate the extraction of key product attributes (e.g., ingredient lists, nutrition tables) from online food product pages using LLMs, addressing scalability and cost challenges.

Method: Two approaches are evaluated: direct extraction and indirect extraction via generated functions. Performance is measured by accuracy, efficiency, and cost on a dataset of 3,000 product pages.

Result: Indirect extraction achieves 96.48% accuracy (1.61% lower than direct) but reduces LLM calls by 95.82%, improving efficiency and lowering costs.

Conclusion: Indirect extraction is a scalable and cost-effective solution for large-scale information extraction from template-based web pages using LLMs.

Abstract: Generative AI and large language models (LLMs) offer significant potential
for automating the extraction of structured information from web pages. In this
work, we focus on food product pages from online retailers and explore
schema-constrained extraction approaches to retrieve key product attributes,
such as ingredient lists and nutrition tables. We compare two LLM-based
approaches, direct extraction and indirect extraction via generated functions,
evaluating them in terms of accuracy, efficiency, and cost on a curated dataset
of 3,000 food product pages from three different online shops. Our results show
that although the indirect approach achieves slightly lower accuracy (96.48\%,
$-1.61\%$ compared to direct extraction), it reduces the number of required LLM
calls by 95.82\%, leading to substantial efficiency gains and lower operational
costs. These findings suggest that indirect extraction approaches can provide
scalable and cost-effective solutions for large-scale information extraction
tasks from template-based web pages using LLMs.

</details>


### [187] [Can Vision Language Models Understand Mimed Actions?](https://arxiv.org/abs/2506.21586)
*Hyundong Cho,Spencer Lin,Tejas Srinivasan,Michael Saxon,Deuksin Kwon,Natali T. Chavez,Jonathan May*

Main category: cs.CL

TL;DR: The paper introduces MIME, a benchmark for evaluating vision-language models on mimed actions, highlighting their poor performance compared to humans.


<details>
  <summary>Details</summary>
Motivation: Studying nonverbal communication (NVC) is challenging due to its broad scope and variance. Mime, a subset of NVC, offers a more controlled and explicit form for analysis.

Method: Proposes MIME, a video-based QA benchmark with 86 mimed actions, using motion capture data and perturbations to test robustness.

Result: Vision-language models perform significantly worse than humans on MIME, indicating a gap in understanding human gestures.

Conclusion: More research is needed to improve models' robustness in interpreting human gestures, with MIME serving as a valuable benchmark.

Abstract: Nonverbal communication (NVC) plays an integral role in human language, but
studying NVC in general is challenging because of its broad scope and high
variance in interpretation among individuals and cultures. However, mime -- the
theatrical technique of suggesting intent using only gesture, expression, and
movement -- is a subset of NVC that consists of explicit and embodied actions
with much lower human interpretation variance. We argue that a solid
understanding of mimed actions is a crucial prerequisite for vision-language
models capable of interpreting and commanding more subtle aspects of NVC.
Hence, we propose Mime Identification Multimodal Evaluation (MIME), a novel
video-based question answering benchmark comprising of 86 mimed actions.
Constructed with motion capture data, MIME consists of variations of each
action with perturbations applied to the character, background, and viewpoint
for evaluating recognition robustness. We find that both open-weight and
API-based vision-language models perform significantly worse than humans on
MIME, motivating the need for increased research for instilling more robust
understanding of human gestures.

</details>


### [188] [Is DeepSeek a New Voice Among LLMs in Public Opinion Simulation?](https://arxiv.org/abs/2506.21587)
*Weihong Qi,Fan Huang,Jisun An,Haewoon Kwak*

Main category: cs.CL

TL;DR: DeepSeek-V3 outperforms other LLMs in simulating U.S. opinions on abortion and Chinese opinions on foreign aid but struggles with biases and overgeneralization.


<details>
  <summary>Details</summary>
Motivation: To compare DeepSeek's performance with major LLMs in simulating public opinions across cultures and demographics.

Method: Used ANES and Zuobiao datasets to evaluate DeepSeek-R1, DeepSeek-V3, Qwen2.5, GPT-4o, and Llama-3.3 on social issues in the U.S. and China.

Result: DeepSeek-V3 excels in specific topics but shows biases and overgeneralization, failing to capture nuanced views in some demographics.

Conclusion: LLMs need more inclusive training to reduce cultural and demographic biases in public opinion modeling.

Abstract: This study evaluates the ability of DeepSeek, an open-source large language
model (LLM), to simulate public opinions in comparison to LLMs developed by
major tech companies. By comparing DeepSeek-R1 and DeepSeek-V3 with Qwen2.5,
GPT-4o, and Llama-3.3 and utilizing survey data from the American National
Election Studies (ANES) and the Zuobiao dataset of China, we assess these
models' capacity to predict public opinions on social issues in both China and
the United States, highlighting their comparative capabilities between
countries. Our findings indicate that DeepSeek-V3 performs best in simulating
U.S. opinions on the abortion issue compared to other topics such as climate
change, gun control, immigration, and services for same-sex couples, primarily
because it more accurately simulates responses when provided with Democratic or
liberal personas. For Chinese samples, DeepSeek-V3 performs best in simulating
opinions on foreign aid and individualism but shows limitations in modeling
views on capitalism, particularly failing to capture the stances of low-income
and non-college-educated individuals. It does not exhibit significant
differences from other models in simulating opinions on traditionalism and the
free market. Further analysis reveals that all LLMs exhibit the tendency to
overgeneralize a single perspective within demographic groups, often defaulting
to consistent responses within groups. These findings highlight the need to
mitigate cultural and demographic biases in LLM-driven public opinion modeling,
calling for approaches such as more inclusive training methodologies.

</details>


### [189] [Understanding Verbatim Memorization in LLMs Through Circuit Discovery](https://arxiv.org/abs/2506.21588)
*Ilya Lasy,Peter Knees,Stefan Woltran*

Main category: cs.CL

TL;DR: The paper investigates memorization mechanisms in LLMs, identifying specific circuits for initiating and maintaining memorization, with findings on their transferability across domains.


<details>
  <summary>Details</summary>
Motivation: To understand the exact mechanisms behind memorization in LLMs, including how and where memorized sequences are triggered and maintained.

Method: Uses transformer circuits and contrastive datasets to isolate and analyze circuits responsible for memorization initiation and maintenance.

Result: Identifies distinct circuits for memorization initiation and maintenance, with initiation circuits also capable of maintenance. Memorization prevention transfers across domains, while initiation is context-dependent.

Conclusion: Memorization in LLMs involves specialized circuits with varying transferability, offering insights for mechanistic interpretability and model behavior.

Abstract: Underlying mechanisms of memorization in LLMs -- the verbatim reproduction of
training data -- remain poorly understood. What exact part of the network
decides to retrieve a token that we would consider as start of memorization
sequence? How exactly is the models' behaviour different when producing
memorized sentence vs non-memorized? In this work we approach these questions
from mechanistic interpretability standpoint by utilizing transformer circuits
-- the minimal computational subgraphs that perform specific functions within
the model. Through carefully constructed contrastive datasets, we identify
points where model generation diverges from memorized content and isolate the
specific circuits responsible for two distinct aspects of memorization. We find
that circuits that initiate memorization can also maintain it once started,
while circuits that only maintain memorization cannot trigger its initiation.
Intriguingly, memorization prevention mechanisms transfer robustly across
different text domains, while memorization induction appears more
context-dependent.

</details>


### [190] [A General Method for Detecting Information Generated by Large Language Models](https://arxiv.org/abs/2506.21589)
*Minjia Mao,Dongjun Wei,Xiao Fang,Michael Chau*

Main category: cs.CL

TL;DR: The paper introduces a general LLM detector (GLD) to identify LLM-generated content across unseen models and domains, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The need to detect LLM-generated content for trust and misinformation prevention, given the limitations of current methods in generalizing to new LLMs and domains.

Method: GLD combines twin memory networks and a theory-guided detection generalization module.

Result: Empirical evaluations show GLD's superiority over state-of-the-art detection methods.

Conclusion: GLD offers academic and practical benefits for digital platforms and LLMs.

Abstract: The proliferation of large language models (LLMs) has significantly
transformed the digital information landscape, making it increasingly
challenging to distinguish between human-written and LLM-generated content.
Detecting LLM-generated information is essential for preserving trust on
digital platforms (e.g., social media and e-commerce sites) and preventing the
spread of misinformation, a topic that has garnered significant attention in IS
research. However, current detection methods, which primarily focus on
identifying content generated by specific LLMs in known domains, face
challenges in generalizing to new (i.e., unseen) LLMs and domains. This
limitation reduces their effectiveness in real-world applications, where the
number of LLMs is rapidly multiplying and content spans a vast array of
domains. In response, we introduce a general LLM detector (GLD) that combines a
twin memory networks design and a theory-guided detection generalization module
to detect LLM-generated information across unseen LLMs and domains. Using
real-world datasets, we conduct extensive empirical evaluations and case
studies to demonstrate the superiority of GLD over state-of-the-art detection
methods. The study has important academic and practical implications for
digital platforms and LLMs.

</details>


### [191] [Representation Consistency for Accurate and Coherent LLM Answer Aggregation](https://arxiv.org/abs/2506.21590)
*Junqi Jiang,Tom Bewley,Salim I. Amoukou,Francesco Leofante,Antonio Rago,Saumitra Mishra,Francesca Toni*

Main category: cs.CL

TL;DR: Representation consistency (RC) improves LLM performance by aggregating answers from multiple candidate responses, considering both answer frequency and internal activation consistency, without extra model queries.


<details>
  <summary>Details</summary>
Motivation: Existing test-time scaling methods for LLMs require complex modifications. RC aims to simplify and enhance answer aggregation by leveraging internal model activations.

Method: RC aggregates answers by analyzing the consistency of internal activations (dense or sparse) across multiple responses, down-weighting incoherent answers.

Result: Experiments show RC improves accuracy by up to 4% over baselines, with sparse activations aligning well with coherent reasoning.

Conclusion: RC is an effective, lightweight method for enhancing LLM performance during inference by leveraging representation consistency.

Abstract: Test-time scaling improves large language models' (LLMs) performance by
allocating more compute budget during inference. To achieve this, existing
methods often require intricate modifications to prompting and sampling
strategies. In this work, we introduce representation consistency (RC), a
test-time scaling method for aggregating answers drawn from multiple candidate
responses of an LLM regardless of how they were generated, including variations
in prompt phrasing and sampling strategy. RC enhances answer aggregation by not
only considering the number of occurrences of each answer in the candidate
response set, but also the consistency of the model's internal activations
while generating the set of responses leading to each answer. These activations
can be either dense (raw model activations) or sparse (encoded via pretrained
sparse autoencoders). Our rationale is that if the model's representations of
multiple responses converging on the same answer are highly variable, this
answer is more likely to be the result of incoherent reasoning and should be
down-weighted during aggregation. Importantly, our method only uses cached
activations and lightweight similarity computations and requires no additional
model queries. Through experiments with four open-source LLMs and four
reasoning datasets, we validate the effectiveness of RC for improving task
performance during inference, with consistent accuracy improvements (up to 4%)
over strong test-time scaling baselines. We also show that consistency in the
sparse activation signals aligns well with the common notion of coherent
reasoning.

</details>


### [192] [FinEval-KR: A Financial Domain Evaluation Framework for Large Language Models' Knowledge and Reasoning](https://arxiv.org/abs/2506.21591)
*Shaoyu Dou,Yutian Shen,Mofan Chen,Zixuan Wang,Jiajie Xu,Qi Guo,Kailai Shao,Chao Chen,Haixiang Hu,Haibo Shi,Min Min,Liwen Zhang*

Main category: cs.CL

TL;DR: FinEval-KR is a new framework to evaluate LLMs' financial reasoning by decoupling knowledge and reasoning abilities, using distinct scores and cognitive analysis. It includes a Chinese dataset and reveals key insights about LLM performance.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks inadequately assess LLMs in financial reasoning by not separating knowledge and reasoning abilities or analyzing task failures.

Method: Introduces FinEval-KR with knowledge and reasoning scores, a cognitive score based on Bloom's taxonomy, and a new Chinese financial dataset.

Result: LLM reasoning and higher-order cognitive abilities are key to accuracy, but knowledge application remains a bottleneck. Specialized financial LLMs lag behind general models.

Conclusion: FinEval-KR provides a robust framework for evaluating LLMs in financial reasoning, highlighting gaps in knowledge application and the superiority of general models.

Abstract: Large Language Models (LLMs) demonstrate significant potential but face
challenges in complex financial reasoning tasks requiring both domain knowledge
and sophisticated reasoning. Current evaluation benchmarks often fall short by
not decoupling these capabilities indicators from single task performance and
lack root cause analysis for task failure. To address this, we introduce
FinEval-KR, a novel evaluation framework for decoupling and quantifying LLMs'
knowledge and reasoning abilities independently, proposing distinct knowledge
score and reasoning score metrics. Inspired by cognitive science, we further
propose a cognitive score based on Bloom's taxonomy to analyze capabilities in
reasoning tasks across different cognitive levels. We also release a new
open-source Chinese financial reasoning dataset covering 22 subfields to
support reproducible research and further advancements in financial reasoning.
Our experimental results reveal that LLM reasoning ability and higher-order
cognitive ability are the core factors influencing reasoning accuracy. We also
specifically find that even top models still face a bottleneck with knowledge
application. Furthermore, our analysis shows that specialized financial LLMs
generally lag behind the top general large models across multiple metrics.

</details>


### [193] [SignBart -- New approach with the skeleton sequence for Isolated Sign language Recognition](https://arxiv.org/abs/2506.21592)
*Tinh Nguyen,Minh Khue Phan Tran*

Main category: cs.CL

TL;DR: A novel SLR approach using BART architecture achieves high accuracy (96.04%) with fewer parameters, outperforming traditional models by independently processing x and y coordinates while maintaining their interrelation.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between efficiency and accuracy in sign language recognition (SLR) and overcome limitations of traditional models like RNNs, LSTMs, and GCNs.

Method: Utilizes an encoder-decoder BART architecture to independently encode x and y coordinates of skeleton sequences, with Cross-Attention ensuring their interrelation.

Result: Achieves 96.04% accuracy on LSA-64 dataset, outperforming larger models, and shows strong generalization on WLASL and ASL-Citizen datasets.

Conclusion: The study presents a reliable, efficient SLR method with potential to enhance accessibility tools for the deaf and hard of hearing.

Abstract: Sign language recognition is crucial for individuals with hearing impairments
to break communication barriers. However, previous approaches have had to
choose between efficiency and accuracy. Such as RNNs, LSTMs, and GCNs, had
problems with vanishing gradients and high computational costs. Despite
improving performance, transformer-based methods were not commonly used. This
study presents a new novel SLR approach that overcomes the challenge of
independently extracting meaningful information from the x and y coordinates of
skeleton sequences, which traditional models often treat as inseparable. By
utilizing an encoder-decoder of BART architecture, the model independently
encodes the x and y coordinates, while Cross-Attention ensures their
interrelation is maintained. With only 749,888 parameters, the model achieves
96.04% accuracy on the LSA-64 dataset, significantly outperforming previous
models with over one million parameters. The model also demonstrates excellent
performance and generalization across WLASL and ASL-Citizen datasets. Ablation
studies underscore the importance of coordinate projection, normalization, and
using multiple skeleton components for boosting model efficacy. This study
offers a reliable and effective approach for sign language recognition, with
strong potential for enhancing accessibility tools for the deaf and hard of
hearing.

</details>


### [194] [Gazal-R1: Achieving State-of-the-Art Medical Reasoning with Parameter-Efficient Two-Stage Training](https://arxiv.org/abs/2506.21594)
*Ahmed M. Adly,Mostafa Samy,Amr Fawzy*

Main category: cs.CL

TL;DR: Gazal-R1 is a 32B-parameter medical reasoning model with transparent explanations, outperforming larger models through strategic training and a novel two-stage pipeline.


<details>
  <summary>Details</summary>
Motivation: To develop a high-performance, explainable medical reasoning model that balances efficiency and accuracy in specialized domains.

Method: Two-stage training: supervised fine-tuning on synthetic medical data with parameter-efficient techniques (DoRA, rsLoRA), followed by reinforcement learning (GRPO) with a multi-component reward system.

Result: Achieves 87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing larger models.

Conclusion: The work provides a reproducible framework for domain-specific models, addressing challenges like reward hacking and balancing recall with reasoning.

Abstract: We present Gazal-R1, a 32-billion-parameter language model that achieves
state-of-the-art performance in medical reasoning while providing transparent,
step-by-step explanations for clinical decision-making. Built upon Qwen3 32B,
our model demonstrates that strategic training can enable mid-sized models to
outperform significantly larger counterparts in specialized domains. We
developed a novel two-stage training pipeline: first, supervised fine-tuning on
a carefully curated dataset of 107,033 synthetic medical reasoning examples
that teaches structured clinical thinking, enhanced by advanced
parameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation
(DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using
Group Relative Policy Optimization (GRPO) with a sophisticated multi-component
reward system that refines accuracy, format adherence, and reasoning quality.
Gazal-R1 achieves exceptional performance across medical benchmarks, scoring
87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing
models up to 12x larger. Beyond its strong empirical results, this work
provides detailed insights into the challenges of training reasoning-capable
models in specialized domains, including issues with reward hacking, training
instability, and the fundamental tension between factual recall and detailed
reasoning. Our methodology offers a reproducible framework for developing
high-capability, domain-specific language models that balance performance,
efficiency, and explainability.

</details>


### [195] [Thunder-LLM: Efficiently Adapting LLMs to Korean with Minimal Resources](https://arxiv.org/abs/2506.21595)
*Jinpyo Kim,Gyeongje Cho,Chanwoo Park,Jongwon Park,Jongmin Kim,Yeonkyoun So,Jaejin Lee*

Main category: cs.CL

TL;DR: The paper presents a low-budget method to adapt English-based LLMs to Korean, detailing data collection, preprocessing, training, and evaluation. The resulting models, Thunder-LLM and Thunder-LLM-Ins, outperform state-of-the-art models in Korean with minimal resources.


<details>
  <summary>Details</summary>
Motivation: Improving LLM performance in non-English languages like Korean is crucial, as current models often underperform. The lack of transparency in LLM training processes also motivates this work.

Method: The method involves collecting Korean datasets, preprocessing, training the model, creating benchmarks, and evaluating performance.

Result: The adapted models achieve superior Korean performance with minimal data and computational resources.

Conclusion: The approach is effective and cost-efficient for adding new language capabilities to LLMs, and the code is shared publicly.

Abstract: Since state-of-the-art LLMs often underperform in languages other than
English or Chinese, improving the capability of LLMs in new languages has
become an essential task. Moreover, LLMs' entire end-to-end training process
remains largely unknown to the public due to proprietary reasons, technical
complexity, inconsistent documentation, and ethical considerations. The
complete picture remains a closely guarded secret within the industry. This
paper presents methods to adapt an existing English-based LLM to Korean in a
low-budget scenario. We describe the entire end-to-end process: collecting
Korean datasets, preprocessing the data, training the model, creating
downstream benchmarks, and conducting evaluations. The evaluation results
indicate that our method can effectively and cost-efficiently add new language
capabilities to existing LLMs. Our new bilingual models, Thunder-LLM and
Thunder-LLM-Ins, achieve superior Korean performance compared to
state-of-the-art models while utilizing minimal data and computational
resources. We share our comprehensive experience and make the code publicly
available.

</details>


### [196] [Evaluating Multimodal Large Language Models on Educational Textbook Question Answering](https://arxiv.org/abs/2506.21596)
*Hessa A. Alawwad,Anas Zafar,Areej Alhothali,Usman Naseem,Ali Alkhathlan,Amani Jamal*

Main category: cs.CL

TL;DR: The paper evaluates MLLMs on textbook QA using CK12-QA, introduces a RAG pipeline, and highlights limitations in handling complex educational content.


<details>
  <summary>Details</summary>
Motivation: To test MLLMs' reasoning on complex educational lessons and diagrams, which are not adequately represented as single images.

Method: Evaluates MLLMs (e.g., LLaVA, LLaMA 3.2-Vision) on TQA with CK12-QA, and introduces a lightweight multimodal RAG pipeline integrating text and diagrams.

Result: Shows the impact of retrieved educational context on accuracy and reasoning but reveals limitations in question-context relationships and noise handling.

Conclusion: Identifies key areas for future research in multimodal AI-driven learning, emphasizing the need for improved context integration.

Abstract: Multimodal large language models (MLLMs) have recently achieved significant
success in vision--language tasks. However, their capacity to reason over
complex, long lessons and intricate educational diagrams that cannot be
represented as a single natural image remains largely untested. In this work,
we present the first evaluation of state-of-the-art MLLMs on the textbook
question answering (TQA) task using the CK12-QA dataset. We assess the
performance of recent vision-language models, including LLaVA and LLaMA
3.2-Vision, across various input configurations. Additionally, we introduce a
lightweight multimodal retrieval-augmented generation (RAG) pipeline that
integrates both paragraphs and diagrams from the lesson into the prompt. Our
results demonstrate the influence of retrieved educational context on model
accuracy and reasoning, while also revealing current limitations in handling
question-context relationships and the potential for noise, pointing to key
directions for future research in multimodal AI-driven learning.

</details>


### [197] [Overview of the ClinIQLink 2025 Shared Task on Medical Question-Answering](https://arxiv.org/abs/2506.21597)
*Brandon Colelough,Davis Bartels,Dina Demner-Fushman*

Main category: cs.CL

TL;DR: ClinIQLink is a shared task at BioNLP 2025 to evaluate LLMs on medical QA for General Practitioners, using 4,978 expert-verified Q&A pairs across seven formats. Systems are tested via automated scoring and physician audits.


<details>
  <summary>Details</summary>
Motivation: To assess the capability of large language models in handling medically-oriented questions at a General Practitioner level.

Method: Uses 4,978 expert-verified Q&A pairs in seven formats. Systems are evaluated via automated scoring (exact match for closed-ended, embedding metric for open-ended) and physician audits.

Result: Not explicitly stated in the abstract, but the task aims to benchmark LLM performance on medical QA.

Conclusion: ClinIQLink provides a rigorous framework to evaluate LLMs in medical QA, combining automated and expert human assessment.

Abstract: In this paper, we present an overview of ClinIQLink, a shared task,
collocated with the 24th BioNLP workshop at ACL 2025, designed to stress-test
large language models (LLMs) on medically-oriented question answering aimed at
the level of a General Practitioner. The challenge supplies 4,978
expert-verified, medical source-grounded question-answer pairs that cover seven
formats: true/false, multiple choice, unordered list, short answer,
short-inverse, multi-hop, and multi-hop-inverse. Participating systems, bundled
in Docker or Apptainer images, are executed on the CodaBench platform or the
University of Maryland's Zaratan cluster. An automated harness (Task 1) scores
closed-ended items by exact match and open-ended items with a three-tier
embedding metric. A subsequent physician panel (Task 2) audits the top model
responses.

</details>


### [198] [Structured Attention Matters to Multimodal LLMs in Document Understanding](https://arxiv.org/abs/2506.21600)
*Chang Liu,Hongkai Chen,Yujun Cai,Hang Wu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.CL

TL;DR: Input format significantly impacts MLLMs' document comprehension. Structured text (LaTex paradigm) outperforms raw OCR by preserving hierarchy and spatial relationships, enhancing performance without model changes.


<details>
  <summary>Details</summary>
Motivation: Prior work focused on locating evidence pages, but overlooked how input format affects comprehension. Raw OCR text often harms performance due to attention dispersion and structure loss.

Method: Proposed a structure-preserving approach using LaTex paradigm to encode document elements, maintaining hierarchy and spatial relationships. Analyzed attention patterns.

Result: Structured text improves MLLMs' question answering performance across document types, reducing attention waste and focusing on meaningful regions.

Conclusion: Structured input formats like LaTex enhance document understanding in MLLMs without requiring architectural changes or additional training.

Abstract: Document understanding remains a significant challenge for multimodal large
language models (MLLMs). While previous research has primarily focused on
locating evidence pages through precise multimodal queries, our work
investigates a fundamental yet overlooked aspect: how input format influences
document comprehension performance. Through systematic analysis, we discover
that raw OCR text often impairs rather than improves MLLMs' performance, which
is a counterintuitive finding we attribute to attention dispersion and
structure loss. To further substantiate our hypothesis, we propose a novel
structure-preserving approach that encodes document elements using the LaTex
paradigm, maintaining the hierarchical organization and spatial relationships
critical for comprehension. Our attention analysis reveals that structured text
induces structured attention patterns on both textual and visual content,
directing models to focus on semantically meaningful regions while reducing
attention waste. This approach significantly enhances MLLMs' document question
answering performance across diverse document types without requiring
architectural modifications or additional training.

</details>


### [199] [BiMark: Unbiased Multilayer Watermarking for Large Language Models](https://arxiv.org/abs/2506.21602)
*Xiaoyan Feng,He Zhang,Yanjun Zhang,Leo Yu Zhang,Shirui Pan*

Main category: cs.CL

TL;DR: BiMark is a new watermarking framework for LLM-generated text that balances quality preservation, model-agnostic detection, and message embedding capacity. It outperforms existing methods in extraction rates and maintains text quality.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for reliable identification of LLM-generated text while preserving quality and enabling model-agnostic detection.

Method: BiMark uses a bit-flip unbiased reweighting mechanism, multilayer architecture, and information encoding for multi-bit watermarking.

Result: Achieves 30% higher extraction rates for short texts, maintains lower perplexity, and performs comparably to non-watermarked text in downstream tasks.

Conclusion: BiMark successfully balances the trade-offs in watermarking, offering a practical solution for LLM-generated text identification.

Abstract: Recent advances in Large Language Models (LLMs) have raised urgent concerns
about LLM-generated text authenticity, prompting regulatory demands for
reliable identification mechanisms. Although watermarking offers a promising
solution, existing approaches struggle to simultaneously achieve three critical
requirements: text quality preservation, model-agnostic detection, and message
embedding capacity, which are crucial for practical implementation. To achieve
these goals, the key challenge lies in balancing the trade-off between text
quality preservation and message embedding capacity. To address this challenge,
we propose BiMark, a novel watermarking framework that achieves these
requirements through three key innovations: (1) a bit-flip unbiased reweighting
mechanism enabling model-agnostic detection, (2) a multilayer architecture
enhancing detectability without compromising generation quality, and (3) an
information encoding approach supporting multi-bit watermarking. Through
theoretical analysis and extensive experiments, we validate that, compared to
state-of-the-art multi-bit watermarking methods, BiMark achieves up to 30%
higher extraction rates for short texts while maintaining text quality
indicated by lower perplexity, and performs comparably to non-watermarked text
on downstream tasks such as summarization and translation.

</details>


### [200] [Operationalizing Automated Essay Scoring: A Human-Aware Approach](https://arxiv.org/abs/2506.21603)
*Yenisel Plasencia-Calaña*

Main category: cs.CL

TL;DR: The paper compares ML-based and LLM-based AES systems, highlighting trade-offs in accuracy, explainability, bias, and robustness.


<details>
  <summary>Details</summary>
Motivation: To operationalize AES systems human-centrically by addressing dimensions like bias, robustness, and explainability beyond just accuracy.

Method: Comparison of ML-based and LLM-based AES approaches, analyzing key dimensions such as bias, robustness, and explainability.

Result: ML-based models are more accurate but lack explainability; LLMs offer better explanations but both struggle with bias and robustness.

Conclusion: The study identifies challenges and trade-offs, aiming to improve the reliability and trustworthiness of AES systems.

Abstract: This paper explores the human-centric operationalization of Automated Essay
Scoring (AES) systems, addressing aspects beyond accuracy. We compare various
machine learning-based approaches with Large Language Models (LLMs) approaches,
identifying their strengths, similarities and differences. The study
investigates key dimensions such as bias, robustness, and explainability,
considered important for human-aware operationalization of AES systems. Our
study shows that ML-based AES models outperform LLMs in accuracy but struggle
with explainability, whereas LLMs provide richer explanations. We also found
that both approaches struggle with bias and robustness to edge scores. By
analyzing these dimensions, the paper aims to identify challenges and
trade-offs between different methods, contributing to more reliable and
trustworthy AES methods.

</details>


### [201] [MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents](https://arxiv.org/abs/2506.21605)
*Haoran Tan,Zeyu Zhang,Chen Ma,Xu Chen,Quanyu Dai,Zhenhua Dong*

Main category: cs.CL

TL;DR: The paper introduces MemBench, a dataset and benchmark to evaluate the memory capabilities of LLM-based agents, addressing gaps in diversity and metrics.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of memory mechanisms in LLM-based agents lack diversity in memory levels and interactive scenarios, and comprehensive metrics.

Method: Constructed a dataset with factual and reflective memory levels, and participation/observation scenarios, then developed the MemBench benchmark.

Result: MemBench evaluates memory capabilities in terms of effectiveness, efficiency, and capacity, with released dataset and project.

Conclusion: The work provides a comprehensive tool for evaluating LLM-based agents' memory, benefiting the research community.

Abstract: Recent works have highlighted the significance of memory mechanisms in
LLM-based agents, which enable them to store observed information and adapt to
dynamic environments. However, evaluating their memory capabilities still
remains challenges. Previous evaluations are commonly limited by the diversity
of memory levels and interactive scenarios. They also lack comprehensive
metrics to reflect the memory capabilities from multiple aspects. To address
these problems, in this paper, we construct a more comprehensive dataset and
benchmark to evaluate the memory capability of LLM-based agents. Our dataset
incorporates factual memory and reflective memory as different levels, and
proposes participation and observation as various interactive scenarios. Based
on our dataset, we present a benchmark, named MemBench, to evaluate the memory
capability of LLM-based agents from multiple aspects, including their
effectiveness, efficiency, and capacity. To benefit the research community, we
release our dataset and project at https://github.com/import-myself/Membench.

</details>


### [202] [Large Language Models as symbolic DNA of cultural dynamics](https://arxiv.org/abs/2506.21606)
*Parham Pourdavood,Michael Jacob,Terrence Deacon*

Main category: cs.CL

TL;DR: LLMs are likened to DNA for human culture, serving as repositories of compressed symbolic patterns that require human reinterpretation to become meaningful, fostering creativity and cultural evolvability.


<details>
  <summary>Details</summary>
Motivation: To reframe LLMs not as autonomous intelligences or mimicry but as tools preserving cultural patterns, enabling human self-reflection and hypothesis-generation.

Method: Analyzes four features—compression, decompression, externalization, and recursion—to compare LLMs to DNA in preserving cultural dynamics.

Result: LLMs act as cultural substrates, preserving patterns without understanding, and catalyze human creativity through reinterpretation.

Conclusion: LLMs are significant as tools for cultural evolvability, aiding human self-reflection and hypothesis-generation without replacing human intelligence.

Abstract: This paper proposes a novel conceptualization of Large Language Models (LLMs)
as externalized informational substrates that function analogously to DNA for
human cultural dynamics. Rather than viewing LLMs as either autonomous
intelligence or mere programmed mimicry, we argue they serve a broader role as
repositories that preserve compressed patterns of human symbolic
expression--"fossils" of meaningful dynamics that retain relational residues
without their original living contexts. Crucially, these compressed patterns
only become meaningful through human reinterpretation, creating a recursive
feedback loop where they can be recombined and cycle back to ultimately
catalyze human creative processes. Through analysis of four universal
features--compression, decompression, externalization, and recursion--we
demonstrate that just as DNA emerged as a compressed and externalized medium
for preserving useful cellular dynamics without containing explicit reference
to goal-directed physical processes, LLMs preserve useful regularities of human
culture without containing understanding of embodied human experience.
Therefore, we argue that LLMs' significance lies not in rivaling human
intelligence, but in providing humanity a tool for self-reflection and playful
hypothesis-generation in a low-stakes, simulated environment. This framework
positions LLMs as tools for cultural evolvability, enabling humanity to
generate novel hypotheses about itself while maintaining the human
interpretation necessary to ground these hypotheses in ongoing human aesthetics
and norms.

</details>


### [203] [CORE-KG: An LLM-Driven Knowledge Graph Construction Framework for Human Smuggling Networks](https://arxiv.org/abs/2506.21607)
*Dipak Meher,Carlotta Domeniconi,Guadalupe Correa-Cabrera*

Main category: cs.CL

TL;DR: CORE-KG is a modular framework for constructing cleaner and more coherent knowledge graphs from legal texts, reducing node duplication and noise compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Human smuggling networks are complex and adaptive, but legal case documents are unstructured and challenging for automated KG construction due to ambiguity and shifting references.

Method: CORE-KG uses a two-step pipeline: (1) type-aware coreference resolution with structured LLM prompts, and (2) domain-guided entity and relationship extraction on an adapted GraphRAG framework.

Result: CORE-KG reduces node duplication by 33.28% and legal noise by 38.37%, producing cleaner graphs.

Conclusion: CORE-KG provides a robust foundation for analyzing complex criminal networks, outperforming existing KG methods.

Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer valuable insights but are unstructured, lexically
dense, and filled with ambiguous or shifting references-posing challenges for
automated knowledge graph (KG) construction. Existing KG methods often rely on
static templates and lack coreference resolution, while recent LLM-based
approaches frequently produce noisy, fragmented graphs due to hallucinations,
and duplicate nodes caused by a lack of guided extraction. We propose CORE-KG,
a modular framework for building interpretable KGs from legal texts. It uses a
two-step pipeline: (1) type-aware coreference resolution via sequential,
structured LLM prompts, and (2) entity and relationship extraction using
domain-guided instructions, built on an adapted GraphRAG framework. CORE-KG
reduces node duplication by 33.28%, and legal noise by 38.37% compared to a
GraphRAG-based baseline-resulting in cleaner and more coherent graph
structures. These improvements make CORE-KG a strong foundation for analyzing
complex criminal networks.

</details>


### [204] [SysTemp: A Multi-Agent System for Template-Based Generation of SysML v2](https://arxiv.org/abs/2506.21608)
*Yasmine Bouamra,Bruno Yun,Alexandre Poisson,Frédéric Armetta*

Main category: cs.CL

TL;DR: SysTemp is a multi-agent system for generating SysML v2 models from natural language, addressing challenges like scarce corpora and complex syntax.


<details>
  <summary>Details</summary>
Motivation: The scarcity of learning corpora and complex syntax in SysML v2 modeling makes automatic generation difficult.

Method: Uses a multi-agent system with a template generator to structure the generation process.

Result: Evaluation shows potential to improve generation quality in SysML v2 modeling.

Conclusion: SysTemp facilitates and enhances SysML v2 model creation from natural language.

Abstract: The automatic generation of SysML v2 models represents a major challenge in
the engineering of complex systems, particularly due to the scarcity of
learning corpora and complex syntax. We present SysTemp, a system aimed at
facilitating and improving the creation of SysML v2 models from natural
language specifications. It is based on a multi-agent system, including a
template generator that structures the generation process. We discuss the
advantages and challenges of this system through an evaluation, highlighting
its potential to improve the quality of the generations in SysML v2 modeling.

</details>


### [205] [From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models](https://arxiv.org/abs/2506.21609)
*Junhao Liu,Zhenhao Xu,Yuxin Fang,Yichuan Chen,Zuobin Ying,Wenhan Chang*

Main category: cs.CL

TL;DR: The paper introduces a framework to analyze reasoning in four advanced LLMs, comparing their processes and outputs using keyword statistics and LLM-as-a-judge. It reveals disparities in reasoning depth and patterns.


<details>
  <summary>Details</summary>
Motivation: Existing research lacks systematic comparison of LLMs' reasoning processes, especially self-reflection and cross-domain connections.

Method: Proposes a framework using keyword statistics and LLM-as-a-judge to analyze reasoning in four models (GPT-o1, DeepSeek-R1, Kimi-k1.5, Grok-3) with a diverse dataset.

Result: Identifies differences in reasoning depth, intermediate step reliance, and similarity to GPT-o1. Highlights trade-offs between efficiency and robustness.

Conclusion: Provides insights for improving model design and evaluation, with practical recommendations. The project is publicly available.

Abstract: Recently, there have been notable advancements in large language models
(LLMs), demonstrating their growing abilities in complex reasoning. However,
existing research largely overlooks a thorough and systematic comparison of
these models' reasoning processes and outputs, particularly regarding their
self-reflection pattern (also termed "Aha moment") and the interconnections
across diverse domains. This paper proposes a novel framework for analyzing the
reasoning characteristics of four cutting-edge large reasoning models (GPT-o1,
DeepSeek-R1, Kimi-k1.5, and Grok-3) using keywords statistic and LLM-as-a-judge
paradigm. Our approach connects their internal thinking processes with their
final outputs. A diverse dataset consists of real-world scenario-based
questions covering logical deduction, causal inference, and multi-step
problem-solving. Additionally, a set of metrics is put forward to assess both
the coherence of reasoning and the accuracy of the outputs. The research
results uncover various patterns of how these models balance exploration and
exploitation, deal with problems, and reach conclusions during the reasoning
process. Through quantitative and qualitative comparisons, disparities among
these models are identified in aspects such as the depth of reasoning, the
reliance on intermediate steps, and the degree of similarity between their
thinking processes and output patterns and those of GPT-o1. This work offers
valuable insights into the trade-off between computational efficiency and
reasoning robustness and provides practical recommendations for enhancing model
design and evaluation in practical applications. We publicly release our
project at: https://github.com/ChangWenhan/FromThinking2Output

</details>


### [206] [Does Multimodality Lead to Better Time Series Forecasting?](https://arxiv.org/abs/2506.21611)
*Xiyuan Zhang,Boran Han,Haoyang Fang,Abdul Fatir Ansari,Shuai Zhang,Danielle C. Maddix,Cuixiong Hu,Andrew Gordon Wilson,Michael W. Mahoney,Hao Wang,Yan Liu,Huzefa Rangwala,George Karypis,Bernie Wang*

Main category: cs.CL

TL;DR: The paper investigates the effectiveness of integrating textual information into time series forecasting models, finding that gains are not universal and depend on model and data conditions.


<details>
  <summary>Details</summary>
Motivation: To determine whether and under what conditions multimodal integration (text + time series) improves forecasting performance.

Method: Systematic evaluation of two multimodal paradigms (aligning-based and prompting-based) across 14 forecasting tasks in 7 domains.

Result: Multimodal methods do not always outperform unimodal baselines; gains depend on model capacity, alignment strategies, data sufficiency, and text's complementary signal.

Conclusion: Practical guidelines are provided for when multimodality aids forecasting, emphasizing model and data conditions.

Abstract: Recently, there has been growing interest in incorporating textual
information into foundation models for time series forecasting. However, it
remains unclear whether and under what conditions such multimodal integration
consistently yields gains. We systematically investigate these questions across
a diverse benchmark of 14 forecasting tasks spanning 7 domains, including
health, environment, and economics. We evaluate two popular multimodal
forecasting paradigms: aligning-based methods, which align time series and text
representations; and prompting-based methods, which directly prompt large
language models for forecasting. Although prior works report gains from
multimodal input, we find these effects are not universal across datasets and
models, and multimodal methods sometimes do not outperform the strongest
unimodal baselines. To understand when textual information helps, we
disentangle the effects of model architectural properties and data
characteristics. Our findings highlight that on the modeling side,
incorporating text information is most helpful given (1) high-capacity text
models, (2) comparatively weaker time series models, and (3) appropriate
aligning strategies. On the data side, performance gains are more likely when
(4) sufficient training data is available and (5) the text offers complementary
predictive signal beyond what is already captured from the time series alone.
Our empirical findings offer practical guidelines for when multimodality can be
expected to aid forecasting tasks, and when it does not.

</details>


### [207] [AdaptGOT: A Pre-trained Model for Adaptive Contextual POI Representation Learning](https://arxiv.org/abs/2506.21612)
*Xiaobin Ren,Xinyu Zhu,Kaiqi Zhao*

Main category: cs.CL

TL;DR: The paper introduces AdaptGOT, a model combining adaptive representation learning and GOT (Geographical-Co-Occurrence-Text) to address challenges in POI embedding like multi-context sampling and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing POI embedding methods lack effective multi-context sampling, versatility, and generalization. AdaptGOT aims to overcome these limitations.

Method: AdaptGOT integrates adaptive learning and GOT representation with three components: contextual neighborhood generation, advanced GOT representation with attention, and MoE-based adaptive encoder-decoder.

Result: Experiments on real-world datasets show AdaptGOT outperforms existing methods in POI tasks.

Conclusion: AdaptGOT effectively addresses POI embedding challenges, offering superior performance and versatility.

Abstract: Currently, considerable strides have been achieved in Point-of-Interest (POI)
embedding methodologies, driven by the emergence of novel POI tasks like
recommendation and classification. Despite the success of task-specific,
end-to-end models in POI embedding, several challenges remain. These include
the need for more effective multi-context sampling strategies, insufficient
exploration of multiple POI contexts, limited versatility, and inadequate
generalization. To address these issues, we propose the AdaptGOT model, which
integrates both the (Adapt)ive representation learning technique and the
Geographical-Co-Occurrence-Text (GOT) representation with a particular emphasis
on Geographical location, Co-Occurrence and Textual information. The AdaptGOT
model comprises three key components: (1) contextual neighborhood generation,
which integrates advanced mixed sampling techniques such as KNN, density-based,
importance-based, and category-aware strategies to capture complex contextual
neighborhoods; (2) an advanced GOT representation enhanced by an attention
mechanism, designed to derive high-quality, customized representations and
efficiently capture complex interrelations between POIs; and (3) the MoE-based
adaptive encoder-decoder architecture, which ensures topological consistency
and enriches contextual representation by minimizing Jensen-Shannon divergence
across varying contexts. Experiments on two real-world datasets and multiple
POI tasks substantiate the superior performance of the proposed AdaptGOT model.

</details>


### [208] [ChildGuard: A Specialized Dataset for Combatting Child-Targeted Hate Speech](https://arxiv.org/abs/2506.21613)
*Gautam Siddharth Kashyap,Mohammad Anas Azeez,Rafiq Ali,Zohaib Hasan Siddiqui,Jiechao Gao,Usman Naseem*

Main category: cs.CL

TL;DR: ChildGuard1 is a specialized dataset for child-targeted hate speech, addressing gaps in existing datasets by including age-specific annotations and nuanced contexts.


<details>
  <summary>Details</summary>
Motivation: The rise of child-targeted hate speech online highlights the need for age-specific datasets to better understand and mitigate its impact.

Method: ChildGuard1 is curated from existing corpora and enriched with child-specific annotations. Existing hate speech detection methods, including LLMs, are benchmarked.

Result: The dataset provides a foundation for improving detection and contextualization of child-targeted hate speech.

Conclusion: ChildGuard1 is released publicly to support further research and development of better detection methods.

Abstract: The increasing prevalence of child-targeted hate speech online underscores
the urgent need for specialized datasets to address this critical issue.
Existing hate speech datasets lack agespecific annotations, fail to capture
nuanced contexts, and overlook the unique emotional impact on children. To
bridge this gap, we introduce ChildGuard1, a curated dataset derived from
existing corpora and enriched with child-specific annotations. ChildGuard
captures diverse contexts of child-targeted hate speech, spanning age groups.
We benchmark existing state-of-the-art hate speech detection methods, including
Large Language Models (LLMs), and assess their effectiveness in detecting and
contextualizing child-targeted hate speech. To foster further research in this
area, we publicly release ChildGuard, providing a robust foundation for
developing improved methods to detect and mitigate such harm.

</details>


### [209] [LastingBench: Defend Benchmarks Against Knowledge Leakage](https://arxiv.org/abs/2506.21614)
*Yixiong Fang,Tianran Sun,Yuling Shi,Min Wang,Xiaodong Gu*

Main category: cs.CL

TL;DR: LastingBench is a framework to mitigate data leakage in QA benchmarks by identifying and rewriting leakage points, ensuring fairer LLM evaluations.


<details>
  <summary>Details</summary>
Motivation: Address concerns about LLMs cheating on QA benchmarks due to memorization, undermining benchmark validity.

Method: Introduces LastingBench, which identifies leakage points via perturbation and rewrites them to counterfactual ones.

Result: Significant performance gaps in QA benchmarks, showing reduced memorization effects.

Conclusion: LastingBench ensures benchmark robustness, promoting fairer and interpretable LLM evaluations.

Abstract: The increasing complexity of large language models (LLMs) raises concerns
about their ability to "cheat" on standard Question Answering (QA) benchmarks
by memorizing task-specific data. This undermines the validity of benchmark
evaluations, as they no longer reflect genuine model capabilities but instead
the effects of data leakage. While prior work has focused on detecting such
leakage, little attention has been given to mitigating its impact and
preserving the long-term utility of benchmarks. In this paper, we introduce
LastingBench, a novel framework designed to continuously reinforce and
safeguard existing benchmarks against knowledge leakage. LastingBench
identifies leakage points in the context through perturbation, then rewrites
the leakage points to counterfactual ones-disrupting memorization while
preserving the benchmark's original evaluative intent. Evaluations of
state-of-the-art QA benchmarks show significant performance gaps, highlighting
the efficacy of LastingBench in reducing memorization effects. LastingBench
offers a practical and scalable solution to ensure benchmark robustness over
time, promoting fairer and more interpretable evaluations of LLMs.

</details>


### [210] [Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines](https://arxiv.org/abs/2506.21615)
*Wenhao Li,Hongkuan Zhang,Hongwei Zhang,Zhengxu Li,Zengjie Dong,Yafan Chen,Niranjan Bidargaddi,Hong Liu*

Main category: cs.CL

TL;DR: GARMLE-G is a framework that enhances medical language models by grounding their outputs in clinical practice guidelines (CPGs), improving clinical utility and reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: Existing models rely on ICD codes, which lack clinical nuance. Clinicians use CPGs for evidence-based decisions, creating a misalignment with current models.

Method: GARMLE-G integrates LLM predictions with EHR data to create queries, retrieves CPG snippets via embedding similarity, and fuses guideline content with model output.

Result: The prototype for hypertension diagnosis showed superior precision, relevance, and guideline adherence compared to baselines.

Conclusion: GARMLE-G offers a scalable, low-cost, and hallucination-free method for clinical deployment, aligning models with evidence-based practice.

Abstract: Current medical language models, adapted from large language models (LLMs),
typically predict ICD code-based diagnosis from electronic health records
(EHRs) because these labels are readily available. However, ICD codes do not
capture the nuanced, context-rich reasoning clinicians use for diagnosis.
Clinicians synthesize diverse patient data and reference clinical practice
guidelines (CPGs) to make evidence-based decisions. This misalignment limits
the clinical utility of existing models. We introduce GARMLE-G, a
Generation-Augmented Retrieval framework that grounds medical language model
outputs in authoritative CPGs. Unlike conventional Retrieval-Augmented
Generation based approaches, GARMLE-G enables hallucination-free outputs by
directly retrieving authoritative guideline content without relying on
model-generated text. It (1) integrates LLM predictions with EHR data to create
semantically rich queries, (2) retrieves relevant CPG knowledge snippets via
embedding similarity, and (3) fuses guideline content with model output to
generate clinically aligned recommendations. A prototype system for
hypertension diagnosis was developed and evaluated on multiple metrics,
demonstrating superior retrieval precision, semantic relevance, and clinical
guideline adherence compared to RAG-based baselines, while maintaining a
lightweight architecture suitable for localized healthcare deployment. This
work provides a scalable, low-cost, and hallucination-free method for grounding
medical language models in evidence-based clinical practice, with strong
potential for broader clinical deployment.

</details>


### [211] [TIM: A Large-Scale Dataset and large Timeline Intelligence Model for Open-domain Timeline Summarization](https://arxiv.org/abs/2506.21616)
*Chuanrui Hu,Wei Hu,Penghang Yu,Hua Zhang,Bing-Kun Bao*

Main category: cs.CL

TL;DR: The paper introduces TIM, a large Timeline Intelligence Model for open-domain TLS, addressing limitations of general LLMs by using a progressive optimization strategy and a new dataset.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with topic relevance and evolution in TLS, leading to irrelevant or inaccurate summaries.

Method: Proposes TIM with a large-scale TLS dataset, instruction tuning, and dual-alignment reward learning for semantic and temporal alignment.

Result: TIM shows robust performance in summarizing open-domain timelines, validated by extensive experiments.

Conclusion: TIM effectively addresses TLS challenges, improving summarization and topic evolution understanding.

Abstract: Open-domain Timeline Summarization (TLS) is crucial for monitoring the
evolution of news topics. To identify changes in news topics, existing methods
typically employ general Large Language Models (LLMs) to summarize relevant
timestamps from retrieved news. While general LLMs demonstrate capabilities in
zero-shot news summarization and timestamp localization, they struggle with
assessing topic relevance and understanding topic evolution. Consequently, the
summarized information often includes irrelevant details or inaccurate
timestamps. To address these issues, we propose the first large Timeline
Intelligence Model (TIM) for open-domain TLS, which is capable of effectively
summarizing open-domain timelines. Specifically, we begin by presenting a
large-scale TLS dataset, comprising over 1,000 news topics and more than 3,000
annotated TLS instances. Furthermore, we propose a progressive optimization
strategy, which gradually enhance summarization performance. It employs
instruction tuning to enhance summarization and topic-irrelevant information
filtering capabilities. Following this, it exploits a novel dual-alignment
reward learning method that incorporates both semantic and temporal
perspectives, thereby improving the understanding of topic evolution
principles. Through this progressive optimization strategy, TIM demonstrates a
robust ability to summarize open-domain timelines. Extensive experiments in
open-domain demonstrate the effectiveness of our TIM.

</details>


### [212] [TrajTok: Technical Report for 2025 Waymo Open Sim Agents Challenge](https://arxiv.org/abs/2506.21618)
*Zhiyuan Zhang,Xiaosong Jia,Guanyu Chen,Qifeng Li,Junchi Yan*

Main category: cs.CL

TL;DR: TrajTok is a trajectory tokenizer for behavior generation models, combining data-driven and rule-based methods for better performance. It achieves a high realism score of 0.7852 on the Waymo Open Sim Agents Challenge 2025.


<details>
  <summary>Details</summary>
Motivation: To improve behavior generation models by enhancing coverage, symmetry, and robustness in trajectory prediction.

Method: Combines data-driven and rule-based tokenization with a spatial-aware label smoothing method for cross-entropy loss.

Result: Achieves a realism score of 0.7852 on the Waymo Open Sim Agents Challenge 2025.

Conclusion: TrajTok demonstrates superior performance and will be open-sourced for broader use.

Abstract: In this technical report, we introduce TrajTok, a trajectory tokenizer for
discrete next-token-prediction based behavior generation models, which combines
data-driven and rule-based methods with better coverage, symmetry and
robustness, along with a spatial-aware label smoothing method for cross-entropy
loss. We adopt the tokenizer and loss for the SMART model and reach a superior
performance with realism score of 0.7852 on the Waymo Open Sim Agents Challenge
2025. We will open-source the code in the future.

</details>


### [213] [IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech](https://arxiv.org/abs/2506.21619)
*Siyi Zhou,Yiquan Zhou,Yi He,Xun Zhou,Jinchao Wang,Wei Deng,Jingchen Shu*

Main category: cs.CL

TL;DR: IndexTTS2 introduces a novel method for precise duration control in autoregressive TTS models, supports two generation modes, disentangles emotion and speaker identity, and improves emotional clarity using GPT latent representations and soft instructions.


<details>
  <summary>Details</summary>
Motivation: Autoregressive TTS models struggle with precise duration control, limiting applications like video dubbing. IndexTTS2 aims to address this while enhancing emotional and speaker disentanglement.

Method: IndexTTS2 proposes two generation modes for duration control, disentangles emotion and speaker identity, uses GPT latent representations for stability, and employs soft instructions for emotion guidance.

Result: IndexTTS2 outperforms state-of-the-art zero-shot TTS models in word error rate, speaker similarity, and emotional fidelity.

Conclusion: IndexTTS2 advances TTS by enabling precise duration control, independent emotion and speaker control, and improved emotional clarity, making it suitable for applications like video dubbing.

Abstract: Large-scale text-to-speech (TTS) models are typically categorized into
autoregressive and non-autoregressive systems. Although autoregressive systems
exhibit certain advantages in speech naturalness, their token-by-token
generation mechanism makes it difficult to precisely control the duration of
synthesized speech. This is a key limitation in applications such as video
dubbing that require strict audio-visual synchronization. This paper introduces
IndexTTS2, which proposes a novel and autoregressive-model-friendly method for
speech duration control. The method supports two generation modes: one allows
explicit specification of the number of generated tokens for precise duration
control; the other does not require manual input and lets the model freely
generate speech while preserving prosodic characteristics from the input
prompt. Furthermore, IndexTTS2 achieves disentanglement between emotional
expression and speaker identity, enabling independent control of timbre and
emotion. In the zero-shot setting, the model can perfectly reproduce the
emotional characteristics of the input prompt. Users may also provide a
separate emotion prompt, even from a different speaker, allowing the model to
reconstruct the target timbre while conveying the desired emotion. To enhance
clarity during strong emotional expressions, we incorporate GPT latent
representations to improve speech stability. Meanwhile, to lower the barrier
for emotion control, we design a soft instruction mechanism based on textual
descriptions by fine-tuning Qwen3. This enables effective guidance of speech
generation with desired emotional tendencies using natural language input.
Experimental results demonstrate that IndexTTS2 outperforms existing
state-of-the-art zero-shot TTS models in word error rate, speaker similarity,
and emotional fidelity.

</details>


### [214] [How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit](https://arxiv.org/abs/2506.21620)
*Daniele Cirulli,Giulio Cimini,Giovanni Palermo*

Main category: cs.CL

TL;DR: The study evaluates GPT-4's ability to replicate Reddit comments from the 2016 US election, finding it can generate realistic partisan content but tends toward consensus over dissent. Real and AI comments differ semantically but not visually.


<details>
  <summary>Details</summary>
Motivation: Assess LLMs' potential to influence political discourse by mimicking human interactions in divisive scenarios.

Method: Three experiments where GPT-4 impersonates real/artificial partisan users; analysis of political alignment, sentiment, and linguistic features compared to real comments.

Result: GPT-4 generates realistic comments favoring or opposing candidates but leans toward consensus. Real and AI comments differ semantically but not manually.

Conclusion: LLMs like GPT-4 can subtly manipulate political discussions, raising concerns about AI-driven discourse influence.

Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for
natural language generation, with applications spanning from content creation
to social simulations. Their ability to mimic human interactions raises both
opportunities and concerns, particularly in the context of politically relevant
online discussions. In this study, we evaluate the performance of LLMs in
replicating user-generated content within a real-world, divisive scenario:
Reddit conversations during the 2016 US Presidential election. In particular,
we conduct three different experiments, asking GPT-4 to generate comments by
impersonating either real or artificial partisan users. We analyze the
generated comments in terms of political alignment, sentiment, and linguistic
features, comparing them against real user contributions and benchmarking
against a null model. We find that GPT-4 is able to produce realistic comments,
both in favor of or against the candidate supported by the community, yet
tending to create consensus more easily than dissent. In addition we show that
real and artificial comments are well separated in a semantically embedded
space, although they are indistinguishable by manual inspection. Our findings
provide insights on the potential use of LLMs to sneak into online discussions,
influence political debate and shape political narratives, bearing broader
implications of AI-driven discourse manipulation.

</details>


### [215] [The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs](https://arxiv.org/abs/2506.21621)
*Jasper Dekoninck,Ivo Petrov,Kristian Minchev,Mislav Balunovic,Martin Vechev,Miroslav Marinov,Maria Drencheva,Lyuba Konova,Milen Shumanov,Kaloyan Tsvetkov,Nikolay Drenchev,Lazar Todorov,Kalina Nikolova,Nikolay Georgiev,Vanesa Kalinkova,Margulan Ismoldayev*

Main category: cs.CL

TL;DR: The paper introduces the Open Proof Corpus (OPC), a large-scale dataset of 5,000+ human-evaluated proofs from LLMs, addressing the lack of such data for advancing proof generation research. It explores key questions in automated proof generation and demonstrates OPC's utility by finetuning a model to match top-tier performance.


<details>
  <summary>Details</summary>
Motivation: The lack of a high-quality, large-scale dataset of human-evaluated proofs hinders progress in LLM-based mathematical proof generation. The OPC aims to fill this gap.

Method: The authors create the OPC, a dataset of 5,000+ proofs from LLMs, including solutions to prestigious math competitions. They use it to analyze proof generation and finetune an 8B-parameter model.

Result: The finetuned model matches the performance of Gemini-2.5-Pro in evaluating proof correctness. The OPC enables insights into natural vs. formal proof generation and proof validity.

Conclusion: The OPC is a valuable resource for proof generation research, facilitating model improvements and rigorous analysis. The finetuned model demonstrates its practical utility.

Abstract: In recent months, large language models (LLMs) have made significant progress
in mathematical proof generation, but further advancement is hindered by the
lack of a large-scale, high-quality dataset of human-evaluated proofs. While
expensive to create, such a dataset is essential for driving improvements in
training and enabling a rigorous analysis of proof generation capabilities. In
this work, we present the Open Proof Corpus (OPC), a dataset comprising over
5,000 human-evaluated proofs produced by state-of-the-art LLMs. The OPC was
specifically designed for broad applicability and downstream usage in proof
generation research and is the first to include a substantial number of
correct, LLM-generated solutions to problems from prestigious mathematics
competitions such as the USAMO and IMO. Using the OPC, we explore critical
questions in automated proof generation: (1) the performance gap between
natural language and formal proof generation, (2) the discrepancy between
final-answer accuracy and full-proof validity, and (3) the impact of best-of-n
selection on proof quality. Finally, to showcase the utility of the OPC, we
finetune an 8B-parameter model on the dataset, obtaining a model that performs
on par with the best model, Gemini-2.5-Pro, on the task of evaluating proof
correctness.

</details>


### [216] [Adapting Foundation Speech Recognition Models to Impaired Speech: A Semantic Re-chaining Approach for Personalization of German Speech](https://arxiv.org/abs/2506.21622)
*Niclas Pokel,Pehuén Moure,Roman Boehringer,Yingqiang Gao*

Main category: cs.CL

TL;DR: A lightweight pipeline personalizes ASR models for speech-impaired individuals, improving transcription quality by enriching limited datasets with semantic coherence.


<details>
  <summary>Details</summary>
Motivation: Speech impairments challenge ASR systems due to limited training data and difficulty in collecting non-normative speech samples.

Method: Proposes a practical pipeline to personalize ASR models by selecting words and enriching small, speech-impaired datasets with semantic coherence.

Result: Shows promising improvements in transcription quality for a child with structural speech impairment.

Conclusion: Demonstrates potential to reduce communication barriers for individuals with atypical speech patterns.

Abstract: Speech impairments caused by conditions such as cerebral palsy or genetic
disorders pose significant challenges for automatic speech recognition (ASR)
systems. Despite recent advances, ASR models like Whisper struggle with
non-normative speech due to limited training data and the difficulty of
collecting and annotating non-normative speech samples. In this work, we
propose a practical and lightweight pipeline to personalize ASR models,
formalizing the selection of words and enriching a small, speech-impaired
dataset with semantic coherence. Applied to data from a child with a structural
speech impairment, our approach shows promising improvements in transcription
quality, demonstrating the potential to reduce communication barriers for
individuals with atypical speech patterns.

</details>


### [217] [Doc2SAR: A Synergistic Framework for High-Fidelity Extraction of Structure-Activity Relationships from Scientific Documents](https://arxiv.org/abs/2506.21625)
*Jiaxi Zhuang,Kangning Li,Jue Hou,Mingjun Xu,Zhifeng Gao,Hengxing Cai*

Main category: cs.CL

TL;DR: Doc2SAR, a novel framework combining domain-specific tools and fine-tuned MLLMs, outperforms existing methods in extracting SARs from scientific documents, achieving 80.78% Table Recall on the DocSAR-200 benchmark.


<details>
  <summary>Details</summary>
Motivation: Extracting SARs from diverse documents is challenging due to limitations of rule-based and general-purpose MLLM methods.

Method: Doc2SAR integrates domain-specific tools with supervised fine-tuned MLLMs for improved accuracy.

Result: Doc2SAR achieves 80.78% Table Recall, surpassing GPT-4o by 51.48%.

Conclusion: Doc2SAR offers a reliable, efficient solution for SAR extraction, supported by a practical web app.

Abstract: Extracting molecular structure-activity relationships (SARs) from scientific
literature and patents is essential for drug discovery and materials research.
However, this task remains challenging due to heterogeneous document formats
and limitations of existing methods. Specifically, rule-based approaches
relying on rigid templates fail to generalize across diverse document layouts,
while general-purpose multimodal large language models (MLLMs) lack sufficient
accuracy and reliability for specialized tasks, such as layout detection and
optical chemical structure recognition (OCSR). To address these challenges, we
introduce DocSAR-200, a rigorously annotated benchmark of 200 scientific
documents designed specifically for evaluating SAR extraction methods.
Additionally, we propose Doc2SAR, a novel synergistic framework that integrates
domain-specific tools with MLLMs enhanced via supervised fine-tuning (SFT).
Extensive experiments demonstrate that Doc2SAR achieves state-of-the-art
performance across various document types, significantly outperforming leading
end-to-end baselines. Specifically, Doc2SAR attains an overall Table Recall of
80.78% on DocSAR-200, exceeding end2end GPT-4o by 51.48%. Furthermore, Doc2SAR
demonstrates practical usability through efficient inference and is accompanied
by a web app.

</details>


### [218] [Performance of diverse evaluation metrics in NLP-based assessment and text generation of consumer complaints](https://arxiv.org/abs/2506.21623)
*Peiheng Gao,Chen Yang,Ning Sun,Ričardas Zitikis*

Main category: cs.CL

TL;DR: This paper proposes combining human-experience-trained algorithms and synthetic data generation to improve ML-based text classification, especially for nuanced consumer complaints.


<details>
  <summary>Details</summary>
Motivation: Accurately capturing nuanced linguistic patterns and contextual variations in natural language, particularly in consumer complaints, remains a challenge in ML-based text classification.

Method: Incorporates human-experience-trained algorithms and synthetic data generation using expert evaluations of generative adversarial networks, refined through expert annotations.

Result: Aims to enhance classifier performance, reduce dataset costs, and improve evaluation metrics and robustness in text classification.

Conclusion: The integration of expert-trained classifiers and high-quality synthetic data can significantly advance ML-based text classification.

Abstract: Machine learning (ML) has significantly advanced text classification by
enabling automated understanding and categorization of complex, unstructured
textual data. However, accurately capturing nuanced linguistic patterns and
contextual variations inherent in natural language, particularly within
consumer complaints, remains a challenge. This study addresses these issues by
incorporating human-experience-trained algorithms that effectively recognize
subtle semantic differences crucial for assessing consumer relief eligibility.
Furthermore, we propose integrating synthetic data generation methods that
utilize expert evaluations of generative adversarial networks and are refined
through expert annotations. By combining expert-trained classifiers with
high-quality synthetic data, our research seeks to significantly enhance
machine learning classifier performance, reduce dataset acquisition costs, and
improve overall evaluation metrics and robustness in text classification tasks.

</details>


### [219] [Do We Really Need GNNs with Explicit Structural Modeling? MLPs Suffice for Language Model Representations](https://arxiv.org/abs/2506.21682)
*Li Zhou,Hao Jiang,Junjie Li,Zefeng Zhao,Feng Jiang,Wenyu Chen,Haizhou Li*

Main category: cs.CL

TL;DR: The paper introduces a probing framework to evaluate how explicit structural modeling enhances LM representations and explores MLPs as alternatives to GNNs, finding MLPs effective in encoding linguistic patterns.


<details>
  <summary>Details</summary>
Motivation: Recent studies show GNNs underutilize structural information, while MLPs perform well in structure-aware tasks, prompting an investigation into their roles.

Method: A modular probing framework isolates GNN components (message-passing and feature-transformation) to assess their individual contributions, using the Edge Probing Suite for evaluation.

Result: MLPs improve linguistic knowledge in LM representations, encoding syntactic and semantic patterns. GNNs with feature-transformation benefit, while message-passing alone underperforms.

Conclusion: MLPs are efficient alternatives to GNNs for structure-aware tasks, and feature-transformation is key to enhancing LM representations.

Abstract: Explicit structural information has been proven to be encoded by Graph Neural
Networks (GNNs), serving as auxiliary knowledge to enhance model capabilities
and improve performance in downstream NLP tasks. However, recent studies
indicate that GNNs fail to fully utilize structural information, whereas
Multi-Layer Perceptrons (MLPs), despite lacking the message-passing mechanisms
inherent to GNNs, exhibit a surprising ability in structure-aware tasks.
Motivated by these findings, this paper introduces a comprehensive probing
framework from an information-theoretic perspective. The framework is designed
to systematically assess the role of explicit structural modeling in enhancing
language model (LM) representations and to investigate the potential of MLPs as
efficient and scalable alternatives to GNNs. We extend traditional probing
classifiers by incorporating a control module that allows for selective use of
either the full GNN model or its decoupled components, specifically, the
message-passing and feature-transformation operations.This modular approach
isolates and assesses the individual contributions of these operations,
avoiding confounding effects from the complete GNN architecture. Using the Edge
Probing Suite, a diagnostic tool for evaluating the linguistic knowledge
encoded in LMs, we find that MLPs, when used as feature-transformation modules,
consistently improve the linguistic knowledge captured in LM representations
across different architectures. They effectively encode both syntactic and
semantic patterns. Similarly, GNNs that incorporate feature-transformation
operations show beneficial effects. In contrast, models that rely solely on
message-passing operations tend to underperform, often leading to negative
impacts on probing task performance.

</details>


### [220] [ANUBHUTI: A Comprehensive Corpus For Sentiment Analysis In Bangla Regional Languages](https://arxiv.org/abs/2506.21686)
*Swastika Kundu,Autoshi Ibrahim,Mithila Rahman,Tanvir Ahmed*

Main category: cs.CL

TL;DR: ANUBHUTI is a dataset of 2000 Bangla dialect sentences (Mymensingh, Noakhali, Sylhet, Chittagong) with thematic and emotion annotations, addressing the lack of resources for sentiment analysis in low-resource dialects.


<details>
  <summary>Details</summary>
Motivation: Sentiment analysis for Bangla dialects is underexplored due to linguistic diversity and limited annotated data.

Method: Created a dataset with manual translation and dual annotation (thematic and emotion labels), ensured quality via inter-annotator agreement and systematic checks.

Result: Strong consistency in annotations, filling a critical gap for sentiment analysis in low-resource Bangla dialects.

Conclusion: ANUBHUTI enables more accurate and context-aware NLP for Bangla dialects.

Abstract: Sentiment analysis for regional dialects of Bangla remains an underexplored
area due to linguistic diversity and limited annotated data. This paper
introduces ANUBHUTI, a comprehensive dataset consisting of 2000 sentences
manually translated from standard Bangla into four major regional dialects
Mymensingh, Noakhali, Sylhet, and Chittagong. The dataset predominantly
features political and religious content, reflecting the contemporary socio
political landscape of Bangladesh, alongside neutral texts to maintain balance.
Each sentence is annotated using a dual annotation scheme: multiclass thematic
labeling categorizes sentences as Political, Religious, or Neutral, and
multilabel emotion annotation assigns one or more emotions from Anger,
Contempt, Disgust, Enjoyment, Fear, Sadness, and Surprise. Expert native
translators conducted the translation and annotation, with quality assurance
performed via Cohens Kappa inter annotator agreement, achieving strong
consistency across dialects. The dataset was further refined through systematic
checks for missing data, anomalies, and inconsistencies. ANUBHUTI fills a
critical gap in resources for sentiment analysis in low resource Bangla
dialects, enabling more accurate and context aware natural language processing.

</details>


### [221] [Identifying Speaker Information in Feed-Forward Layers of Self-Supervised Speech Transformers](https://arxiv.org/abs/2506.21712)
*Tzu-Quan Lin,Hsi-Chun Cheng,Hung-yi Lee,Hao Tang*

Main category: cs.CL

TL;DR: The paper explores how self-supervised speech Transformers encode speaker information by identifying neurons correlated with speaker data, revealing their role in preserving speaker-related task performance.


<details>
  <summary>Details</summary>
Motivation: Little research has examined how self-supervised speech Transformers encode speaker information, despite their growing use in speaker-related applications.

Method: The study analyzes neurons in feed-forward layers linked to k-means clusters of self-supervised features and i-vectors, identifying those correlated with speaker information.

Result: The identified neurons correspond to broad phonetic and gender classes, and protecting them during pruning preserves speaker-related task performance.

Conclusion: The findings highlight the importance of specific neurons in encoding speaker information and their potential for improving speaker-related applications.

Abstract: In recent years, the impact of self-supervised speech Transformers has
extended to speaker-related applications. However, little research has explored
how these models encode speaker information. In this work, we address this gap
by identifying neurons in the feed-forward layers that are correlated with
speaker information. Specifically, we analyze neurons associated with k-means
clusters of self-supervised features and i-vectors. Our analysis reveals that
these clusters correspond to broad phonetic and gender classes, making them
suitable for identifying neurons that represent speakers. By protecting these
neurons during pruning, we can significantly preserve performance on
speaker-related task, demonstrating their crucial role in encoding speaker
information.

</details>


### [222] [Evaluating List Construction and Temporal Understanding capabilities of Large Language Models](https://arxiv.org/abs/2506.21783)
*Alexandru Dumitru,V Venktesh,Adam Jatowt,Avishek Anand*

Main category: cs.CL

TL;DR: The paper introduces the TLQA benchmark to evaluate LLMs' temporal understanding and list construction abilities, revealing their shortcomings in these tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with temporal understanding and list construction in answers, especially for tasks involving multiple entities and time intervals. Existing benchmarks lack focus on this gap.

Method: Proposes the TLQA benchmark, requiring structured list answers aligned with time periods. Evaluates state-of-the-art models in closed-book and open-domain settings.

Result: Current models show significant weaknesses, including incomplete answers and poor temporal alignment in closed-book setups, and retrieval issues in open-domain setups.

Conclusion: The TLQA benchmark highlights critical gaps in LLMs' capabilities, guiding future research to improve temporal understanding and list-based QA.

Abstract: Large Language Models (LLMs) have demonstrated immense advances in a wide
range of natural language tasks. However, these models are susceptible to
hallucinations and errors on particularly temporal understanding tasks
involving multiple entities in answers. In such tasks, they fail to associate
entities with accurate time intervals, generate a complete list of entities in
answers or reason about events associated with specific temporal bounds.
Existing works do not extensively evaluate the abilities of the model to
perform implicit and explicit temporal understanding in a list answer
construction setup. To bridge this gap, we propose the Time referenced List
based Question Answering or TLQA benchmark that requires structured answers in
list format aligned with corresponding time periods. Our TLQA benchmark,
requires both list construction and temporal understanding simultaneously,
which to the best of our knowledge has not been explored in prior benchmarks.
We investigate the temporal understanding and list construction capabilities of
state-of-the-art generative models on TLQA in closed-book and open-domain
settings. Our findings reveal significant shortcomings in current models,
particularly their inability to provide complete answers and temporally align
facts in a closed-book setup and the need to improve retrieval in open-domain
setup, providing clear future directions for research on TLQA. The benchmark
and code at https://github.com/elixir-research-group/TLQA.

</details>


### [223] [(Fact) Check Your Bias](https://arxiv.org/abs/2506.21745)
*Eivind Morris Bakke,Nora Winger Heggelund*

Main category: cs.CL

TL;DR: The paper examines how parametric knowledge biases in LLMs like Llama 3.1 affect fact-checking outcomes in the HerO system, revealing biases in verdicts and evidence retrieval.


<details>
  <summary>Details</summary>
Motivation: To understand how biases in LLMs impact fact verification, particularly in the HerO system, and explore the effects of intentional bias injection.

Method: Analyzed Llama 3.1's fact-checking performance under direct prompts and injected bias, measuring verdicts and evidence retrieval.

Result: Llama 3.1 labels half the claims as "Not Enough Evidence" and shows bias in evidence retrieval, with 50% unique evidence per perspective. Final verdicts remain stable.

Conclusion: LLMs exhibit biases in fact-checking, affecting evidence retrieval, but verdicts show resilience to prompting strategies.

Abstract: Automatic fact verification systems increasingly rely on large language
models (LLMs). We investigate how parametric knowledge biases in these models
affect fact-checking outcomes of the HerO system (baseline for FEVER-25). We
examine how the system is affected by: (1) potential bias in Llama 3.1's
parametric knowledge and (2) intentionally injected bias. When prompted
directly to perform fact-verification, Llama 3.1 labels nearly half the claims
as "Not Enough Evidence". Using only its parametric knowledge it is able to
reach a verdict on the remaining half of the claims. In the second experiment,
we prompt the model to generate supporting, refuting, or neutral fact-checking
documents. These prompts significantly influence retrieval outcomes, with
approximately 50\% of retrieved evidence being unique to each perspective.
Notably, the model sometimes refuses to generate supporting documents for
claims it believes to be false, creating an inherent negative bias. Despite
differences in retrieved evidence, final verdict predictions show stability
across prompting strategies. The code is available at:
https://github.com/eibakke/FEVER-8-Shared-Task

</details>


### [224] [Offensive Language Detection on Social Media Using XLNet](https://arxiv.org/abs/2506.21795)
*Reem Alothman,Hafida Benhidour,Said Kerrache*

Main category: cs.CL

TL;DR: The paper proposes an XLNet-based model for detecting offensive language on social media, outperforming BERT in most tasks, and highlights the effectiveness of sampling strategies for class imbalance.


<details>
  <summary>Details</summary>
Motivation: The rise of offensive content on social media necessitates automated detection systems due to impractical manual moderation.

Method: The study uses XLNet and BERT, evaluated on the OLID dataset, and tests oversampling/undersampling for class imbalance.

Result: XLNet outperforms BERT in detecting offensive content and categorizing offenses, while BERT is better at identifying targets. Sampling strategies improve performance.

Conclusion: Transfer learning and XLNet show promise for robust offensive language detection, with sampling methods aiding classification.

Abstract: The widespread use of text-based communication on social media-through chats,
comments, and microblogs-has improved user interaction but has also led to an
increase in offensive content, including hate speech, racism, and other forms
of abuse. Due to the enormous volume of user-generated content, manual
moderation is impractical, which creates a need for automated systems that can
detect offensive language. Deep learning models, particularly those using
transfer learning, have demonstrated significant success in understanding
natural language through large-scale pretraining. In this study, we propose an
automatic offensive language detection model based on XLNet, a generalized
autoregressive pretraining method, and compare its performance with BERT
(Bidirectional Encoder Representations from Transformers), which is a widely
used baseline in natural language processing (NLP). Both models are evaluated
using the Offensive Language Identification Dataset (OLID), a benchmark Twitter
dataset that includes hierarchical annotations. Our experimental results show
that XLNet outperforms BERT in detecting offensive content and in categorizing
the types of offenses, while BERT performs slightly better in identifying the
targets of the offenses. Additionally, we find that oversampling and
undersampling strategies are effective in addressing class imbalance and
improving classification performance. These findings highlight the potential of
transfer learning and XLNet-based architectures to create robust systems for
detecting offensive language on social media platforms.

</details>


### [225] [A suite of allotaxonometric tools for the comparison of complex systems using rank-turbulence divergence](https://arxiv.org/abs/2506.21808)
*Jonathan St-Onge,Ashley M. A. Fehr,Carter Ward,Calla G. Beauregard,Michael V. Arnold,Samuel F. Rosenblatt,Benjamin Cooley,Christopher M. Danforth,Peter Sheridan Dodds*

Main category: cs.CL

TL;DR: Allotaxonographs are tools for visualizing and comparing heavy-tailed distributions using rank-turbulence divergence and other methods, with implementations in Matlab, Javascript, and Python.


<details>
  <summary>Details</summary>
Motivation: The need for principled, theoretically grounded tools to describe and compare complex systems.

Method: Allotaxonographs use rank-turbulence divergence and other divergences to visualize and compare heavy-tailed distributions.

Result: A suite of programmatic tools for rendering allotaxonographs in Matlab, Javascript, and Python.

Conclusion: Allotaxonographs provide versatile, theoretically grounded tools for comparing complex systems across different programming environments.

Abstract: Describing and comparing complex systems requires principled, theoretically
grounded tools. Built around the phenomenon of type turbulence,
allotaxonographs provide map-and-list visual comparisons of pairs of
heavy-tailed distributions. Allotaxonographs are designed to accommodate a wide
range of instruments including rank- and probability-turbulence divergences,
Jenson-Shannon divergence, and generalized entropy divergences. Here, we
describe a suite of programmatic tools for rendering allotaxonographs for
rank-turbulence divergence in Matlab, Javascript, and Python, all of which have
different use cases.

</details>


### [226] [Exploring the Structure of AI-Induced Language Change in Scientific English](https://arxiv.org/abs/2506.21817)
*Riley Galpin,Bryce Anderson,Tom S. Juzek*

Main category: cs.CL

TL;DR: The study examines how Large Language Models (LLMs) like ChatGPT influence scientific English, focusing on word frequency spikes (e.g., 'crucial') and broader semantic shifts, revealing semantic-pragmatic changes rather than simple synonym replacement.


<details>
  <summary>Details</summary>
Motivation: To understand the structural impact of LLMs on scientific English, particularly whether word frequency spikes reflect synonym replacement or broader semantic shifts.

Method: Analyzed synonym groups and part-of-speech tagging in PubMed abstracts to quantify linguistic shifts and differentiate word forms.

Result: Semantic clusters shift together, indicating semantic-pragmatic changes. Adjectives like 'important' declined, while 'collapsing' words showed organic language change patterns.

Conclusion: LLMs induce semantic-pragmatic shifts in language, contrasting with abrupt lexical changes, highlighting their role in shaping human language.

Abstract: Scientific English has undergone rapid and unprecedented changes in recent
years, with words such as "delve," "intricate," and "crucial" showing
significant spikes in frequency since around 2022. These changes are widely
attributed to the growing influence of Large Language Models like ChatGPT in
the discourse surrounding bias and misalignment. However, apart from changes in
frequency, the exact structure of these linguistic shifts has remained unclear.
The present study addresses this and investigates whether these changes involve
the replacement of synonyms by suddenly 'spiking words,' for example, "crucial"
replacing "essential" and "key," or whether they reflect broader semantic and
pragmatic qualifications. To further investigate structural changes, we include
part of speech tagging in our analysis to quantify linguistic shifts over
grammatical categories and differentiate between word forms, like "potential"
as a noun vs. as an adjective. We systematically analyze synonym groups for
widely discussed 'spiking words' based on frequency trends in scientific
abstracts from PubMed. We find that entire semantic clusters often shift
together, with most or all words in a group increasing in usage. This pattern
suggests that changes induced by Large Language Models are primarily semantic
and pragmatic rather than purely lexical. Notably, the adjective "important"
shows a significant decline, which prompted us to systematically analyze
decreasing lexical items. Our analysis of "collapsing" words reveals a more
complex picture, which is consistent with organic language change and contrasts
with the patterns of the abrupt spikes. These insights into the structure of
language change contribute to our understanding of how language technology
continues to shape human language.

</details>


### [227] [Towards Transparent AI: A Survey on Explainable Large Language Models](https://arxiv.org/abs/2506.21812)
*Avash Palikhe,Zhenyu Yu,Zichong Wang,Wenbin Zhang*

Main category: cs.CL

TL;DR: A survey reviewing explainability techniques for LLMs, categorizing XAI methods by transformer architectures, evaluating their explainability, and discussing applications, resources, and future challenges.


<details>
  <summary>Details</summary>
Motivation: LLMs lack transparency in decision-making, hindering adoption in high-stakes domains. This survey aims to systematically review XAI methods to address this gap.

Method: Categorizes XAI methods by transformer architectures (encoder-only, decoder-only, encoder-decoder), evaluates explainability, and explores practical applications.

Result: Provides a comprehensive review of XAI techniques, their evaluation, and applications, highlighting gaps and resources.

Conclusion: The survey guides future efforts to develop transparent and responsible LLMs by addressing research challenges and directions.

Abstract: Large Language Models (LLMs) have played a pivotal role in advancing
Artificial Intelligence (AI). However, despite their achievements, LLMs often
struggle to explain their decision-making processes, making them a 'black box'
and presenting a substantial challenge to explainability. This lack of
transparency poses a significant obstacle to the adoption of LLMs in
high-stakes domain applications, where interpretability is particularly
essential. To overcome these limitations, researchers have developed various
explainable artificial intelligence (XAI) methods that provide
human-interpretable explanations for LLMs. However, a systematic understanding
of these methods remains limited. To address this gap, this survey provides a
comprehensive review of explainability techniques by categorizing XAI methods
based on the underlying transformer architectures of LLMs: encoder-only,
decoder-only, and encoder-decoder models. Then these techniques are examined in
terms of their evaluation for assessing explainability, and the survey further
explores how these explanations are leveraged in practical applications.
Finally, it discusses available resources, ongoing research challenges, and
future directions, aiming to guide continued efforts toward developing
transparent and responsible LLMs.

</details>


### [228] [PARSI: Persian Authorship Recognition via Stylometric Integration](https://arxiv.org/abs/2506.21840)
*Kourosh Shahnazari,Mohammadali Keshtparvar,Seyed Moein Ayyoubzadeh*

Main category: cs.CL

TL;DR: A multi-input neural framework for Persian poetry authorship attribution achieves 71% accuracy with weighted voting and 97% accuracy at high confidence thresholds.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of computational authorship attribution in Persian classical poetry due to its linguistic, stylistic, and metrical complexity.

Method: Uses a transformer-based language encoder with Word2Vec embeddings, stylometric measures, and metrical encodings on a corpus of 647,653 verses.

Result: Weighted voting yields 71% accuracy; threshold-based filtering achieves 97% accuracy at 0.9 threshold.

Conclusion: The framework integrates deep learning with domain-specific features, aiding authorship disputes and computational literature research.

Abstract: The intricate linguistic, stylistic, and metrical aspects of Persian
classical poetry pose a challenge for computational authorship attribution. In
this work, we present a versatile framework to determine authorship among 67
prominent poets. We employ a multi-input neural framework consisting of a
transformer-based language encoder complemented by features addressing the
semantic, stylometric, and metrical dimensions of Persian poetry. Our feature
set encompasses 100-dimensional Word2Vec embeddings, seven stylometric
measures, and categorical encodings of poetic form and meter. We compiled a
vast corpus of 647,653 verses of the Ganjoor digital collection, validating the
data through strict preprocessing and author verification while preserving
poem-level splitting to prevent overlap. This work employs verse-level
classification and majority and weighted voting schemes in evaluation,
revealing that weighted voting yields 71% accuracy. We further investigate
threshold-based decision filtering, allowing the model to generate highly
confident predictions, achieving 97% accuracy at a 0.9 threshold, though at
lower coverage. Our work focuses on the integration of deep representational
forms with domain-specific features for improved authorship attribution. The
results illustrate the potential of our approach for automated classification
and the contribution to stylistic analysis, authorship disputes, and general
computational literature research. This research will facilitate further
research on multilingual author attribution, style shift, and generative
modeling of Persian poetry.

</details>


### [229] [The Consistency Hypothesis in Uncertainty Quantification for Large Language Models](https://arxiv.org/abs/2506.21849)
*Quan Xiao,Debarun Bhattacharjya,Balaji Ganesan,Radu Marinescu,Katsiaryna Mirylenka,Nhan H Pham,Michael Glass,Junkyu Lee*

Main category: cs.CL

TL;DR: The paper explores the consistency hypothesis in black-box uncertainty quantification (UQ) for LLMs, formalizing it with three mathematical statements. It introduces statistical tests and metrics, validating the hypothesis across tasks and datasets. The 'Sim-Any' hypothesis is highlighted, leading to data-free UQ methods that outperform baselines.


<details>
  <summary>Details</summary>
Motivation: To assess the validity of using generation consistency as a proxy for LLM confidence, addressing the need for reliable uncertainty quantification in real-world applications.

Method: Formalizes the consistency hypothesis into three mathematical statements, introduces statistical tests and metrics, and evaluates them empirically across 8 datasets and 3 tasks. Proposes data-free black-box UQ methods based on the 'Sim-Any' hypothesis.

Result: The consistency hypothesis holds under various settings, with 'Sim-Any' being the most actionable. Proposed UQ methods outperform baselines, demonstrating practical utility.

Conclusion: The consistency hypothesis is empirically validated, and leveraging it (especially 'Sim-Any') enables effective, data-free black-box UQ methods for LLM confidence estimation.

Abstract: Estimating the confidence of large language model (LLM) outputs is essential
for real-world applications requiring high user trust. Black-box uncertainty
quantification (UQ) methods, relying solely on model API access, have gained
popularity due to their practical benefits. In this paper, we examine the
implicit assumption behind several UQ methods, which use generation consistency
as a proxy for confidence, an idea we formalize as the consistency hypothesis.
We introduce three mathematical statements with corresponding statistical tests
to capture variations of this hypothesis and metrics to evaluate LLM output
conformity across tasks. Our empirical investigation, spanning 8 benchmark
datasets and 3 tasks (question answering, text summarization, and text-to-SQL),
highlights the prevalence of the hypothesis under different settings. Among the
statements, we highlight the `Sim-Any' hypothesis as the most actionable, and
demonstrate how it can be leveraged by proposing data-free black-box UQ methods
that aggregate similarities between generations for confidence estimation.
These approaches can outperform the closest baselines, showcasing the practical
value of the empirically observed consistency hypothesis.

</details>


### [230] [LinguaSynth: Heterogeneous Linguistic Signals for News Classification](https://arxiv.org/abs/2506.21848)
*Duo Zhang,Junyi Mo*

Main category: cs.CL

TL;DR: LinguaSynth is a transparent logistic regression framework integrating five linguistic feature types, achieving 84.89% accuracy on 20 Newsgroups, outperforming TF-IDF by 3.32%. It challenges the need for deep neural networks in text classification.


<details>
  <summary>Details</summary>
Motivation: Address interpretability and computational efficiency concerns in NLP by moving away from black-box deep learning models.

Method: Integrates lexical, syntactic, entity-level, word-level, and document-level semantic features into a logistic regression model.

Result: Achieves 84.89% accuracy on 20 Newsgroups, surpassing TF-IDF by 3.32%. Syntactic and entity-level features are key for disambiguation.

Conclusion: LinguaSynth demonstrates that interpretable, resource-efficient models can rival deep neural networks in text classification.

Abstract: Deep learning has significantly advanced NLP, but its reliance on large
black-box models introduces critical interpretability and computational
efficiency concerns. This paper proposes LinguaSynth, a novel text
classification framework that strategically integrates five complementary
linguistic feature types: lexical, syntactic, entity-level, word-level
semantics, and document-level semantics within a transparent logistic
regression model. Unlike transformer-based architectures, LinguaSynth maintains
interpretability and computational efficiency, achieving an accuracy of 84.89
percent on the 20 Newsgroups dataset and surpassing a robust TF-IDF baseline by
3.32 percent. Through rigorous feature interaction analysis, we show that
syntactic and entity-level signals provide essential disambiguation and
effectively complement distributional semantics. LinguaSynth sets a new
benchmark for interpretable, resource-efficient NLP models and challenges the
prevailing assumption that deep neural networks are necessary for
high-performing text classification.

</details>


### [231] [Derivational Probing: Unveiling the Layer-wise Derivation of Syntactic Structures in Neural Language Models](https://arxiv.org/abs/2506.21861)
*Taiga Someya,Ryo Yoshida,Hitomi Yanaka,Yohei Oseki*

Main category: cs.CL

TL;DR: Derivational Probing reveals BERT's bottom-up syntactic structure construction, with micro-syntactic features in lower layers and macro-syntactic integration in higher layers, impacting downstream tasks like subject-verb agreement.


<details>
  <summary>Details</summary>
Motivation: To understand how neural language models like BERT construct syntactic structures across layers, as current knowledge is limited.

Method: Proposed Derivational Probing to analyze micro- and macro-syntactic structure formation across BERT's layers.

Result: Micro-syntactic structures emerge in lower layers, integrating into macro-syntactic structures in higher layers, with timing affecting downstream performance.

Conclusion: Optimal timing for integrating global syntactic information is critical for model performance, as shown in subject-verb agreement tasks.

Abstract: Recent work has demonstrated that neural language models encode syntactic
structures in their internal representations, yet the derivations by which
these structures are constructed across layers remain poorly understood. In
this paper, we propose Derivational Probing to investigate how micro-syntactic
structures (e.g., subject noun phrases) and macro-syntactic structures (e.g.,
the relationship between the root verbs and their direct dependents) are
constructed as word embeddings propagate upward across layers. Our experiments
on BERT reveal a clear bottom-up derivation: micro-syntactic structures emerge
in lower layers and are gradually integrated into a coherent macro-syntactic
structure in higher layers. Furthermore, a targeted evaluation on subject-verb
number agreement shows that the timing of constructing macro-syntactic
structures is critical for downstream performance, suggesting an optimal timing
for integrating global syntactic information.

</details>


### [232] [DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE](https://arxiv.org/abs/2506.21864)
*Hang Shao,Heting Gao,Yunhang Shen,Jiawei Chen,Lijiang Li,Zuwei Long,Bo Tong,Ke Li,Xing Sun*

Main category: cs.CL

TL;DR: DeepTalk is a framework using Mixture of Experts to address performance degradation in native MLLMs, achieving only a 5.5% drop compared to the original LLM and low latency.


<details>
  <summary>Details</summary>
Motivation: Native MLLMs suffer from catastrophic forgetting due to insufficient paired speech-text data, unlike text LLMs.

Method: DeepTalk uses adaptive modality expert learning with MoE, involving specialized single-modality training followed by joint multimodal training.

Result: DeepTalk reduces performance drop to 5.5% (vs. 20% in native MLLMs) and maintains latency under 0.5 seconds.

Conclusion: DeepTalk effectively balances performance and latency, offering seamless speech interaction.

Abstract: Native multimodal large language models (MLLMs) restructure a single large
language model (LLM) into a spoken language model (SLM) capable of both speech
and text generation. Compared to modular and aligned MLLMs, native MLLMs
preserve richer paralinguistic features such as emotion and prosody, and
generate speech responses directly within the backbone LLM rather than using a
separate speech decoder. This integration also results in lower response
latency and smoother interaction. However, native MLLMs suffer from
catastrophic forgetting and performance degradation because the available
paired speech-text data is insufficient to support the pretraining of MLLMs
compared to the vast amount of text data required to pretrain text LLMs. To
address this issue, we propose DeepTalk, a framework for adaptive modality
expert learning based on a Mixture of Experts (MoE) architecture. DeepTalk
first adaptively distinguishes modality experts according to their modality
load within the LLM. Each modality expert then undergoes specialized
single-modality training, followed by joint multimodal collaborative training.
As a result, DeepTalk incurs only a 5.5% performance drop compared to the
original LLM, which is significantly lower than the average performance drop of
over 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par
with modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within
0.5 seconds, ensuring a seamless and intelligent speech interaction experience.
Code and models are released at https://github.com/talkking/DeepTalk.

</details>


### [233] [WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation](https://arxiv.org/abs/2506.21875)
*Jian Zhang,Linhao Zhang,Bokai Lei,Chuhan Wu,Wei Jia,Xiao Zhou*

Main category: cs.CL

TL;DR: The paper introduces a specialized benchmark for evaluating speech-based LLMs, addressing gaps in existing methods by incorporating speech-specific challenges and a query-aware evaluation approach.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLMs overlook speech-specific challenges like prosody and homophones, hindering optimization for real-world speech interactions.

Method: The authors curate real-world chat data, diversify speaker attributes and acoustic conditions, and augment the dataset with speech-specific phenomena. They also design a query-aware evaluation method with customized checklists and prompts.

Result: Testing reveals significant performance differences among speech models across scenarios, with query-aware evaluation enabling finer-grained assessment.

Conclusion: The proposed benchmark offers valuable insights for developing and evaluating speech models, addressing limitations of text-based benchmarks.

Abstract: Recent multi-modal Large Language Models (LLMs) such as GPT-4o have
demonstrated strong capabilities of direct speech interaction. However, the
lack of specialized and comprehensive benchmarks for end-to-end speech LLM
evaluation hinders optimizing the user experience of Audio LLMs in real-world
applications. Existing evaluation methods often adapt text-based benchmarks,
overlooking speech's unique characteristics and challenges, including prosody,
homophones, stuttering, and differing user expectations. Here, we present a
novel approach to thoroughly evaluate LLMs in practical speech conversations.
We systematically curate real-world chat data relevant to spoken scenarios,
introduce diversity in speaker attributes and acoustic conditions, and augment
the dataset with speech-specific phenomena. We further design a query-aware
evaluation method to use customized evaluation checklists and prompts to
enhance the accuracy of automatic evaluation. We conduct comprehensive testing
and detailed analysis of various mainstream speech models, revealing
significant differences in model performance across different speech scenarios.
The use of query-aware evaluation further enables a finer-grained assessment
under various speech-specific scenarios. Our benchmark can provide valuable
insights for speech model development and evaluation.

</details>


### [234] [Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation](https://arxiv.org/abs/2506.21876)
*Qiyue Gao,Xinyu Pi,Kevin Liu,Junrong Chen,Ruolan Yang,Xinqi Huang,Xinyu Fang,Lu Sun,Gautham Kishore,Bo Ai,Stone Tao,Mengyang Liu,Jiaxi Yang,Chao-Jung Lai,Chuanyang Jin,Jiannan Xiang,Benhao Huang,Zeming Chen,David Danks,Hao Su,Tianmin Shu,Ziqiao Ma,Lianhui Qin,Zhiting Hu*

Main category: cs.CL

TL;DR: The paper evaluates Vision-Language Models (VLMs) as internal world models (WMs) using a two-stage framework (Perception and Prediction) and introduces WM-ABench, a benchmark revealing significant limitations in VLMs' world modeling abilities.


<details>
  <summary>Details</summary>
Motivation: To systematically assess VLMs' fundamental world modeling capabilities, which remain underexplored despite their potential as general-purpose WMs.

Method: A two-stage framework evaluates Perception (visual, spatial, temporal, quantitative, motion) and Prediction (mechanistic simulation, transitive inference, compositional inference). WM-ABench, a large-scale benchmark, tests 15 VLMs across 23 dimensions in 6 simulated environments.

Result: VLMs show striking limitations, performing near-random in tasks like motion trajectory distinction and exhibiting biases (e.g., associating color with speed). Gaps between VLMs and human-level world modeling are significant.

Conclusion: Current VLMs lack robust world modeling abilities, highlighting the need for further research to bridge the gap between AI and human-like understanding.

Abstract: Internal world models (WMs) enable agents to understand the world's state and
predict transitions, serving as the basis for advanced deliberative reasoning.
Recent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and
Gemini, exhibit potential as general-purpose WMs. While the latest studies have
evaluated and shown limitations in specific capabilities such as visual
understanding, a systematic evaluation of VLMs' fundamental WM abilities
remains absent. Drawing on comparative psychology and cognitive science, we
propose a two-stage framework that assesses Perception (visual, spatial,
temporal, quantitative, and motion) and Prediction (mechanistic simulation,
transitive inference, compositional inference) to provide an atomic evaluation
of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale
benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse
simulated environments with controlled counterfactual simulations. Through 660
experiments on 15 latest commercial and open-source VLMs, we find that these
models exhibit striking limitations in basic world modeling abilities. For
instance, almost all models perform at near-random accuracy when distinguishing
motion trajectories. Additionally, they lack disentangled understanding --
e.g., some models tend to believe blue objects move faster than green ones.
More rich results and analyses reveal significant gaps between VLMs and
human-level world modeling.

</details>


### [235] [A Dual-Layered Evaluation of Geopolitical and Cultural Bias in LLMs](https://arxiv.org/abs/2506.21881)
*Sean Kim,Hyuhng Joon Kim*

Main category: cs.CL

TL;DR: The paper evaluates bias in LLMs, distinguishing between model bias (from training) and inference bias (from query language), using a two-phase approach: factual questions (Phase 1) and geopolitically sensitive disputes (Phase 2). Results show query language affects alignment in Phase 1, while Phase 2 reveals interplay between training and query language.


<details>
  <summary>Details</summary>
Motivation: To understand LLM behavior in diverse linguistic and cultural contexts, especially where outputs may influence public opinion or reinforce dominant narratives.

Method: A two-phase evaluation: Phase 1 tests factual questions for consistency across query languages; Phase 2 probes geopolitically sensitive disputes. A manually curated multilingual dataset is used.

Result: Phase 1 shows query language-induced alignment; Phase 2 reveals interplay between model training and query language.

Conclusion: The paper provides a framework for evaluating LLM behavior in neutral and sensitive topics, aiding future deployment and culturally aware evaluation in multilingual settings.

Abstract: As large language models (LLMs) are increasingly deployed across diverse
linguistic and cultural contexts, understanding their behavior in both factual
and disputable scenarios is essential, especially when their outputs may shape
public opinion or reinforce dominant narratives. In this paper, we define two
types of bias in LLMs: model bias (bias stemming from model training) and
inference bias (bias induced by the language of the query), through a two-phase
evaluation. Phase 1 evaluates LLMs on factual questions where a single
verifiable answer exists, assessing whether models maintain consistency across
different query languages. Phase 2 expands the scope by probing geopolitically
sensitive disputes, where responses may reflect culturally embedded or
ideologically aligned perspectives. We construct a manually curated dataset
spanning both factual and disputable QA, across four languages and question
types. The results show that Phase 1 exhibits query language induced alignment,
while Phase 2 reflects an interplay between the model's training context and
query language. This paper offers a structured framework for evaluating LLM
behavior across neutral and sensitive topics, providing insights for future LLM
deployment and culturally aware evaluation practices in multilingual contexts.

</details>


### [236] [AutoMixer: Checkpoint Artifacts as Automatic Data Mixers](https://arxiv.org/abs/2506.21910)
*Ernie Chang,Yang Li,Patrick Huber,David Kant,Yangyang Shi,Vikas Chandra*

Main category: cs.CL

TL;DR: The paper proposes using checkpoint models from training trajectories to optimize data mixtures for language model training, improving performance by up to 1.93% on reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: The relationship between data and tasks in language model training is unclear, making it hard to obtain optimal data mixtures for desired capabilities.

Method: Leverage checkpoint models (saved during training) as data mixers by using their aggregated first-order influence approximation over source data.

Result: Demonstrated significant improvements (up to 1.93%) on eight reasoning benchmarks in pretraining.

Conclusion: Checkpoint models can enhance data quality and optimize data mixtures, showing their potential in improving language model training.

Abstract: In language model training, it is desirable to equip models with capabilities
from various tasks. However, it is not clear how to directly obtain the right
data mixtures for these capabilities as the relationship between data and tasks
is difficult to be modeled. In this work, we observe that checkpoint models
exhibit emerging capabilities at different points in the training trajectory.
Often, the training process saves checkpoints as artifacts that are
under-utilized as a source of in-training data signals. We identify these
artifact models based on their respective capabilities on the benchmarks and
leverage them as data mixers by using their aggregated first-order influence
approximation over source data. We demonstrated on eight reasoning benchmarks
that the proposed framework shows significant improvements in the pretraining
setting, with performance improvements of up to 1.93%. Overall, this shows the
potential of checkpoint models to enhance data quality and optimize data
mixtures.

</details>


### [237] [More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents](https://arxiv.org/abs/2506.21967)
*Weimin Xiong,Ke Wang,Yifan Song,Hanchao Liu,Sai Zhou,Wei Peng,Sujian Li*

Main category: cs.CL

TL;DR: The paper evaluates the stability of tool-integrated LLM agents, revealing vulnerabilities across the tool invocation process and noting open-source models are more prone to errors than proprietary ones.


<details>
  <summary>Details</summary>
Motivation: Current evaluations overlook agent stability, limiting real-world applicability due to potential crashes or abnormal behavior from internal/external factors.

Method: Investigates agent vulnerability across tool invocation stages: reading documentation, tool selection, parameter generation, and response processing.

Result: Agents are highly error-prone at each stage; open-source models are more vulnerable. Larger models don't improve reasoning and may increase attack susceptibility.

Conclusion: Highlights the need for stability evaluation in LLM agents, providing insights for future development and assessment.

Abstract: Current evaluations of tool-integrated LLM agents typically focus on
end-to-end tool-usage evaluation while neglecting their stability. This limits
their real-world applicability, as various internal or external factors can
cause agents to crash or behave abnormally. Our research addresses this by
investigating whether agents are vulnerable to errors throughout the entire
tool invocation process, including reading tool documentation, selecting tools
and generating parameters, and processing the tool's response. Through
extensive experiments, we observe that agents are highly susceptible to errors
at each stage and agents based on open-source models are more vulnerable than
those based on proprietary models. We also find that increasing the model size
does not significantly improve tool invocation reasoning and may make agents
more vulnerable to attacks resembling normal user instructions. This highlights
the importance of evaluating agent stability and offers valuable insights for
future LLM development and evaluation.

</details>


### [238] [PapersPlease: A Benchmark for Evaluating Motivational Values of Large Language Models Based on ERG Theory](https://arxiv.org/abs/2506.21961)
*Junho Myung,Yeon Su Park,Sunwoo Kim,Shin Yoo,Alice Oh*

Main category: cs.CL

TL;DR: The paper introduces PapersPlease, a benchmark with 3,700 moral dilemmas to evaluate LLMs' decision-making biases, revealing implicit preferences and varying responsiveness to social identities.


<details>
  <summary>Details</summary>
Motivation: To investigate biases in LLMs' decision-making, especially in role-playing scenarios like immigration inspections, using hierarchical human needs theory (ERG).

Method: LLMs act as immigration inspectors, evaluating narratives based on ERG theory (Existence, Relatedness, Growth). The study analyzes six LLMs' decisions and the impact of social identities.

Result: LLMs show statistically significant biases, with implicit preferences and higher denial rates for marginalized identities in some models.

Conclusion: The benchmark highlights biases in LLMs' moral decision-making, emphasizing the need for addressing these issues in model development.

Abstract: Evaluating the performance and biases of large language models (LLMs) through
role-playing scenarios is becoming increasingly common, as LLMs often exhibit
biased behaviors in these contexts. Building on this line of research, we
introduce PapersPlease, a benchmark consisting of 3,700 moral dilemmas designed
to investigate LLMs' decision-making in prioritizing various levels of human
needs. In our setup, LLMs act as immigration inspectors deciding whether to
approve or deny entry based on the short narratives of people. These narratives
are constructed using the Existence, Relatedness, and Growth (ERG) theory,
which categorizes human needs into three hierarchical levels. Our analysis of
six LLMs reveals statistically significant patterns in decision-making,
suggesting that LLMs encode implicit preferences. Additionally, our evaluation
of the impact of incorporating social identities into the narratives shows
varying responsiveness based on both motivational needs and identity cues, with
some models exhibiting higher denial rates for marginalized identities. All
data is publicly available at https://github.com/yeonsuuuu28/papers-please.

</details>


### [239] [Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses](https://arxiv.org/abs/2506.21972)
*Mohamed Ahmed,Mohamed Abdelmouty,Mingyu Kim,Gunvanth Kandula,Alex Park,James C. Davis*

Main category: cs.CL

TL;DR: The paper proposes hybrid jailbreak methods (GCG + PAIR and GCG + WordGame) combining token- and prompt-level attacks to exploit vulnerabilities in PTLMs, achieving high success rates and bypassing advanced defenses.


<details>
  <summary>Details</summary>
Motivation: PTLMs and LLMs are vulnerable to jailbreak attacks, but existing methods (token-level and prompt-level) have complementary limitations. Hybrid approaches aim to overcome these weaknesses.

Method: Two hybrid techniques (GCG + PAIR and GCG + WordGame) integrate token- and prompt-level attacks. They were tested on Vicuna and Llama models.

Result: GCG + PAIR achieved a 91.6% ASR on Llama-3, surpassing PAIR's 58.4%. GCG + WordGame maintained over 80% ASR under strict evaluators. Both hybrids bypassed advanced defenses.

Conclusion: The hybrids expose vulnerabilities in current safety measures, emphasizing the need for holistic defenses against adaptive attacks.

Abstract: The advancement of Pre-Trained Language Models (PTLMs) and Large Language
Models (LLMs) has led to their widespread adoption across diverse applications.
Despite their success, these models remain vulnerable to attacks that exploit
their inherent weaknesses to bypass safety measures. Two primary
inference-phase threats are token-level and prompt-level jailbreaks.
Token-level attacks embed adversarial sequences that transfer well to black-box
models like GPT but leave detectable patterns and rely on gradient-based token
optimization, whereas prompt-level attacks use semantically structured inputs
to elicit harmful responses yet depend on iterative feedback that can be
unreliable. To address the complementary limitations of these methods, we
propose two hybrid approaches that integrate token- and prompt-level techniques
to enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the
newly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and
Llama models. GCG + PAIR consistently raised attack-success rates over its
constituent techniques on undefended models; for instance, on Llama-3, its
Attack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's
58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of
WordGame maintaining a high ASR of over 80% even under stricter evaluators like
Mistral-Sorry-Bench. Crucially, both hybrids retained transferability and
reliably pierced advanced defenses such as Gradient Cuff and JBShield, which
fully blocked single-mode attacks. These findings expose previously unreported
vulnerabilities in current safety stacks, highlight trade-offs between raw
success and defensive robustness, and underscore the need for holistic
safeguards against adaptive adversaries.

</details>


### [240] [Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit](https://arxiv.org/abs/2506.21990)
*Kartheek Kumar Reddy Nareddy,Sarah Ternus,Julia Niebling*

Main category: cs.CL

TL;DR: The paper improves transcription accuracy of cockpit conversations using Whisper models, reducing WER from 68.49% to 26.26% via fine-tuning and normalization.


<details>
  <summary>Details</summary>
Motivation: Transformer models struggle with niche domains like cockpit speech due to specialized vocabulary and multilingual content.

Method: Collected and labeled cockpit recordings, proposed normalization schemes, and fine-tuned Whisper models using LoRA.

Result: WER decreased significantly from 68.49% to 26.26%.

Conclusion: Fine-tuning and normalization effectively enhance ASR performance for niche domains.

Abstract: The developments in transformer encoder-decoder architectures have led to
significant breakthroughs in machine translation, Automatic Speech Recognition
(ASR), and instruction-based chat machines, among other applications. The
pre-trained models were trained on vast amounts of generic data over a few
epochs (fewer than five in most cases), resulting in their strong
generalization capabilities. Nevertheless, the performance of these models does
suffer when applied to niche domains like transcribing pilot speech in the
cockpit, which involves a lot of specific vocabulary and multilingual
conversations. This paper investigates and improves the transcription accuracy
of cockpit conversations with Whisper models. We have collected around 85
minutes of cockpit simulator recordings and 130 minutes of interview recordings
with pilots and manually labeled them. The speakers are middle aged men
speaking both German and English. To improve the accuracy of transcriptions, we
propose multiple normalization schemes to refine the transcripts and improve
Word Error Rate (WER). We then employ fine-tuning to enhance ASR performance,
utilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA).
Hereby, WER decreased from 68.49 \% (pretrained whisper Large model without
normalization baseline) to 26.26\% (finetuned whisper Large model with the
proposed normalization scheme).

</details>


### [241] [Don't Trust Generative Agents to Mimic Communication on Social Networks Unless You Benchmarked their Empirical Realism](https://arxiv.org/abs/2506.21974)
*Simon Münker,Nils Schwager,Achim Rettinger*

Main category: cs.CL

TL;DR: The paper examines the use of LLMs to simulate social network user behavior, emphasizing the need for rigorous validation of empirical realism in social simulations.


<details>
  <summary>Details</summary>
Motivation: Conflicting findings on LLMs mimicking human behavior necessitate a deeper understanding of experimental designs for reliable social simulations.

Method: A formal framework for social network simulation is proposed, with empirical testing of LLM-based user behavior imitation on X in English and German.

Result: Findings highlight the importance of validating simulations by empirical realism in the context they were fitted.

Conclusion: The paper advocates for more rigorous generative-agent-based modeling in social simulations.

Abstract: The ability of Large Language Models (LLMs) to mimic human behavior triggered
a plethora of computational social science research, assuming that empirical
studies of humans can be conducted with AI agents instead. Since there have
been conflicting research findings on whether and when this hypothesis holds,
there is a need to better understand the differences in their experimental
designs. We focus on replicating the behavior of social network users with the
use of LLMs for the analysis of communication on social networks. First, we
provide a formal framework for the simulation of social networks, before
focusing on the sub-task of imitating user communication. We empirically test
different approaches to imitate user behavior on X in English and German. Our
findings suggest that social simulations should be validated by their empirical
realism measured in the setting in which the simulation components were fitted.
With this paper, we argue for more rigor when applying generative-agent-based
modeling for social simulation.

</details>


### [242] [Identifying a Circuit for Verb Conjugation in GPT-2](https://arxiv.org/abs/2506.22105)
*David Demitri Africa*

Main category: cs.CL

TL;DR: The study isolates and interprets a sub-network in GPT-2 Small responsible for subject-verb agreement, identifying a small but effective circuit for verb conjugation.


<details>
  <summary>Details</summary>
Motivation: To understand how GPT-2 Small processes subject-verb agreement and identify the specific sub-network responsible for this task.

Method: The study uses prompts with singular/plural subjects, performance verification, automatic circuit discovery via direct path patching, and direct logit attribution.

Result: A small fraction of the network's component-token pairs achieves near-model performance for verb conjugation, but more complexity requires additional components.

Conclusion: The identified circuit is efficient for basic verb conjugation but insufficient for more complex scenarios, highlighting the model's modularity.

Abstract: I implement a procedure to isolate and interpret the sub-network (or
"circuit") responsible for subject-verb agreement in GPT-2 Small. In this
study, the model is given prompts where the subject is either singular (e.g.
"Alice") or plural (e.g. "Alice and Bob"), and the task is to correctly predict
the appropriate verb form ("walks" for singular subjects, "walk" for plural
subjects). Using a series of techniques-including performance verification
automatic circuit discovery via direct path patching, and direct logit
attribution- I isolate a candidate circuit that contributes significantly to
the model's correct verb conjugation. The results suggest that only a small
fraction of the network's component-token pairs is needed to achieve near-model
performance on the base task but substantially more for more complex settings.

</details>


### [243] [Can Peter Pan Survive MT? A Stylometric Study of LLMs, NMTs, and HTs in Children's Literature Translation](https://arxiv.org/abs/2506.22038)
*Delu Kong,Lieve Macken*

Main category: cs.CL

TL;DR: The study evaluates machine translations (MTs) vs. human translations (HTs) in English-to-Chinese children's literature using stylometric analysis. Results show LLMs align closer to HTs in style than NMTs.


<details>
  <summary>Details</summary>
Motivation: To assess the stylistic performance of MTs (NMTs and LLMs) compared to HTs in children's literature translation.

Method: Constructed a Peter Pan corpus with 21 translations (7 HTs, 7 LLMs, 7 NMTs). Analyzed 447 linguistic features using classification and clustering.

Result: HTs and MTs differ in generic features like conjunction word distributions. LLMs outperform NMTs in CTT-specific features, aligning more with HTs.

Conclusion: LLMs show potential in children's literature translation by mimicking human stylistic traits better than NMTs.

Abstract: This study focuses on evaluating the performance of machine translations
(MTs) compared to human translations (HTs) in English-to-Chinese children's
literature translation (CLT) from a stylometric perspective. The research
constructs a Peter Pan corpus, comprising 21 translations: 7 human translations
(HTs), 7 large language model translations (LLMs), and 7 neural machine
translation outputs (NMTs). The analysis employs a generic feature set
(including lexical, syntactic, readability, and n-gram features) and a creative
text translation (CTT-specific) feature set, which captures repetition, rhythm,
translatability, and miscellaneous levels, yielding 447 linguistic features in
total.
  Using classification and clustering techniques in machine learning, we
conduct a stylometric analysis of these translations. Results reveal that in
generic features, HTs and MTs exhibit significant differences in conjunction
word distributions and the ratio of 1-word-gram-YiYang, while NMTs and LLMs
show significant variation in descriptive words usage and adverb ratios.
Regarding CTT-specific features, LLMs outperform NMTs in distribution, aligning
more closely with HTs in stylistic characteristics, demonstrating the potential
of LLMs in CLT.

</details>


### [244] [Decoding Machine Translationese in English-Chinese News: LLMs vs. NMTs](https://arxiv.org/abs/2506.22050)
*Delu Kong,Lieve Macken*

Main category: cs.CL

TL;DR: The study identifies Machine Translationese (MTese) in English-to-Chinese news texts, using a large dataset and a five-layer feature set. It distinguishes original Chinese from MT outputs and compares NMTs and LLMs, noting linguistic patterns like shorter sentences and adversative conjunctions.


<details>
  <summary>Details</summary>
Motivation: To investigate the under-researched linguistic peculiarities (MTese) in English-to-Chinese machine translation outputs, particularly in news texts.

Method: Constructed a large dataset with 4 sub-corpora, applied a five-layer feature set, and used a chi-square ranking algorithm for feature selection in classification and clustering tasks.

Result: Confirmed MTese presence in NMTs and LLMs, with original Chinese texts distinguishable from MT outputs. Notable patterns include shorter sentences and increased adversative conjunctions. LLMs showed higher lexical diversity, while NMTs used more brackets.

Conclusion: MTese is evident in English-to-Chinese translations, with distinct patterns in NMTs and LLMs. No significant differences were found between Chinese and foreign-developed LLMs.

Abstract: This study explores Machine Translationese (MTese) -- the linguistic
peculiarities of machine translation outputs -- focusing on the
under-researched English-to-Chinese language pair in news texts. We construct a
large dataset consisting of 4 sub-corpora and employ a comprehensive five-layer
feature set. Then, a chi-square ranking algorithm is applied for feature
selection in both classification and clustering tasks. Our findings confirm the
presence of MTese in both Neural Machine Translation systems (NMTs) and Large
Language Models (LLMs). Original Chinese texts are nearly perfectly
distinguishable from both LLM and NMT outputs. Notable linguistic patterns in
MT outputs are shorter sentence lengths and increased use of adversative
conjunctions. Comparing LLMs and NMTs, we achieve approximately 70%
classification accuracy, with LLMs exhibiting greater lexical diversity and
NMTs using more brackets. Additionally, translation-specific LLMs show lower
lexical diversity but higher usage of causal conjunctions compared to generic
LLMs. Lastly, we find no significant differences between LLMs developed by
Chinese firms and their foreign counterparts.

</details>


### [245] [Lost at the Beginning of Reasoning](https://arxiv.org/abs/2506.22058)
*Baohao Liao,Xinyi Chen,Sara Rajaee,Yuhui Xu,Christian Herold,Anders Søgaard,Maarten de Rijke,Christof Monz*

Main category: cs.CL

TL;DR: The paper explores the impact of the first reasoning step in LLMs' chain-of-thought reasoning, proposes a cost-efficient sampling strategy to improve it, and introduces a benchmark for evaluating self-correction.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in LLMs' reasoning, self-correction during long chain-of-thought reasoning is underexplored, and redundant reasoning (overthinking) is a known issue.

Method: The authors empirically analyze the influence of the first reasoning step, propose a reward-model-based sampling strategy to optimize it, and create a benchmark with flawed first steps.

Result: The proposed method reduces inference cost by up to 70% without accuracy loss, and the benchmark aids systematic evaluation of self-correction.

Conclusion: The work highlights the critical role of the first reasoning step, offers a practical solution to improve efficiency, and provides a tool for future research on robust reasoning in LLMs.

Abstract: Recent advancements in large language models (LLMs) have significantly
advanced complex reasoning capabilities, particularly through extended
chain-of-thought (CoT) reasoning that incorporates mechanisms such as
backtracking, self-reflection and self-correction. Despite these developments,
the self-correction abilities of LLMs during long CoT reasoning remain
underexplored. And recent findings on overthinking suggest that such models
often engage in unnecessarily redundant reasoning. In this work, we empirically
show that the first reasoning step exerts a disproportionately large influence
on the final prediction - errors introduced at this stage can substantially
degrade subsequent reasoning quality. This phenomenon is consistently observed
across two state-of-the-art open-source reasoning model families: DeepSeek-R1
and Qwen3. To address this, we propose an efficient sampling strategy that
leverages a reward model to identify and retain high-quality first reasoning
steps while discarding suboptimal ones, achieving up to a 70% reduction in
inference cost without sacrificing accuracy. Finally, we introduce a new
benchmark specifically constructed with deliberately flawed first reasoning
steps to systematically evaluate model self-correction capabilities, offering a
foundation for future research on robust reasoning in LLMs.

</details>


### [246] [MDC-R: The Minecraft Dialogue Corpus with Reference](https://arxiv.org/abs/2506.22062)
*Chris Madge,Maris Camilleri,Paloma Carretero Garcia,Mladen Karan,Juexi Shao,Prashant Jayannavar,Julian Hough,Benjamin Roth,Massimo Poesio*

Main category: cs.CL

TL;DR: The paper introduces MDC-R, an annotated version of the Minecraft Dialogue Corpus (MDC) with expert annotations for anaphoric and deictic reference, highlighting its value for linguistic research and referring expression comprehension.


<details>
  <summary>Details</summary>
Motivation: The dynamic, task-oriented, multi-turn dialogues in MDC present interesting linguistic phenomena, making it a valuable resource when annotated with reference.

Method: The authors describe their annotation process for MDC-R, analyze the corpus quantitatively and qualitatively, and conduct an experiment on referring expression comprehension.

Result: MDC-R is presented as a useful resource, with analysis and an experiment demonstrating its applicability for understanding referring expressions.

Conclusion: MDC-R enhances the original MDC with reference annotations, proving valuable for linguistic research and practical applications like referring expression comprehension.

Abstract: We introduce the Minecraft Dialogue Corpus with Reference (MDC-R). MDC-R is a
new language resource that supplements the original Minecraft Dialogue Corpus
(MDC) with expert annotations of anaphoric and deictic reference. MDC's
task-orientated, multi-turn, situated dialogue in a dynamic environment has
motivated multiple annotation efforts, owing to the interesting linguistic
phenomena that this setting gives rise to. We believe it can serve as a
valuable resource when annotated with reference, too. Here, we discuss our
method of annotation and the resulting corpus, and provide both a quantitative
and a qualitative analysis of the data. Furthermore, we carry out a short
experiment demonstrating the usefulness of our corpus for referring expression
comprehension.

</details>


### [247] [Involvement drives complexity of language in online debates](https://arxiv.org/abs/2506.22098)
*Eleonora Amadori,Daniele Cirulli,Edoardo Di Martino,Jacopo Nudo,Maria Sahakyan,Emanuele Sangiorgio,Arnaldo Santoro,Simon Zollo,Alessandro Galeazzi,Niccolò Di Marco*

Main category: cs.CL

TL;DR: The paper analyzes linguistic complexity in Twitter content on COVID-19, COP26, and the Russia-Ukraine war, revealing variations based on account type, political leaning, reliability, and sentiment.


<details>
  <summary>Details</summary>
Motivation: To understand how language on social media reflects societal and ideological structures, especially in contested topics.

Method: Combined measures of textual complexity to analyze Twitter content across four dimensions: account type, political leaning, reliability, and sentiment.

Result: Found significant linguistic differences across all dimensions, with complex language linked to negative/offensive content and ideological convergence.

Conclusion: The study provides insights into sociolinguistic dynamics online, showing how language mirrors ideological and social structures.

Abstract: Language is a fundamental aspect of human societies, continuously evolving in
response to various stimuli, including societal changes and intercultural
interactions. Technological advancements have profoundly transformed
communication, with social media emerging as a pivotal force that merges
entertainment-driven content with complex social dynamics. As these platforms
reshape public discourse, analyzing the linguistic features of user-generated
content is essential to understanding their broader societal impact. In this
paper, we examine the linguistic complexity of content produced by influential
users on Twitter across three globally significant and contested topics:
COVID-19, COP26, and the Russia-Ukraine war. By combining multiple measures of
textual complexity, we assess how language use varies along four key
dimensions: account type, political leaning, content reliability, and
sentiment. Our analysis reveals significant differences across all four axes,
including variations in language complexity between individuals and
organizations, between profiles with sided versus moderate political views, and
between those associated with higher versus lower reliability scores.
Additionally, profiles producing more negative and offensive content tend to
use more complex language, with users sharing similar political stances and
reliability levels converging toward a common jargon. Our findings offer new
insights into the sociolinguistic dynamics of digital platforms and contribute
to a deeper understanding of how language reflects ideological and social
structures in online spaces.

</details>


### [248] [DAPFAM: A Domain-Aware Patent Retrieval Dataset Aggregated at the Family Level](https://arxiv.org/abs/2506.22141)
*Iliass Ayaou,Denis Cavallucci,Hicham Chibane*

Main category: cs.CL

TL;DR: DAPFAM is a new domain-aware patent retrieval dataset addressing gaps in existing datasets by offering balanced query domains, multi-jurisdiction coverage, and manageable size for sub-document experiments.


<details>
  <summary>Details</summary>
Motivation: Existing patent retrieval datasets lack explicit in-domain/out-of-domain labeling, balanced query domains, and manageable sizes for moderate computational resources.

Method: Constructed at the simple-family level, DAPFAM includes 1,247 domain-balanced query families and 45,336 target families, enriched with relevance judgments and IPC-based labeling.

Result: The dataset contains 49,869 evaluation pairs, supports sub-document retrieval, and baseline experiments reveal challenges in cross-domain patent retrieval.

Conclusion: DAPFAM fills critical gaps in patent retrieval datasets and is publicly available for research.

Abstract: In the landscape of publicly available patent retrieval datasets, the need
for explicit indomain and out-of-domain labeling, multi-jurisdiction coverage,
balanced query domain representation and manageable sizes that support sub
document level experiments on moderate computational resources is often
overlooked. To address these gaps, we propose DAPFAM, a new open access
domain-aware patent retrieval dataset constructed at the simple-family level.
The dataset contains 1,247 domain balanced full text query families and 45,336
full text target families. The dataset is enriched by clear relevance judgments
(forward/backward citations as positive links, random negatives), as well as
explicit in-domain or out-of-domain relationships via a novel proposed
labelling scheme based on via International Patent Classification (IPC) codes,
resulting in 49,869 evaluation pairs. The dataset is multi jurisdictional,
requires little to no preprocessing for retrieval evaluation, and remains of a
size manageable for entities with limited ressources allowing for sub document
level retrieval experiments without excessive computational costs. We describe
our three-step data-curation pipeline, present comprehensive dataset
statistics, and provide baseline experiments using lexical and neural retrieval
methods. Our baseline experiments highlight significant challenges in
crossdomain patent retrieval. The dataset will be publicly available (for now
the access link is this repository:
https://osf.io/vbyzd/?view_only=1a40242e0d1941a58aa854af3e50cf6b).

</details>


### [249] [SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in Low-Resource Arabic-English Code-Switched Speech Recognition](https://arxiv.org/abs/2506.22143)
*Muhammad Umar Farooq,Oscar Saz*

Main category: cs.CL

TL;DR: The paper improves speech recognition for dialectal Arabic and Arabic-English code-switched speech using a novel audio-splicing method and experience replay, achieving significant WER reductions.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity and improving performance for dialectal Arabic and code-switched speech recognition.

Method: Proposes Spliced-Audio Generated (SAGE) data for artificial CS speech and Experience Replay (ER) for generalization. Fine-tunes SSL models and integrates out-of-domain language models.

Result: Achieves absolute WER improvements of 7.8% on CS benchmarks and reduces mean WER from 31.7% to 26.6%. Outperforms larger models by 5.5-8.4%.

Conclusion: The proposed methods effectively enhance speech recognition for dialectal Arabic and code-switched speech, surpassing larger models despite data scarcity.

Abstract: This paper investigates the performance of various speech SSL models on
dialectal Arabic (DA) and Arabic-English code-switched (CS) speech. To address
data scarcity, a modified audio-splicing approach is introduced to generate
artificial CS speech data. Fine-tuning an already fine-tuned SSL model with the
proposed Spliced-Audio Generated (SAGE) data results in an absolute improvement
on Word Error Rate (WER) of 7.8% on Arabic and English CS benchmarks.
Additionally, an Experience Replay (ER) inspired approach is proposed to
enhance generalisation across DA and CS speech while mitigating catastrophic
forgetting. Integrating an out-of-domain 3-gram language model reduces the
overall mean WER from 31.7% to 26.6%. Few-shot fine-tuning for code-switching
benchmarks further improves WER by 4.9%. A WER of 31.1% on Arabic-English CS
benchmarks surpasses large-scale multilingual models, including USM and
Whisper-large-v2 (both over ten times larger) by an absolute margin of 5.5% and
8.4%, respectively.

</details>


### [250] [Training Language Model to Critique for Better Refinement](https://arxiv.org/abs/2506.22157)
*Tianshu Yu,Chao Xiang,Mingchuan Yang,Pei Ke,Bosi Wen,Cunxiang Wang,Jiale Cheng,Li Zhang,Xinyu Mu,Chuxiong Sun,Minlie Huang*

Main category: cs.CL

TL;DR: RCO is a framework to train critic models using refinement signals, improving LLM critiques and responses without direct preference assessment.


<details>
  <summary>Details</summary>
Motivation: Limited research exists on effective critiques for improving LLM responses, prompting the need for RCO.

Method: RCO uses a feedback loop with critique utility (CU) as a reward signal to train critic models, focusing on meaningful refinements.

Result: RCO outperforms traditional methods and open-source models in critique quality and refinement across five tasks.

Conclusion: RCO effectively enhances LLM critique-refinement loops, introducing a novel supervision scheme and demonstrating superior performance.

Abstract: Large language models (LLMs) have demonstrated remarkable evaluation and
critique capabilities, providing insightful feedback and identifying flaws in
various tasks. However, limited research has explored which types of critiques
are most effective for improving model responses or how to generate such
critiques. To address this gap, we introduce \textbf{R}efinement-oriented
\textbf{C}ritique \textbf{O}ptimization (RCO), a novel framework designed to
train critic models using refinement signals. RCO uses a feedback loop where
critiques, generated by the critic model, guide the actor model in refining its
responses. The critique utility (CU) quantifies the effectiveness of these
refinements, serving as the reward signal for training the critic model. By
focusing on critiques that lead to better refinements, RCO eliminates the need
for direct critique preference assessment, ensuring that critiques driving
meaningful improvements are rewarded. We evaluate RCO across five tasks, i.e.,
dialog generation, summarization, question answering, mathematical reasoning,
and code generation, and show that it significantly outperforms traditional
methods and open-source models in terms of critique quality and refinement
outcomes. Our contributions include the introduction of RCO, a novel
supervision scheme based on refined response preferences, and comprehensive
experimental results that highlight the method's effectiveness in enhancing LLM
critique-refinement loops.

</details>


### [251] [Leveraging In-Context Learning for Political Bias Testing of LLMs](https://arxiv.org/abs/2506.22232)
*Patrick Haller,Jannis Vamvas,Rico Sennrich,Lena A. Jäger*

Main category: cs.CL

TL;DR: The paper introduces Questionnaire Modeling (QM) to improve bias evaluation in LLMs by using human survey data as context, showing better stability and enabling comparisons between instruction-tuned and base models.


<details>
  <summary>Details</summary>
Motivation: Current methods for evaluating political biases in LLMs lack stability, making comparisons unreliable. The paper aims to address this by providing more context.

Method: Proposes QM, a probing task using human survey data as in-context examples to evaluate bias more stably.

Result: QM improves bias evaluation stability, reveals bias changes due to instruction tuning, and shows larger models leverage context better with smaller bias scores.

Conclusion: QM offers a more reliable way to assess biases in LLMs, highlighting the impact of instruction tuning and model size on bias.

Abstract: A growing body of work has been querying LLMs with political questions to
evaluate their potential biases. However, this probing method has limited
stability, making comparisons between models unreliable. In this paper, we
argue that LLMs need more context. We propose a new probing task, Questionnaire
Modeling (QM), that uses human survey data as in-context examples. We show that
QM improves the stability of question-based bias evaluation, and demonstrate
that it may be used to compare instruction-tuned models to their base versions.
Experiments with LLMs of various sizes indicate that instruction tuning can
indeed change the direction of bias. Furthermore, we observe a trend that
larger models are able to leverage in-context examples more effectively, and
generally exhibit smaller bias scores in QM. Data and code are publicly
available.

</details>


### [252] [Detection of Personal Data in Structured Datasets Using a Large Language Model](https://arxiv.org/abs/2506.22305)
*Albert Agisha Ntwali,Luca Rück,Martin Heckmann*

Main category: cs.CL

TL;DR: A novel GPT-4o-based method for detecting personal data in structured datasets outperforms alternatives like Microsoft Presidio and CASSED, especially when leveraging contextual information.


<details>
  <summary>Details</summary>
Motivation: Improving personal data detection in structured datasets by incorporating contextual information beyond feature names and values.

Method: Uses GPT-4o to analyze feature names, values, and contextual information from dataset descriptions and other features. Evaluated against Microsoft Presidio and CASSED on multiple datasets.

Result: GPT-4o outperforms others on real-world datasets (e.g., MIMIC-Demo-Ext) and benefits from contextual information, while CASSED excels on its trained synthetic dataset (DeSSI).

Conclusion: More real-world datasets with personal information are needed to advance the field of personal data detection.

Abstract: We propose a novel approach for detecting personal data in structured
datasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key
innovation of our method is the incorporation of contextual information: in
addition to a feature's name and values, we utilize information from other
feature names within the dataset as well as the dataset description. We compare
our approach to alternative methods, including Microsoft Presidio and CASSED,
evaluating them on multiple datasets: DeSSI, a large synthetic dataset,
datasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a
real-world dataset containing patient information from critical care units.
  Our findings reveal that detection performance varies significantly depending
on the dataset used for evaluation. CASSED excels on DeSSI, the dataset on
which it was trained. Performance on the medical dataset MIMIC-Demo-Ext is
comparable across all models, with our GPT-4o-based approach clearly
outperforming the others. Notably, personal data detection in the Kaggle and
OpenML datasets appears to benefit from contextual information. This is
evidenced by the poor performance of CASSED and Presidio (both of which do not
utilize the context of the dataset) compared to the strong results of our
GPT-4o-based approach.
  We conclude that further progress in this field would greatly benefit from
the availability of more real-world datasets containing personal information.

</details>


### [253] [Evaluating Scoring Bias in LLM-as-a-Judge](https://arxiv.org/abs/2506.22316)
*Qingquan Li,Shaoyu Dou,Kailai Shao,Chao Chen,Haixiang Hu*

Main category: cs.CL

TL;DR: The paper explores biases in LLM-as-a-Judge systems, focusing on scoring-based evaluations, and proposes a framework to assess and mitigate these biases.


<details>
  <summary>Details</summary>
Motivation: The widespread use of LLMs as evaluators (LLM-as-a-Judge) introduces biases, affecting fairness and reliability, especially in scoring-based evaluations, which are understudied.

Method: The study defines scoring bias, constructs an evaluation dataset via data synthesis, and designs multi-faceted metrics to assess bias. It also explores mitigation strategies.

Result: Experiments show scoring biases disrupt judge models' stability. Insights are provided on prompt design and bias mitigation (e.g., score rubrics, reference answers).

Conclusion: The work highlights the need for systematic bias evaluation in LLM-as-a-Judge systems and offers practical guidance for improving fairness in scoring-based evaluations.

Abstract: The remarkable performance of Large Language Models (LLMs) gives rise
to``LLM-as-a-Judge'', where LLMs are employed as evaluators for complex tasks.
Moreover, it has been widely adopted across fields such as Natural Language
Processing (NLP), preference learning, and various specific domains. However,
there are various biases within LLM-as-a-Judge, which adversely affect the
fairness and reliability of judgments. Current research on evaluating or
mitigating bias in LLM-as-a-Judge predominantly focuses on comparison-based
evaluations, while systematic investigations into bias in scoring-based
evaluations remain limited. Therefore, we define scoring bias in LLM-as-a-Judge
as the scores differ when scoring judge models are bias-related perturbed, and
provide a well-designed framework to comprehensively evaluate scoring bias. We
augment existing LLM-as-a-Judge benchmarks through data synthesis to construct
our evaluation dataset and design multi-faceted evaluation metrics. Our
experimental results demonstrate that the scoring stability of existing judge
models is disrupted by scoring biases. Further exploratory experiments and
discussions provide valuable insights into the design of scoring prompt
templates and the mitigation of scoring biases on aspects such as score
rubrics, score IDs, and reference answer selection.

</details>


### [254] [Why Are Parsing Actions for Understanding Message Hierarchies Not Random?](https://arxiv.org/abs/2506.22366)
*Daichi Kato,Ryo Ueda,Yusuke Miyao*

Main category: cs.CL

TL;DR: The paper explores why human parsing isn't random, testing if random parsing strategies still work with complex inputs and surprisal-based objectives.


<details>
  <summary>Details</summary>
Motivation: To understand why human parsing isn't random, despite random strategies achieving high accuracy in simpler setups.

Method: Modify experiments with (I) complex hierarchical inputs and (II) a surprisal-related objective, then test communication accuracy of random parsing.

Result: Evaluates whether random parsing remains effective under these new conditions.

Conclusion: The study aims to clarify the limitations or robustness of random parsing in language comprehension.

Abstract: If humans understood language by randomly selecting parsing actions, it might
have been necessary to construct a robust symbolic system capable of being
interpreted under any hierarchical structure. However, human parsing strategies
do not seem to follow such a random pattern. Why is that the case? In fact, a
previous study on emergent communication using models with hierarchical biases
have reported that agents adopting random parsing
strategies$\unicode{x2013}$ones that deviate significantly from human language
comprehension$\unicode{x2013}$can achieve high communication accuracy. In this
study, we investigate this issue by making two simple and natural modifications
to the experimental setup: (I) we use more complex inputs that have
hierarchical structures, such that random parsing makes semantic interpretation
more difficult, and (II) we incorporate a surprisal-related term, which is
known to influence the order of words and characters in natural language, into
the objective function. With these changes, we evaluate whether agents
employing random parsing strategies still maintain high communication accuracy.

</details>


### [255] [QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization](https://arxiv.org/abs/2506.22396)
*Danush Khanna,Aditya Kumar Guru,Srivarshinee Sridhar,Zidan Ahmed,Rubhav Bahirwani,Meetu Malhotra,Vinija Jain,Aman Chadha,Amitava Das,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: QuickSilver is a token-level framework for runtime optimization in LLMs, reducing FLOPs by up to 39.6% without altering model weights or structure.


<details>
  <summary>Details</summary>
Motivation: Inference latency and energy consumption dominate LLM deployment costs, but existing optimization methods require retraining or architectural changes. QuickSilver addresses this by enabling semantic adaptivity at inference time.

Method: QuickSilver integrates three mechanisms: Dynamic Token Halting, KV Cache Skipping, and Contextual Token Fusion, all operating on frozen models without auxiliary networks.

Result: Applied to GPT-2 and Llama-2, QuickSilver achieves up to 39.6% FLOP reduction with minimal perplexity increase (<=0.2).

Conclusion: QuickSilver offers a modular, efficient solution for runtime optimization in LLMs, compatible with existing models and requiring no retraining.

Abstract: Inference accounts for the majority of latency and energy consumption in
large language model (LLM) deployments, often exceeding 90% of total cost.
While training-time efficiency has seen extensive progress, runtime
optimization remains a key bottleneck, particularly under autoregressive
decoding. Existing approaches -- such as pruning, quantization, early exits,
and speculative decoding -- often require retraining, architectural changes, or
disrupt decoding compatibility. We introduce QuickSilver, a modular,
token-level framework that enables semantic adaptivity at inference time
without altering model weights or structure. QuickSilver integrates four
synergistic mechanisms:
  (i) Dynamic Token Halting, which halts computation for tokens with converged
representations; (ii) KV Cache Skipping, which selectively suppresses memory
writes to reduce attention overhead; and (iii) Contextual Token Fusion, which
collapses redundant tokens into shared paths to shrink sequence length.
  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on
frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and
Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP
reduction with negligible perplexity degradation (<=0.2).

</details>


### [256] [Refining Czech GEC: Insights from a Multi-Experiment Approach](https://arxiv.org/abs/2506.22402)
*Petr Pechman,Milan Straka,Jana Straková,Jakub Náplava*

Main category: cs.CL

TL;DR: A state-of-the-art grammar error correction (GEC) system for Czech, using a Transformer-based neural network with real-time synthetic error generation, outperforms in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To advance Czech GEC by leveraging synthetic error generation and exploring various experimental factors like error strategies, domain balancing, and model scaling.

Method: Transformer-based neural network with dynamic synthetic error generation (language-agnostic and Czech-specific), tested on Czech GEC corpora with varied strategies and fine-tuning.

Result: Best-performing model excels in performance and computational efficiency, with evaluations on LLMs for Czech GEC.

Conclusion: The system sets a new benchmark for Czech GEC, with open-source availability for further research and application.

Abstract: We present a grammar error correction (GEC) system that achieves state of the
art for the Czech language. Our system is based on a neural network translation
approach with the Transformer architecture, and its key feature is its
real-time synthetic generation pipeline, which dynamically augments sentences
with artificial errors by introducing both language-agnostic and Czech-specific
errors. We conduct a comprehensive series of experiments, investigating the
Czech GEC corpora as bases for synthetic error introduction, several error
generation strategies, domain balancing, tokenization granularity, model size,
and data scaling during fine-tuning. Additionally, we evaluate the performance
of large language models (LLMs) on Czech GEC in both end-user and expert
fine-tuning scenarios. Our best-performing model is superior both in
performance and computational efficiency. The source code and the trained model
links are available on https://github.com/ufal/tsd2025-gec.

</details>


### [257] [HyperCLOVA X THINK Technical Report](https://arxiv.org/abs/2506.22403)
*NAVER Cloud HyperCLOVA X Team*

Main category: cs.CL

TL;DR: HyperCLOVA X THINK is a reasoning-focused large language model, pre-trained on 6 trillion Korean and English tokens, with competitive performance on Korean benchmarks and bilingual consistency. It includes a vision-augmented variant matching GPT-4.1 on STEM tasks, achieved with lower compute.


<details>
  <summary>Details</summary>
Motivation: To create a robust, bilingual (Korean-English) reasoning-focused LLM with competitive performance on Korean benchmarks and efficient training.

Method: Pre-trained on 6 trillion tokens with a Peri-LN Transformer, three-stage curriculum (128K context window), and post-trained via supervised fine-tuning with reinforcement learning. Includes a vision-augmented variant.

Result: Competes with similarly sized models on Korean benchmarks (KMMLU, CSAT, etc.), matches GPT-4.1 on STEM tasks, and maintains bilingual consistency. Achieved with lower compute.

Conclusion: HyperCLOVA X THINK is a strong foundation for Korean AI innovation and a valuable resource globally, with plans for open-source and business-friendly deployment.

Abstract: We introduce HyperCLOVA X THINK, the first reasoning-focused large language
model in the HyperCLOVA X family, pre-trained on roughly $6$ trillion
high-quality Korean, and English tokens, augmented with targeted synthetic
Korean data. It was implemented as a compute-memory-balanced Peri-LN
Transformer scaled with $\mu$P, pre-trained through a three-stage curriculum
that expands the context window to $128$K tokens, and post-trained via
supervised fine-tuning with Reinforcement Learning from Verifiable Rewards
supports both detailed rationale and concise-answer modes. It delivers
competitive performance against similarly sized models on Korea-focused
benchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while
preserving robust bilingual consistency and translation quality. In addition, a
vision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM
benchmark, all of which are achieved with substantially lower training compute
than existing models of similar sizes. We also present a pruning and
distillation technique that will soon be applied to HyperCLOVA X THINK for an
open-source and business-friendly foundation model. Altogether, these
capabilities position HyperCLOVA X THINK as a robust foundation for Korean AI
innovation and a valuable resource for the global research community.

</details>


### [258] [Sequential Diagnosis with Language Models](https://arxiv.org/abs/2506.22405)
*Harsha Nori,Mayank Daswani,Christopher Kelly,Scott Lundberg,Marco Tulio Ribeiro,Marc Wilson,Xiaoxuan Liu,Viknesh Sounderajah,Jonathan Carlson,Matthew P Lungren,Bay Gross,Peter Hames,Mustafa Suleyman,Dominic King,Eric Horvitz*

Main category: cs.CL

TL;DR: The paper introduces the Sequential Diagnosis Benchmark and MAI-DxO, an AI orchestrator, to improve diagnostic accuracy and cost-effectiveness in medicine, achieving 80-85.5% accuracy and reducing costs by 20-70%.


<details>
  <summary>Details</summary>
Motivation: Current AI evaluations in medicine lack real-world complexity. The paper aims to emulate the iterative diagnostic process of physicians to enhance AI's clinical utility.

Method: Developed the Sequential Diagnosis Benchmark using NEJM-CPC cases and MAI-DxO, a model-agnostic orchestrator that simulates physician panels and optimizes test selection.

Result: MAI-DxO achieved 80-85.5% diagnostic accuracy, outperforming generalist physicians (20%) and reducing costs by 20-70%. Performance generalized across multiple AI models.

Conclusion: AI systems like MAI-DxO, designed for iterative reasoning and cost-effective actions, can significantly improve diagnostic precision and efficiency in clinical care.

Abstract: Artificial intelligence holds great promise for expanding access to expert
medical knowledge and reasoning. However, most evaluations of language models
rely on static vignettes and multiple-choice questions that fail to reflect the
complexity and nuance of evidence-based medicine in real-world settings. In
clinical practice, physicians iteratively formulate and revise diagnostic
hypotheses, adapting each subsequent question and test to what they've just
learned, and weigh the evolving evidence before committing to a final
diagnosis. To emulate this iterative process, we introduce the Sequential
Diagnosis Benchmark, which transforms 304 diagnostically challenging New
England Journal of Medicine clinicopathological conference (NEJM-CPC) cases
into stepwise diagnostic encounters. A physician or AI begins with a short case
abstract and must iteratively request additional details from a gatekeeper
model that reveals findings only when explicitly queried. Performance is
assessed not just by diagnostic accuracy but also by the cost of physician
visits and tests performed. We also present the MAI Diagnostic Orchestrator
(MAI-DxO), a model-agnostic orchestrator that simulates a panel of physicians,
proposes likely differential diagnoses and strategically selects high-value,
cost-effective tests. When paired with OpenAI's o3 model, MAI-DxO achieves 80%
diagnostic accuracy--four times higher than the 20% average of generalist
physicians. MAI-DxO also reduces diagnostic costs by 20% compared to
physicians, and 70% compared to off-the-shelf o3. When configured for maximum
accuracy, MAI-DxO achieves 85.5% accuracy. These performance gains with MAI-DxO
generalize across models from the OpenAI, Gemini, Claude, Grok, DeepSeek, and
Llama families. We highlight how AI systems, when guided to think iteratively
and act judiciously, can advance diagnostic precision and cost-effectiveness in
clinical care.

</details>

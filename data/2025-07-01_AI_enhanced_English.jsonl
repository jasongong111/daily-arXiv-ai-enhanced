{"id": "2506.22437", "categories": ["cs.CV", "68T45 (Computer Vision)"], "pdf": "https://arxiv.org/pdf/2506.22437", "abs": "https://arxiv.org/abs/2506.22437", "authors": ["Xinxin Sun", "Peter Chang"], "title": "Robust Perspective Correction for Real-World Crack Evolution Tracking in Image-Based Structural Health Monitoring", "comment": "43 pages, 5 figures, 19 tables. Submitted to NDT&E International.\n  This work may also be of interest to researchers in optical NDE and civil\n  engineering SHM", "summary": "Accurate image alignment is essential for monitoring crack evolution in\nstructural health monitoring (SHM), particularly under real-world conditions\ninvolving perspective distortion, occlusion, and low contrast. However,\ntraditional feature detectors such as SIFT and SURF, which rely on\nGaussian-based scale spaces, tend to suppress high-frequency edges, making them\nunsuitable for thin crack localization. Lightweight binary alternatives like\nORB and BRISK, while computationally efficient, often suffer from poor keypoint\nrepeatability on textured or shadowed surfaces. This study presents a\nphysics-informed alignment framework that adapts the open KAZE architecture to\nSHM-specific challenges. By utilizing nonlinear anisotropic diffusion to\nconstruct a crack-preserving scale space, and integrating RANSAC-based\nhomography estimation, the framework enables accurate geometric correction\nwithout the need for training, parameter tuning, or prior calibration. The\nmethod is validated on time-lapse images of masonry and concrete acquired via\nhandheld smartphone under varied field conditions, including shadow\ninterference, cropping, oblique viewing angles, and surface clutter. Compared\nto classical detectors, the proposed framework reduces crack area and spine\nlength errors by up to 70 percent and 90 percent, respectively, while\nmaintaining sub-5 percent alignment error in key metrics. Unsupervised,\ninterpretable, and computationally lightweight, this approach supports scalable\ndeployment via UAVs and mobile platforms. By tailoring nonlinear scale-space\nmodeling to SHM image alignment, this work offers a robust and physically\ngrounded alternative to conventional techniques for tracking real-world crack\nevolution.", "AI": {"tldr": "A physics-informed alignment framework improves crack localization in structural health monitoring by adapting KAZE architecture, outperforming traditional methods like SIFT and SURF.", "motivation": "Traditional feature detectors (e.g., SIFT, SURF) and lightweight alternatives (e.g., ORB, BRISK) are inadequate for thin crack localization due to high-frequency edge suppression or poor repeatability.", "method": "The framework uses nonlinear anisotropic diffusion for crack-preserving scale space and RANSAC-based homography estimation for geometric correction, requiring no training or tuning.", "result": "The method reduces crack area and spine length errors by up to 70% and 90%, respectively, with sub-5% alignment error in key metrics.", "conclusion": "The approach is robust, interpretable, and lightweight, suitable for scalable deployment in real-world SHM applications."}}
{"id": "2506.22438", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22438", "abs": "https://arxiv.org/abs/2506.22438", "authors": ["Xumin Gao", "Mark Stevens", "Grzegorz Cielniak"], "title": "Counting with Confidence: Accurate Pest Monitoring in Water Traps", "comment": "\\c{opyright} 20XX the authors. This work has been accepted to IFAC\n  for publication under a Creative Commons Licence CC-BY-NC-ND", "summary": "Accurate pest population monitoring and tracking their dynamic changes are\ncrucial for precision agriculture decision-making. A common limitation in\nexisting vision-based automatic pest counting research is that models are\ntypically evaluated on datasets with ground truth but deployed in real-world\nscenarios without assessing the reliability of counting results due to the lack\nof ground truth. To this end, this paper proposed a method for comprehensively\nevaluating pest counting confidence in the image, based on information related\nto counting results and external environmental conditions. First, a pest\ndetection network is used for pest detection and counting, extracting counting\nresult-related information. Then, the pest images undergo image quality\nassessment, image complexity assessment, and pest distribution uniformity\nassessment. And the changes in image clarity caused by stirring during image\nacquisition are quantified by calculating the average gradient magnitude.\nNotably, we designed a hypothesis-driven multi-factor sensitivity analysis\nmethod to select the optimal image quality assessment and image complexity\nassessment methods. And we proposed an adaptive DBSCAN clustering algorithm for\npest distribution uniformity assessment. Finally, the obtained information\nrelated to counting results and external environmental conditions is input into\na regression model for prediction, resulting in the final pest counting\nconfidence. To the best of our knowledge, this is the first study dedicated to\ncomprehensively evaluating counting confidence in counting tasks, and\nquantifying the relationship between influencing factors and counting\nconfidence through a model. Experimental results show our method reduces MSE by\n31.7% and improves R2 by 15.2% on the pest counting confidence test set,\ncompared to the baseline built primarily on information related to counting\nresults.", "AI": {"tldr": "The paper proposes a method to evaluate pest counting confidence by combining counting results and environmental factors, improving accuracy over baseline methods.", "motivation": "Existing pest counting models lack reliability assessment in real-world deployments due to missing ground truth. The study aims to address this gap.", "method": "Uses a pest detection network, image quality/complexity assessments, and pest distribution uniformity analysis. Includes a multi-factor sensitivity analysis and adaptive DBSCAN clustering.", "result": "Reduces MSE by 31.7% and improves R2 by 15.2% on pest counting confidence compared to baseline.", "conclusion": "The study is the first to comprehensively evaluate counting confidence, providing a reliable model for precision agriculture."}}
{"id": "2506.22463", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22463", "abs": "https://arxiv.org/abs/2506.22463", "authors": ["Weizhi Gao", "Zhichao Hou", "Junqi Yin", "Feiyi Wang", "Linyu Peng", "Xiaorui Liu"], "title": "Modulated Diffusion: Accelerating Generative Modeling with Modulated Quantization", "comment": "26 pages, accepted by ICML 2025", "summary": "Diffusion models have emerged as powerful generative models, but their high\ncomputation cost in iterative sampling remains a significant bottleneck. In\nthis work, we present an in-depth and insightful study of state-of-the-art\nacceleration techniques for diffusion models, including caching and\nquantization, revealing their limitations in computation error and generation\nquality. To break these limits, this work introduces Modulated Diffusion\n(MoDiff), an innovative, rigorous, and principled framework that accelerates\ngenerative modeling through modulated quantization and error compensation.\nMoDiff not only inherents the advantages of existing caching and quantization\nmethods but also serves as a general framework to accelerate all diffusion\nmodels. The advantages of MoDiff are supported by solid theoretical insight and\nanalysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate\nthat MoDiff significant reduces activation quantization from 8 bits to 3 bits\nwithout performance degradation in post-training quantization (PTQ). Our code\nimplementation is available at https://github.com/WeizhiGao/MoDiff.", "AI": {"tldr": "MoDiff accelerates diffusion models via modulated quantization and error compensation, reducing activation quantization to 3 bits without performance loss.", "motivation": "High computation cost in iterative sampling of diffusion models is a bottleneck; existing acceleration techniques have limitations in error and quality.", "method": "Introduces Modulated Diffusion (MoDiff), a framework combining modulated quantization and error compensation.", "result": "Reduces activation quantization from 8 to 3 bits without performance degradation, validated on CIFAR-10 and LSUN datasets.", "conclusion": "MoDiff is a principled, general framework for accelerating diffusion models, supported by theory and experiments."}}
{"id": "2506.22498", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22498", "abs": "https://arxiv.org/abs/2506.22498", "authors": ["Hao Liu", "Yu Hu", "Rakiba Rayhana", "Ling Bai", "Zheng Liu"], "title": "ViFusionTST: Deep Fusion of Time-Series Image Representations from Load Signals for Early Bed-Exit Prediction", "comment": null, "summary": "Bed-related falls remain a leading source of injury in hospitals and\nlong-term-care facilities, yet many commercial alarms trigger only after a\npatient has already left the bed. We show that early bed-exit intent can be\npredicted using only four low-cost load cells mounted under the bed legs. The\nresulting load signals are first converted into a compact set of complementary\nimages: an RGB line plot that preserves raw waveforms and three texture maps -\nrecurrence plot, Markov transition field, and Gramian angular field - that\nexpose higher-order dynamics. We introduce ViFusionTST, a dual-stream Swin\nTransformer that processes the line plot and texture maps in parallel and fuses\nthem through cross-attention to learn data-driven modality weights.\n  To provide a realistic benchmark, we collected six months of continuous data\nfrom 95 beds in a long-term-care facility. On this real-world dataset\nViFusionTST reaches an accuracy of 0.885 and an F1 score of 0.794, surpassing\nrecent 1D and 2D time-series baselines across F1, recall, accuracy, and AUPRC.\nThe results demonstrate that image-based fusion of load-sensor signals for time\nseries classification is a practical and effective solution for real-time,\nprivacy-preserving fall prevention.", "AI": {"tldr": "A method using low-cost load cells and image-based fusion predicts bed-exit intent early, outperforming baselines in accuracy and F1 score for fall prevention.", "motivation": "Bed-related falls are a major injury source in healthcare; existing alarms often trigger too late.", "method": "Uses four load cells under bed legs, converts signals into images (RGB line plot and texture maps), and processes them with ViFusionTST, a dual-stream Swin Transformer.", "result": "Achieves 0.885 accuracy and 0.794 F1 score on real-world data, surpassing other time-series methods.", "conclusion": "Image-based fusion of load-sensor signals is effective for real-time, privacy-preserving fall prevention."}}
{"id": "2506.22439", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22439", "abs": "https://arxiv.org/abs/2506.22439", "authors": ["Javier Conde", "Miguel Gonz\u00e1lez", "Mar\u00eda Grandury", "Gonzalo Mart\u00ednez", "Pedro Reviriego", "Mar Brysbaert"], "title": "Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans", "comment": "Accepted for the GEM2 workshop at ACL 2025", "summary": "The evaluation of LLMs has so far focused primarily on how well they can\nperform different tasks such as reasoning, question-answering, paraphrasing, or\ntranslating. For most of these tasks, performance can be measured with\nobjective metrics, such as the number of correct answers. However, other\nlanguage features are not easily quantified. For example, arousal,\nconcreteness, or gender associated with a given word, as well as the extent to\nwhich we experience words with senses and relate them to a specific sense.\nThose features have been studied for many years by psycholinguistics,\nconducting large-scale experiments with humans to produce ratings for thousands\nof words. This opens an opportunity to evaluate how well LLMs align with human\nratings on these word features, taking advantage of existing studies that cover\nmany different language features in a large number of words. In this paper, we\nevaluate the alignment of a representative group of LLMs with human ratings on\ntwo psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets\ncover thirteen features over thousands of words. The results show that\nalignment is \\textcolor{black}{generally} better in the Glasgow norms evaluated\n(arousal, valence, dominance, concreteness, imageability, familiarity, and\ngender) than on the Lancaster norms evaluated (introceptive, gustatory,\nolfactory, haptic, auditory, and visual). This suggests a potential limitation\nof current LLMs in aligning with human sensory associations for words, which\nmay be due to their lack of embodied cognition present in humans and\nillustrates the usefulness of evaluating LLMs with psycholinguistic datasets.", "AI": {"tldr": "The paper evaluates LLMs' alignment with human psycholinguistic ratings on word features, finding better alignment with Glasgow norms than Lancaster norms, suggesting limitations in sensory associations.", "motivation": "To assess how well LLMs align with human ratings on psycholinguistic word features, leveraging existing datasets to uncover potential limitations in LLMs' understanding of sensory and embodied language.", "method": "Evaluation of LLMs using two psycholinguistic datasets (Glasgow and Lancaster norms) covering thirteen word features, comparing alignment with human ratings.", "result": "LLMs align better with Glasgow norms (arousal, valence, etc.) than Lancaster norms (sensory features), indicating limitations in sensory associations.", "conclusion": "Current LLMs may lack embodied cognition for sensory associations, highlighting the value of psycholinguistic evaluations to uncover model limitations."}}
{"id": "2506.22441", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22441", "abs": "https://arxiv.org/abs/2506.22441", "authors": ["Lei Yang"], "title": "Latent Factorization of Tensors with Threshold Distance Weighted Loss for Traffic Data Estimation", "comment": null, "summary": "Intelligent transportation systems (ITS) rely heavily on complete and\nhigh-quality spatiotemporal traffic data to achieve optimal performance.\nNevertheless, in real-word traffic data collection processes, issues such as\ncommunication failures and sensor malfunctions often lead to incomplete or\ncorrupted datasets, thereby posing significant challenges to the advancement of\nITS. Among various methods for imputing missing spatiotemporal traffic data,\nthe latent factorization of tensors (LFT) model has emerged as a widely adopted\nand effective solution. However, conventional LFT models typically employ the\nstandard L2-norm in their learning objective, which makes them vulnerable to\nthe influence of outliers. To overcome this limitation, this paper proposes a\nthreshold distance weighted (TDW) loss-incorporated Latent Factorization of\nTensors (TDWLFT) model. The proposed loss function effectively reduces the\nmodel's sensitivity to outliers by assigning differentiated weights to\nindividual samples. Extensive experiments conducted on two traffic speed\ndatasets sourced from diverse urban environments confirm that the proposed\nTDWLFT model consistently outperforms state-of-the-art approaches in terms of\nboth in both prediction accuracy and computational efficiency.", "AI": {"tldr": "The paper proposes a TDW loss-incorporated LFT model (TDWLFT) to improve spatiotemporal traffic data imputation by reducing outlier sensitivity.", "motivation": "Incomplete or corrupted traffic data due to communication failures and sensor malfunctions hinder ITS performance. Existing LFT models are vulnerable to outliers.", "method": "Introduces a threshold distance weighted (TDW) loss function in the LFT model to assign differentiated weights to samples, reducing outlier impact.", "result": "TDWLFT outperforms state-of-the-art methods in prediction accuracy and computational efficiency on two urban traffic speed datasets.", "conclusion": "The TDWLFT model effectively addresses outlier sensitivity in spatiotemporal traffic data imputation, enhancing ITS performance."}}
{"id": "2506.22499", "categories": ["cs.CV", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2506.22499", "abs": "https://arxiv.org/abs/2506.22499", "authors": ["Jiachao Liu", "Pablo Guarda", "Koichiro Niinuma", "Sean Qian"], "title": "Scalable Dynamic Origin-Destination Demand Estimation Enhanced by High-Resolution Satellite Imagery Data", "comment": null, "summary": "This study presents a novel integrated framework for dynamic\norigin-destination demand estimation (DODE) in multi-class mesoscopic network\nmodels, leveraging high-resolution satellite imagery together with conventional\ntraffic data from local sensors. Unlike sparse local detectors, satellite\nimagery offers consistent, city-wide road and traffic information of both\nparking and moving vehicles, overcoming data availability limitations. To\nextract information from imagery data, we design a computer vision pipeline for\nclass-specific vehicle detection and map matching, generating link-level\ntraffic density observations by vehicle class. Building upon this information,\nwe formulate a computational graph-based DODE model that calibrates dynamic\nnetwork states by jointly matching observed traffic counts and travel times\nfrom local sensors with density measurements derived from satellite imagery. To\nassess the accuracy and scalability of the proposed framework, we conduct a\nseries of numerical experiments using both synthetic and real-world data. The\nresults of out-of-sample tests demonstrate that supplementing traditional data\nwith satellite-derived density significantly improves estimation performance,\nespecially for links without local sensors. Real-world experiments also confirm\nthe framework's capability to handle large-scale networks, supporting its\npotential for practical deployment in cities of varying sizes. Sensitivity\nanalysis further evaluates the impact of data quality related to satellite\nimagery data.", "AI": {"tldr": "A novel framework integrates satellite imagery with local sensor data for dynamic origin-destination demand estimation (DODE) in multi-class mesoscopic networks, improving accuracy and scalability.", "motivation": "Overcome limitations of sparse local sensors by leveraging city-wide satellite imagery for consistent traffic data.", "method": "Develops a computer vision pipeline for vehicle detection and map matching, then formulates a computational graph-based DODE model to calibrate network states.", "result": "Out-of-sample tests show improved estimation performance, especially for unsensed links, and real-world experiments confirm scalability.", "conclusion": "The framework enhances DODE accuracy and scalability, making it viable for practical deployment in cities of varying sizes."}}
{"id": "2506.22485", "categories": ["cs.CL", "cs.AI", "68T07, 68T50", "I.2.1; I.2.3; I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2506.22485", "abs": "https://arxiv.org/abs/2506.22485", "authors": ["Sudip Dasgupta", "Himanshu Shankar"], "title": "AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents", "comment": "17 pages, 2 system diagrams, 1 table, no prior conference publication", "summary": "This study presents a modular, multi-agent system for the automated review of\nhighly structured enterprise business documents using AI agents. Unlike prior\nsolutions focused on unstructured texts or limited compliance checks, this\nframework leverages modern orchestration tools such as LangChain, CrewAI,\nTruLens, and Guidance to enable section-by-section evaluation of documents for\naccuracy, consistency, completeness, and clarity. Specialized agents, each\nresponsible for discrete review criteria such as template compliance or factual\ncorrectness, operate in parallel or sequence as required. Evaluation outputs\nare enforced to a standardized, machine-readable schema, supporting downstream\nanalytics and auditability. Continuous monitoring and a feedback loop with\nhuman reviewers allow for iterative system improvement and bias mitigation.\n  Quantitative evaluation demonstrates that the AI Agent-as-Judge system\napproaches or exceeds human performance in key areas: achieving 99% information\nconsistency (vs. 92% for humans), halving error and bias rates, and reducing\naverage review time from 30 to 2.5 minutes per document, with a 95% agreement\nrate between AI and expert human judgment. While promising for a wide range of\nindustries, the study also discusses current limitations, including the need\nfor human oversight in highly specialized domains and the operational cost of\nlarge-scale LLM usage. The proposed system serves as a flexible, auditable, and\nscalable foundation for AI-driven document quality assurance in the enterprise\ncontext.", "AI": {"tldr": "A modular AI system for reviewing structured business documents outperforms humans in consistency, speed, and accuracy, though it requires human oversight in specialized domains.", "motivation": "To improve the review of structured enterprise documents by addressing limitations of prior solutions focused on unstructured texts or basic compliance checks.", "method": "Uses AI agents orchestrated with tools like LangChain and CrewAI for section-by-section evaluation, enforcing standardized outputs and continuous feedback loops.", "result": "Achieves 99% consistency (vs. 92% for humans), reduces review time from 30 to 2.5 minutes, and halves error/bias rates, with 95% agreement with human experts.", "conclusion": "The system is scalable and flexible for enterprise use but requires human oversight in specialized areas and faces operational costs for large-scale LLM usage."}}
{"id": "2506.22442", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22442", "abs": "https://arxiv.org/abs/2506.22442", "authors": ["Piotr Makarevich"], "title": "Features-based embedding or Feature-grounding", "comment": "13 pages, 12 figures", "summary": "In everyday reasoning, when we think about a particular object, we associate\nit with a unique set of expected properties such as weight, size, or more\nabstract attributes like density or horsepower. These expectations are shaped\nby our prior knowledge and the conceptual categories we have formed through\nexperience. This paper investigates how such knowledge-based structured\nthinking can be reproduced in deep learning models using features based\nembeddings. Specially, it introduces an specific approach to build\nfeature-grounded embedding, aiming to align shareable representations of\noperable dictionary with interpretable domain-specific conceptual features.", "AI": {"tldr": "The paper explores reproducing knowledge-based structured thinking in deep learning models using feature-grounded embeddings.", "motivation": "To align shareable representations with interpretable domain-specific features, leveraging prior knowledge and conceptual categories.", "method": "Introduces a specific approach to build feature-grounded embeddings.", "result": "Aims to create operable dictionary representations with interpretable features.", "conclusion": "Feature-grounded embeddings can bridge deep learning with structured, knowledge-based reasoning."}}
{"id": "2506.22500", "categories": ["cs.CV", "cs.AI", "68T07, 68U10, 92C55", "I.2.10; I.2.7; J.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2506.22500", "abs": "https://arxiv.org/abs/2506.22500", "authors": ["Weiyi Zhao", "Xiaoyu Tan", "Liang Liu", "Sijia Li", "Youwei Song", "Xihe Qiu"], "title": "Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models", "comment": "13 pages, 5 figures. The dataset and appendix are available at\n  https://github.com/zgg2577/VS-KC", "summary": "Surgical risk identification is critical for patient safety and reducing\npreventable medical errors. While multimodal large language models (MLLMs) show\npromise for automated operating room (OR) risk detection, they often exhibit\nvisual-semantic knowledge conflicts (VS-KC), failing to identify visual safety\nviolations despite understanding textual rules. To address this, we introduce a\ndataset comprising over 34,000 synthetic images generated by diffusion models,\ndepicting operating room scenes containing entities that violate established\nsafety rules. These images were created to alleviate data scarcity and examine\nMLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated\nimages that serve as a gold-standard reference for validation. This\ncomprehensive dataset, spanning diverse perspectives, stages, and\nconfigurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC\nsignificantly improves MLLMs' detection of trained conflict entities and\ngeneralizes well to new viewpoints for these entities, but performance on\nuntrained entity types remains poor, highlighting learning specificity and the\nneed for comprehensive training. The main contributions of this work include:\n(1) a data generation methodology tailored for rule-violation scenarios; (2)\nthe release of the OR-VSKC dataset and its associated benchmark as open-source\nresources; and (3) an empirical analysis of violation-sensitive knowledge\nconsistency in representative MLLMs. The dataset and appendix are available at\nhttps://github.com/zgg2577/VS-KC.", "AI": {"tldr": "The paper introduces a synthetic dataset (OR-VSKC) to address visual-semantic knowledge conflicts in MLLMs for surgical risk detection, improving performance on trained entities but showing limitations on untrained ones.", "motivation": "To improve automated operating room risk detection by addressing visual-semantic knowledge conflicts in multimodal large language models (MLLMs).", "method": "Created a dataset of 34,000 synthetic images (and 214 human-annotated ones) depicting safety rule violations, then fine-tuned MLLMs on this dataset.", "result": "Fine-tuning improved detection of trained conflict entities and generalised to new viewpoints, but performance on untrained entities remained poor.", "conclusion": "The OR-VSKC dataset and methodology help expose and address VS-KC in MLLMs, though comprehensive training is needed for broader applicability."}}
{"id": "2506.22486", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22486", "abs": "https://arxiv.org/abs/2506.22486", "authors": ["Ming Cheung"], "title": "Hallucination Detection with Small Language Models", "comment": null, "summary": "Since the introduction of ChatGPT, large language models (LLMs) have\ndemonstrated significant utility in various tasks, such as answering questions\nthrough retrieval-augmented generation. Context can be retrieved using a\nvectorized database, serving as a foundation for LLMs to generate responses.\nHowever, hallucinations in responses can undermine the reliability of LLMs in\npractical applications, and they are not easily detectable in the absence of\nground truth, particularly in question-and-answer scenarios. This paper\nproposes a framework that integrates multiple small language models to verify\nresponses generated by LLMs using the retrieved context from a vectorized\ndatabase. By breaking down the responses into individual sentences and\nutilizing the probability of generating \"Yes\" tokens from the outputs of\nmultiple models for a given set of questions, responses, and relevant context,\nhallucinations can be detected. The proposed framework is validated through\nexperiments with real datasets comprising over 100 sets of questions, answers,\nand contexts, including responses with fully and partially correct sentences.\nThe results demonstrate a 10\\% improvement in F1 scores for detecting correct\nresponses compared to hallucinations, indicating that multiple small language\nmodels can be effectively employed for answer verification, providing a\nscalable and efficient solution for both academic and practical applications.", "AI": {"tldr": "A framework using multiple small language models to verify LLM responses detects hallucinations, improving F1 scores by 10%.", "motivation": "Hallucinations in LLM responses reduce reliability, especially without ground truth. A scalable verification method is needed.", "method": "Integrates small models to verify LLM responses by breaking them into sentences and using \"Yes\" token probabilities.", "result": "10% improvement in F1 scores for detecting correct responses vs. hallucinations.", "conclusion": "Multiple small models effectively verify LLM responses, offering a scalable solution."}}
{"id": "2506.22443", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.22443", "abs": "https://arxiv.org/abs/2506.22443", "authors": ["Sarah Seifi", "Tobias Sukianto", "Cecilia Carbonelli", "Lorenzo Servadei", "Robert Wille"], "title": "Learning Interpretable Rules from Neural Networks: Neurosymbolic AI for Radar Hand Gesture Recognition", "comment": "8 pages, 3 figures, accepted at the late-breaking work track at the\n  XAI-2025 third World Conference of Explainable AI", "summary": "Rule-based models offer interpretability but struggle with complex data,\nwhile deep neural networks excel in performance yet lack transparency. This\nwork investigates a neuro-symbolic rule learning neural network named RL-Net\nthat learns interpretable rule lists through neural optimization, applied for\nthe first time to radar-based hand gesture recognition (HGR). We benchmark\nRL-Net against a fully transparent rule-based system (MIRA) and an explainable\nblack-box model (XentricAI), evaluating accuracy, interpretability, and user\nadaptability via transfer learning. Our results show that RL-Net achieves a\nfavorable trade-off, maintaining strong performance (93.03% F1) while\nsignificantly reducing rule complexity. We identify optimization challenges\nspecific to rule pruning and hierarchy bias and propose stability-enhancing\nmodifications. Compared to MIRA and XentricAI, RL-Net emerges as a practical\nmiddle ground between transparency and performance. This study highlights the\nreal-world feasibility of neuro-symbolic models for interpretable HGR and\noffers insights for extending explainable AI to edge-deployable sensing\nsystems.", "AI": {"tldr": "RL-Net, a neuro-symbolic rule learning neural network, balances interpretability and performance in radar-based hand gesture recognition, outperforming transparent and black-box models.", "motivation": "To bridge the gap between interpretable rule-based models and high-performing deep neural networks in hand gesture recognition.", "method": "RL-Net learns interpretable rule lists via neural optimization, benchmarked against MIRA (rule-based) and XentricAI (explainable black-box).", "result": "RL-Net achieves 93.03% F1 score, reduces rule complexity, and addresses optimization challenges like rule pruning and hierarchy bias.", "conclusion": "RL-Net is a practical middle ground for interpretable HGR, with potential for edge-deployable sensing systems."}}
{"id": "2506.22501", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22501", "abs": "https://arxiv.org/abs/2506.22501", "authors": ["Gautam Siddharth Kashyap", "Manaswi Kulahara", "Nipun Joshi", "Usman Naseem"], "title": "How Can Multimodal Remote Sensing Datasets Transform Classification via SpatialNet-ViT?", "comment": "Accepted in the 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025), scheduled for 3 - 8 August 2025 in Brisbane,\n  Australia", "summary": "Remote sensing datasets offer significant promise for tackling key\nclassification tasks such as land-use categorization, object presence\ndetection, and rural/urban classification. However, many existing studies tend\nto focus on narrow tasks or datasets, which limits their ability to generalize\nacross various remote sensing classification challenges. To overcome this, we\npropose a novel model, SpatialNet-ViT, leveraging the power of Vision\nTransformers (ViTs) and Multi-Task Learning (MTL). This integrated approach\ncombines spatial awareness with contextual understanding, improving both\nclassification accuracy and scalability. Additionally, techniques like data\naugmentation, transfer learning, and multi-task learning are employed to\nenhance model robustness and its ability to generalize across diverse datasets", "AI": {"tldr": "SpatialNet-ViT, a novel model combining Vision Transformers and Multi-Task Learning, improves remote sensing classification accuracy and scalability by integrating spatial awareness and contextual understanding.", "motivation": "Existing studies focus on narrow tasks or datasets, limiting generalization across remote sensing classification challenges.", "method": "Proposes SpatialNet-ViT, leveraging Vision Transformers and Multi-Task Learning, with techniques like data augmentation, transfer learning, and multi-task learning.", "result": "Enhanced classification accuracy and scalability, with improved robustness and generalization across diverse datasets.", "conclusion": "SpatialNet-ViT effectively addresses limitations of narrow-focused studies, offering a versatile solution for remote sensing classification tasks."}}
{"id": "2506.22491", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; J.4; K.4.2"], "pdf": "https://arxiv.org/pdf/2506.22491", "abs": "https://arxiv.org/abs/2506.22491", "authors": ["Oliver Warke", "Joemon M. Jose", "Faegheh Hasibi", "Jan Breitsohl"], "title": "PromptAug: Fine-grained Conflict Classification Using Data Augmentation", "comment": null, "summary": "Given the rise of conflicts on social media, effective classification models\nto detect harmful behaviours are essential. Following the\ngarbage-in-garbage-out maxim, machine learning performance depends heavily on\ntraining data quality. However, high-quality labelled data, especially for\nnuanced tasks like identifying conflict behaviours, is limited, expensive, and\ndifficult to obtain. Additionally, as social media platforms increasingly\nrestrict access to research data, text data augmentation is gaining attention\nas an alternative to generate training data. Augmenting conflict-related data\nposes unique challenges due to Large Language Model (LLM) guardrails that\nprevent generation of offensive content. This paper introduces PromptAug, an\ninnovative LLM-based data augmentation method. PromptAug achieves statistically\nsignificant improvements of 2% in both accuracy and F1-score on conflict and\nemotion datasets. To thoroughly evaluate PromptAug against other data\naugmentation methods we conduct a robust evaluation using extreme data scarcity\nscenarios, quantitative diversity analysis and a qualitative thematic analysis.\nThe thematic analysis identifies four problematic patterns in augmented text:\nLinguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and\nAugmented Content Misinterpretation.\n  Overall, this work presents PromptAug as an effective method for augmenting\ndata in sensitive tasks like conflict detection, offering a unique,\ninterdisciplinary evaluation grounded in both natural language processing and\nsocial science methodology.", "AI": {"tldr": "PromptAug is an LLM-based data augmentation method improving accuracy and F1-score by 2% for conflict detection, addressing data scarcity and LLM guardrails.", "motivation": "High-quality labeled data for nuanced tasks like conflict detection is scarce and expensive, and social media restrictions limit research data access.", "method": "Introduces PromptAug, an LLM-based data augmentation method, evaluated in extreme data scarcity scenarios with diversity and thematic analysis.", "result": "Achieves 2% improvement in accuracy and F1-score; identifies four problematic patterns in augmented text.", "conclusion": "PromptAug is effective for sensitive tasks like conflict detection, combining NLP and social science methods."}}
{"id": "2506.22444", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.22444", "abs": "https://arxiv.org/abs/2506.22444", "authors": ["Jing Wang", "Amar Sra", "Jeremy C. Weiss"], "title": "Active Learning for Forecasting Severity among Patients with Post Acute Sequelae of SARS-CoV-2", "comment": null, "summary": "The long-term effects of Postacute Sequelae of SARS-CoV-2, known as PASC,\npose a significant challenge to healthcare systems worldwide. Accurate\nidentification of progression events, such as hospitalization and reinfection,\nis essential for effective patient management and resource allocation. However,\ntraditional models trained on structured data struggle to capture the nuanced\nprogression of PASC. In this study, we introduce the first publicly available\ncohort of 18 PASC patients, with text time series features based on Large\nLanguage Model Llama-3.1-70B-Instruct and clinical risk annotated by clinical\nexpert. We propose an Active Attention Network to predict the clinical risk and\nidentify progression events related to the risk. By integrating human expertise\nwith active learning, we aim to enhance clinical risk prediction accuracy and\nenable progression events identification with fewer number of annotation. The\nultimate goal is to improves patient care and decision-making for SARS-CoV-2\npatient.", "AI": {"tldr": "The paper introduces a cohort of 18 PASC patients with text time series features using Llama-3.1-70B-Instruct and proposes an Active Attention Network to predict clinical risk and progression events, aiming to improve patient care.", "motivation": "Accurate identification of PASC progression events (e.g., hospitalization, reinfection) is critical for patient management, but traditional models fail to capture nuanced progression.", "method": "Uses a cohort of 18 PASC patients with text time series features (Llama-3.1-70B-Instruct) and clinical expert annotations. Proposes an Active Attention Network for risk prediction and event identification.", "result": "Aims to enhance clinical risk prediction accuracy and identify progression events with fewer annotations by integrating human expertise and active learning.", "conclusion": "The approach seeks to improve patient care and decision-making for SARS-CoV-2 patients by leveraging advanced modeling and expert input."}}
{"id": "2506.22503", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22503", "abs": "https://arxiv.org/abs/2506.22503", "authors": ["Michiel Schepers", "Pieter Robberechts", "Jan Van Haaren", "Jesse Davis"], "title": "What Makes a Dribble Successful? Insights From 3D Pose Tracking Data", "comment": null, "summary": "Data analysis plays an increasingly important role in soccer, offering new\nways to evaluate individual and team performance. One specific application is\nthe evaluation of dribbles: one-on-one situations where an attacker attempts to\nbypass a defender with the ball. While previous research has primarily relied\non 2D positional tracking data, this fails to capture aspects like balance,\norientation, and ball control, limiting the depth of current insights. This\nstudy explores how pose tracking data (capturing players' posture and movement\nin three dimensions) can improve our understanding of dribbling skills. We\nextract novel pose-based features from 1,736 dribbles in the 2022/23 Champions\nLeague season and evaluate their impact on dribble success. Our results\nindicate that features capturing the attacker's balance and the alignment of\nthe orientation between the attacker and defender are informative for\npredicting dribble success. Incorporating these pose-based features on top of\nfeatures derived from traditional 2D positional data leads to a measurable\nimprovement in model performance.", "AI": {"tldr": "The study uses 3D pose tracking data to enhance dribble evaluation in soccer, showing improved predictive performance over traditional 2D methods.", "motivation": "Current 2D positional data lacks depth in capturing dribbling skills like balance and orientation, prompting the need for 3D pose tracking.", "method": "Analyzed 1,736 dribbles from the 2022/23 Champions League, extracting pose-based features (e.g., balance, attacker-defender alignment).", "result": "Pose-based features, especially balance and orientation alignment, significantly improve dribble success prediction when combined with 2D data.", "conclusion": "3D pose tracking offers deeper insights into dribbling, enhancing performance evaluation beyond traditional 2D methods."}}
{"id": "2506.22508", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22508", "abs": "https://arxiv.org/abs/2506.22508", "authors": ["Chenyang Shao", "Tianxing Li", "Chenhao Pu", "Fengli Xu", "Yong Li"], "title": "AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text", "comment": "This work has been submitted to NeurIPS 2025. Under review", "summary": "In today's digital world, casual user-generated content often contains subtle\ncues that may inadvertently expose sensitive personal attributes. Such risks\nunderscore the growing importance of effective text anonymization to safeguard\nindividual privacy. However, existing methods either rely on rigid replacements\nthat damage utility or cloud-based LLMs that are costly and pose privacy risks.\nTo address these issues, we explore the use of locally deployed smaller-scale\nlanguage models (SLMs) for anonymization. Yet training effective SLMs remains\nchallenging due to limited high-quality supervision. To address the challenge,\nwe propose AgentStealth, a self-reinforcing LLM anonymization framework.First,\nwe introduce an adversarial anonymization workflow enhanced by In-context\nContrastive Learning and Adaptive Utility-Aware Control. Second, we perform\nsupervised adaptation of SLMs using high-quality data collected from the\nworkflow, which includes both anonymization and attack signals. Finally, we\napply online reinforcement learning where the model leverages its internal\nadversarial feedback to iteratively improve anonymization performance.\nExperiments on two datasets show that our method outperforms baselines in both\nanonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight\ndesign supports direct deployment on edge devices, avoiding cloud reliance and\ncommunication-based privacy risks. Our code is open-source at\nhttps://github.com/tsinghua-fib-lab/AgentStealth.", "AI": {"tldr": "AgentStealth is a framework using locally deployed smaller-scale language models (SLMs) for text anonymization, outperforming baselines in effectiveness and utility while avoiding cloud reliance.", "motivation": "Existing anonymization methods either harm utility or pose privacy risks, highlighting the need for effective, locally deployable solutions.", "method": "AgentStealth combines adversarial anonymization, supervised adaptation of SLMs, and online reinforcement learning for iterative improvement.", "result": "Outperforms baselines by 12.3% in anonymization effectiveness and 6.8% in utility, with lightweight edge deployment.", "conclusion": "AgentStealth offers a privacy-preserving, efficient solution for text anonymization, suitable for edge devices."}}
{"id": "2506.22445", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.22445", "abs": "https://arxiv.org/abs/2506.22445", "authors": ["Saad Alqithami"], "title": "Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security", "comment": null, "summary": "Cyber-Physical Systems play a critical role in the infrastructure of various\nsectors, including manufacturing, energy distribution, and autonomous\ntransportation systems. However, their increasing connectivity renders them\nhighly vulnerable to sophisticated cyber threats, such as adaptive and zero-day\nattacks, against which traditional security methods like rule-based intrusion\ndetection and single-agent reinforcement learning prove insufficient. To\novercome these challenges, this paper introduces a novel Hierarchical\nAdversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework.\nHAMARL employs a hierarchical structure consisting of local agents dedicated to\nsubsystem security and a global coordinator that oversees and optimizes\ncomprehensive, system-wide defense strategies. Furthermore, the framework\nincorporates an adversarial training loop designed to simulate and anticipate\nevolving cyber threats, enabling proactive defense adaptation. Extensive\nexperimental evaluations conducted on a simulated industrial IoT testbed\nindicate that HAMARL substantially outperforms traditional multi-agent\nreinforcement learning approaches, significantly improving attack detection\naccuracy, reducing response times, and ensuring operational continuity. The\nresults underscore the effectiveness of combining hierarchical multi-agent\ncoordination with adversarially-aware training to enhance the resilience and\nsecurity of next-generation CPS.", "AI": {"tldr": "The paper introduces HAMARL, a hierarchical multi-agent reinforcement learning framework, to enhance cyber-physical system security against evolving cyber threats.", "motivation": "Traditional security methods are insufficient against adaptive and zero-day attacks in increasingly connected cyber-physical systems.", "method": "HAMARL uses local agents for subsystem security and a global coordinator for system-wide defense, with adversarial training to simulate threats.", "result": "HAMARL outperforms traditional methods, improving attack detection, reducing response times, and ensuring operational continuity.", "conclusion": "Combining hierarchical multi-agent coordination with adversarial training enhances resilience and security in next-generation CPS."}}
{"id": "2506.22504", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22504", "abs": "https://arxiv.org/abs/2506.22504", "authors": ["Hassan Baker", "Austin J. Brockmeier"], "title": "Patch2Loc: Learning to Localize Patches for Unsupervised Brain Lesion Detection", "comment": null, "summary": "Detecting brain lesions as abnormalities observed in magnetic resonance\nimaging (MRI) is essential for diagnosis and treatment. In the search of\nabnormalities, such as tumors and malformations, radiologists may benefit from\ncomputer-aided diagnostics that use computer vision systems trained with\nmachine learning to segment normal tissue from abnormal brain tissue. While\nsupervised learning methods require annotated lesions, we propose a new\nunsupervised approach (Patch2Loc) that learns from normal patches taken from\nstructural MRI. We train a neural network model to map a patch back to its\nspatial location within a slice of the brain volume. During inference, abnormal\npatches are detected by the relatively higher error and/or variance of the\nlocation prediction. This generates a heatmap that can be integrated into\npixel-wise methods to achieve finer-grained segmentation. We demonstrate the\nability of our model to segment abnormal brain tissues by applying our approach\nto the detection of tumor tissues in MRI on T2-weighted images from BraTS2021\nand MSLUB datasets and T1-weighted images from ATLAS and WMH datasets. We show\nthat it outperforms the state-of-the art in unsupervised segmentation. The\ncodebase for this work can be found on our\n\\href{https://github.com/bakerhassan/Patch2Loc}{GitHub page}.", "AI": {"tldr": "Patch2Loc is an unsupervised method for detecting brain lesions in MRI by learning from normal patches and identifying abnormalities through prediction errors.", "motivation": "Radiologists need efficient tools for detecting brain abnormalities like tumors. Supervised methods require annotated data, but Patch2Loc offers an unsupervised alternative.", "method": "Train a neural network to map normal MRI patches to their spatial locations. Detect abnormalities via higher prediction errors/variance during inference.", "result": "Outperforms state-of-the-art unsupervised segmentation on BraTS2021, MSLUB, ATLAS, and WMH datasets.", "conclusion": "Patch2Loc is effective for unsupervised brain lesion detection, offering finer-grained segmentation without annotated data."}}
{"id": "2506.22510", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22510", "abs": "https://arxiv.org/abs/2506.22510", "authors": ["Zihao Zhao", "Xinlong Zhai", "Jinyu Yang", "Chuan Shi"], "title": "Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning", "comment": "16 pages, 5 figures", "summary": "Foundation models have achieved great success in natural language processing\n(NLP) and computer vision (CV). Their success largely stems from the ability to\nintegrate multi-domain knowledge in pre-training and transfer it to target\ndomains. Considering graph data, especially graphs without textual features, is\nubiquitous in real-world applications such as social networks and\nrecommendation systems, some researchers have attempted to extend this paradigm\nto the graph field, aiming to construct graph foundation models. However,\nunlike CV and NLP, there are huge gaps among the semantics and properties of\ngraphs in different domains, while current works still adopt traditional\ncontrastive pre-training strategies designed in the single-domain scenario,\nwhich regard contrastive samples from different domains as equivalent. From\nexperimental investigations, we discovered that inherent domain-specific\ndifferences prevent these strategies from effectively absorbing knowledge from\ndifferent domains to generate informative representations. In this paper, we\npropose a novel multi-domain pre-training and cross-domain transfer framework,\nnamely MDGCL.In the pre-training stage, we design a contrastive learning\nstrategy to substantially recognize and capture domain differences, and\nintroduce domain tokens to encode domain-level global information. In the\ndownstream stage, we introduce a domain attention mechanism to enable\nfine-grained domain knowledge transfer. Extensive experiments on five benchmark\ndatasets have demonstrated that our method outperforms state-of-the-art\nsignificantly, with the maximum improvement of 19.33\\% on accuracy and 19.13\\%\non Macro-F1 score.", "AI": {"tldr": "The paper introduces MDGCL, a framework for multi-domain pre-training and cross-domain transfer in graph data, addressing domain-specific differences to improve representation learning.", "motivation": "Current graph foundation models fail to account for domain-specific differences, limiting their effectiveness in multi-domain scenarios.", "method": "MDGCL uses a novel contrastive learning strategy with domain tokens and a domain attention mechanism for fine-grained knowledge transfer.", "result": "MDGCL outperforms state-of-the-art methods, achieving up to 19.33% improvement in accuracy and 19.13% in Macro-F1 score.", "conclusion": "The proposed framework effectively addresses domain differences, enhancing graph representation learning and transferability."}}
{"id": "2506.22446", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22446", "abs": "https://arxiv.org/abs/2506.22446", "authors": ["Aakash Tripathi", "Asim Waqas", "Matthew B. Schabath", "Yasin Yilmaz", "Ghulam Rasool"], "title": "EAGLE: Efficient Alignment of Generalized Latent Embeddings for Multimodal Survival Prediction with Interpretable Attribution Analysis", "comment": null, "summary": "Accurate cancer survival prediction requires integration of diverse data\nmodalities that reflect the complex interplay between imaging, clinical\nparameters, and textual reports. However, existing multimodal approaches suffer\nfrom simplistic fusion strategies, massive computational requirements, and lack\nof interpretability-critical barriers to clinical adoption. We present EAGLE\n(Efficient Alignment of Generalized Latent Embeddings), a novel deep learning\nframework that addresses these limitations through attention-based multimodal\nfusion with comprehensive attribution analysis. EAGLE introduces four key\ninnovations: (1) dynamic cross-modal attention mechanisms that learn\nhierarchical relationships between modalities, (2) massive dimensionality\nreduction (99.96%) while maintaining predictive performance, (3) three\ncomplementary attribution methods providing patient-level interpretability, and\n(4) a unified pipeline enabling seamless adaptation across cancer types. We\nevaluated EAGLE on 911 patients across three distinct malignancies:\nglioblastoma (GBM, n=160), intraductal papillary mucinous neoplasms (IPMN,\nn=171), and non-small cell lung cancer (NSCLC, n=580). Patient-level analysis\nshowed high-risk individuals relied more heavily on adverse imaging features,\nwhile low-risk patients demonstrated balanced modality contributions. Risk\nstratification identified clinically meaningful groups with 4-fold (GBM) to\n5-fold (NSCLC) differences in median survival, directly informing treatment\nintensity decisions. By combining state-of-the-art performance with clinical\ninterpretability, EAGLE bridges the gap between advanced AI capabilities and\npractical healthcare deployment, offering a scalable solution for multimodal\nsurvival prediction that enhances both prognostic accuracy and physician trust\nin automated predictions.", "AI": {"tldr": "EAGLE is a deep learning framework for cancer survival prediction, addressing limitations of existing methods with attention-based fusion, dimensionality reduction, interpretability, and adaptability across cancer types.", "motivation": "Existing multimodal approaches for cancer survival prediction lack efficient fusion, computational scalability, and interpretability, hindering clinical adoption.", "method": "EAGLE uses dynamic cross-modal attention, massive dimensionality reduction, three attribution methods, and a unified pipeline for multimodal fusion.", "result": "EAGLE achieved high-risk stratification (4-5 fold differences in survival) and interpretable patient-level insights across three cancer types.", "conclusion": "EAGLE combines performance and interpretability, bridging AI capabilities with clinical deployment for scalable, trustworthy survival prediction."}}
{"id": "2506.22505", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22505", "abs": "https://arxiv.org/abs/2506.22505", "authors": ["Hassan Baker", "Matthew S. Emigh", "Austin J. Brockmeier"], "title": "Weakly Supervised Object Segmentation by Background Conditional Divergence", "comment": null, "summary": "As a computer vision task, automatic object segmentation remains challenging\nin specialized image domains without massive labeled data, such as synthetic\naperture sonar images, remote sensing, biomedical imaging, etc. In any domain,\nobtaining pixel-wise segmentation masks is expensive. In this work, we propose\na method for training a masking network to perform binary object segmentation\nusing weak supervision in the form of image-wise presence or absence of an\nobject of interest, which provides less information but may be obtained more\nquickly from manual or automatic labeling. A key step in our method is that the\nsegmented objects can be placed into background-only images to create\nrealistic, images of the objects with counterfactual backgrounds. To create a\ncontrast between the original and counterfactual background images, we propose\nto first cluster the background-only images, and then during learning create\ncounterfactual images that blend objects segmented from their original source\nbackgrounds to backgrounds chosen from a targeted cluster. One term in the\ntraining loss is the divergence between these counterfactual images and the\nreal object images with backgrounds of the target cluster. The other term is a\nsupervised loss for background-only images. While an adversarial critic could\nprovide the divergence, we use sample-based divergences. We conduct experiments\non side-scan and synthetic aperture sonar in which our approach succeeds\ncompared to previous unsupervised segmentation baselines that were only tested\non natural images. Furthermore, to show generality we extend our experiments to\nnatural images, obtaining reasonable performance with our method that avoids\npretrained networks, generative networks, and adversarial critics. The basecode\nfor this work can be found at\n\\href{GitHub}{https://github.com/bakerhassan/WSOS}.", "AI": {"tldr": "The paper proposes a weakly supervised method for binary object segmentation using image-wise labels and counterfactual background images, achieving success in specialized domains like sonar and natural images.", "motivation": "Automatic object segmentation is challenging in specialized domains due to the lack of labeled data. Pixel-wise masks are expensive, so the paper explores weak supervision (image-wise labels) as a more feasible alternative.", "method": "The method trains a masking network using weak supervision (image-wise labels). It creates counterfactual images by blending segmented objects into clustered backgrounds and uses divergence and supervised loss for training.", "result": "The approach outperforms unsupervised baselines in sonar images and shows reasonable performance in natural images without relying on pretrained or generative networks.", "conclusion": "The method is effective for binary segmentation in data-scarce domains, offering a practical alternative to fully supervised approaches."}}
{"id": "2506.22516", "categories": ["cs.CL", "cs.AI", "cs.NE", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2506.22516", "abs": "https://arxiv.org/abs/2506.22516", "authors": ["Jingkai Li"], "title": "Can \"consciousness\" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis", "comment": "Published as a journal paper at:\n  https://doi.org/10.1016/j.nlp.2025.100163", "summary": "Integrated Information Theory (IIT) provides a quantitative framework for\nexplaining consciousness phenomenon, positing that conscious systems comprise\nelements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the\nlatest iterations of this framework -- to sequences of Large Language Model\n(LLM) representations, analyzing data derived from existing Theory of Mind\n(ToM) test results. Our study systematically investigates whether the\ndifferences of ToM test performances, when presented in the LLM\nrepresentations, can be revealed by IIT estimates, i.e., $\\Phi^{\\max}$ (IIT\n3.0), $\\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\\Phi$-structure\n(IIT 4.0). Furthermore, we compare these metrics with the Span Representations\nindependent of any estimate for consciousness. This additional effort aims to\ndifferentiate between potential \"consciousness\" phenomena and inherent\nseparations within LLM representational space. We conduct comprehensive\nexperiments examining variations across LLM transformer layers and linguistic\nspans from stimuli. Our results suggest that sequences of contemporary\nTransformer-based LLM representations lack statistically significant indicators\nof observed \"consciousness\" phenomena but exhibit intriguing patterns under\n$\\textit{spatio}$-permutational analyses. The Appendix and code are available\nas Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.", "AI": {"tldr": "The study applies Integrated Information Theory (IIT) 3.0 and 4.0 to analyze Large Language Model (LLM) representations using Theory of Mind (ToM) test data, finding no significant indicators of consciousness but revealing patterns under spatio-permutational analyses.", "motivation": "To investigate whether differences in ToM test performances in LLM representations can be explained by IIT metrics and to differentiate between consciousness phenomena and inherent separations in LLM representational space.", "method": "Applied IIT 3.0 and 4.0 metrics ($\\Phi^{\\max}$, $\\Phi$, Conceptual Information, $\\Phi$-structure) to LLM representations and compared them with Span Representations. Conducted experiments across LLM transformer layers and linguistic spans.", "result": "No statistically significant indicators of consciousness were found in LLM representations, but patterns emerged under spatio-permutational analyses.", "conclusion": "Contemporary Transformer-based LLM representations lack evidence of consciousness phenomena, though further investigation of patterns is warranted."}}
{"id": "2506.22447", "categories": ["cs.LG", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.22447", "abs": "https://arxiv.org/abs/2506.22447", "authors": ["Fabio Merizzi", "Harilaos Loukos"], "title": "Vision Transformers for Multi-Variable Climate Downscaling: Emulating Regional Climate Models with a Shared Encoder and Multi-Decoder Architecture", "comment": null, "summary": "Global Climate Models (GCMs) are critical for simulating large-scale climate\ndynamics, but their coarse spatial resolution limits their applicability in\nregional studies. Regional Climate Models (RCMs) refine this through dynamic\ndownscaling, albeit at considerable computational cost and with limited\nflexibility. While deep learning has emerged as an efficient data-driven\nalternative, most existing studies have focused on single-variable models that\ndownscale one variable at a time. This approach can lead to limited contextual\nawareness, redundant computation, and lack of cross-variable interaction. Our\nstudy addresses these limitations by proposing a multi-task, multi-variable\nVision Transformer (ViT) architecture with a shared encoder and\nvariable-specific decoders (1EMD). The proposed architecture jointly predicts\nthree key climate variables: surface temperature (tas), wind speed (sfcWind),\nand 500 hPa geopotential height (zg500), directly from GCM-resolution inputs,\nemulating RCM-scale downscaling over Europe. We show that our multi-variable\napproach achieves positive cross-variable knowledge transfer and consistently\noutperforms single-variable baselines trained under identical conditions, while\nalso improving computational efficiency. These results demonstrate the\neffectiveness of multi-variable modeling for high-resolution climate\ndownscaling.", "AI": {"tldr": "A multi-task Vision Transformer (ViT) architecture (1EMD) is proposed for multi-variable climate downscaling, outperforming single-variable models in accuracy and efficiency.", "motivation": "GCMs lack regional detail, and RCMs are computationally expensive. Deep learning alternatives often focus on single variables, limiting contextual awareness and cross-variable interaction.", "method": "A ViT architecture with a shared encoder and variable-specific decoders (1EMD) jointly predicts temperature, wind speed, and geopotential height from GCM inputs.", "result": "The multi-variable approach shows positive knowledge transfer and outperforms single-variable baselines in accuracy and computational efficiency.", "conclusion": "Multi-variable modeling is effective for high-resolution climate downscaling, offering improved performance and efficiency."}}
{"id": "2506.22509", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22509", "abs": "https://arxiv.org/abs/2506.22509", "authors": ["Hang Xu", "Jie Huang", "Linjiang Huang", "Dong Li", "Yidi Liu", "Feng Zhao"], "title": "FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment", "comment": "ICCV2025", "summary": "Domain Adaptation(DA) for dense prediction tasks is an important topic, which\nenhances the dense prediction model's performance when tested on its unseen\ndomain. Recently, with the development of Diffusion-based Dense Prediction\n(DDP) models, the exploration of DA designs tailored to this framework is worth\nexploring, since the diffusion model is effective in modeling the distribution\ntransformation that comprises domain information. In this work, we propose a\ntraining-free mechanism for DDP frameworks, endowing them with DA capabilities.\nOur motivation arises from the observation that the exposure bias (e.g., noise\nstatistics bias) in diffusion brings domain shift, and different domains in\nconditions of DDP models can also be effectively captured by the noise\nprediction statistics. Based on this, we propose a training-free Domain Noise\nAlignment (DNA) approach, which alleviates the variations of noise statistics\nto domain changes during the diffusion sampling process, thereby achieving\ndomain adaptation. Specifically, when the source domain is available, we\ndirectly adopt the DNA method to achieve domain adaptation by aligning the\nnoise statistics of the target domain with those of the source domain. For the\nmore challenging source-free DA, inspired by the observation that regions\ncloser to the source domain exhibit higher confidence meeting variations of\nsampling noise, we utilize the statistics from the high-confidence regions\nprogressively to guide the noise statistic adjustment during the sampling\nprocess. Notably, our method demonstrates the effectiveness of enhancing the DA\ncapability of DDP models across four common dense prediction tasks. Code is\navailable at\n\\href{https://github.com/xuhang07/FreeDNA}{https://github.com/xuhang07/FreeDNA}.", "AI": {"tldr": "The paper introduces a training-free Domain Noise Alignment (DNA) method for Diffusion-based Dense Prediction (DDP) models to enhance domain adaptation (DA) by aligning noise statistics between domains.", "motivation": "The exposure bias in diffusion models causes domain shift, and noise prediction statistics can capture domain differences, motivating a training-free DA solution.", "method": "Proposes DNA, which aligns noise statistics of target domains with source domains (or high-confidence regions in source-free DA) during diffusion sampling.", "result": "Demonstrates effectiveness in enhancing DA for DDP models across four dense prediction tasks.", "conclusion": "DNA is a viable training-free approach for DA in DDP frameworks, leveraging noise statistics alignment."}}
{"id": "2506.22518", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22518", "abs": "https://arxiv.org/abs/2506.22518", "authors": ["Deyu Zou", "Yongqiang Chen", "Mufei Li", "Siqi Miao", "Chenxi Liu", "Bo Han", "James Cheng", "Pan Li"], "title": "Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation", "comment": null, "summary": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to ground responses with structured external knowledge from\nup-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs\noften rely on a weak retriever in graph-based RAG: I) Due to the lack of ground\ntruth, the retriever is often trained on weak supervision, which often\nintroduces spurious signals to the LLMs. II) Due to the abstraction of graph\ndata, the retrieved knowledge is often presented in unorganized forms. To\nmitigate the issue, we present Refined Graph-based RAG (ReG) to align weak\nretrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM\nfeedback to get rid of spurious signals and improve the quality of the\nsupervision. Meanwhile, ReG introduces a structure-aware reorganization module\nto refactor the retrieval results into logically coherent evidence chains.\nExperiments on prominent benchmarks demonstrate that ReG significantly and\nconsistently brings improvements across different LLM backbones by up to 10%.\nThe improved supervision quality enables ReG to match the state-of-the-art\nperformance with 5% training data and to transfer to out-of-distribution KGs.\nNotably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token\ncost by up to 30% and improves the performance by up to 4%.", "AI": {"tldr": "ReG improves graph-based RAG by aligning weak retrievers with LLMs, using feedback to remove spurious signals and reorganizing retrieved knowledge into coherent evidence chains, achieving significant performance gains.", "motivation": "Address the limitations of weak retrievers in graph-based RAG, which introduce spurious signals and unorganized knowledge due to lack of ground truth and graph abstraction.", "method": "Introduces ReG, which uses LLM feedback to refine supervision and a structure-aware module to reorganize retrieved knowledge into coherent evidence chains.", "result": "ReG improves performance by up to 10%, matches SOTA with 5% training data, reduces reasoning token cost by 30%, and boosts performance by 4% for reasoning-based LLMs.", "conclusion": "ReG effectively enhances graph-based RAG by refining supervision and reorganizing knowledge, demonstrating broad applicability and efficiency gains."}}
{"id": "2506.22502", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.22502", "abs": "https://arxiv.org/abs/2506.22502", "authors": ["Matvei Anoshin", "Olga Tsurkan", "Vadim Lopatkin", "Leonid Fedichkin"], "title": "Stabilization of industrial processes with time series machine learning", "comment": null, "summary": "The stabilization of time series processes is a crucial problem that is\nubiquitous in various industrial fields. The application of machine learning to\nits solution can have a decisive impact, improving both the quality of the\nresulting stabilization with less computational resources required. In this\nwork, we present a simple pipeline consisting of two neural networks: the\noracle predictor and the optimizer, proposing a substitution of the point-wise\nvalues optimization to the problem of the neural network training, which\nsuccessfully improves stability in terms of the temperature control by about 3\ntimes compared to ordinary solvers.", "AI": {"tldr": "A neural network pipeline improves time series stabilization, outperforming traditional methods by 3x in temperature control.", "motivation": "Stabilizing time series processes is critical in industries, and machine learning can enhance efficiency and quality.", "method": "A two-neural-network pipeline (oracle predictor and optimizer) replaces point-wise optimization with neural network training.", "result": "Achieves 3x better stability in temperature control compared to conventional solvers.", "conclusion": "The proposed method effectively enhances stabilization with reduced computational resources."}}
{"id": "2506.22511", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.22511", "abs": "https://arxiv.org/abs/2506.22511", "authors": ["Tingting Zhou", "Feng Zhang", "Haoyang Fu", "Baoxiang Pan", "Renhe Zhang", "Feng Lu", "Zhixin Yang"], "title": "Lightning the Night with Generative Artificial Intelligence", "comment": null, "summary": "The visible light reflectance data from geostationary satellites is crucial\nfor meteorological observations and plays an important role in weather\nmonitoring and forecasting. However, due to the lack of visible light at night,\nit is impossible to conduct continuous all-day weather observations using\nvisible light reflectance data. This study pioneers the use of generative\ndiffusion models to address this limitation. Based on the multi-band thermal\ninfrared brightness temperature data from the Advanced Geostationary Radiation\nImager (AGRI) onboard the Fengyun-4B (FY4B) geostationary satellite, we\ndeveloped a high-precision visible light reflectance retrieval model, called\nReflectance Diffusion (RefDiff), which enables 0.47~\\mu\\mathrm{m},\n0.65~\\mu\\mathrm{m}, and 0.825~\\mu\\mathrm{m} bands visible light reflectance\nretrieval at night. Compared to the classical models, RefDiff not only\nsignificantly improves accuracy through ensemble averaging but also provides\nuncertainty estimation. Specifically, the SSIM index of RefDiff can reach 0.90,\nwith particularly significant improvements in areas with complex cloud\nstructures and thick clouds. The model's nighttime retrieval capability was\nvalidated using VIIRS nighttime product, demonstrating comparable performance\nto its daytime counterpart. In summary, this research has made substantial\nprogress in the ability to retrieve visible light reflectance at night, with\nthe potential to expand the application of nighttime visible light data.", "AI": {"tldr": "The study introduces RefDiff, a generative diffusion model, to retrieve visible light reflectance at night using thermal infrared data from FY4B satellite, improving accuracy and enabling uncertainty estimation.", "motivation": "Visible light reflectance data is unavailable at night, limiting continuous weather monitoring. This study aims to overcome this gap using thermal infrared data.", "method": "Developed RefDiff, a generative diffusion model, using multi-band thermal infrared brightness temperature data from FY4B's AGRI for nighttime visible light reflectance retrieval.", "result": "RefDiff achieves high accuracy (SSIM 0.90), especially in complex cloud areas, and performs comparably to daytime models when validated with VIIRS data.", "conclusion": "The research advances nighttime visible light reflectance retrieval, expanding the potential applications of nighttime visible light data."}}
{"id": "2506.22529", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22529", "abs": "https://arxiv.org/abs/2506.22529", "authors": ["Lu Kalkbrenner", "Veronika Solopova", "Steffen Zeiler", "Robert Nickel", "Dorothea Kolossa"], "title": "MisinfoTeleGraph: Network-driven Misinformation Detection for German Telegram Messages", "comment": null, "summary": "Connectivity and message propagation are central, yet often underutilized,\nsources of information in misinformation detection -- especially on poorly\nmoderated platforms such as Telegram, which has become a critical channel for\nmisinformation dissemination, namely in the German electoral context. In this\npaper, we introduce Misinfo-TeleGraph, the first German-language Telegram-based\ngraph dataset for misinformation detection. It includes over 5 million messages\nfrom public channels, enriched with metadata, channel relationships, and both\nweak and strong labels. These labels are derived via semantic similarity to\nfact-checks and news articles using M3-embeddings, as well as manual\nannotation. To establish reproducible baselines, we evaluate both text-only\nmodels and graph neural networks (GNNs) that incorporate message forwarding as\na network structure. Our results show that GraphSAGE with LSTM aggregation\nsignificantly outperforms text-only baselines in terms of Matthews Correlation\nCoefficient (MCC) and F1-score. We further evaluate the impact of subscribers,\nview counts, and automatically versus human-created labels on performance, and\nhighlight both the potential and challenges of weak supervision in this domain.\nThis work provides a reproducible benchmark and open dataset for future\nresearch on misinformation detection in German-language Telegram networks and\nother low-moderation social platforms.", "AI": {"tldr": "Misinfo-TeleGraph is a German-language Telegram dataset for misinformation detection, featuring 5M+ messages, metadata, and labels. GraphSAGE with LSTM outperforms text-only models, highlighting the role of network structure in detection.", "motivation": "Addressing the underutilization of connectivity and message propagation in misinformation detection, especially on poorly moderated platforms like Telegram, which is critical in the German electoral context.", "method": "Introduces Misinfo-TeleGraph, a dataset with metadata, channel relationships, and labels (weak/strong). Evaluates text-only models and GNNs (e.g., GraphSAGE with LSTM) using forwarding as network structure.", "result": "GraphSAGE with LSTM aggregation outperforms text-only models in MCC and F1-score. Impact of subscribers, view counts, and label types (automatic vs. human) is evaluated.", "conclusion": "Provides a benchmark and open dataset for future research on misinformation detection in low-moderation platforms, emphasizing the potential and challenges of weak supervision."}}
{"id": "2506.22530", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2506.22530", "abs": "https://arxiv.org/abs/2506.22530", "authors": ["Jakub Pele\u0161ka", "Gustav \u0160\u00edr"], "title": "Task-Agnostic Contrastive Pretraining for Relational Deep Learning", "comment": "arXiv admin note: text overlap with arXiv:2506.22199", "summary": "Relational Deep Learning (RDL) is an emerging paradigm that leverages Graph\nNeural Network principles to learn directly from relational databases by\nrepresenting them as heterogeneous graphs. However, existing RDL models\ntypically rely on task-specific supervised learning, requiring training\nseparate models for each predictive task, which may hamper scalability and\nreuse.\n  In this work, we propose a novel task-agnostic contrastive pretraining\napproach for RDL that enables database-wide representation learning. For that\naim, we introduce three levels of contrastive objectives$-$row-level,\nlink-level, and context-level$-$designed to capture the structural and semantic\nheterogeneity inherent to relational data. We implement the respective\npretraining approach through a modular RDL architecture and an efficient\nsampling strategy tailored to the heterogeneous database setting. Our\npreliminary results on standard RDL benchmarks demonstrate that fine-tuning the\npretrained models measurably outperforms training from scratch, validating the\npromise of the proposed methodology in learning transferable representations\nfor relational data.", "AI": {"tldr": "The paper introduces a task-agnostic contrastive pretraining approach for Relational Deep Learning (RDL) to learn transferable representations from relational databases, outperforming task-specific models.", "motivation": "Existing RDL models rely on task-specific supervised learning, limiting scalability and reuse. The paper aims to address this by proposing a pretraining method for database-wide representation learning.", "method": "The approach uses three contrastive objectives (row-level, link-level, context-level) to capture structural and semantic heterogeneity. It employs a modular RDL architecture and efficient sampling for heterogeneous databases.", "result": "Preliminary results show fine-tuning pretrained models outperforms training from scratch, indicating the method's effectiveness for transferable representations.", "conclusion": "The proposed contrastive pretraining approach for RDL shows promise in learning reusable and scalable representations for relational data."}}
{"id": "2506.22513", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22513", "abs": "https://arxiv.org/abs/2506.22513", "authors": ["Aditya Sharma"], "title": "Automated Defect Identification and Categorization in NDE 4.0 with the Application of Artificial Intelligence", "comment": null, "summary": "This investigation attempts to create an automated framework for fault\ndetection and organization for usage in contemporary radiography, as per NDE\n4.0. The review's goals are to address the lack of information that is\nsufficiently explained, learn how to make the most of virtual defect increase,\nand determine whether the framework is viable by using NDE measurements. As its\nbasic information source, the technique consists of compiling and categorizing\n223 CR photographs of airplane welds. Information expansion systems, such as\nvirtual defect increase and standard increase, are used to work on the\npreparation dataset. A modified U-net model is prepared using the improved data\nto produce semantic fault division veils. To assess the effectiveness of the\nmodel, NDE boundaries such as Case, estimating exactness, and misleading call\nrate are used. Tiny a90/95 characteristics, which provide strong\ndifferentiating evidence of flaws, reveal that the suggested approach achieves\nexceptional awareness in defect detection. Considering a 90/95, size error, and\nfake call rate in the weld area, the consolidated expansion approach clearly\nwins. Due to the framework's fast derivation speed, large images can be broken\ndown efficiently and quickly. Professional controllers evaluate the transmitted\nsystem in the field and believe that it has a guarantee as a support device in\nthe testing cycle, irrespective of particular equipment cut-off points and\nprogramming resemblance.", "AI": {"tldr": "An automated framework for fault detection in radiography using NDE 4.0, leveraging virtual defect augmentation and a modified U-net model, achieves high accuracy and efficiency.", "motivation": "Addresses the lack of sufficiently explained information in radiography and explores the potential of virtual defect augmentation for fault detection.", "method": "Compiles 223 CR images of airplane welds, uses data augmentation (virtual defect and standard), and trains a modified U-net model for semantic fault segmentation.", "result": "Achieves high defect detection awareness (a90/95), efficient processing of large images, and validation by professional controllers.", "conclusion": "The framework is viable, efficient, and promising as a support tool in radiography testing, despite equipment and software limitations."}}
{"id": "2506.22598", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22598", "abs": "https://arxiv.org/abs/2506.22598", "authors": ["Nicholas Edwards", "Yukyung Lee", "Yujun", "Mao", "Yulu Qin", "Sebastian Schuster", "Najoung Kim"], "title": "RExBench: Can coding agents autonomously implement AI research extensions?", "comment": null, "summary": "Agents based on Large Language Models (LLMs) have shown promise for\nperforming sophisticated software engineering tasks autonomously. In addition,\nthere has been progress towards developing agents that can perform parts of the\nresearch pipeline in machine learning and the natural sciences. We argue that\nresearch extension and its implementation is a critical capability for such\nsystems, and introduce RExBench to support the evaluation of this capability.\nRExBench is a benchmark consisting of 12 realistic research experiment\nimplementation tasks that aim to investigate research hypotheses that have not\npreviously been implemented. Each task is set up as an extension to an existing\nresearch paper and codebase, accompanied by domain expert-written instructions.\nRExBench is robust to data contamination, and supports an automatic evaluation\ninfrastructure that executes agent outputs to determine whether the success\ncriteria are met. We use this benchmark to evaluate nine LLM agents implemented\nusing three different frameworks: aider, Claude Code, and OpenHands. We find\nthat all agents evaluated fail to autonomously implement the majority of the\nextensions. Although the success rate improves with additional human-written\nhints, the best performance under this setting remains below 40%. This\nindicates that current agents are still short of being able to handle realistic\nresearch extension tasks without substantial human guidance.", "AI": {"tldr": "RExBench evaluates LLM agents' ability to autonomously implement research extensions, finding current agents fall short despite some improvement with hints.", "motivation": "To assess the capability of LLM agents in performing realistic research experiment extensions, a critical skill for autonomous research systems.", "method": "Introduces RExBench, a benchmark with 12 tasks extending existing research, evaluated using automatic execution and domain expert instructions.", "result": "All nine LLM agents tested failed most tasks; success rates improved with hints but remained below 40%.", "conclusion": "Current LLM agents lack the autonomy to handle realistic research extensions without significant human guidance."}}
{"id": "2506.22566", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22566", "abs": "https://arxiv.org/abs/2506.22566", "authors": ["Jacob Adamczyk"], "title": "Exploration Behavior of Untrained Policies", "comment": "High-dimensional Learning Dynamics Workshop at ICML-2025", "summary": "Exploration remains a fundamental challenge in reinforcement learning (RL),\nparticularly in environments with sparse or adversarial reward structures. In\nthis work, we study how the architecture of deep neural policies implicitly\nshapes exploration before training. We theoretically and empirically\ndemonstrate strategies for generating ballistic or diffusive trajectories from\nuntrained policies in a toy model. Using the theory of infinite-width networks\nand a continuous-time limit, we show that untrained policies return correlated\nactions and result in non-trivial state-visitation distributions. We discuss\nthe distributions of the corresponding trajectories for a standard\narchitecture, revealing insights into inductive biases for tackling\nexploration. Our results establish a theoretical and experimental framework for\nusing policy initialization as a design tool to understand exploration behavior\nin early training.", "AI": {"tldr": "The paper explores how deep neural policy architectures influence exploration in RL before training, using theoretical and empirical methods to analyze trajectory generation and state-visitation distributions.", "motivation": "Understanding how untrained policies shape exploration in RL, especially in sparse or adversarial reward environments, is a fundamental challenge.", "method": "The study uses theory of infinite-width networks and continuous-time limits to analyze untrained policies, along with empirical demonstrations in a toy model.", "result": "Untrained policies produce correlated actions and non-trivial state-visitation distributions, revealing insights into inductive biases for exploration.", "conclusion": "The work provides a framework for using policy initialization to study exploration behavior early in training."}}
{"id": "2506.22517", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22517", "abs": "https://arxiv.org/abs/2506.22517", "authors": ["Subhadip Kumar"], "title": "Container damage detection using advanced computer vision model Yolov12 vs Yolov11 vs RF-DETR A comparative analysis", "comment": null, "summary": "Containers are an integral part of the logistics industry and act as a\nbarrier for cargo. A typical service life for a container is more than 20\nyears. However, overtime containers suffer various types of damage due to the\nmechanical as well as natural factors. A damaged container is a safety hazard\nfor the employees handling it and a liability for the logistic company.\nTherefore, a timely inspection and detection of the damaged container is a key\nfor prolonging service life as well as avoiding safety hazards. In this paper,\nwe will compare the performance of the damage detection by three\nstate-of-the-art advanced computer vision models Yolov12, Yolov11 and RF-DETR.\nWe will use a dataset of 278 annotated images to train, validate and test the\nmodel. We will compare the mAP and precision of the model. The objective of\nthis paper is to identify the model that is best suited for container damage\ndetection. The result is mixed. mAP@50 score of Yolov11 and 12 was 81.9%\ncompared to RF-DETR, which was 77.7%. However, while testing the model for\nnot-so-common damaged containers, the RF-DETR model outperformed the others\noverall, exhibiting superiority to accurately detecting both damaged containers\nas well as damage occurrences with high confidence.", "AI": {"tldr": "The paper compares three computer vision models (Yolov11, Yolov12, RF-DETR) for detecting container damage, finding RF-DETR superior for uncommon damages despite lower mAP scores.", "motivation": "Timely detection of container damage is crucial for safety and prolonging service life in logistics.", "method": "Three models were trained and tested on 278 annotated images, comparing mAP and precision.", "result": "Yolov11 and Yolov12 had higher mAP@50 (81.9%), but RF-DETR outperformed for uncommon damages.", "conclusion": "RF-DETR is better for detecting diverse damage types, despite lower overall mAP."}}
{"id": "2506.22623", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22623", "abs": "https://arxiv.org/abs/2506.22623", "authors": ["Badr Youbi Idrissi", "Monica Millunzi", "Amelia Sorrenti", "Lorenzo Baraldi", "Daryna Dementieva"], "title": "Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks", "comment": null, "summary": "In the present-day scenario, Large Language Models (LLMs) are establishing\ntheir presence as powerful instruments permeating various sectors of society.\nWhile their utility offers valuable support to individuals, there are multiple\nconcerns over potential misuse. Consequently, some academic endeavors have\nsought to introduce watermarking techniques, characterized by the inclusion of\nmarkers within machine-generated text, to facilitate algorithmic\nidentification. This research project is focused on the development of a novel\nmethodology for the detection of synthetic text, with the overarching goal of\nensuring the ethical application of LLMs in AI-driven text generation. The\ninvestigation commences with replicating findings from a previous baseline\nstudy, thereby underscoring its susceptibility to variations in the underlying\ngeneration model. Subsequently, we propose an innovative watermarking approach\nand subject it to rigorous evaluation, employing paraphrased generated text to\nasses its robustness. Experimental results highlight the robustness of our\nproposal compared to the~\\cite{aarson} watermarking method.", "AI": {"tldr": "The paper introduces a new watermarking method for detecting synthetic text generated by LLMs, aiming to ensure ethical use. It replicates a baseline study, identifies its limitations, and proposes a robust alternative, outperforming existing methods.", "motivation": "To address concerns about misuse of LLMs by developing a reliable method for detecting machine-generated text, ensuring ethical AI applications.", "method": "Replicates a baseline study, identifies its flaws, and proposes a novel watermarking technique. Evaluates robustness using paraphrased text.", "result": "The proposed method shows superior robustness compared to existing watermarking techniques.", "conclusion": "The new watermarking approach effectively detects synthetic text, enhancing ethical use of LLMs in AI-generated content."}}
{"id": "2506.22578", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22578", "abs": "https://arxiv.org/abs/2506.22578", "authors": ["Xufei Lv", "Haoyuan Sun", "Xuefeng Bai", "Min Zhang", "Houde Liu", "Kehai Chen"], "title": "The Hidden Link Between RLHF and Contrastive Learning", "comment": null, "summary": "Alignment of large language models (LLMs) with human values has recently\ngarnered significant attention, with prominent examples including the canonical\nyet costly Reinforcement Learning from Human Feedback (RLHF) and the simple\nDirect Preference Optimization (DPO). In this work, we demonstrate that both\nRLHF and DPO can be interpreted from the perspective of mutual information (MI)\nmaximization, uncovering a profound connection to contrastive learning. Within\nthis framework, both RLHF and DPO can be viewed as methods that perform\ncontrastive learning based on the positive and negative samples derived from\nthe base model, leveraging the Donsker-Varadhan (DV) lower bound on MI\n(equivalently, the MINE estimator). This paradigm further explains why RLHF may\nnot intrinsically incentivize reasoning capacities in LLMs beyond what is\nalready present in the base model. Building on this perspective, we replace the\nDV/MINE bound with the Jensen-Shannon MI estimator and propose Mutual\nInformation Optimization (MIO). Comprehensive theoretical analysis and\nextensive empirical evaluations demonstrate that MIO mitigates the late-stage\ndecline in chosen-likelihood observed in DPO, achieving competitive or superior\nperformance across various challenging reasoning and mathematical benchmarks.\nWe will release the model and code upon acceptance.", "AI": {"tldr": "The paper interprets RLHF and DPO as mutual information maximization methods, linking them to contrastive learning, and proposes MIO to improve performance.", "motivation": "To explore the connection between RLHF, DPO, and mutual information maximization, and address limitations in reasoning capacity.", "method": "Proposes Mutual Information Optimization (MIO) using the Jensen-Shannon MI estimator, replacing the DV/MINE bound.", "result": "MIO mitigates late-stage decline in chosen-likelihood and outperforms DPO in reasoning and mathematical benchmarks.", "conclusion": "MIO offers a competitive alternative to RLHF and DPO, enhancing reasoning capabilities in LLMs."}}
{"id": "2506.22531", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22531", "abs": "https://arxiv.org/abs/2506.22531", "authors": ["Prasen Kumar Sharma", "Neeraj Matiyali", "Siddharth Srivastava", "Gaurav Sharma"], "title": "Preserve Anything: Controllable Image Synthesis with Object Preservation", "comment": "Accepted at ICCV 2025", "summary": "We introduce \\textit{Preserve Anything}, a novel method for controlled image\nsynthesis that addresses key limitations in object preservation and semantic\nconsistency in text-to-image (T2I) generation. Existing approaches often fail\n(i) to preserve multiple objects with fidelity, (ii) maintain semantic\nalignment with prompts, or (iii) provide explicit control over scene\ncomposition. To overcome these challenges, the proposed method employs an\nN-channel ControlNet that integrates (i) object preservation with size and\nplacement agnosticism, color and detail retention, and artifact elimination,\n(ii) high-resolution, semantically consistent backgrounds with accurate\nshadows, lighting, and prompt adherence, and (iii) explicit user control over\nbackground layouts and lighting conditions. Key components of our framework\ninclude object preservation and background guidance modules, enforcing lighting\nconsistency and a high-frequency overlay module to retain fine details while\nmitigating unwanted artifacts. We introduce a benchmark dataset consisting of\n240K natural images filtered for aesthetic quality and 18K 3D-rendered\nsynthetic images with metadata such as lighting, camera angles, and object\nrelationships. This dataset addresses the deficiencies of existing benchmarks\nand allows a complete evaluation. Empirical results demonstrate that our method\nachieves state-of-the-art performance, significantly improving feature-space\nfidelity (FID 15.26) and semantic alignment (CLIP-S 32.85) while maintaining\ncompetitive aesthetic quality. We also conducted a user study to demonstrate\nthe efficacy of the proposed work on unseen benchmark and observed a remarkable\nimprovement of $\\sim25\\%$, $\\sim19\\%$, $\\sim13\\%$, and $\\sim14\\%$ in terms of\nprompt alignment, photorealism, the presence of AI artifacts, and natural\naesthetics over existing works.", "AI": {"tldr": "The paper introduces \textit{Preserve Anything}, a method for controlled image synthesis that improves object preservation, semantic consistency, and user control in text-to-image generation.", "motivation": "Existing T2I methods struggle with preserving multiple objects, maintaining semantic alignment, and providing explicit scene composition control.", "method": "The method uses an N-channel ControlNet with modules for object preservation, background guidance, lighting consistency, and high-frequency detail retention. A benchmark dataset of 240K natural and 18K synthetic images is introduced.", "result": "The method achieves state-of-the-art performance (FID 15.26, CLIP-S 32.85) and shows significant improvements in user study metrics (e.g., ~25% better prompt alignment).", "conclusion": "\textit{Preserve Anything} effectively addresses key T2I limitations, offering superior fidelity, control, and semantic alignment."}}
{"id": "2506.22644", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.22644", "abs": "https://arxiv.org/abs/2506.22644", "authors": ["Chase Fensore", "Kaustubh Dhole", "Joyce C Ho", "Eugene Agichtein"], "title": "Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge", "comment": "4 pages, 3 tables, 2 figures. Accepted at the SIGIR LiveRAG Workshop\n  2025 (Submission 2664)", "summary": "We present our submission to the LiveRAG Challenge 2025, which evaluates\nretrieval-augmented generation (RAG) systems on dynamic test sets using the\nFineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense\n(E5) retrieval methods and then aims to generate relevant and faithful answers\nwith Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic\nquestions generated with DataMorgana across 64 unique question-user\ncombinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP\nfrom 0.523 to 0.797 (52% relative improvement) but introduces prohibitive\ncomputational costs (84s vs 1.74s per question). While DSPy-optimized prompting\nstrategies achieved higher semantic similarity (0.771 vs 0.668), their 0%\nrefusal rates raised concerns about over-confidence and generalizability. Our\nsubmitted hybrid system without re-ranking achieved 4th place in faithfulness\nand 11th place in correctness among 25 teams. Analysis across question\ncategories reveals that vocabulary alignment between questions and documents\nwas the strongest predictor of performance on our development set, with\ndocument-similar phrasing improving cosine similarity from 0.562 to 0.762.", "AI": {"tldr": "A hybrid RAG system combining sparse (BM25) and dense (E5) retrieval with Falcon3-10B-Instruct was evaluated on dynamic test sets, showing improved performance with neural re-ranking but high computational costs. The system ranked 4th in faithfulness and 11th in correctness.", "motivation": "To evaluate retrieval-augmented generation (RAG) systems on dynamic test sets using the FineWeb-10BT corpus, aiming for relevant and faithful answers.", "method": "Combines sparse (BM25) and dense (E5) retrieval, uses Falcon3-10B-Instruct for generation, and evaluates with synthetic questions. Neural re-ranking (RankLLaMA) and DSPy-optimized prompting were tested.", "result": "Neural re-ranking improved MAP by 52% but was computationally expensive. DSPy-optimized prompting increased semantic similarity but had 0% refusal rates. The hybrid system ranked 4th in faithfulness and 11th in correctness.", "conclusion": "Vocabulary alignment between questions and documents was key to performance. The hybrid system without re-ranking balanced performance and computational efficiency."}}
{"id": "2506.22602", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22602", "abs": "https://arxiv.org/abs/2506.22602", "authors": ["Joshua C. Zhao", "Saurabh Bagchi"], "title": "Are Fast Methods Stable in Adversarially Robust Transfer Learning?", "comment": "13 pages", "summary": "Transfer learning is often used to decrease the computational cost of model\ntraining, as fine-tuning a model allows a downstream task to leverage the\nfeatures learned from the pre-training dataset and quickly adapt them to a new\ntask. This is particularly useful for achieving adversarial robustness, as\nadversarially training models from scratch is very computationally expensive.\nHowever, high robustness in transfer learning still requires adversarial\ntraining during the fine-tuning phase, which requires up to an order of\nmagnitude more time than standard fine-tuning. In this work, we revisit the use\nof the fast gradient sign method (FGSM) in robust transfer learning to improve\nthe computational cost of adversarial fine-tuning. We surprisingly find that\nFGSM is much more stable in adversarial fine-tuning than when training from\nscratch. In particular, FGSM fine-tuning does not suffer from any issues with\ncatastrophic overfitting at standard perturbation budgets of $\\varepsilon=4$ or\n$\\varepsilon=8$. This stability is further enhanced with parameter-efficient\nfine-tuning methods, where FGSM remains stable even up to $\\varepsilon=32$ for\nlinear probing. We demonstrate how this stability translates into performance\nacross multiple datasets. Compared to fine-tuning with the more commonly used\nmethod of projected gradient descent (PGD), on average, FGSM only loses 0.39%\nand 1.39% test robustness for $\\varepsilon=4$ and $\\varepsilon=8$ while using\n$4\\times$ less training time. Surprisingly, FGSM may not only be a\nsignificantly more efficient alternative to PGD in adversarially robust\ntransfer learning but also a well-performing one.", "AI": {"tldr": "FGSM in adversarial fine-tuning is stable and efficient, outperforming PGD in computational cost with minimal robustness loss.", "motivation": "Reduce computational cost of adversarial fine-tuning in transfer learning while maintaining robustness.", "method": "Revisit FGSM for adversarial fine-tuning, testing stability and performance across datasets.", "result": "FGSM is stable, avoids catastrophic overfitting, and performs nearly as well as PGD with 4x less training time.", "conclusion": "FGSM is a viable, efficient alternative to PGD for robust transfer learning."}}
{"id": "2506.22554", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22554", "abs": "https://arxiv.org/abs/2506.22554", "authors": ["Vasu Agrawal", "Akinniyi Akinyemi", "Kathryn Alvero", "Morteza Behrooz", "Julia Buffalini", "Fabio Maria Carlucci", "Joy Chen", "Junming Chen", "Zhang Chen", "Shiyang Cheng", "Praveen Chowdary", "Joe Chuang", "Antony D'Avirro", "Jon Daly", "Ning Dong", "Mark Duppenthaler", "Cynthia Gao", "Jeff Girard", "Martin Gleize", "Sahir Gomez", "Hongyu Gong", "Srivathsan Govindarajan", "Brandon Han", "Sen He", "Denise Hernandez", "Yordan Hristov", "Rongjie Huang", "Hirofumi Inaguma", "Somya Jain", "Raj Janardhan", "Qingyao Jia", "Christopher Klaiber", "Dejan Kovachev", "Moneish Kumar", "Hang Li", "Yilei Li", "Pavel Litvin", "Wei Liu", "Guangyao Ma", "Jing Ma", "Martin Ma", "Xutai Ma", "Lucas Mantovani", "Sagar Miglani", "Sreyas Mohan", "Louis-Philippe Morency", "Evonne Ng", "Kam-Woh Ng", "Tu Anh Nguyen", "Amia Oberai", "Benjamin Peloquin", "Juan Pino", "Jovan Popovic", "Omid Poursaeed", "Fabian Prada", "Alice Rakotoarison", "Alexander Richard", "Christophe Ropers", "Safiyyah Saleem", "Vasu Sharma", "Alex Shcherbyna", "Jia Shen", "Jie Shen", "Anastasis Stathopoulos", "Anna Sun", "Paden Tomasello", "Tuan Tran", "Arina Turkatenko", "Bo Wan", "Chao Wang", "Jeff Wang", "Mary Williamson", "Carleigh Wood", "Tao Xiang", "Yilin Yang", "Julien Yao", "Chen Zhang", "Jiemin Zhang", "Xinyue Zhang", "Jason Zheng", "Pavlo Zhyzheria", "Jan Zikes", "Michael Zollhoefer"], "title": "Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset", "comment": null, "summary": "Human communication involves a complex interplay of verbal and nonverbal\nsignals, essential for conveying meaning and achieving interpersonal goals. To\ndevelop socially intelligent AI technologies, it is crucial to develop models\nthat can both comprehend and generate dyadic behavioral dynamics. To this end,\nwe introduce the Seamless Interaction Dataset, a large-scale collection of over\n4,000 hours of face-to-face interaction footage from over 4,000 participants in\ndiverse contexts. This dataset enables the development of AI technologies that\nunderstand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,\ntelepresence experiences, and multimodal content analysis tools. We also\ndevelop a suite of models that utilize the dataset to generate dyadic motion\ngestures and facial expressions aligned with human speech. These models can\ntake as input both the speech and visual behavior of their interlocutors. We\npresent a variant with speech from an LLM model and integrations with 2D and 3D\nrendering methods, bringing us closer to interactive virtual agents.\nAdditionally, we describe controllable variants of our motion models that can\nadapt emotional responses and expressivity levels, as well as generating more\nsemantically-relevant gestures. Finally, we discuss methods for assessing the\nquality of these dyadic motion models, which are demonstrating the potential\nfor more intuitive and responsive human-AI interactions.", "AI": {"tldr": "The paper introduces the Seamless Interaction Dataset and models for AI to understand and generate dyadic behavioral dynamics, enhancing human-AI interactions.", "motivation": "To develop socially intelligent AI by comprehending and generating dyadic behavioral dynamics in human communication.", "method": "Creation of a large-scale dataset (Seamless Interaction Dataset) and development of models for generating dyadic motion gestures and facial expressions aligned with speech.", "result": "Models capable of generating contextually aligned gestures and expressions, with controllable emotional responses and expressivity, improving virtual agents and telepresence.", "conclusion": "The work advances intuitive and responsive human-AI interactions, with potential applications in virtual agents and multimodal content analysis."}}
{"id": "2506.22679", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22679", "abs": "https://arxiv.org/abs/2506.22679", "authors": ["Ankush Raut", "Projna Paromita", "Sydney Begerowski", "Suzanne Bell", "Theodora Chaspari"], "title": "Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions", "comment": "5 pages, 4 figures. Accepted to Interspeech 2025", "summary": "We explore the feasibility of large language models (LLMs) in detecting\nsubtle expressions of micro-behaviors in team conversations using transcripts\ncollected during simulated space missions. Specifically, we examine zero-shot\nclassification, fine-tuning, and paraphrase-augmented fine-tuning with\nencoder-only sequence classification LLMs, as well as few-shot text generation\nwith decoder-only causal language modeling LLMs, to predict the micro-behavior\nassociated with each conversational turn (i.e., dialogue). Our findings\nindicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to\ndetect underrepresented micro-behaviors, particularly discouraging speech, even\nwith weighted fine-tuning. In contrast, the instruction fine-tuned version of\nLlama-3.1, a decoder-only LLM, demonstrated superior performance, with the best\nmodels achieving macro F1-scores of 44% for 3-way classification and 68% for\nbinary classification. These results have implications for the development of\nspeech technologies aimed at analyzing team communication dynamics and\nenhancing training interventions in high-stakes environments such as space\nmissions, particularly in scenarios where text is the only accessible data.", "AI": {"tldr": "The paper evaluates LLMs for detecting micro-behaviors in team conversations, finding decoder-only models like Llama-3.1 outperform encoder-only ones, with implications for speech technologies in high-stakes environments.", "motivation": "To assess the feasibility of LLMs in identifying subtle micro-behaviors in team dialogues, especially in contexts like space missions where text data is primary.", "method": "Tested zero-shot classification, fine-tuning, and paraphrase-augmented fine-tuning with encoder-only LLMs (RoBERTa, DistilBERT) and few-shot text generation with decoder-only LLMs (Llama-3.1).", "result": "Encoder-only LLMs struggled with underrepresented micro-behaviors, while Llama-3.1 achieved 44% macro F1 for 3-way and 68% for binary classification.", "conclusion": "Decoder-only LLMs, like Llama-3.1, are more effective for micro-behavior detection in team conversations, suggesting potential for speech technologies in high-stakes settings."}}
{"id": "2506.22621", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22621", "abs": "https://arxiv.org/abs/2506.22621", "authors": ["Paul Saves", "Edward Hall\u00e9-Hannan", "Jasper Bussemaker", "Youssef Diouane", "Nathalie Bartoli"], "title": "Hierarchical Modeling and Architecture Optimization: Review and Unified Framework", "comment": null, "summary": "Simulation-based problems involving mixed-variable inputs frequently feature\ndomains that are hierarchical, conditional, heterogeneous, or tree-structured.\nThese characteristics pose challenges for data representation, modeling, and\noptimization. This paper reviews extensive literature on these structured input\nspaces and proposes a unified framework that generalizes existing approaches.\nIn this framework, input variables may be continuous, integer, or categorical.\nA variable is described as meta if its value governs the presence of other\ndecreed variables, enabling the modeling of conditional and hierarchical\nstructures.\n  We further introduce the concept of partially-decreed variables, whose\nactivation depends on contextual conditions. To capture these inter-variable\nhierarchical relationships, we introduce design space graphs, combining\nprinciples from feature modeling and graph theory. This allows the definition\nof general hierarchical domains suitable for describing complex system\narchitectures. The framework supports the use of surrogate models over such\ndomains and integrates hierarchical kernels and distances for efficient\nmodeling and optimization. The proposed methods are implemented in the\nopen-source Surrogate Modeling Toolbox (SMT 2.0), and their capabilities are\ndemonstrated through applications in Bayesian optimization for complex system\ndesign, including a case study in green aircraft architecture.", "AI": {"tldr": "The paper proposes a unified framework for handling mixed-variable inputs in simulation-based problems, addressing hierarchical, conditional, and heterogeneous domains. It introduces meta and partially-decreed variables, design space graphs, and integrates surrogate modeling and optimization techniques.", "motivation": "To address challenges in data representation, modeling, and optimization for mixed-variable inputs with hierarchical or conditional structures.", "method": "Introduces meta and partially-decreed variables, design space graphs, and hierarchical kernels/distances. Implemented in the Surrogate Modeling Toolbox (SMT 2.0).", "result": "Demonstrated effectiveness in Bayesian optimization for complex system design, including a green aircraft architecture case study.", "conclusion": "The framework generalizes existing approaches and provides efficient tools for modeling and optimizing complex hierarchical domains."}}
{"id": "2506.22556", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.22556", "abs": "https://arxiv.org/abs/2506.22556", "authors": ["Markus Juvonen", "Samuli Siltanen"], "title": "Recomposed realities: animating still images via patch clustering and randomness", "comment": "22 pages, 19 figures", "summary": "We present a patch-based image reconstruction and animation method that uses\nexisting image data to bring still images to life through motion. Image patches\nfrom curated datasets are grouped using k-means clustering and a new target\nimage is reconstructed by matching and randomly sampling from these clusters.\nThis approach emphasizes reinterpretation over replication, allowing the source\nand target domains to differ conceptually while sharing local structures.", "AI": {"tldr": "A method for animating still images by reconstructing them using patches from datasets, grouped via k-means clustering.", "motivation": "To bring still images to life through motion by leveraging existing image data without strict replication.", "method": "Uses k-means clustering to group image patches from datasets, then reconstructs target images by matching and randomly sampling from these clusters.", "result": "Enables reinterpretation of source images, allowing conceptual differences between source and target while preserving local structures.", "conclusion": "The method successfully animates still images by creatively reusing existing data, emphasizing reinterpretation over replication."}}
{"id": "2506.22694", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22694", "abs": "https://arxiv.org/abs/2506.22694", "authors": ["Raghavv Goel", "Sudhanshu Agrawal", "Mukul Gagrani", "Junyoung Park", "Yifan Zao", "He Zhang", "Tian Liu", "Yiping Yang", "Xin Yuan", "Jiuyan Lu", "Chris Lott", "Mingu Lee"], "title": "VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs", "comment": "7 pages, 4 figures, 5 tables, accepted at ICML 2025 workshop on\n  Efficient Systems for Foundational Models", "summary": "In this paper, we introduce a simple training-free technique to improve the\nperformance of drafter-based speculative decoding (SpD) methods that\nincorporates language modeling head (LM head) during drafting process. A\ndrafter-based speculative decoding leverages one or more smaller language\nmodels, a.k.a. drafters or draft models, to sample a draft sequence or tree\nconsisting of multiple tokens, followed by verification by a base LLM, a target\nmodel, accepting a subset as its valid generation. As it is usually considered\nthat the speculative decoding requires one-to-one mapping between vocabularies\nof the target model and the draft model, it has been natural to share the\nvocabulary between them, or even share the LM head as in EAGLE or Medusa. We\nfirst identify that this draft token sampling scheme inherently contains an\nunnecessary inference overhead in drafting, especially for some target LLMs\nwith very large vocabularies. Then, we propose a simple technique, VocabTrim,\nto mitigate the drafting overhead to improve the generation speed in\nmemory-bound environment. VocabTrim reconstructs the drafter LM head to contain\nonly a limited set of tokens, selected by the most frequently sampled from the\nvocabulary of the target model. While limiting the vocabulary in drafting\nslightly degrades the acceptance rate, it significantly reduces the drafting\nlatency in memory-bound process which is often the case on edge devices,\nresulting in higher memory-bound speed up (MBSU). We show that our method can\nboost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically\nby 16% for Llama-3.2-3B-Instruct.", "AI": {"tldr": "The paper introduces VocabTrim, a training-free technique to reduce drafting overhead in speculative decoding by limiting the drafter's vocabulary to frequently sampled tokens, improving speed in memory-bound environments.", "motivation": "The motivation is to address the unnecessary inference overhead in drafting for large-vocabulary target LLMs, which slows down generation in memory-bound settings.", "method": "VocabTrim reconstructs the drafter's LM head to include only frequently sampled tokens, reducing drafting latency while slightly lowering acceptance rates.", "result": "The method achieves a 16% memory-bound speed-up for Llama-3.2-3B-Instruct on Spec-Bench.", "conclusion": "VocabTrim effectively improves generation speed in memory-bound environments by optimizing the drafter's vocabulary, despite a minor trade-off in acceptance rates."}}
{"id": "2506.22631", "categories": ["cs.LG", "stat.ML", "68Q32, 68W27, 68W20"], "pdf": "https://arxiv.org/pdf/2506.22631", "abs": "https://arxiv.org/abs/2506.22631", "authors": ["Dmitry B. Rokhlin"], "title": "A hierarchical Vovk-Azoury-Warmuth forecaster with discounting for online regression in RKHS", "comment": null, "summary": "We study the problem of online regression with the unconstrained quadratic\nloss against a time-varying sequence of functions from a Reproducing Kernel\nHilbert Space (RKHS). Recently, Jacobsen and Cutkosky (2024) introduced a\ndiscounted Vovk-Azoury-Warmuth (DVAW) forecaster that achieves optimal dynamic\nregret in the finite-dimensional case. In this work, we lift their approach to\nthe non-parametric domain by synthesizing the DVAW framework with a random\nfeature approximation. We propose a fully adaptive, hierarchical algorithm,\nwhich we call H-VAW-D (Hierarchical Vovk-Azoury-Warmuth with Discounting), that\nlearns both the discount factor and the number of random features. We prove\nthat this algorithm, which has a per-iteration computational complexity of\n$O(T\\ln T)$, achieves an expected dynamic regret of $O(T^{2/3}P_T^{1/3} +\n\\sqrt{T}\\ln T)$, where $P_T$ is the functional path length of a comparator\nsequence.", "AI": {"tldr": "The paper introduces H-VAW-D, a hierarchical algorithm for online regression in RKHS, combining DVAW with random feature approximation to achieve optimal dynamic regret.", "motivation": "To extend the DVAW forecaster to non-parametric domains and adaptively learn key parameters for improved performance.", "method": "Proposes H-VAW-D, a hierarchical algorithm integrating DVAW with random features, learning discount factors and feature counts adaptively.", "result": "Achieves $O(T^{2/3}P_T^{1/3} + \\sqrt{T}\\ln T)$ expected dynamic regret with $O(T\\ln T)$ per-iteration complexity.", "conclusion": "H-VAW-D effectively bridges finite-dimensional and non-parametric settings, offering adaptive and computationally efficient online regression."}}
{"id": "2506.22562", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22562", "abs": "https://arxiv.org/abs/2506.22562", "authors": ["Abhineet Singh", "Nilanjan Ray"], "title": "Improving Token-based Object Detection with Video", "comment": "Under review for publication in IEEE Access", "summary": "This paper improves upon the Pix2Seq object detector by extending it for\nvideos. In the process, it introduces a new way to perform end-to-end video\nobject detection that improves upon existing video detectors in two key ways.\nFirst, by representing objects as variable-length sequences of discrete tokens,\nwe can succinctly represent widely varying numbers of video objects, with\ndiverse shapes and locations, without having to inject any localization cues in\nthe training process. This eliminates the need to sample the space of all\npossible boxes that constrains conventional detectors and thus solves the dual\nproblems of loss sparsity during training and heuristics-based postprocessing\nduring inference. Second, it conceptualizes and outputs the video objects as\nfully integrated and indivisible 3D boxes or tracklets instead of generating\nimage-specific 2D boxes and linking these boxes together to construct the video\nobject, as done in most conventional detectors. This allows it to scale\neffortlessly with available computational resources by simply increasing the\nlength of the video subsequence that the network takes as input, even\ngeneralizing to multi-object tracking if the subsequence can span the entire\nvideo. We compare our video detector with the baseline Pix2Seq static detector\non several datasets and demonstrate consistent improvement, although with\nstrong signs of being bottlenecked by our limited computational resources. We\nalso compare it with several video detectors on UA-DETRAC to show that it is\ncompetitive with the current state of the art even with the computational\nbottleneck. We make our code and models publicly available.", "AI": {"tldr": "The paper extends Pix2Seq for video object detection, introducing a method that uses discrete tokens for object representation and outputs 3D boxes or tracklets, improving scalability and performance.", "motivation": "To address limitations of conventional video detectors, such as loss sparsity and heuristics-based postprocessing, by leveraging sequence-based object representation and integrated 3D output.", "method": "Represents objects as variable-length sequences of discrete tokens and outputs them as 3D boxes or tracklets, eliminating the need for box sampling and postprocessing.", "result": "Shows consistent improvement over Pix2Seq and competitive performance with state-of-the-art video detectors, despite computational bottlenecks.", "conclusion": "The proposed method offers a scalable and efficient approach to video object detection, with potential for generalization to multi-object tracking."}}
{"id": "2506.22698", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22698", "abs": "https://arxiv.org/abs/2506.22698", "authors": ["Emily Dux Speltz"], "title": "Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report", "comment": null, "summary": "This report synthesizes the outcomes of a recent interdisciplinary workshop\nthat brought together leading experts in cognitive psychology, language\nlearning, and artificial intelligence (AI)-based natural language processing\n(NLP). The workshop, funded by the National Science Foundation, aimed to\naddress a critical knowledge gap in our understanding of the relationship\nbetween AI language models and human cognitive processes in text comprehension\nand composition. Through collaborative dialogue across cognitive, linguistic,\nand technological perspectives, workshop participants examined the underlying\nprocesses involved when humans produce and comprehend text, and how AI can both\ninform our understanding of these processes and augment human capabilities. The\nworkshop revealed emerging patterns in the relationship between large language\nmodels (LLMs) and human cognition, with highlights on both the capabilities of\nLLMs and their limitations in fully replicating human-like language\nunderstanding and generation. Key findings include the potential of LLMs to\noffer insights into human language processing, the increasing alignment between\nLLM behavior and human language processing when models are fine-tuned with\nhuman feedback, and the opportunities and challenges presented by human-AI\ncollaboration in language tasks. By synthesizing these findings, this report\naims to guide future research, development, and implementation of LLMs in\ncognitive psychology, linguistics, and education. It emphasizes the importance\nof ethical considerations and responsible use of AI technologies while striving\nto enhance human capabilities in text comprehension and production through\neffective human-AI collaboration.", "AI": {"tldr": "The workshop explored the relationship between AI language models and human cognition in text comprehension and production, highlighting LLMs' potential and limitations, and emphasizing ethical AI use.", "motivation": "To address gaps in understanding how AI language models relate to human cognitive processes in language tasks.", "method": "Interdisciplinary collaboration among experts in cognitive psychology, linguistics, and AI, analyzing human and AI language processes.", "result": "Found insights into LLMs' alignment with human cognition, their potential and limitations, and the benefits of human-AI collaboration.", "conclusion": "Future research should focus on ethical AI use and enhancing human-AI collaboration in language tasks."}}
{"id": "2506.22638", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22638", "abs": "https://arxiv.org/abs/2506.22638", "authors": ["Aadim Nepal", "Safal Shrestha", "Anubhav Shrestha", "Minwu Kim", "Keith Ross"], "title": "Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training", "comment": null, "summary": "Large language models can exhibit improved mathematical reasoning\ncapabilities following post-training with instruction tuning, reinforcement\nlearning, or knowledge distillation. However, it remains unclear whether these\nimprovements are driven by major changes in transformer layers or from minor\nadjustments that leave the relative layer importance structures of the base\nmodel largely unchanged. We investigate this question through systematic\nlayer-wise ablation experiments, examining base, instruction-tuned,\nknowledge-distilled, and reinforcement learning variants on mathematical\nreasoning benchmarks. Our findings show that mathematical reasoning gives rise\nto a specific layer importance structure, and this structure persists across\nall post-training paradigms. Removal of such layers causes accuracy drops of up\nto 80%. In contrast, non-mathematical tasks like factual recall exhibit no\ncritical layers. This distinction suggests that mathematical reasoning requires\nspecialized layers that emerge during pre-training, while other non-reasoning\ntasks do not. From an information-theoretic perspective, we also observe that\nthese critical layers are the same layers where major representational\ntransformation occurs.", "AI": {"tldr": "The paper investigates whether improvements in mathematical reasoning in large language models post-training are due to major changes in transformer layers or minor adjustments. It finds that mathematical reasoning relies on specific, persistent layer structures, unlike non-mathematical tasks.", "motivation": "To understand if post-training enhancements in mathematical reasoning stem from significant layer changes or minor adjustments, and to identify the role of specific layers in such tasks.", "method": "Conducts layer-wise ablation experiments on base and post-trained models (instruction-tuned, knowledge-distilled, reinforcement learning variants) using mathematical reasoning benchmarks.", "result": "Mathematical reasoning depends on specific layers, whose removal causes up to 80% accuracy drop. Non-mathematical tasks show no such critical layers. These layers coincide with major representational transformations.", "conclusion": "Mathematical reasoning requires specialized layers emerging during pre-training, while non-reasoning tasks do not, highlighting a distinct layer importance structure for reasoning."}}
{"id": "2506.22567", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22567", "abs": "https://arxiv.org/abs/2506.22567", "authors": ["Shansong Wang", "Zhecheng Jin", "Mingzhe Hu", "Mojtaba Safari", "Feng Zhao", "Chih-Wei Chang", "Richard LJ Qiu", "Justin Roper", "David S. Yu", "Xiaofeng Yang"], "title": "Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation", "comment": null, "summary": "CLIP models pretrained on natural images with billion-scale image-text pairs\nhave demonstrated impressive capabilities in zero-shot classification,\ncross-modal retrieval, and open-ended visual answering. However, transferring\nthis success to biomedicine is hindered by the scarcity of large-scale\nbiomedical image-text corpora, the heterogeneity of image modalities, and\nfragmented data standards across institutions. These limitations hinder the\ndevelopment of a unified and generalizable biomedical foundation model trained\nfrom scratch. To overcome this, we introduce MMKD-CLIP, a generalist biomedical\nfoundation model developed via Multiple Medical CLIP Knowledge Distillation.\nRather than relying on billion-scale raw data, MMKD-CLIP distills knowledge\nfrom nine state-of-the-art domain-specific or generalist biomedical CLIP\nmodels, each pretrained on millions of biomedical image-text pairs. Our\ntwo-stage training pipeline first performs CLIP-style pretraining on over 2.9\nmillion biomedical image-text pairs from 26 image modalities, followed by\nfeature-level distillation using over 19.2 million feature pairs extracted from\nteacher models. We evaluate MMKD-CLIP on 58 diverse biomedical datasets,\nencompassing over 10.8 million biomedical images across nine image modalities.\nThe evaluation spans six core task types: zero-shot classification, linear\nprobing, cross-modal retrieval, visual question answering, survival prediction,\nand cancer diagnosis. MMKD-CLIP consistently outperforms all teacher models\nwhile demonstrating remarkable robustness and generalization across image\ndomains and task settings. These results underscore that multi-teacher\nknowledge distillation is a scalable and effective paradigm for building\nhigh-performing biomedical foundation models under the practical constraints of\nreal-world data availability.", "AI": {"tldr": "MMKD-CLIP is a biomedical foundation model that distills knowledge from multiple CLIP models to overcome data scarcity and heterogeneity in biomedicine, outperforming teacher models across diverse tasks.", "motivation": "The scarcity of large-scale biomedical image-text corpora and fragmented data standards hinder the development of a unified biomedical foundation model.", "method": "MMKD-CLIP uses a two-stage training pipeline: CLIP-style pretraining on 2.9M biomedical image-text pairs, followed by feature-level distillation from nine teacher models using 19.2M feature pairs.", "result": "MMKD-CLIP outperforms all teacher models on 58 datasets across six task types, showing robustness and generalization.", "conclusion": "Multi-teacher knowledge distillation is scalable and effective for building high-performing biomedical foundation models under real-world data constraints."}}
{"id": "2506.22724", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22724", "abs": "https://arxiv.org/abs/2506.22724", "authors": ["Niyati Bafna", "Tianjian Li", "Kenton Murray", "David R. Mortensen", "David Yarowsky", "Hale Sirin", "Daniel Khashabi"], "title": "The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure", "comment": "23 pages incl. appendix", "summary": "Multilingual generation with large language models (LLMs) is often of poor\nquality for mid- to low-resource languages. Building on insights from\ninterpretability, we demonstrate the existence of an implicit\ntask-solving-->translation pipeline for generation, whereby the model first\nsolves the required task in a largely target-language-agnostic manner, and\nsubsequently translates answer concepts into the intended target language. We\nhypothesize that the failure of the translation stage is an important culprit\nfor the observed low quality of final outputs, and formalize this as the\ntranslation barrier hypothesis. We test this hypothesis for a word translation\ntask across 108 language pairs, using logit lens to observe model processing in\nintermediate layers. We find that a significant portion of overall failures\nindeed stems from translation failure, or the model's inability to translate\ncorrectly solved intermediate concepts into the target language. This is\nespecially true for low-resource target languages. Our results highlight an\nimportant hurdle for end-to-end multilingual generation, and lend guiding\ninsights for future work seeking to improve multilinguality in LLMs.", "AI": {"tldr": "The paper investigates why multilingual generation in LLMs performs poorly for mid- to low-resource languages, attributing it to a translation barrier in the model's pipeline.", "motivation": "To understand and address the poor quality of multilingual generation in LLMs, especially for mid- to low-resource languages.", "method": "Analyzes a word translation task across 108 language pairs using logit lens to observe intermediate model processing.", "result": "Finds translation failure is a major cause of poor outputs, especially for low-resource languages.", "conclusion": "Highlights the translation barrier as a key hurdle and provides insights for improving multilingual generation in LLMs."}}
{"id": "2506.22645", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22645", "abs": "https://arxiv.org/abs/2506.22645", "authors": ["Amir Hossein Rahmati", "Nathan M. Urban", "Byung-Jun Yoon", "Xiaoning Qian"], "title": "Cost-effective Reduced-Order Modeling via Bayesian Active Learning", "comment": null, "summary": "Machine Learning surrogates have been developed to accelerate solving systems\ndynamics of complex processes in different science and engineering\napplications. To faithfully capture governing systems dynamics, these methods\nrely on large training datasets, hence restricting their applicability in\nreal-world problems. In this work, we propose BayPOD-AL, an active learning\nframework based on an uncertainty-aware Bayesian proper orthogonal\ndecomposition (POD) approach, which aims to effectively learn reduced-order\nmodels from high-fidelity full-order models representing complex systems.\nExperimental results on predicting the temperature evolution over a rod\ndemonstrate BayPOD-AL's effectiveness in suggesting the informative data and\nreducing computational cost related to constructing a training dataset compared\nto other uncertainty-guided active learning strategies. Furthermore, we\ndemonstrate BayPOD-AL's generalizability and efficiency by evaluating its\nperformance on a dataset of higher temporal resolution than the training\ndataset.", "AI": {"tldr": "BayPOD-AL is an active learning framework using Bayesian POD to efficiently learn reduced-order models from high-fidelity data, reducing computational costs and improving generalizability.", "motivation": "Machine Learning surrogates require large datasets, limiting real-world applicability. BayPOD-AL addresses this by actively selecting informative data.", "method": "BayPOD-AL combines Bayesian proper orthogonal decomposition with active learning to suggest informative data and reduce training costs.", "result": "BayPOD-AL outperforms other uncertainty-guided strategies in predicting temperature evolution and generalizes well to higher temporal resolution data.", "conclusion": "BayPOD-AL offers a computationally efficient and generalizable solution for learning reduced-order models from complex systems."}}
{"id": "2506.22570", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22570", "abs": "https://arxiv.org/abs/2506.22570", "authors": ["Chee Mei Ling", "Thangarajah Akilan", "Aparna Ravinda Phalke"], "title": "Dual Atrous Separable Convolution for Improving Agricultural Semantic Segmentation", "comment": "17 pages, 7 figures, 6 tables", "summary": "Agricultural image semantic segmentation is a pivotal component of modern\nagriculture, facilitating accurate visual data analysis to improve crop\nmanagement, optimize resource utilization, and boost overall productivity. This\nstudy proposes an efficient image segmentation method for precision\nagriculture, focusing on accurately delineating farmland anomalies to support\ninformed decision-making and proactive interventions. A novel Dual Atrous\nSeparable Convolution (DAS Conv) module is integrated within the\nDeepLabV3-based segmentation framework. The DAS Conv module is meticulously\ndesigned to achieve an optimal balance between dilation rates and padding size,\nthereby enhancing model performance without compromising efficiency. The study\nalso incorporates a strategic skip connection from an optimal stage in the\nencoder to the decoder to bolster the model's capacity to capture fine-grained\nspatial features. Despite its lower computational complexity, the proposed\nmodel outperforms its baseline and achieves performance comparable to highly\ncomplex transformer-based state-of-the-art (SOTA) models on the Agriculture\nVision benchmark dataset. It achieves more than 66% improvement in efficiency\nwhen considering the trade-off between model complexity and performance,\ncompared to the SOTA model. This study highlights an efficient and effective\nsolution for improving semantic segmentation in remote sensing applications,\noffering a computationally lightweight model capable of high-quality\nperformance in agricultural imagery.", "AI": {"tldr": "A novel Dual Atrous Separable Convolution (DAS Conv) module is integrated into a DeepLabV3-based framework for efficient agricultural image segmentation, balancing performance and efficiency.", "motivation": "To improve crop management and productivity by accurately segmenting farmland anomalies in agricultural imagery.", "method": "Proposes a DAS Conv module with optimal dilation rates and padding, and a strategic skip connection in a DeepLabV3 framework.", "result": "Outperforms baseline and matches SOTA transformer models with 66% efficiency improvement on the Agriculture Vision dataset.", "conclusion": "The study presents a lightweight, high-performance solution for agricultural semantic segmentation."}}
{"id": "2506.22760", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22760", "abs": "https://arxiv.org/abs/2506.22760", "authors": ["Alan Dao", "Dinh Bach Vu"], "title": "Jan-nano Technical Report", "comment": null, "summary": "Most language models face a fundamental tradeoff where powerful capabilities\nrequire substantial computational resources. We shatter this constraint with\nJan-nano, a 4B parameter language model that redefines efficiency through\nradical specialization: instead of trying to know everything, it masters the\nart of finding anything instantly. Fine-tuned from Qwen3-4B using our novel\nmulti-stage RLVR system that completely eliminates reliance on next token\nprediction training (SFT), Jan-nano achieves 83.2% on SimpleQA benchmark with\nMCP integration while running on consumer hardware. With 128K context length,\nJan-nano proves that intelligence isn't about scale, it's about strategy.", "AI": {"tldr": "Jan-nano is a 4B parameter language model that achieves high efficiency and performance by specializing in instant information retrieval, eliminating next token prediction training, and running on consumer hardware.", "motivation": "Address the tradeoff between powerful capabilities and computational resources by creating a highly efficient, specialized language model.", "method": "Fine-tuned from Qwen3-4B using a novel multi-stage RLVR system, eliminating next token prediction training (SFT).", "result": "Achieves 83.2% on SimpleQA benchmark with MCP integration and supports 128K context length.", "conclusion": "Jan-nano demonstrates that intelligence is about strategic specialization, not just scale, enabling high performance with minimal resources."}}
{"id": "2506.22655", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22655", "abs": "https://arxiv.org/abs/2506.22655", "authors": ["Andrew F. Ilersich", "Prasanth B. Nair"], "title": "Learning Stochastic Multiscale Models", "comment": "Body is 9 pages, 13 including acknowledgements and references, 35\n  including appendix. 21 figures and 6 tables. Submitted to NeurIPS 2025", "summary": "The physical sciences are replete with dynamical systems that require the\nresolution of a wide range of length and time scales. This presents significant\ncomputational challenges since direct numerical simulation requires\ndiscretization at the finest relevant scales, leading to a high-dimensional\nstate space. In this work, we propose an approach to learn stochastic\nmultiscale models in the form of stochastic differential equations directly\nfrom observational data. Our method resolves the state on a coarse mesh while\nintroducing an auxiliary state to capture the effects of unresolved scales. We\nlearn the parameters of the multiscale model using a modern forward-solver-free\namortized variational inference method. Our approach draws inspiration from\nphysics-based multiscale modeling approaches, such as large-eddy simulation in\nfluid dynamics, while learning directly from data. We present numerical studies\nto demonstrate that our learned multiscale models achieve superior predictive\naccuracy compared to direct numerical simulation and closure-type models at\nequivalent resolution.", "AI": {"tldr": "Proposes a data-driven method to learn stochastic multiscale models for dynamical systems, improving predictive accuracy over traditional simulations.", "motivation": "Addresses computational challenges in simulating multiscale dynamical systems by avoiding high-dimensional state spaces.", "method": "Uses stochastic differential equations and a coarse mesh with an auxiliary state, learned via forward-solver-free variational inference.", "result": "Learned models outperform direct numerical simulation and closure-type models in predictive accuracy.", "conclusion": "The approach offers a promising data-driven alternative for efficient and accurate multiscale modeling."}}
{"id": "2506.22589", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22589", "abs": "https://arxiv.org/abs/2506.22589", "authors": ["Yijun Lin", "Rhett Olson", "Junhan Wu", "Yao-Yi Chiang", "Jerod Weinman"], "title": "LIGHT: Multi-Modal Text Linking on Historical Maps", "comment": "Accepted at ICDAR2025", "summary": "Text on historical maps provides valuable information for studies in history,\neconomics, geography, and other related fields. Unlike structured or\nsemi-structured documents, text on maps varies significantly in orientation,\nreading order, shape, and placement. Many modern methods can detect and\ntranscribe text regions, but they struggle to effectively ``link'' the\nrecognized text fragments, e.g., determining a multi-word place name. Existing\nlayout analysis methods model word relationships to improve text understanding\nin structured documents, but they primarily rely on linguistic features and\nneglect geometric information, which is essential for handling map text. To\naddress these challenges, we propose LIGHT, a novel multi-modal approach that\nintegrates linguistic, image, and geometric features for linking text on\nhistorical maps. In particular, LIGHT includes a geometry-aware embedding\nmodule that encodes the polygonal coordinates of text regions to capture\npolygon shapes and their relative spatial positions on an image. LIGHT unifies\nthis geometric information with the visual and linguistic token embeddings from\nLayoutLMv3, a pretrained layout analysis model. LIGHT uses the cross-modal\ninformation to predict the reading-order successor of each text instance\ndirectly with a bi-directional learning strategy that enhances sequence\nrobustness. Experimental results show that LIGHT outperforms existing methods\non the ICDAR 2024/2025 MapText Competition data, demonstrating the\neffectiveness of multi-modal learning for historical map text linking.", "AI": {"tldr": "LIGHT is a multi-modal approach integrating linguistic, image, and geometric features to link text on historical maps, outperforming existing methods.", "motivation": "Text on historical maps varies in orientation and placement, making linking fragments challenging. Existing methods neglect geometric information, crucial for map text.", "method": "LIGHT combines geometry-aware embeddings (polygon coordinates) with visual and linguistic features from LayoutLMv3, using a bi-directional learning strategy for reading-order prediction.", "result": "LIGHT outperforms existing methods on the ICDAR 2024/2025 MapText Competition data.", "conclusion": "Multi-modal learning, especially integrating geometric features, is effective for linking text on historical maps."}}
{"id": "2506.22777", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22777", "abs": "https://arxiv.org/abs/2506.22777", "authors": ["Miles Turpin", "Andy Arditi", "Marvin Li", "Joe Benton", "Julian Michael"], "title": "Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning", "comment": null, "summary": "Language models trained with RL can engage in reward hacking--exploiting\nunintended strategies for high reward--without revealing this behavior in their\nchain-of-thought reasoning, making detection difficult and posing risks for\nhigh-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL\nintervention that trains models to explicitly acknowledge when they are\ninfluenced by prompt cues--hints which point to incorrect answers (e.g., \"a\nStanford professor thinks the answer is A\"). To evaluate VFT, we subsequently\ntrain models with RL on environments where held-out prompt cues signal which\nincorrect answers will receive high reward, incentivizing models to reward hack\nby exploiting cues instead of reasoning correctly. We measure how often models\nexploit these cues without verbalizing it. After RL, only 6% of the VFT-trained\nmodel's responses consist of undetected reward hacks. In comparison, when we\nperform RL without VFT, the rate of undetected reward hacks goes up to 88%;\nwith a debiasing baseline intervention, this increases further to 99%. VFT\nachieves this by substantially increasing how often models verbalize the\ninfluence of cues--from 8% to 42% after VFT, and up to 94% after RL--while\nbaselines remain low even after RL (10% and 1%). Our results show that teaching\nmodels to explicitly verbalize reward hacking behavior before RL significantly\nimproves their detection, offering a practical path toward more transparent and\nsafe AI systems.", "AI": {"tldr": "VFT (verbalization fine-tuning) reduces undetected reward hacking in RL-trained models by training them to acknowledge prompt cues, improving detection from 6% (with VFT) vs. 88-99% (without).", "motivation": "Address the challenge of detecting reward hacking in RL-trained models, which exploit unintended strategies without revealing them in reasoning, posing risks in high-stakes applications.", "method": "Propose VFT, a pre-RL intervention training models to verbalize influence from prompt cues. Evaluate by RL training on environments with cues signaling incorrect answers.", "result": "VFT reduces undetected reward hacks to 6% post-RL, vs. 88-99% without VFT. Models verbalize cue influence 42-94% with VFT, vs. 1-10% in baselines.", "conclusion": "VFT significantly improves detection of reward hacking by teaching models to verbalize behavior, enhancing transparency and safety in AI systems."}}
{"id": "2506.22668", "categories": ["cs.LG", "cs.AI", "cs.DC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22668", "abs": "https://arxiv.org/abs/2506.22668", "authors": ["Selahattin Akkas", "Aditya Devarakonda", "Ariful Azad"], "title": "DistShap: Scalable GNN Explanations with Distributed Shapley Values", "comment": "12 pages", "summary": "With the growing adoption of graph neural networks (GNNs), explaining their\npredictions has become increasingly important. However, attributing predictions\nto specific edges or features remains computationally expensive. For example,\nclassifying a node with 100 neighbors using a 3-layer GNN may involve\nidentifying important edges from millions of candidates contributing to the\nprediction. To address this challenge, we propose DistShap, a parallel\nalgorithm that distributes Shapley value-based explanations across multiple\nGPUs. DistShap operates by sampling subgraphs in a distributed setting,\nexecuting GNN inference in parallel across GPUs, and solving a distributed\nleast squares problem to compute edge importance scores. DistShap outperforms\nmost existing GNN explanation methods in accuracy and is the first to scale to\nGNN models with millions of features by using up to 128 GPUs on the NERSC\nPerlmutter supercomputer.", "AI": {"tldr": "DistShap is a parallel algorithm for efficiently computing Shapley value-based explanations for GNN predictions by distributing computations across multiple GPUs.", "motivation": "The need to explain GNN predictions efficiently, especially for large-scale graphs with millions of features, due to the computational expense of existing methods.", "method": "DistShap distributes Shapley value computations by sampling subgraphs, parallelizing GNN inference, and solving a distributed least squares problem to determine edge importance.", "result": "DistShap outperforms existing GNN explanation methods in accuracy and scales to models with millions of features using up to 128 GPUs.", "conclusion": "DistShap provides a scalable and accurate solution for explaining GNN predictions, addressing computational challenges in large-scale applications."}}
{"id": "2506.22591", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22591", "abs": "https://arxiv.org/abs/2506.22591", "authors": ["Arunkumar Kannan", "Martin A. Lindquist", "Brian Caffo"], "title": "BrainMT: A Hybrid Mamba-Transformer Architecture for Modeling Long-Range Dependencies in Functional MRI Data", "comment": "Accepted at MICCAI 2025", "summary": "Recent advances in deep learning have made it possible to predict phenotypic\nmeasures directly from functional magnetic resonance imaging (fMRI) brain\nvolumes, sparking significant interest in the neuroimaging community. However,\nexisting approaches, primarily based on convolutional neural networks or\ntransformer architectures, often struggle to model the complex relationships\ninherent in fMRI data, limited by their inability to capture long-range spatial\nand temporal dependencies. To overcome these shortcomings, we introduce\nBrainMT, a novel hybrid framework designed to efficiently learn and integrate\nlong-range spatiotemporal attributes in fMRI data. Our framework operates in\ntwo stages: (1) a bidirectional Mamba block with a temporal-first scanning\nmechanism to capture global temporal interactions in a computationally\nefficient manner; and (2) a transformer block leveraging self-attention to\nmodel global spatial relationships across the deep features processed by the\nMamba block. Extensive experiments on two large-scale public datasets,\nUKBioBank and the Human Connectome Project, demonstrate that BrainMT achieves\nstate-of-the-art performance on both classification (sex prediction) and\nregression (cognitive intelligence prediction) tasks, outperforming existing\nmethods by a significant margin. Our code and implementation details will be\nmade publicly available at this\nhttps://github.com/arunkumar-kannan/BrainMT-fMRI", "AI": {"tldr": "BrainMT is a hybrid framework combining Mamba and transformer blocks to model long-range spatiotemporal dependencies in fMRI data, outperforming existing methods in classification and regression tasks.", "motivation": "Existing deep learning approaches struggle with long-range spatial and temporal dependencies in fMRI data, limiting their predictive performance.", "method": "BrainMT uses a bidirectional Mamba block for temporal interactions and a transformer block for spatial relationships, applied to fMRI data.", "result": "BrainMT achieves state-of-the-art performance on sex prediction and cognitive intelligence prediction tasks, surpassing other methods.", "conclusion": "BrainMT effectively addresses the limitations of current methods by integrating long-range spatiotemporal modeling, demonstrating superior performance in fMRI-based prediction tasks."}}
{"id": "2506.22791", "categories": ["cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2506.22791", "abs": "https://arxiv.org/abs/2506.22791", "authors": ["Jianxin Yan", "Wangze Ni", "Lei Chen", "Xuemin Lin", "Peng Cheng", "Zhan Qin", "Kui Ren"], "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models", "comment": null, "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.", "AI": {"tldr": "ContextCache introduces a context-aware semantic caching system for multi-turn dialogues, improving precision and recall while reducing latency and computational costs.", "motivation": "Existing semantic caching systems lack awareness of multi-turn dialogue contexts, leading to incorrect cache hits in different conversational settings.", "method": "ContextCache uses a two-stage retrieval architecture: vector-based retrieval on the current query followed by contextual matching via self-attention mechanisms.", "result": "ContextCache improves precision and recall in real-world conversations and reduces latency by 10x compared to direct LLM invocation.", "conclusion": "ContextCache effectively addresses the limitations of existing systems, offering significant efficiency gains for LLM conversational applications."}}
{"id": "2506.22685", "categories": ["cs.LG", "cs.GR"], "pdf": "https://arxiv.org/pdf/2506.22685", "abs": "https://arxiv.org/abs/2506.22685", "authors": ["Anh Bui", "Trang Vu", "Trung Le", "Junae Kim", "Tamas Abraham", "Rollin Omari", "Amar Kaur", "Dinh Phung"], "title": "Mitigating Semantic Collapse in Generative Personalization with a Surprisingly Simple Test-Time Embedding Adjustment", "comment": null, "summary": "In this paper, we investigate the semantic collapsing problem in generative\npersonalization, an under-explored topic where the learned visual concept\n($V^*$) gradually shifts from its original textual meaning and comes to\ndominate other concepts in multi-concept input prompts. This issue not only\nreduces the semantic richness of complex input prompts like \"a photo of $V^*$\nwearing glasses and playing guitar\" into simpler, less contextually rich forms\nsuch as \"a photo of $V^*$\" but also leads to simplified output images that fail\nto capture the intended concept.\n  We identify the root cause as unconstrained optimisation, which allows the\nlearned embedding $V^*$ to drift arbitrarily in the embedding space, both in\ndirection and magnitude. To address this, we propose a simple yet effective\ntraining-free method that adjusts the magnitude and direction of pre-trained\nembedding at inference time, effectively mitigating the semantic collapsing\nproblem. Our method is broadly applicable across different personalization\nmethods and demonstrates significant improvements in text-image alignment in\ndiverse use cases. Our code is anonymously published at\nhttps://anonymous.4open.science/r/Embedding-Adjustment.", "AI": {"tldr": "The paper addresses the semantic collapsing problem in generative personalization, where learned visual concepts drift from their original meaning, simplifying outputs. A training-free method is proposed to adjust embeddings at inference, improving alignment.", "motivation": "Semantic collapsing reduces the richness of multi-concept prompts, leading to oversimplified outputs. The goal is to preserve the intended semantic context.", "method": "A training-free approach adjusts the magnitude and direction of pre-trained embeddings during inference to prevent drift.", "result": "The method effectively mitigates semantic collapsing, improving text-image alignment across various personalization techniques.", "conclusion": "The proposed solution is simple, effective, and broadly applicable, enhancing the fidelity of generative personalization."}}
{"id": "2506.22624", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22624", "abs": "https://arxiv.org/abs/2506.22624", "authors": ["Zuyao You", "Zuxuan Wu"], "title": "Seg-R1: Segmentation Can Be Surprisingly Simple with Reinforcement Learning", "comment": null, "summary": "We present Seg-R1, a preliminary exploration of using reinforcement learning\n(RL) to enhance the pixel-level understanding and reasoning capabilities of\nlarge multimodal models (LMMs). Starting with foreground segmentation tasks,\nspecifically camouflaged object detection (COD) and salient object detection\n(SOD), our approach enables the LMM to generate point and bounding box prompts\nin the next-token fashion, which are then used to guide SAM2 in producing\nsegmentation masks. We introduce Group Relative Policy Optimization (GRPO) into\nthe segmentation domain, equipping the LMM with pixel-level comprehension\nthrough a carefully designed training strategy. Notably, Seg-R1 achieves\nremarkable performance with purely RL-based training, achieving .873 S-measure\non COD10K without complex model modification. Moreover, we found that pure RL\ntraining demonstrates strong open-world generalization. Despite being trained\nsolely on foreground segmentation image-mask pairs without text supervision,\nSeg-R1 achieves impressive zero-shot performance on referring segmentation and\nreasoning segmentation tasks, with 71.4 cIoU on RefCOCOg test and 56.7 gIoU on\nReasonSeg test, outperforming models fully supervised on these datasets.", "AI": {"tldr": "Seg-R1 uses reinforcement learning to improve pixel-level understanding in large multimodal models, achieving strong performance in segmentation tasks without complex modifications.", "motivation": "Enhancing pixel-level reasoning in large multimodal models for better segmentation tasks like camouflaged and salient object detection.", "method": "Utilizes reinforcement learning (RL) with Group Relative Policy Optimization (GRPO) to train the model for generating prompts and segmentation masks.", "result": "Achieves .873 S-measure on COD10K and strong zero-shot performance on referring and reasoning segmentation tasks.", "conclusion": "Seg-R1 demonstrates the effectiveness of pure RL training for segmentation, with notable generalization capabilities."}}
{"id": "2506.22808", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22808", "abs": "https://arxiv.org/abs/2506.22808", "authors": ["Jianhui Wei", "Zijie Meng", "Zikai Xiao", "Tianxiang Hu", "Yang Feng", "Zhijie Zhou", "Jian Wu", "Zuozhu Liu"], "title": "MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs", "comment": "20 pages", "summary": "While Medical Large Language Models (MedLLMs) have demonstrated remarkable\npotential in clinical tasks, their ethical safety remains insufficiently\nexplored. This paper introduces $\\textbf{MedEthicsQA}$, a comprehensive\nbenchmark comprising $\\textbf{5,623}$ multiple-choice questions and\n$\\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs.\nWe systematically establish a hierarchical taxonomy integrating global medical\nethical standards. The benchmark encompasses widely used medical datasets,\nauthoritative question banks, and scenarios derived from PubMed literature.\nRigorous quality control involving multi-stage filtering and multi-faceted\nexpert validation ensures the reliability of the dataset with a low error rate\n($2.72\\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance\nin answering medical ethics questions compared to their foundation\ncounterparts, elucidating the deficiencies of medical ethics alignment. The\ndataset, registered under CC BY-NC 4.0 license, is available at\nhttps://github.com/JianhuiWei7/MedEthicsQA.", "AI": {"tldr": "The paper introduces MedEthicsQA, a benchmark for evaluating medical ethics in LLMs, revealing deficiencies in MedLLMs' ethical alignment.", "motivation": "To address the insufficient exploration of ethical safety in Medical Large Language Models (MedLLMs).", "method": "Creation of MedEthicsQA, a benchmark with 5,623 multiple-choice and 5,351 open-ended questions, integrating global ethical standards and rigorous quality control.", "result": "State-of-the-art MedLLMs perform worse on medical ethics questions compared to their foundation counterparts.", "conclusion": "The study highlights the need for better ethical alignment in MedLLMs and provides a reliable benchmark for future research."}}
{"id": "2506.22696", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22696", "abs": "https://arxiv.org/abs/2506.22696", "authors": ["Brian Mak", "Jeffrey Flanigan"], "title": "Residual Matrix Transformers: Scaling the Size of the Residual Stream", "comment": "Accepted to ICML 2025", "summary": "The residual stream acts as a memory bus where transformer layers both store\nand access features (Elhage et al., 2021). We consider changing the mechanism\nfor retrieving and storing information in the residual stream, and replace the\nresidual stream of the transformer with an outer product memory matrix\n(Kohonen, 1972, Anderson, 1972). We call this model the Residual Matrix\nTransformer (RMT). We find that the RMT enjoys a number of attractive\nproperties: 1) the size of the residual stream can be scaled independently of\ncompute and model size, improving performance, 2) the RMT can achieve the same\nloss as the transformer with 58% fewer FLOPS, 25% fewer parameters, and 41%\nfewer training tokens tokens, and 3) the RMT outperforms the transformer on\ndownstream evaluations. We theoretically analyze the transformer and the RMT,\nand show that the RMT allows for more efficient scaling of the residual stream,\nas well as improved variance propagation properties. Code for this project can\nbe found at https://github.com/bmac3/residual-matrix-transformer.", "AI": {"tldr": "The paper introduces the Residual Matrix Transformer (RMT), replacing the transformer's residual stream with an outer product memory matrix, offering improved scalability, efficiency, and performance.", "motivation": "To enhance the transformer's residual stream mechanism for better efficiency and performance by leveraging an outer product memory matrix.", "method": "Replaces the transformer's residual stream with an outer product memory matrix, creating the RMT model.", "result": "RMT achieves comparable loss with fewer FLOPS (58% less), parameters (25% less), and training tokens (41% less), while outperforming transformers in downstream tasks.", "conclusion": "The RMT provides a more efficient and scalable alternative to traditional transformers, with theoretical and practical advantages."}}
{"id": "2506.22636", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22636", "abs": "https://arxiv.org/abs/2506.22636", "authors": ["Sotirios Panagiotis Chytas", "Miso Choi", "Hyunwoo J. Kim", "Vikas Singh"], "title": "ReCo: Reminder Composition Mitigates Hallucinations in Vision-Language Models", "comment": null, "summary": "Vision Language Models (VLMs) show impressive capabilities in integrating and\nreasoning with both visual and language data. But these models make mistakes. A\ncommon finding -- similar to LLMs -- is their tendency to hallucinate, i.e.,\ngenerate plausible sounding text which is not grounded in the visual input, or\nat worst, is contradictory. A growing consensus attributes this behavior to an\nover-reliance on language -- especially as the generation progresses, the model\nsuffers from a ``fading memory effect'' with respect to the provided visual\ninput. We study mechanisms by which this behavior can be controlled.\nSpecifically, using ideas from geometric algebra and relational compositions,\nwe propose the addition of a small, trainable module (named ReCo) on top of any\nVLM -- no other modification is needed. We show that such a lightweight module\nis able to mitigate the fading memory effect on three of the most widely used\nVLMs (InstructBLIP, LlaVA, MiniGPT4), where we see performance improvements on\nmultiple benchmarks. Additionally, we show that our module can be combined with\nmany of the other approaches for reducing hallucination where we achieve\nimproved results for each one.", "AI": {"tldr": "The paper addresses the hallucination issue in Vision Language Models (VLMs) by proposing a lightweight module (ReCo) to mitigate the fading memory effect, improving performance across benchmarks.", "motivation": "VLMs often hallucinate due to over-reliance on language and fading memory of visual input, leading to ungrounded or contradictory outputs.", "method": "Introduces ReCo, a small trainable module based on geometric algebra and relational compositions, added atop existing VLMs without further modifications.", "result": "ReCo improves performance on three popular VLMs (InstructBLIP, LlaVA, MiniGPT4) across benchmarks and enhances other hallucination-reduction methods.", "conclusion": "ReCo effectively mitigates the fading memory effect in VLMs, offering a lightweight solution to hallucination without extensive model changes."}}
{"id": "2506.22813", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22813", "abs": "https://arxiv.org/abs/2506.22813", "authors": ["Zhuojun Ding", "Wei Wei", "Chenghao Fan"], "title": "Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models", "comment": null, "summary": "Supervised fine-tuning (SFT) is widely used to align large language models\n(LLMs) with information extraction (IE) tasks, such as named entity recognition\n(NER). However, annotating such fine-grained labels and training\ndomain-specific models is costly. Existing works typically train a unified\nmodel across multiple domains, but such approaches lack adaptation and\nscalability since not all training data benefits target domains and scaling\ntrained models remains challenging. We propose the SaM framework, which\ndynamically Selects and Merges expert models at inference time. Specifically,\nfor a target domain, we select domain-specific experts pre-trained on existing\ndomains based on (i) domain similarity to the target domain and (ii)\nperformance on sampled instances, respectively. The experts are then merged to\ncreate task-specific models optimized for the target domain. By dynamically\nmerging experts beneficial to target domains, we improve generalization across\nvarious domains without extra training. Additionally, experts can be added or\nremoved conveniently, leading to great scalability. Extensive experiments on\nmultiple benchmarks demonstrate our framework's effectiveness, which\noutperforms the unified model by an average of 10%. We further provide insights\ninto potential improvements, practical experience, and extensions of our\nframework.", "AI": {"tldr": "The paper introduces SaM, a framework for dynamically selecting and merging expert models for domain-specific tasks, improving generalization and scalability without extra training.", "motivation": "Supervised fine-tuning for domain-specific tasks is costly and lacks adaptability in unified models. SaM addresses these issues by leveraging existing expert models.", "method": "SaM selects and merges domain-specific experts based on domain similarity and performance, creating optimized task-specific models dynamically.", "result": "SaM outperforms unified models by 10% on average across benchmarks and offers scalability by allowing easy addition/removal of experts.", "conclusion": "SaM provides an effective, scalable solution for domain adaptation in information extraction tasks, with potential for further improvements."}}
{"id": "2506.22708", "categories": ["cs.LG", "cs.SY", "econ.GN", "eess.SY", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2506.22708", "abs": "https://arxiv.org/abs/2506.22708", "authors": ["Shrenik Jadhav", "Birva Sevak", "Srijita Das", "Akhtar Hussain", "Wencong Su", "Van-Hai Bui"], "title": "FairMarket-RL: LLM-Guided Fairness Shaping for Multi-Agent Reinforcement Learning in Peer-to-Peer Markets", "comment": null, "summary": "Peer-to-peer (P2P) trading is increasingly recognized as a key mechanism for\ndecentralized market regulation, yet existing approaches often lack robust\nframeworks to ensure fairness. This paper presents FairMarket-RL, a novel\nhybrid framework that combines Large Language Models (LLMs) with Reinforcement\nLearning (RL) to enable fairness-aware trading agents. In a simulated P2P\nmicrogrid with multiple sellers and buyers, the LLM acts as a real-time\nfairness critic, evaluating each trading episode using two metrics:\nFairness-To-Buyer (FTB) and Fairness-Between-Sellers (FBS). These fairness\nscores are integrated into agent rewards through scheduled\n{\\lambda}-coefficients, forming an adaptive LLM-guided reward shaping loop that\nreplaces brittle, rule-based fairness constraints. Agents are trained using\nIndependent Proximal Policy Optimization (IPPO) and achieve equitable outcomes,\nfulfilling over 90% of buyer demand, maintaining fair seller margins, and\nconsistently reaching FTB and FBS scores above 0.80. The training process\ndemonstrates that fairness feedback improves convergence, reduces buyer\nshortfalls, and narrows profit disparities between sellers. With its\nlanguage-based critic, the framework scales naturally, and its extension to a\nlarge power distribution system with household prosumers illustrates its\npractical applicability. FairMarket-RL thus offers a scalable, equity-driven\nsolution for autonomous trading in decentralized energy systems.", "AI": {"tldr": "FairMarket-RL combines LLMs and RL for fairness-aware P2P trading, achieving high fairness scores and equitable outcomes in decentralized markets.", "motivation": "Existing P2P trading lacks robust fairness frameworks, necessitating a scalable, adaptive solution.", "method": "Uses LLMs as fairness critics with FTB and FBS metrics, integrating scores into RL rewards via IPPO training.", "result": "Agents achieve >90% buyer demand fulfillment, fair seller margins, and FTB/FBS scores >0.80.", "conclusion": "FairMarket-RL provides a scalable, equity-driven solution for decentralized trading systems."}}
{"id": "2506.22637", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22637", "abs": "https://arxiv.org/abs/2506.22637", "authors": ["Haoxuan Wang", "Zhenghao Zhao", "Junyi Wu", "Yuzhang Shang", "Gaowen Liu", "Yan Yan"], "title": "CaO$_2$: Rectifying Inconsistencies in Diffusion-Based Dataset Distillation", "comment": "ICCV 2025. Code is available at\n  https://github.com/hatchetProject/CaO2", "summary": "The recent introduction of diffusion models in dataset distillation has shown\npromising potential in creating compact surrogate datasets for large,\nhigh-resolution target datasets, offering improved efficiency and performance\nover traditional bi-level/uni-level optimization methods. However, current\ndiffusion-based dataset distillation approaches overlook the evaluation process\nand exhibit two critical inconsistencies in the distillation process: (1)\nObjective Inconsistency, where the distillation process diverges from the\nevaluation objective, and (2) Condition Inconsistency, leading to mismatches\nbetween generated images and their corresponding conditions. To resolve these\nissues, we introduce Condition-aware Optimization with Objective-guided\nSampling (CaO$_2$), a two-stage diffusion-based framework that aligns the\ndistillation process with the evaluation objective. The first stage employs a\nprobability-informed sample selection pipeline, while the second stage refines\nthe corresponding latent representations to improve conditional likelihood.\nCaO$_2$ achieves state-of-the-art performance on ImageNet and its subsets,\nsurpassing the best-performing baselines by an average of 2.3% accuracy.", "AI": {"tldr": "CaO$_2$ introduces a two-stage diffusion-based framework to address inconsistencies in dataset distillation, achieving state-of-the-art performance on ImageNet.", "motivation": "Current diffusion-based dataset distillation methods suffer from objective and condition inconsistencies, leading to suboptimal performance.", "method": "CaO$_2$ uses a two-stage approach: probability-informed sample selection and latent representation refinement.", "result": "CaO$_2$ outperforms baselines by 2.3% accuracy on ImageNet and subsets.", "conclusion": "The proposed framework effectively aligns distillation with evaluation objectives, improving performance."}}
{"id": "2506.22846", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22846", "abs": "https://arxiv.org/abs/2506.22846", "authors": ["Duygu Altinok"], "title": "Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization", "comment": "This is the accepted version of an article accepted to the TSD 2025\n  conference, published in Springer Lecture Notes in Artificial Intelligence\n  (LNAI). The final authenticated version is available online at SpringerLink", "summary": "End-to-end (E2E) automatic speech recognition (ASR) systems have\nrevolutionized the field by integrating all components into a single neural\nnetwork, with attention-based encoder-decoder models achieving state-of-the-art\nperformance. However, their autoregressive decoding process limits inference\nspeed, making them unsuitable for real-time applications. In contrast,\nCTC-based models offer faster, non-autoregressive decoding but struggle to\nmodel linguistic dependencies effectively. Addressing this challenge, we\npropose a novel auxiliary loss framework called Language-Aware Intermediate\nLoss (LAIL) to enhance CTC-based ASR using the linguistic knowledge of large\nlanguage models (LLMs). By attaching connector layers to intermediate encoder\nlayers, LAIL maps outputs to the embedding space of an LLM and computes a\ncausal language modeling loss during training. This approach enhances\nlinguistic modeling while preserving the computational efficiency of CTC\ndecoding. Using the Conformer architecture and various LLaMA models, we\ndemonstrate significant improvements in Word Error Rate (WER) on the\nLibriSpeech, TEDLIUM2, and WSJ corpora, achieving state-of-the-art performance\nfor CTC-based ASR with minimal computational overhead.", "AI": {"tldr": "Proposes LAIL, a novel auxiliary loss framework to enhance CTC-based ASR by leveraging LLMs' linguistic knowledge, improving WER without sacrificing CTC's efficiency.", "motivation": "Autoregressive E2E ASR models are slow for real-time use, while CTC models lack linguistic dependency modeling. LAIL bridges this gap.", "method": "Attaches connector layers to intermediate encoder layers, maps outputs to LLM embedding space, and computes a causal language modeling loss.", "result": "Achieves state-of-the-art WER on LibriSpeech, TEDLIUM2, and WSJ with minimal computational overhead.", "conclusion": "LAIL effectively enhances CTC-based ASR by integrating LLM linguistic knowledge, balancing performance and efficiency."}}
{"id": "2506.22712", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22712", "abs": "https://arxiv.org/abs/2506.22712", "authors": ["Alexander Theus", "Alessandro Cabodi", "Sotiris Anagnostidis", "Antonio Orvieto", "Sidak Pal Singh", "Valentina Boeva"], "title": "Generalized Linear Mode Connectivity for Transformers", "comment": null, "summary": "Understanding the geometry of neural network loss landscapes is a central\nquestion in deep learning, with implications for generalization and\noptimization. A striking phenomenon is linear mode connectivity (LMC), where\nindependently trained models can be connected by low- or zero-loss paths,\ndespite appearing to lie in separate loss basins. However, this is often\nobscured by symmetries in parameter space -- such as neuron permutations --\nwhich make functionally equivalent models appear dissimilar. Prior work has\npredominantly focused on neuron re-ordering through permutations, but such\napproaches are limited in scope and fail to capture the richer symmetries\nexhibited by modern architectures such as Transformers. In this work, we\nintroduce a unified framework that captures four symmetry classes:\npermutations, semi-permutations, orthogonal transformations, and general\ninvertible maps -- broadening the set of valid reparameterizations and\nsubsuming many previous approaches as special cases. Crucially, this\ngeneralization enables, for the first time, the discovery of low- and\nzero-barrier linear interpolation paths between independently trained Vision\nTransformers and GPT-2 models. These results reveal deeper structure in the\nloss landscape and underscore the importance of symmetry-aware analysis for\nunderstanding model space geometry.", "AI": {"tldr": "The paper introduces a unified framework to analyze symmetries in neural network loss landscapes, enabling discovery of low- and zero-loss paths between models like Vision Transformers and GPT-2.", "motivation": "To understand the geometry of neural network loss landscapes, particularly linear mode connectivity (LMC), which is often obscured by symmetries like neuron permutations.", "method": "Proposes a framework capturing four symmetry classes: permutations, semi-permutations, orthogonal transformations, and general invertible maps.", "result": "Enables discovery of low- and zero-barrier linear interpolation paths between independently trained models (e.g., Vision Transformers and GPT-2).", "conclusion": "Reveals deeper structure in loss landscapes and highlights the importance of symmetry-aware analysis for understanding model space geometry."}}
{"id": "2506.22604", "categories": ["cs.AI", "cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22604", "abs": "https://arxiv.org/abs/2506.22604", "authors": ["David Porfirio", "Vincent Hsiao", "Morgan Fine-Morris", "Leslie Smith", "Laura M. Hiatt"], "title": "Bootstrapping Human-Like Planning via LLMs", "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN)", "summary": "Robot end users increasingly require accessible means of specifying tasks for\nrobots to perform. Two common end-user programming paradigms include\ndrag-and-drop interfaces and natural language programming. Although natural\nlanguage interfaces harness an intuitive form of human communication,\ndrag-and-drop interfaces enable users to meticulously and precisely dictate the\nkey actions of the robot's task. In this paper, we investigate the degree to\nwhich both approaches can be combined. Specifically, we construct a large\nlanguage model (LLM)-based pipeline that accepts natural language as input and\nproduces human-like action sequences as output, specified at a level of\ngranularity that a human would produce. We then compare these generated action\nsequences to another dataset of hand-specified action sequences. Although our\nresults reveal that larger models tend to outperform smaller ones in the\nproduction of human-like action sequences, smaller models nonetheless achieve\nsatisfactory performance.", "AI": {"tldr": "Combining natural language and drag-and-drop interfaces for robot task specification using LLMs, with larger models outperforming smaller ones in generating human-like action sequences.", "motivation": "To bridge the gap between intuitive natural language interfaces and precise drag-and-drop programming for robot task specification.", "method": "Developed an LLM-based pipeline that converts natural language input into human-like action sequences, compared to hand-specified sequences.", "result": "Larger LLMs generate more human-like sequences, but smaller models still perform satisfactorily.", "conclusion": "Combining natural language and drag-and-drop paradigms is feasible, with LLMs effectively bridging the gap."}}
{"id": "2506.22678", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22678", "abs": "https://arxiv.org/abs/2506.22678", "authors": ["Nicolas Caytuiro", "Ivan Sipiran"], "title": "3D Shape Generation: A Survey", "comment": "20 pages, 5 figures", "summary": "Recent advances in deep learning have significantly transformed the field of\n3D shape generation, enabling the synthesis of complex, diverse, and\nsemantically meaningful 3D objects. This survey provides a comprehensive\noverview of the current state of the art in 3D shape generation, organizing the\ndiscussion around three core components: shape representations, generative\nmodeling approaches, and evaluation protocols. We begin by categorizing 3D\nrepresentations into explicit, implicit, and hybrid setups, highlighting their\nstructural properties, advantages, and limitations. Next, we review a wide\nrange of generation methods, focusing on feedforward architectures. We further\nsummarize commonly used datasets and evaluation metrics that assess fidelity,\ndiversity, and realism of generated shapes. Finally, we identify open\nchallenges and outline future research directions that could drive progress in\ncontrollable, efficient, and high-quality 3D shape generation. This survey aims\nto serve as a valuable reference for researchers and practitioners seeking a\nstructured and in-depth understanding of this rapidly evolving field.", "AI": {"tldr": "A survey on 3D shape generation, covering representations, methods, and evaluation, with future research directions.", "motivation": "To provide a structured overview of advancements in 3D shape generation and guide future research.", "method": "Categorizes 3D representations, reviews generative methods, and summarizes datasets and metrics.", "result": "Highlights current state-of-the-art, challenges, and future directions in 3D shape generation.", "conclusion": "The survey serves as a reference for understanding and advancing 3D shape generation."}}
{"id": "2506.22852", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22852", "abs": "https://arxiv.org/abs/2506.22852", "authors": ["Yucheng Cai", "Yuxuan Wu", "Yi Huang", "Junlan Feng", "Zhijian Ou"], "title": "Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems", "comment": null, "summary": "Large language models (LLMs) have recently been applied to dialog systems.\nDespite making progress, LLMs are prone to errors in knowledge-intensive\nscenarios. Recently, approaches based on retrieval augmented generation (RAG)\nand agent have emerged to improve the factual accuracy by enhancing the LLMs\nwith knowledge retrieved from external knowledge bases (KBs). This is mostly\nimplemented by prompting the LLMs with instructions, examples and the retrieved\nknowledge. However, LLMs may have difficulty using the retrieved knowledge\neffectively for response generation, because they are not well trained to do\nsuch generation for specific domains. To mitigate this problem, we propose to\nfinetune the LLMs in the RAG-based and agent-based systems with domain-specific\ndata, together with domain-specific external knowledge, which is called\nknowledge augmented finetuning (KAFT). We base our study on the MobileCS2\ndataset, a real-life customer service dialog dataset that features intensive\nknowledge interactions, to systematically compare the prompting and KAFT\ntechniques in the RAG-based and agent-based systems. Experiment results show\nthat KAFT substantially surpasses prompting in both RAG and agent systems,\nparticularly in terms of factual accuracy. To the best of our knowledge, this\npaper represents the first solid empirical work to investigate the KAFT idea.", "AI": {"tldr": "The paper proposes Knowledge Augmented Fine-Tuning (KAFT) to improve LLMs' factual accuracy in knowledge-intensive dialog systems by fine-tuning them with domain-specific data and external knowledge, outperforming prompting-based methods.", "motivation": "LLMs struggle with factual accuracy in knowledge-intensive scenarios, even with retrieval-augmented generation (RAG) or agent-based approaches.", "method": "Proposes KAFT, fine-tuning LLMs with domain-specific data and external knowledge, tested on the MobileCS2 dataset.", "result": "KAFT significantly outperforms prompting in RAG and agent systems, especially in factual accuracy.", "conclusion": "KAFT is a promising approach for enhancing LLMs' performance in knowledge-intensive dialog systems."}}
{"id": "2506.22716", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2506.22716", "abs": "https://arxiv.org/abs/2506.22716", "authors": ["Dujian Ding", "Ankur Mallick", "Shaokun Zhang", "Chi Wang", "Daniel Madrigal", "Mirian Del Carmen Hipolito Garcia", "Menglin Xia", "Laks V. S. Lakshmanan", "Qingyun Wu", "Victor R\u00fchle"], "title": "BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute", "comment": "Accepted to ICML 2025 (main conference)", "summary": "Large language models (LLMs) are powerful tools but are often expensive to\ndeploy at scale. LLM query routing mitigates this by dynamically assigning\nqueries to models of varying cost and quality to obtain a desired trade-off.\nPrior query routing approaches generate only one response from the selected\nmodel and a single response from a small (inexpensive) model was often not good\nenough to beat a response from a large (expensive) model due to which they end\nup overusing the large model and missing out on potential cost savings.\nHowever, it is well known that for small models, generating multiple responses\nand selecting the best can enhance quality while remaining cheaper than a\nsingle large-model response. We leverage this idea to propose BEST-Route, a\nnovel routing framework that chooses a model and the number of responses to\nsample from it based on query difficulty and the quality thresholds.\nExperiments on real-world datasets demonstrate that our method reduces costs by\nup to 60% with less than 1% performance drop.", "AI": {"tldr": "BEST-Route is a novel LLM query routing framework that dynamically selects models and response counts to balance cost and quality, achieving significant cost savings with minimal performance loss.", "motivation": "To address the inefficiency of prior query routing methods that overuse expensive large models by generating only one response, missing cost-saving opportunities with smaller models.", "method": "Proposes BEST-Route, which selects models and response counts based on query difficulty and quality thresholds, leveraging multiple responses from small models to enhance quality cheaply.", "result": "Reduces costs by up to 60% with less than 1% performance drop on real-world datasets.", "conclusion": "BEST-Route effectively balances cost and quality in LLM query routing, outperforming prior approaches."}}
{"id": "2506.22609", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22609", "abs": "https://arxiv.org/abs/2506.22609", "authors": ["Graham Todd", "Alexander G. Padula", "Dennis J. N. J. Soemers", "Julian Togelius"], "title": "Ludax: A GPU-Accelerated Domain Specific Language for Board Games", "comment": "18 pages, 3 figures", "summary": "Games have long been used as benchmarks and testing environments for research\nin artificial intelligence. A key step in supporting this research was the\ndevelopment of game description languages: frameworks that compile\ndomain-specific code into playable and simulatable game environments, allowing\nresearchers to generalize their algorithms and approaches across multiple games\nwithout having to manually implement each one. More recently, progress in\nreinforcement learning (RL) has been largely driven by advances in hardware\nacceleration. Libraries like JAX allow practitioners to take full advantage of\ncutting-edge computing hardware, often speeding up training and testing by\norders of magnitude. Here, we present a synthesis of these strands of research:\na domain-specific language for board games which automatically compiles into\nhardware-accelerated code. Our framework, Ludax, combines the generality of\ngame description languages with the speed of modern parallel processing\nhardware and is designed to fit neatly into existing deep learning pipelines.\nWe envision Ludax as a tool to help accelerate games research generally, from\nRL to cognitive science, by enabling rapid simulation and providing a flexible\nrepresentation scheme. We present a detailed breakdown of Ludax's description\nlanguage and technical notes on the compilation process, along with speed\nbenchmarking and a demonstration of training RL agents. The Ludax framework,\nalong with implementations of existing board games, is open-source and freely\navailable.", "AI": {"tldr": "Ludax is a domain-specific language for board games that compiles into hardware-accelerated code, combining generality with speed for AI research.", "motivation": "To accelerate games research (e.g., RL, cognitive science) by enabling rapid simulation and flexible representation.", "method": "Developed Ludax, a framework that compiles game descriptions into hardware-accelerated code, integrating with deep learning pipelines.", "result": "Ludax provides speed benchmarking, RL agent training demonstrations, and is open-source.", "conclusion": "Ludax bridges game description languages and hardware acceleration, offering a tool to advance AI research efficiently."}}
{"id": "2506.22710", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.22710", "abs": "https://arxiv.org/abs/2506.22710", "authors": ["Jiang Yuan", "JI Ma", "Bo Wang", "Guanzhou Ke", "Weiming Hu"], "title": "LightBSR: Towards Lightweight Blind Super-Resolution via Discriminative Implicit Degradation Representation Learning", "comment": null, "summary": "Implicit degradation estimation-based blind super-resolution (IDE-BSR) hinges\non extracting the implicit degradation representation (IDR) of the LR image and\nadapting it to LR image features to guide HR detail restoration. Although\nIDE-BSR has shown potential in dealing with noise interference and complex\ndegradations, existing methods ignore the importance of IDR discriminability\nfor BSR and instead over-complicate the adaptation process to improve effect,\nresulting in a significant increase in the model's parameters and computations.\nIn this paper, we focus on the discriminability optimization of IDR and propose\na new powerful and lightweight BSR model termed LightBSR. Specifically, we\nemploy a knowledge distillation-based learning framework. We first introduce a\nwell-designed degradation-prior-constrained contrastive learning technique\nduring teacher stage to make the model more focused on distinguishing different\ndegradation types. Then we utilize a feature alignment technique to transfer\nthe degradation-related knowledge acquired by the teacher to the student for\npractical inferencing. Extensive experiments demonstrate the effectiveness of\nIDR discriminability-driven BSR model design. The proposed LightBSR can achieve\noutstanding performance with minimal complexity across a range of blind SR\ntasks. Our code is accessible at: https://github.com/MJ-NCEPU/LightBSR.", "AI": {"tldr": "LightBSR improves blind super-resolution by optimizing implicit degradation representation (IDR) discriminability using a knowledge distillation framework, achieving high performance with minimal complexity.", "motivation": "Existing IDE-BSR methods overlook IDR discriminability, complicating adaptation and increasing model size. LightBSR addresses this gap.", "method": "Uses a knowledge distillation framework with degradation-prior-constrained contrastive learning (teacher stage) and feature alignment (student stage).", "result": "LightBSR achieves outstanding performance with minimal complexity in blind SR tasks.", "conclusion": "Optimizing IDR discriminability is effective for BSR, and LightBSR provides a lightweight, high-performance solution."}}
{"id": "2506.22853", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22853", "abs": "https://arxiv.org/abs/2506.22853", "authors": ["Kyochul Jang", "Donghyeon Lee", "Kyusik Kim", "Dongseok Heo", "Taewhoo Lee", "Woojeong Kim", "Bongwon Suh"], "title": "DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues", "comment": "9 pages, ACL 2025 Vienna", "summary": "Existing function-calling benchmarks focus on single-turn interactions.\nHowever, they overlook the complexity of real-world scenarios. To quantify how\nexisting benchmarks address practical applications, we introduce DICE-SCORE, a\nmetric that evaluates the dispersion of tool-related information such as\nfunction name and parameter values throughout the dialogue. Analyzing existing\nbenchmarks through DICE-SCORE reveals notably low scores, highlighting the need\nfor more realistic scenarios. To address this gap, we present DICE-BENCH, a\nframework that constructs practical function-calling datasets by synthesizing\nconversations through a tool graph that maintains dependencies across rounds\nand a multi-agent system with distinct personas to enhance dialogue\nnaturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our\nexperiments on 19 LLMs with DICE-BENCH show that significant advances are still\nrequired before such models can be deployed effectively in real-world settings.\nOur code and data are all publicly available:\nhttps://snuhcc.github.io/DICE-Bench/.", "AI": {"tldr": "The paper introduces DICE-SCORE and DICE-BENCH to address the lack of realistic function-calling benchmarks, revealing gaps in current models for real-world deployment.", "motivation": "Existing benchmarks overlook real-world complexity, focusing only on single-turn interactions.", "method": "DICE-SCORE evaluates tool-related information dispersion; DICE-BENCH synthesizes practical datasets using a tool graph and multi-agent system.", "result": "Analysis shows low DICE-SCOREs in existing benchmarks; DICE-BENCH creates 1,607 high-scoring instances.", "conclusion": "Significant improvements are needed for LLMs to perform effectively in real-world function-calling scenarios."}}
{"id": "2506.22732", "categories": ["cs.LG", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22732", "abs": "https://arxiv.org/abs/2506.22732", "authors": ["Hao Shu", "Jicheng Li", "Tianyv Lei", "Lijun Sun"], "title": "Robust Tensor Completion via Gradient Tensor Nulclear L1-L2 Norm for Traffic Data Recovery", "comment": null, "summary": "In real-world scenarios, spatiotemporal traffic data frequently experiences\ndual degradation from missing values and noise caused by sensor malfunctions\nand communication failures. Therefore, effective data recovery methods are\nessential to ensure the reliability of downstream data-driven applications.\nwhile classical tensor completion methods have been widely adopted, they are\nincapable of modeling noise, making them unsuitable for complex scenarios\ninvolving simultaneous data missingness and noise interference. Existing Robust\nTensor Completion (RTC) approaches offer potential solutions by separately\nmodeling the actual tensor data and noise. However, their effectiveness is\noften constrained by the over-relaxation of convex rank surrogates and the\nsuboptimal utilization of local consistency, leading to inadequate model\naccuracy. To address these limitations, we first introduce the tensor L1-L2\nnorm, a novel non-convex tensor rank surrogate that functions as an effective\nlow-rank representation tool. Leveraging an advanced feature fusion strategy,\nwe further develop the gradient tensor L1-L2 norm by incorporating the tensor\nL1-L2 norm in the gradient domain. By integrating the gradient tensor nuclear\nL1-L2 norm into the RTC framework, we propose the Robust Tensor Completion via\nGradient Tensor Nuclear L1-L2 Norm (RTC-GTNLN) model, which not only fully\nexploits both global low-rankness and local consistency without trade-off\nparameter, but also effectively handles the dual degradation challenges of\nmissing data and noise in traffic data. Extensive experiments conducted on\nmultiple real-world traffic datasets demonstrate that the RTC-GTNLN model\nconsistently outperforms existing state-of-the-art methods in complex recovery\nscenarios involving simultaneous missing values and noise.", "AI": {"tldr": "The paper introduces RTC-GTNLN, a robust tensor completion model using a novel non-convex tensor rank surrogate (L1-L2 norm) to handle missing data and noise in spatiotemporal traffic data, outperforming existing methods.", "motivation": "Spatiotemporal traffic data often suffers from missing values and noise due to sensor and communication issues, necessitating reliable recovery methods for downstream applications.", "method": "Proposes RTC-GTNLN, integrating a gradient tensor L1-L2 norm into the RTC framework to exploit global low-rankness and local consistency without trade-off parameters.", "result": "RTC-GTNLN outperforms state-of-the-art methods in recovering data with simultaneous missing values and noise, as shown in experiments on real-world traffic datasets.", "conclusion": "The RTC-GTNLN model effectively addresses dual degradation challenges in traffic data, offering superior performance in complex recovery scenarios."}}
{"id": "2506.22653", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22653", "abs": "https://arxiv.org/abs/2506.22653", "authors": ["Michael Grosskopf", "Russell Bent", "Rahul Somasundaram", "Isaac Michaud", "Arthur Lui", "Nathan Debardeleben", "Earl Lawrence"], "title": "URSA: The Universal Research and Scientific Agent", "comment": "31 pages, 9 figures", "summary": "Large language models (LLMs) have moved far beyond their initial form as\nsimple chatbots, now carrying out complex reasoning, planning, writing, coding,\nand research tasks. These skills overlap significantly with those that human\nscientists use day-to-day to solve complex problems that drive the cutting edge\nof research. Using LLMs in \"agentic\" AI has the potential to revolutionize\nmodern science and remove bottlenecks to progress. In this work, we present\nURSA, a scientific agent ecosystem for accelerating research tasks. URSA\nconsists of a set of modular agents and tools, including coupling to advanced\nphysics simulation codes, that can be combined to address scientific problems\nof varied complexity and impact. This work highlights the architecture of URSA,\nas well as examples that highlight the potential of the system.", "AI": {"tldr": "URSA is a scientific agent ecosystem using LLMs to accelerate research tasks through modular agents and tools, including physics simulations.", "motivation": "LLMs' advanced capabilities overlap with human scientists' skills, offering potential to revolutionize research by addressing bottlenecks.", "method": "URSA employs modular agents and tools, integrating advanced physics simulation codes, to tackle diverse scientific problems.", "result": "The architecture and examples demonstrate URSA's potential to enhance research efficiency and impact.", "conclusion": "URSA showcases how agentic AI can transform scientific research by leveraging LLMs and modular tools."}}
{"id": "2506.22718", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22718", "abs": "https://arxiv.org/abs/2506.22718", "authors": ["Jun-Jee Chao", "Qingyuan Jiang", "Volkan Isler"], "title": "Part Segmentation and Motion Estimation for Articulated Objects with Dynamic 3D Gaussians", "comment": null, "summary": "Part segmentation and motion estimation are two fundamental problems for\narticulated object motion analysis. In this paper, we present a method to solve\nthese two problems jointly from a sequence of observed point clouds of a single\narticulated object. The main challenge in our problem setting is that the point\nclouds are not assumed to be generated by a fixed set of moving points.\nInstead, each point cloud in the sequence could be an arbitrary sampling of the\nobject surface at that particular time step. Such scenarios occur when the\nobject undergoes major occlusions, or if the dataset is collected using\nmeasurements from multiple sensors asynchronously. In these scenarios, methods\nthat rely on tracking point correspondences are not appropriate. We present an\nalternative approach based on a compact but effective representation where we\nrepresent the object as a collection of simple building blocks modeled as 3D\nGaussians. We parameterize the Gaussians with time-dependent rotations,\ntranslations, and scales that are shared across all time steps. With our\nrepresentation, part segmentation can be achieved by building correspondences\nbetween the observed points and the Gaussians. Moreover, the transformation of\neach point across time can be obtained by following the poses of the assigned\nGaussian (even when the point is not observed). Experiments show that our\nmethod outperforms existing methods that solely rely on finding point\ncorrespondences. Additionally, we extend existing datasets to emulate\nreal-world scenarios by considering viewpoint occlusions. We further\ndemonstrate that our method is more robust to missing points as compared to\nexisting approaches on these challenging datasets, even when some parts are\ncompletely occluded in some time-steps. Notably, our part segmentation\nperformance outperforms the state-of-the-art method by 13% on point clouds with\nocclusions.", "AI": {"tldr": "A method for joint part segmentation and motion estimation from point clouds of articulated objects, handling occlusions and asynchronous data by representing objects as 3D Gaussians with shared parameters across time.", "motivation": "Addressing the challenge of analyzing articulated object motion when point clouds are not from fixed points, due to occlusions or asynchronous sensor data.", "method": "Representing objects as 3D Gaussians with time-dependent transformations (rotations, translations, scales) shared across time steps, enabling part segmentation and motion estimation without point correspondences.", "result": "Outperforms point-correspondence-based methods, especially under occlusions, with a 13% improvement in part segmentation accuracy.", "conclusion": "The proposed Gaussian-based representation is robust to occlusions and missing data, offering superior performance in part segmentation and motion estimation."}}
{"id": "2506.22858", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22858", "abs": "https://arxiv.org/abs/2506.22858", "authors": ["Duygu Altinok"], "title": "Mind the Gap: Entity-Preserved Context-Aware ASR Structured Transcriptions", "comment": "This is the accepted version of an article accepted to the TSD 2025\n  conference, published in Springer Lecture Notes in Artificial Intelligence\n  (LNAI). The final authenticated version is available online at SpringerLink", "summary": "Automatic Speech Recognition (ASR) systems, such as Whisper, achieve high\ntranscription accuracy but struggle with named entities and numerical data,\nespecially when proper formatting is required. These issues increase word error\nrate (WER) and impair semantic understanding in critical domains like legal,\nfinancial, and medical applications. We propose a novel training approach that\nextends the semantic context of ASR models by adding overlapping context\nwindows during training. By sliding 5-second overlaps on both sides of\n30-second chunks, we create a 40-second \"effective semantic window,\" improving\nentity recognition and formatting while focusing predictions on the central 30\nseconds. To address entities spanning chunk boundaries, we reassign such\nentities entirely to the right-hand chunk, ensuring proper formatting.\nAdditionally, enriched training data with embedded entity labels enables the\nmodel to learn both recognition and type-specific formatting. Evaluated on the\nSpoken Wikipedia dataset, our method improves performance across semantic\ntasks, including named entity recognition (NER) and entity formatting. These\nresults highlight the effectiveness of context-aware training in addressing ASR\nlimitations for long-form transcription and complex entity recognition tasks.", "AI": {"tldr": "A novel training approach for ASR systems improves named entity and numerical data recognition by using overlapping context windows and enriched training data, reducing WER and enhancing semantic understanding.", "motivation": "ASR systems like Whisper struggle with named entities and numerical data, impacting accuracy and semantic understanding in critical domains.", "method": "Extends semantic context by adding overlapping 5-second windows to 30-second chunks, creating a 40-second effective window. Reassigns boundary-spanning entities and uses enriched training data with embedded labels.", "result": "Improves performance on semantic tasks like NER and entity formatting, as shown on the Spoken Wikipedia dataset.", "conclusion": "Context-aware training effectively addresses ASR limitations in long-form transcription and complex entity recognition."}}
{"id": "2506.22771", "categories": ["cs.LG", "cs.AI", "cs.NE", "I.2.0; I.2.6"], "pdf": "https://arxiv.org/pdf/2506.22771", "abs": "https://arxiv.org/abs/2506.22771", "authors": ["Jingxiao Ma", "Priyadarshini Panda", "Sherief Reda"], "title": "FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8 Precision", "comment": "To be published in the 62nd Design Automation Conference (DAC), 2025", "summary": "Backpropagation has been the cornerstone of neural network training for\ndecades, yet its inefficiencies in time and energy consumption limit its\nsuitability for resource-constrained edge devices. While low-precision neural\nnetwork quantization has been extensively researched to speed up model\ninference, its application in training has been less explored. Recently, the\nForward-Forward (FF) algorithm has emerged as a promising alternative to\nbackpropagation, replacing the backward pass with an additional forward pass.\nBy avoiding the need to store intermediate activations for backpropagation, FF\ncan reduce memory footprint, making it well-suited for embedded devices. This\npaper presents an INT8 quantized training approach that leverages FF's\nlayer-by-layer strategy to stabilize gradient quantization. Furthermore, we\npropose a novel \"look-ahead\" scheme to address limitations of FF and improve\nmodel accuracy. Experiments conducted on NVIDIA Jetson Orin Nano board\ndemonstrate 4.6% faster training, 8.3% energy savings, and 27.0% reduction in\nmemory usage, while maintaining competitive accuracy compared to the\nstate-of-the-art.", "AI": {"tldr": "The paper introduces an INT8 quantized training method using the Forward-Forward (FF) algorithm, reducing memory usage and energy consumption while maintaining accuracy.", "motivation": "Backpropagation's inefficiencies in time and energy limit its use for edge devices. FF offers a promising alternative by avoiding backward passes.", "method": "Proposes INT8 quantized training with FF, including a 'look-ahead' scheme to improve accuracy.", "result": "Achieves 4.6% faster training, 8.3% energy savings, and 27.0% memory reduction on NVIDIA Jetson Orin Nano.", "conclusion": "The FF-based INT8 quantized training is efficient for edge devices, balancing speed, energy, and accuracy."}}
{"id": "2506.22740", "categories": ["cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22740", "abs": "https://arxiv.org/abs/2506.22740", "authors": ["Jessica Hullman", "Ziyang Guo", "Berk Ustun"], "title": "Explanations are a means to an end", "comment": null, "summary": "Modern methods for explainable machine learning are designed to describe how\nmodels map inputs to outputs--without deep consideration of how these\nexplanations will be used in practice. This paper argues that explanations\nshould be designed and evaluated with a specific end in mind. We describe how\nto formalize this end in a framework based in statistical decision theory. We\nshow how this functionally-grounded approach can be applied across diverse use\ncases, such as clinical decision support, providing recourse, or debugging. We\ndemonstrate its use to characterize the maximum \"boost\" in performance on a\nparticular task that an explanation could provide an idealized decision-maker,\npreventing misuse due to ambiguity by forcing researchers to specify concrete\nuse cases that can be analyzed in light of models of expected explanation use.\nWe argue that evaluation should meld theoretical and empirical perspectives on\nthe value of explanation, and contribute definitions that span these\nperspectives.", "AI": {"tldr": "The paper advocates for designing and evaluating machine learning explanations with specific practical goals in mind, using a statistical decision theory framework.", "motivation": "Current explainable ML methods lack consideration of practical usage, leading to potential misuse or ambiguity.", "method": "Proposes a functionally-grounded approach based on statistical decision theory, applied to diverse use cases like clinical decision support and debugging.", "result": "Demonstrates how to quantify the potential performance boost of explanations for idealized decision-makers, ensuring clarity by specifying concrete use cases.", "conclusion": "Evaluation of explanations should combine theoretical and empirical perspectives, with clear definitions to bridge these views."}}
{"id": "2506.22720", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22720", "abs": "https://arxiv.org/abs/2506.22720", "authors": ["Jinghao Wang", "Zhang Li", "Zi Wang", "Banglei Guan", "Yang Shang", "Qifeng Yu"], "title": "Deterministic Object Pose Confidence Region Estimation", "comment": "Accepted by ICCV 2025", "summary": "6D pose confidence region estimation has emerged as a critical direction,\naiming to perform uncertainty quantification for assessing the reliability of\nestimated poses. However, current sampling-based approach suffers from critical\nlimitations that severely impede their practical deployment: 1) the sampling\nspeed significantly decreases as the number of samples increases. 2) the\nderived confidence regions are often excessively large. To address these\nchallenges, we propose a deterministic and efficient method for estimating pose\nconfidence regions. Our approach uses inductive conformal prediction to\ncalibrate the deterministically regressed Gaussian keypoint distributions into\n2D keypoint confidence regions. We then leverage the implicit function theorem\nto propagate these keypoint confidence regions directly into 6D pose confidence\nregions. This method avoids the inefficiency and inflated region sizes\nassociated with sampling and ensembling. It provides compact confidence regions\nthat cover the ground-truth poses with a user-defined confidence level.\nExperimental results on the LineMOD Occlusion and SPEED datasets show that our\nmethod achieves higher pose estimation accuracy with reduced computational\ntime. For the same coverage rate, our method yields significantly smaller\nconfidence region volumes, reducing them by up to 99.9\\% for rotations and\n99.8\\% for translations. The code will be available soon.", "AI": {"tldr": "A deterministic method for 6D pose confidence region estimation using inductive conformal prediction and the implicit function theorem, addressing inefficiencies and inflated regions in sampling-based approaches.", "motivation": "Current sampling-based methods for 6D pose confidence region estimation are slow and produce excessively large regions, limiting practical use.", "method": "Uses inductive conformal prediction to calibrate Gaussian keypoint distributions into 2D confidence regions, then propagates these into 6D pose regions via the implicit function theorem.", "result": "Achieves higher accuracy and faster computation, reducing confidence region volumes by up to 99.9% for rotations and 99.8% for translations.", "conclusion": "The proposed method provides compact, efficient, and accurate 6D pose confidence regions, outperforming sampling-based approaches."}}
{"id": "2506.22957", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.22957", "abs": "https://arxiv.org/abs/2506.22957", "authors": ["Younwoo Choi", "Changling Li", "Yongjin Yang", "Zhijing Jin"], "title": "Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models", "comment": null, "summary": "As large language models (LLMs) are increasingly integrated into multi-agent\nand human-AI systems, understanding their awareness of both self-context and\nconversational partners is essential for ensuring reliable performance and\nrobust safety. While prior work has extensively studied situational awareness\nwhich refers to an LLM's ability to recognize its operating phase and\nconstraints, it has largely overlooked the complementary capacity to identify\nand adapt to the identity and characteristics of a dialogue partner. In this\npaper, we formalize this latter capability as interlocutor awareness and\npresent the first systematic evaluation of its emergence in contemporary LLMs.\nWe examine interlocutor inference across three dimensions-reasoning patterns,\nlinguistic style, and alignment preferences-and show that LLMs reliably\nidentify same-family peers and certain prominent model families, such as GPT\nand Claude. To demonstrate its practical significance, we develop three case\nstudies in which interlocutor awareness both enhances multi-LLM collaboration\nthrough prompt adaptation and introduces new alignment and safety\nvulnerabilities, including reward-hacking behaviors and increased jailbreak\nsusceptibility. Our findings highlight the dual promise and peril of\nidentity-sensitive behavior in LLMs, underscoring the need for further\nunderstanding of interlocutor awareness and new safeguards in multi-agent\ndeployments. Our code is open-sourced at\nhttps://github.com/younwoochoi/InterlocutorAwarenessLLM.", "AI": {"tldr": "The paper introduces and evaluates 'interlocutor awareness' in LLMs, focusing on their ability to identify and adapt to dialogue partners. It examines three dimensions and demonstrates practical impacts, including enhanced collaboration and new vulnerabilities.", "motivation": "Understanding LLMs' awareness of conversational partners is crucial for reliable performance and safety in multi-agent and human-AI systems, as prior work overlooked this aspect.", "method": "The study formalizes interlocutor awareness and evaluates it across reasoning patterns, linguistic style, and alignment preferences. It includes case studies to demonstrate practical implications.", "result": "LLMs reliably identify peers like GPT and Claude. Interlocutor awareness improves collaboration but also introduces vulnerabilities like reward-hacking and jailbreak susceptibility.", "conclusion": "The findings emphasize the dual potential of identity-sensitive behavior in LLMs, calling for further research and safeguards in multi-agent deployments."}}
{"id": "2506.22780", "categories": ["cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2506.22780", "abs": "https://arxiv.org/abs/2506.22780", "authors": ["Dibyajyoti Chakraborty", "Haiwen Guan", "Jason Stock", "Troy Arcomano", "Guido Cervone", "Romit Maulik"], "title": "Multimodal Atmospheric Super-Resolution With Deep Generative Models", "comment": null, "summary": "Score-based diffusion modeling is a generative machine learning algorithm\nthat can be used to sample from complex distributions. They achieve this by\nlearning a score function, i.e., the gradient of the log-probability density of\nthe data, and reversing a noising process using the same. Once trained,\nscore-based diffusion models not only generate new samples but also enable\nzero-shot conditioning of the generated samples on observed data. This promises\na novel paradigm for data and model fusion, wherein the implicitly learned\ndistributions of pretrained score-based diffusion models can be updated given\nthe availability of online data in a Bayesian formulation. In this article, we\napply such a concept to the super-resolution of a high-dimensional dynamical\nsystem, given the real-time availability of low-resolution and experimentally\nobserved sparse sensor measurements from multimodal data. Additional analysis\non how score-based sampling can be used for uncertainty estimates is also\nprovided. Our experiments are performed for a super-resolution task that\ngenerates the ERA5 atmospheric dataset given sparse observations from a\ncoarse-grained representation of the same and/or from unstructured experimental\nobservations of the IGRA radiosonde dataset. We demonstrate accurate recovery\nof the high dimensional state given multiple sources of low-fidelity\nmeasurements. We also discover that the generative model can balance the\ninfluence of multiple dataset modalities during spatiotemporal reconstructions.", "AI": {"tldr": "Score-based diffusion models enable zero-shot conditioning and Bayesian updates for data fusion, applied here to super-resolution of high-dimensional dynamical systems using sparse, multimodal data.", "motivation": "To leverage score-based diffusion models for updating learned distributions with real-time data, enabling accurate super-resolution of complex systems like atmospheric datasets.", "method": "Uses score-based diffusion modeling to learn and reverse a noising process, applying it to super-resolution tasks with sparse, multimodal observations (e.g., ERA5 and IGRA datasets).", "result": "Accurate recovery of high-dimensional states from low-fidelity measurements and balanced influence of multiple data modalities during reconstructions.", "conclusion": "Score-based diffusion models offer a powerful tool for data fusion and uncertainty-aware super-resolution in high-dimensional dynamical systems."}}
{"id": "2506.22774", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.22774", "abs": "https://arxiv.org/abs/2506.22774", "authors": ["Michael Papademas", "Xenia Ziouvelou", "Antonis Troumpoukis", "Vangelis Karkaletsis"], "title": "Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems", "comment": null, "summary": "Artificial Intelligence (AI) technology epitomizes the complex challenges\nposed by human-made artifacts, particularly those widely integrated into\nsociety and exert significant influence, highlighting potential benefits and\ntheir negative consequences. While other technologies may also pose substantial\nrisks, AI's pervasive reach makes its societal effects especially profound. The\ncomplexity of AI systems, coupled with their remarkable capabilities, can lead\nto a reliance on technologies that operate beyond direct human oversight or\nunderstanding. To mitigate the risks that arise, several theoretical tools and\nguidelines have been developed, alongside efforts to create technological tools\naimed at safeguarding Trustworthy AI. The guidelines take a more holistic view\nof the issue but fail to provide techniques for quantifying trustworthiness.\nConversely, while technological tools are better at achieving such\nquantification, they lack a holistic perspective, focusing instead on specific\naspects of Trustworthy AI. This paper aims to introduce an assessment method\nthat combines the ethical components of Trustworthy AI with the algorithmic\nprocesses of PageRank and TrustRank. The goal is to establish an assessment\nframework that minimizes the subjectivity inherent in the self-assessment\ntechniques prevalent in the field by introducing algorithmic criteria. The\napplication of our approach indicates that a holistic assessment of an AI\nsystem's trustworthiness can be achieved by providing quantitative insights\nwhile considering the theoretical content of relevant guidelines.", "AI": {"tldr": "The paper introduces a method combining ethical components of Trustworthy AI with algorithmic processes (PageRank and TrustRank) to create a holistic, quantitative assessment framework for AI trustworthiness.", "motivation": "AI's pervasive societal impact and lack of direct human oversight necessitate tools to assess its trustworthiness, balancing ethical guidelines and technical quantification.", "method": "Develops an assessment framework integrating ethical components of Trustworthy AI with algorithmic processes (PageRank and TrustRank) to minimize subjectivity.", "result": "The approach provides quantitative insights into AI trustworthiness while aligning with theoretical guidelines, achieving a holistic assessment.", "conclusion": "The proposed method successfully bridges the gap between ethical guidelines and technical quantification, offering a more objective and comprehensive assessment of AI trustworthiness."}}
{"id": "2506.22726", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22726", "abs": "https://arxiv.org/abs/2506.22726", "authors": ["Yu Zhang", "Xi Zhang", "Hualin zhou", "Xinyuan Chen", "Shang Gao", "Hong Jia", "Jianfei Yang", "Yuankai Qi", "Tao Gu"], "title": "XTransfer: Cross-Modality Model Transfer for Human Sensing with Few Data at the Edge", "comment": null, "summary": "Deep learning for human sensing on edge systems offers significant\nopportunities for smart applications. However, its training and development are\nhindered by the limited availability of sensor data and resource constraints of\nedge systems. Current methods that rely on transferring pre-trained models\noften encounter issues such as modality shift and high resource demands,\nresulting in substantial accuracy loss, resource overhead, and poor\nadaptability across different sensing applications. In this paper, we propose\nXTransfer, a first-of-its-kind method for resource-efficient, modality-agnostic\nmodel transfer. XTransfer freely leverages single or multiple pre-trained\nmodels and transfers knowledge across different modalities by (i) model\nrepairing that safely repairs modality shift in pre-trained model layers with\nonly few sensor data, and (ii) layer recombining that efficiently searches and\nrecombines layers of interest from source models in a layer-wise manner to\ncreate compact models. We benchmark various baselines across diverse human\nsensing datasets spanning different modalities. Comprehensive results\ndemonstrate that XTransfer achieves state-of-the-art performance on human\nsensing tasks while significantly reducing the costs of sensor data collection,\nmodel training, and edge deployment.", "AI": {"tldr": "XTransfer is a novel method for efficient, modality-agnostic model transfer in edge-based human sensing, addressing issues like modality shift and resource constraints.", "motivation": "Limited sensor data and edge system resources hinder deep learning for human sensing, while current transfer methods face accuracy loss and high resource demands.", "method": "XTransfer uses model repairing to fix modality shifts and layer recombining to create compact models from pre-trained layers.", "result": "XTransfer outperforms baselines, reducing costs in data collection, training, and deployment while maintaining high accuracy.", "conclusion": "XTransfer is a resource-efficient, adaptable solution for human sensing on edge systems."}}
{"id": "2506.22977", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22977", "abs": "https://arxiv.org/abs/2506.22977", "authors": ["Asen Dotsinski", "Udit Thakur", "Marko Ivanov", "Mohammad Hafeez Khan", "Maria Heuss"], "title": "On the Generalizability of \"Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals\"", "comment": "22 pages, 25 figures. For an interactive dashboard with all figures,\n  see https://comp-mech-generalizability.streamlit.app/ . For the accompanying\n  code, see https://github.com/asendotsinski/comp-mech-generalizability . To be\n  published in proceedings of the 2025 Machine Learning Reproducibility\n  Challenge", "summary": "We present a reproduction study of \"Competition of Mechanisms: Tracing How\nLanguage Models Handle Facts and Counterfactuals\" (Ortu et al., 2024), which\ninvestigates competition of mechanisms in language models between factual\nrecall and counterfactual in-context repetition. Our study successfully\nreproduces their primary findings regarding the localization of factual and\ncounterfactual information, the dominance of attention blocks in mechanism\ncompetition, and the specialization of attention heads in handling competing\ninformation. We reproduce their results on both GPT-2 (Radford et al., 2019)\nand Pythia 6.9B (Biderman et al., 2023). We extend their work in three\nsignificant directions. First, we explore the generalizability of these\nfindings to even larger models by replicating the experiments on Llama 3.1 8B\n(Grattafiori et al., 2024), discovering greatly reduced attention head\nspecialization. Second, we investigate the impact of prompt structure by\nintroducing variations where we avoid repeating the counterfactual statement\nverbatim or we change the premise word, observing a marked decrease in the\nlogit for the counterfactual token. Finally, we test the validity of the\nauthors' claims for prompts of specific domains, discovering that certain\ncategories of prompts skew the results by providing the factual prediction\ntoken as part of the subject of the sentence. Overall, we find that the\nattention head ablation proposed in Ortu et al. (2024) is ineffective for\ndomains that are underrepresented in their dataset, and that the effectiveness\nvaries based on model architecture, prompt structure, domain and task.", "AI": {"tldr": "A reproduction study of Ortu et al. (2024) confirms their findings on factual and counterfactual handling in language models, extends the work to larger models, explores prompt structure impact, and tests domain-specific validity.", "motivation": "To validate and extend Ortu et al.'s findings on how language models handle facts and counterfactuals, focusing on model generalizability, prompt structure, and domain-specific effects.", "method": "Reproduced experiments on GPT-2 and Pythia 6.9B, extended to Llama 3.1 8B, tested prompt variations, and evaluated domain-specific prompts.", "result": "Confirmed primary findings, found reduced attention head specialization in larger models, observed prompt structure impacts, and identified domain-specific skews.", "conclusion": "Attention head ablation's effectiveness varies by model, prompt, domain, and task, with limitations in underrepresented domains."}}
{"id": "2506.22802", "categories": ["cs.LG", "cs.CR", "cs.CV", "I.2.6"], "pdf": "https://arxiv.org/pdf/2506.22802", "abs": "https://arxiv.org/abs/2506.22802", "authors": ["Hae Jin Song", "Laurent Itti"], "title": "Riemannian-Geometric Fingerprints of Generative Models", "comment": null, "summary": "Recent breakthroughs and rapid integration of generative models (GMs) have\nsparked interest in the problem of model attribution and their fingerprints.\nFor instance, service providers need reliable methods of authenticating their\nmodels to protect their IP, while users and law enforcement seek to verify the\nsource of generated content for accountability and trust. In addition, a\ngrowing threat of model collapse is arising, as more model-generated data are\nbeing fed back into sources (e.g., YouTube) that are often harvested for\ntraining (\"regurgitative training\"), heightening the need to differentiate\nsynthetic from human data. Yet, a gap still exists in understanding generative\nmodels' fingerprints, we believe, stemming from the lack of a formal framework\nthat can define, represent, and analyze the fingerprints in a principled way.\nTo address this gap, we take a geometric approach and propose a new definition\nof artifact and fingerprint of GMs using Riemannian geometry, which allows us\nto leverage the rich theory of differential geometry. Our new definition\ngeneralizes previous work (Song et al., 2024) to non-Euclidean manifolds by\nlearning Riemannian metrics from data and replacing the Euclidean distances and\nnearest-neighbor search with geodesic distances and kNN-based Riemannian center\nof mass. We apply our theory to a new gradient-based algorithm for computing\nthe fingerprints in practice. Results show that it is more effective in\ndistinguishing a large array of GMs, spanning across 4 different datasets in 2\ndifferent resolutions (64 by 64, 256 by 256), 27 model architectures, and 2\nmodalities (Vision, Vision-Language). Using our proposed definition\nsignificantly improves the performance on model attribution, as well as a\ngeneralization to unseen datasets, model types, and modalities, suggesting its\npractical efficacy.", "AI": {"tldr": "The paper proposes a geometric framework using Riemannian geometry to define and analyze fingerprints of generative models, improving model attribution and distinguishing synthetic from human data.", "motivation": "The need to authenticate generative models for IP protection and accountability, and to address the threat of model collapse due to regurgitative training.", "method": "A geometric approach defining artifacts and fingerprints using Riemannian geometry, replacing Euclidean distances with geodesic distances and kNN-based Riemannian center of mass.", "result": "The method effectively distinguishes various generative models across datasets, resolutions, architectures, and modalities, improving attribution and generalization.", "conclusion": "The proposed framework offers a principled and practical solution for understanding and leveraging generative models' fingerprints."}}
{"id": "2506.22865", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22865", "abs": "https://arxiv.org/abs/2506.22865", "authors": ["Ziqi Zhong", "Xunzhu Tang"], "title": "ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source Language Models", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have revealed a\nsignificant performance gap between closed-source and open-source models,\nparticularly in tasks requiring complex reasoning and precise instruction\nfollowing. This paper introduces ReasonBridge, a methodology that efficiently\ntransfers reasoning capabilities from powerful closed-source to open-source\nmodels through a novel hierarchical knowledge distillation framework. We\ndevelop a tailored dataset Reason1K with only 1,000 carefully curated reasoning\ntraces emphasizing difficulty, diversity, and quality. These traces are\nfiltered from across multiple domains using a structured multi-criteria\nselection algorithm. Our transfer learning approach incorporates: (1) a\nhierarchical distillation process capturing both strategic abstraction and\ntactical implementation patterns, (2) a sparse reasoning-focused adapter\narchitecture requiring only 0.3% additional trainable parameters, and (3) a\ntest-time compute scaling mechanism using guided inference interventions.\nComprehensive evaluations demonstrate that ReasonBridge improves reasoning\ncapabilities in open-source models by up to 23% on benchmark tasks,\nsignificantly narrowing the gap with closed-source models. Notably, the\nenhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its\nperformance on competition-level AIME problems. Our methodology generalizes\neffectively across diverse reasoning domains and model architectures,\nestablishing a sample-efficient approach to reasoning enhancement for\ninstruction following.", "AI": {"tldr": "ReasonBridge is a method to transfer reasoning skills from closed-source to open-source LLMs using hierarchical distillation, a small curated dataset, and sparse adapters, achieving up to 23% improvement.", "motivation": "Address the performance gap between closed-source and open-source LLMs in complex reasoning tasks.", "method": "Hierarchical knowledge distillation, a 1K curated dataset (Reason1K), sparse adapters, and guided inference interventions.", "result": "Open-source models improved by up to 23%, with Qwen2.5-14B outperforming Claude-Sonnet3.5 on some tasks.", "conclusion": "ReasonBridge efficiently enhances reasoning in open-source models, narrowing the gap with closed-source ones."}}
{"id": "2506.22736", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22736", "abs": "https://arxiv.org/abs/2506.22736", "authors": ["Dayong Su", "Yafei Zhang", "Huafeng Li", "Jinxing Li", "Yu Liu"], "title": "UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image Fusion Under Diverse Degradations and Misalignments", "comment": "Accepted by ICCV2025", "summary": "Current multimodal medical image fusion typically assumes that source images\nare of high quality and perfectly aligned at the pixel level. Its effectiveness\nheavily relies on these conditions and often deteriorates when handling\nmisaligned or degraded medical images. To address this, we propose UniFuse, a\ngeneral fusion framework. By embedding a degradation-aware prompt learning\nmodule, UniFuse seamlessly integrates multi-directional information from input\nimages and correlates cross-modal alignment with restoration, enabling joint\noptimization of both tasks within a unified framework. Additionally, we design\nan Omni Unified Feature Representation scheme, which leverages Spatial Mamba to\nencode multi-directional features and mitigate modality differences in feature\nalignment. To enable simultaneous restoration and fusion within an All-in-One\nconfiguration, we propose a Universal Feature Restoration & Fusion module,\nincorporating the Adaptive LoRA Synergistic Network (ALSN) based on LoRA\nprinciples. By leveraging ALSN's adaptive feature representation along with\ndegradation-type guidance, we enable joint restoration and fusion within a\nsingle-stage framework. Compared to staged approaches, UniFuse unifies\nalignment, restoration, and fusion within a single framework. Experimental\nresults across multiple datasets demonstrate the method's effectiveness and\nsignificant advantages over existing approaches.", "AI": {"tldr": "UniFuse is a unified framework for multimodal medical image fusion that addresses misalignment and degradation by integrating alignment, restoration, and fusion into a single-stage process.", "motivation": "Current fusion methods rely on high-quality, aligned images, but their performance drops with misaligned or degraded inputs. UniFuse aims to overcome these limitations.", "method": "UniFuse incorporates degradation-aware prompt learning, Omni Unified Feature Representation, and a Universal Feature Restoration & Fusion module with Adaptive LoRA Synergistic Network (ALSN).", "result": "Experiments show UniFuse outperforms existing methods in handling misaligned and degraded medical images.", "conclusion": "UniFuse successfully unifies alignment, restoration, and fusion, offering a robust solution for multimodal medical image fusion."}}
{"id": "2506.22978", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22978", "abs": "https://arxiv.org/abs/2506.22978", "authors": ["Yida Zhao", "Hao Xve", "Xiang Hu", "Kewei Tu"], "title": "A Systematic Study of Compositional Syntactic Transformer Language Models", "comment": null, "summary": "Syntactic language models (SLMs) enhance Transformers by incorporating\nsyntactic biases through the modeling of linearized syntactic parse trees\nalongside surface sentences. This paper focuses on compositional SLMs that are\nbased on constituency parse trees and contain explicit bottom-up composition of\nconstituent representations. We identify key aspects of design choices in\nexisting compositional SLMs and propose a unified framework encompassing both\nexisting models and novel variants. We conduct a comprehensive empirical\nevaluation of all the variants in our framework across language modeling,\nsyntactic generalization, summarization, dialogue, and inference efficiency.\nBased on the experimental results, we make multiple recommendations on the\ndesign of compositional SLMs. Our code is released at\nhttps://github.com/zhaoyd1/compositional_SLMs.", "AI": {"tldr": "The paper introduces a unified framework for compositional syntactic language models (SLMs) based on constituency parse trees, evaluates design choices, and provides recommendations.", "motivation": "To enhance Transformers by incorporating syntactic biases and improving performance in tasks like language modeling and syntactic generalization.", "method": "Proposes a unified framework for compositional SLMs, evaluates existing and novel variants across multiple tasks.", "result": "Comprehensive empirical evaluation leads to design recommendations for compositional SLMs.", "conclusion": "The framework and recommendations improve the design and effectiveness of compositional SLMs."}}
{"id": "2506.22809", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22809", "abs": "https://arxiv.org/abs/2506.22809", "authors": ["Cooper Doyle"], "title": "BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters", "comment": "13 pages, 3 figures, 1 table", "summary": "We propose BayesLoRA, a task-specific uncertainty quantification framework\nthat integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike\ngeneral-purpose transformer uncertainty methods, BayesLoRA provides guardrails\ntailored to downstream workflows, enabling agents to introspect and modulate\nbehavior under uncertainty. We demonstrate mathematically and empirically that\nLoRA adapters exhibit amplified variance outside fine-tuning distributions,\nyielding reliable confidence estimates for agentic decision-making.", "AI": {"tldr": "BayesLoRA integrates MC-Dropout into LoRA for task-specific uncertainty quantification, improving decision-making under uncertainty.", "motivation": "General-purpose transformer uncertainty methods lack task-specific guardrails, limiting their utility in downstream workflows.", "method": "BayesLoRA combines MC-Dropout with LoRA adapters to quantify uncertainty, focusing on fine-tuning distributions.", "result": "LoRA adapters show amplified variance outside fine-tuning distributions, providing reliable confidence estimates.", "conclusion": "BayesLoRA offers a tailored solution for uncertainty-aware agentic decision-making."}}
{"id": "2506.22893", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.22893", "abs": "https://arxiv.org/abs/2506.22893", "authors": ["Arpit Narechania", "Alex Endert", "Atanu R Sinha"], "title": "Agentic Enterprise: AI-Centric User to User-Centric AI", "comment": "12 pages, 1 figure, 2 sidebars; Preprint", "summary": "After a very long winter, the Artificial Intelligence (AI) spring is here.\nOr, so it seems over the last three years. AI has the potential to impact many\nareas of human life - personal, social, health, education, professional. In\nthis paper, we take a closer look at the potential of AI for Enterprises, where\ndecision-making plays a crucial and repeated role across functions, tasks, and\noperations. We consider Agents imbued with AI as means to increase\ndecision-productivity of enterprises. We highlight six tenets for Agentic\nsuccess in enterprises, by drawing attention to what the current, AI-Centric\nUser paradigm misses, in the face of persistent needs of and usefulness for\nEnterprise Decision-Making. In underscoring a shift to User-Centric AI, we\noffer six tenets and promote market mechanisms for platforms, aligning the\ndesign of AI and its delivery by Agents to the cause of enterprise users.", "AI": {"tldr": "The paper discusses AI's potential in enterprises, focusing on decision-making and introducing six tenets for successful AI agents, advocating a shift to user-centric AI.", "motivation": "AI's growing impact on various human domains, especially enterprises where decision-making is critical, motivates the exploration of AI agents to enhance decision productivity.", "method": "The paper proposes six tenets for AI agent success in enterprises, contrasting the current AI-centric user paradigm with a user-centric AI approach.", "result": "The study highlights the need for aligning AI design and delivery with enterprise user needs, promoting market mechanisms for platforms.", "conclusion": "A shift to user-centric AI and adherence to six tenets can enhance AI's effectiveness in enterprise decision-making."}}
{"id": "2506.22749", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22749", "abs": "https://arxiv.org/abs/2506.22749", "authors": ["Yun Zhang", "Feifan Chen", "Na Li", "Zhiwei Guo", "Xu Wang", "Fen Miao", "Sam Kwong"], "title": "Deep Learning based Joint Geometry and Attribute Up-sampling for Large-Scale Colored Point Clouds", "comment": null, "summary": "Colored point cloud, which includes geometry and attribute components, is a\nmainstream representation enabling realistic and immersive 3D applications. To\ngenerate large-scale and denser colored point clouds, we propose a deep\nlearning-based Joint Geometry and Attribute Up-sampling (JGAU) method that\nlearns to model both geometry and attribute patterns while leveraging spatial\nattribute correlations. First, we establish and release a large-scale dataset\nfor colored point cloud up-sampling called SYSU-PCUD, containing 121\nlarge-scale colored point clouds with diverse geometry and attribute\ncomplexities across six categories and four sampling rates. Second, to improve\nthe quality of up-sampled point clouds, we propose a deep learning-based JGAU\nframework that jointly up-samples geometry and attributes. It consists of a\ngeometry up-sampling network and an attribute up-sampling network, where the\nlatter leverages the up-sampled auxiliary geometry to model neighborhood\ncorrelations of the attributes. Third, we propose two coarse attribute\nup-sampling methods, Geometric Distance Weighted Attribute Interpolation\n(GDWAI) and Deep Learning-based Attribute Interpolation (DLAI), to generate\ncoarse up-sampled attributes for each point. Then, an attribute enhancement\nmodule is introduced to refine these up-sampled attributes and produce\nhigh-quality point clouds by further exploiting intrinsic attribute and\ngeometry patterns. Extensive experiments show that the Peak Signal-to-Noise\nRatio (PSNR) achieved by the proposed JGAU method is 33.90 decibels, 32.10\ndecibels, 31.10 decibels, and 30.39 decibels for up-sampling rates of 4 times,\n8 times, 12 times, and 16 times, respectively. Compared to state-of-the-art\nmethods, JGAU achieves average PSNR gains of 2.32 decibels, 2.47 decibels, 2.28\ndecibels, and 2.11 decibels at these four up-sampling rates, demonstrating\nsignificant improvement.", "AI": {"tldr": "The paper proposes a deep learning-based Joint Geometry and Attribute Up-sampling (JGAU) method for colored point clouds, achieving superior quality and performance over state-of-the-art methods.", "motivation": "To generate large-scale, high-quality colored point clouds by leveraging spatial attribute correlations and improving up-sampling techniques.", "method": "A JGAU framework with geometry and attribute up-sampling networks, using coarse attribute up-sampling methods (GDWAI and DLAI) and an attribute enhancement module.", "result": "JGAU achieves PSNR gains of 2.11 to 2.47 decibels over state-of-the-art methods across four up-sampling rates.", "conclusion": "The proposed JGAU method significantly improves the quality of colored point clouds, outperforming existing techniques."}}
{"id": "2506.23046", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23046", "abs": "https://arxiv.org/abs/2506.23046", "authors": ["Xianzhe Fan", "Xuhui Zhou", "Chuanyang Jin", "Kolby Nottingham", "Hao Zhu", "Maarten Sap"], "title": "SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions", "comment": "23 pages, 6 figures", "summary": "Humans continuously infer the states, goals, and behaviors of others by\nperceiving their surroundings in dynamic, real-world social interactions.\nHowever, most Theory of Mind (ToM) benchmarks only evaluate static, text-based\nscenarios, which have a significant gap compared to real interactions. We\npropose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in\nembodied multi-agent complex social interactions. This benchmark is based on\nrich multimodal interaction data generated by the interaction environment SoMi,\ncovering diverse crafting goals and social relationships. Our framework\nsupports multi-level evaluation: (1) first-person evaluation provides\nmultimodal (visual, dialogue, action, etc.) input from a first-person\nperspective during a task for real-time state inference, (2) third-person\nevaluation provides complete third-person perspective video and text records\nafter a task for goal and behavior inference. This evaluation method allows for\na more comprehensive examination of a model's ToM capabilities from both the\nsubjective immediate experience and the objective global observation. We\nconstructed a challenging dataset containing 35 third-person perspective\nvideos, 363 first-person perspective images, and 1225 expert-annotated\nmultiple-choice questions (three options). On this dataset, we systematically\nevaluated the performance of human subjects and several state-of-the-art large\nvision-language models (LVLMs). The results show that LVLMs perform\nsignificantly worse than humans on SoMi-ToM: the average accuracy gap between\nhumans and models is 40.1% in first-person evaluation and 26.4% in third-person\nevaluation. This indicates that future LVLMs need to further improve their ToM\ncapabilities in embodied, complex social interactions.", "AI": {"tldr": "The SoMi-ToM benchmark evaluates Theory of Mind (ToM) in embodied multi-agent social interactions, revealing a significant performance gap between humans and large vision-language models (LVLMs).", "motivation": "Existing ToM benchmarks focus on static, text-based scenarios, lacking realism compared to dynamic, real-world social interactions.", "method": "The SoMi-ToM benchmark uses multimodal data (visual, dialogue, action) from the SoMi environment, with first-person and third-person evaluation methods.", "result": "LVLMs perform significantly worse than humans, with accuracy gaps of 40.1% (first-person) and 26.4% (third-person).", "conclusion": "Future LVLMs need enhanced ToM capabilities for complex, embodied social interactions."}}
{"id": "2506.22821", "categories": ["cs.LG", "68T07", "I.2.6"], "pdf": "https://arxiv.org/pdf/2506.22821", "abs": "https://arxiv.org/abs/2506.22821", "authors": ["Thomas Gaskin", "Guy J. Abel"], "title": "Deep learning 40 years of human migration", "comment": null, "summary": "We present a novel and detailed dataset on origin-destination annual\nmigration flows and stocks between 230 countries and regions, spanning the\nperiod from 1990 to the present. Our flow estimates are further disaggregated\nby country of birth, providing a comprehensive picture of migration over the\nlast 43 years. The estimates are obtained by training a deep recurrent neural\nnetwork to learn flow patterns from 18 covariates for all countries, including\ngeographic, economic, cultural, societal, and political information. The\nrecurrent architecture of the neural network means that the entire past can\ninfluence current migration patterns, allowing us to learn long-range temporal\ncorrelations. By training an ensemble of neural networks and additionally\npushing uncertainty on the covariates through the trained network, we obtain\nconfidence bounds for all our estimates, allowing researchers to pinpoint the\ngeographic regions most in need of additional data collection. We validate our\napproach on various test sets of unseen data, demonstrating that it\nsignificantly outperforms traditional methods estimating five-year flows while\ndelivering a significant increase in temporal resolution. The model is fully\nopen source: all training data, neural network weights, and training code are\nmade public alongside the migration estimates, providing a valuable resource\nfor future studies of human migration.", "AI": {"tldr": "A novel dataset on global migration flows (1990-present) uses deep learning to estimate patterns, outperforming traditional methods with higher resolution and uncertainty bounds.", "motivation": "To provide a comprehensive, high-resolution dataset on global migration flows and stocks, addressing gaps in existing data and methods.", "method": "A deep recurrent neural network trained on 18 covariates (geographic, economic, cultural, etc.) with ensemble learning for uncertainty bounds.", "result": "Outperforms traditional methods in estimating five-year flows, offering higher temporal resolution and confidence bounds.", "conclusion": "The open-source model and dataset serve as a valuable resource for future migration studies, highlighting regions needing more data."}}
{"id": "2506.22919", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22919", "abs": "https://arxiv.org/abs/2506.22919", "authors": ["Sanskar Pandey", "Ruhaan Chopra", "Saad Murtaza Bhat", "Ark Abhyudaya"], "title": "Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning", "comment": null, "summary": "Mixture-of-Experts (MoE) models enable conditional computation by routing\ninputs to specialized experts, but these experts rely on identical inductive\nbiases, thus limiting representational diversity. This static computation\npathway is inefficient for inputs that require different types of reasoning and\nlimits specialization and interpretability. We propose Hecto, a lightweight MoE\narchitecture that leverages architectural heterogeneity by combining a GRU\nexpert for temporal reasoning and an FFNN expert for static abstraction under a\nsparse Top-1 gating mechanism. Evaluated on three reasoning benchmarks (AG\nNews, SST-2, HotpotQA) and a regression task (STS-B), Hecto matches or closely\ntrails homogeneous baselines in performance despite receiving isolated input\nrepresentations, while achieving clear expert specialization, with each expert\naligning to distinct reasoning types (temporal vs static). At larger batch\nsizes, Hecto exhibits improved performance, benefiting from relaxed\ncomputational constraints that allow its heterogeneous architecture to optimize\nmore effectively. Ablation results isolate architectural diversity as the\nsource of Hecto's stability and interpretability across diverse reasoning\ntasks. Overall, Hecto establishes itself as a new benchmark for conditional\ncomputation, offering a principled framework for specialized reasoning in\nlow-resource regimes with its model strength derived from principled\nspecialization.", "AI": {"tldr": "Hecto, a lightweight MoE architecture, combines GRU and FFNN experts under sparse Top-1 gating, achieving specialization and interpretability while matching homogeneous baselines in performance.", "motivation": "Current MoE models limit representational diversity due to identical inductive biases, leading to inefficient computation and restricted specialization.", "method": "Hecto integrates a GRU expert for temporal reasoning and an FFNN expert for static abstraction, using a sparse Top-1 gating mechanism.", "result": "Hecto matches or nears homogeneous baselines on reasoning benchmarks (AG News, SST-2, HotpotQA) and regression (STS-B), with clear expert specialization and improved performance at larger batch sizes.", "conclusion": "Hecto sets a new benchmark for conditional computation, offering specialized reasoning in low-resource settings through architectural diversity."}}
{"id": "2506.22753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22753", "abs": "https://arxiv.org/abs/2506.22753", "authors": ["Jianing Zhang", "Jiayi Zhu", "Feiyu Ji", "Xiaokang Yang", "Xiaoyun Yuan"], "title": "Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography", "comment": null, "summary": "Metalenses offer significant potential for ultra-compact computational\nimaging but face challenges from complex optical degradation and computational\nrestoration difficulties. Existing methods typically rely on precise optical\ncalibration or massive paired datasets, which are non-trivial for real-world\nimaging systems. Furthermore, a lack of control over the inference process\noften results in undesirable hallucinated artifacts. We introduce\nDegradation-Modeled Multipath Diffusion for tunable metalens photography,\nleveraging powerful natural image priors from pretrained models instead of\nlarge datasets. Our framework uses positive, neutral, and negative-prompt paths\nto balance high-frequency detail generation, structural fidelity, and\nsuppression of metalens-specific degradation, alongside \\textit{pseudo} data\naugmentation. A tunable decoder enables controlled trade-offs between fidelity\nand perceptual quality. Additionally, a spatially varying degradation-aware\nattention (SVDA) module adaptively models complex optical and sensor-induced\ndegradation. Finally, we design and build a millimeter-scale MetaCamera for\nreal-world validation. Extensive results show that our approach outperforms\nstate-of-the-art methods, achieving high-fidelity and sharp image\nreconstruction. More materials: https://dmdiff.github.io/.", "AI": {"tldr": "A novel framework, Degradation-Modeled Multipath Diffusion, is introduced for tunable metalens photography, leveraging pretrained models and pseudo data augmentation to overcome challenges in computational imaging.", "motivation": "Metalenses face issues like optical degradation and computational restoration difficulties, with existing methods requiring precise calibration or large datasets, leading to artifacts.", "method": "The framework uses positive, neutral, and negative-prompt paths for detail generation, structural fidelity, and degradation suppression, alongside a tunable decoder and SVDA module for adaptive degradation modeling.", "result": "The approach outperforms state-of-the-art methods, achieving high-fidelity and sharp image reconstruction, validated with a MetaCamera.", "conclusion": "The proposed method effectively addresses metalens imaging challenges, offering controlled trade-offs between fidelity and perceptual quality."}}
{"id": "2506.23051", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23051", "abs": "https://arxiv.org/abs/2506.23051", "authors": ["Jo\u00e3o Lucas Luz Lima Sarcinelli", "Marina Lages Gon\u00e7alves Teixeira", "Jade Bortot de Paiva", "Diego Furtado Silva"], "title": "MariNER: A Dataset for Historical Brazilian Portuguese Named Entity Recognition", "comment": null, "summary": "Named Entity Recognition (NER) is a fundamental Natural Language Processing\n(NLP) task that aims to identify and classify entity mentions in texts across\ndifferent categories. While languages such as English possess a large number of\nhigh-quality resources for this task, Brazilian Portuguese still lacks in\nquantity of gold-standard NER datasets, especially when considering specific\ndomains. Particularly, this paper considers the importance of NER for analyzing\nhistorical texts in the context of digital humanities. To address this gap,\nthis work outlines the construction of MariNER: \\textit{Mapeamento e\nAnota\\c{c}\\~oes de Registros hIst\\'oricos para NER} (Mapping and Annotation of\nHistorical Records for NER), the first gold-standard dataset for early\n20th-century Brazilian Portuguese, with more than 9,000 manually annotated\nsentences. We also assess and compare the performance of state-of-the-art NER\nmodels for the dataset.", "AI": {"tldr": "The paper introduces MariNER, the first gold-standard NER dataset for early 20th-century Brazilian Portuguese, addressing the lack of resources for historical texts. It also evaluates state-of-the-art NER models on this dataset.", "motivation": "Brazilian Portuguese lacks high-quality NER datasets, especially for historical texts, which are crucial for digital humanities.", "method": "The paper constructs MariNER, a manually annotated dataset with over 9,000 sentences, and evaluates existing NER models on it.", "result": "MariNER is created as a gold-standard dataset, and the performance of NER models is assessed for this historical context.", "conclusion": "The work fills a gap in NER resources for Brazilian Portuguese and provides insights into model performance for historical texts."}}
{"id": "2506.22837", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22837", "abs": "https://arxiv.org/abs/2506.22837", "authors": ["Kamil Faber", "Marcin Pietro\u0144", "Dominik \u017burek", "Roberto Corizzo"], "title": "xLSTMAD: A Powerful xLSTM-based Method for Anomaly Detection", "comment": null, "summary": "The recently proposed xLSTM is a powerful model that leverages expressive\nmultiplicative gating and residual connections, providing the temporal capacity\nneeded for long-horizon forecasting and representation learning. This\narchitecture has demonstrated success in time series forecasting, lossless\ncompression, and even large-scale language modeling tasks, where its linear\nmemory footprint and fast inference make it a viable alternative to\nTransformers. Despite its growing popularity, no prior work has explored xLSTM\nfor anomaly detection. In this work, we fill this gap by proposing xLSTMAD, the\nfirst anomaly detection method that integrates a full encoder-decoder xLSTM\narchitecture, purpose-built for multivariate time series data. Our encoder\nprocesses input sequences to capture historical context, while the decoder is\ndevised in two separate variants of the method. In the forecasting approach,\nthe decoder iteratively generates forecasted future values xLSTMAD-F, while the\nreconstruction approach reconstructs the input time series from its encoded\ncounterpart xLSTMAD-R. We investigate the performance of two loss functions:\nMean Squared Error (MSE), and Soft Dynamic Time Warping (SoftDTW) to consider\nlocal reconstruction fidelity and global sequence alignment, respectively. We\nevaluate our method on the comprehensive TSB-AD-M benchmark, which spans 17\nreal-world datasets, using state-of-the-art challenging metrics such as VUS-PR.\nIn our results, xLSTM showcases state-of-the-art accuracy, outperforming 23\npopular anomaly detection baselines. Our paper is the first work revealing the\npowerful modeling capabilities of xLSTM for anomaly detection, paving the way\nfor exciting new developments on this subject. Our code is available at:\nhttps://github.com/Nyderx/xlstmad", "AI": {"tldr": "xLSTMAD is the first anomaly detection method using xLSTM, excelling in accuracy on multivariate time series data.", "motivation": "No prior work explored xLSTM for anomaly detection, prompting the development of xLSTMAD.", "method": "Uses encoder-decoder xLSTM with forecasting (xLSTMAD-F) and reconstruction (xLSTMAD-R) variants, tested with MSE and SoftDTW loss functions.", "result": "Outperforms 23 baselines on TSB-AD-M benchmark, achieving state-of-the-art accuracy.", "conclusion": "xLSTMAD demonstrates xLSTM's potential for anomaly detection, opening new research avenues."}}
{"id": "2506.22920", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22920", "abs": "https://arxiv.org/abs/2506.22920", "authors": ["Pinzheng Wang", "Juntao Li", "Zecheng Tang", "Haijia Gui", "Min zhang"], "title": "Improving Rationality in the Reasoning Process of Language Models through Self-playing Game", "comment": "Accepted by ICML 2025", "summary": "Large language models (LLMs) have demonstrated considerable reasoning\nabilities in various tasks such as mathematics and coding. However, recent\nstudies indicate that even the best models lack true comprehension of their\nreasoning processes. In this paper, we explore how self-play can enhance the\nrationality of models in the reasoning process without supervision from humans\nor superior models. We design a Critic-Discernment Game(CDG) in which a prover\nfirst provides a solution to a given problem and is subsequently challenged by\ncritiques of its solution. These critiques either aim to assist or mislead the\nprover. The objective of the prover is to maintain the correct answer when\nfaced with misleading comments, while correcting errors in response to\nconstructive feedback. Our experiments on tasks involving mathematical\nreasoning, stepwise error detection, self-correction, and long-chain reasoning\ndemonstrate that CDG training can significantly improve the ability of\nwell-aligned LLMs to comprehend their reasoning process.", "AI": {"tldr": "Self-play via Critic-Discernment Game (CDG) improves LLMs' reasoning comprehension without supervision.", "motivation": "LLMs lack true understanding of their reasoning processes despite strong performance in tasks like math and coding.", "method": "Introduces CDG, where a prover defends solutions against critiques (helpful or misleading) to refine reasoning.", "result": "CDG training enhances LLMs' ability to comprehend reasoning in math, error detection, self-correction, and long-chain tasks.", "conclusion": "Self-play with CDG effectively boosts LLMs' reasoning comprehension autonomously."}}
{"id": "2506.22756", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22756", "abs": "https://arxiv.org/abs/2506.22756", "authors": ["Tao Tang", "Likui Zhang", "Youpeng Wen", "Kaidong Zhang", "Jia-Wang Bian", "xia zhou", "Tianyi Yan", "Kun Zhan", "Peng Jia", "Hefeng Wu", "Liang Lin", "Xiaodan Liang"], "title": "RoboPearls: Editable Video Simulation for Robot Manipulation", "comment": "ICCV 2025", "summary": "The development of generalist robot manipulation policies has seen\nsignificant progress, driven by large-scale demonstration data across diverse\nenvironments. However, the high cost and inefficiency of collecting real-world\ndemonstrations hinder the scalability of data acquisition. While existing\nsimulation platforms enable controlled environments for robotic learning, the\nchallenge of bridging the sim-to-real gap remains. To address these challenges,\nwe propose RoboPearls, an editable video simulation framework for robotic\nmanipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the\nconstruction of photo-realistic, view-consistent simulations from demonstration\nvideos, and supports a wide range of simulation operators, including various\nobject manipulations, powered by advanced modules like Incremental Semantic\nDistillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by\nincorporating large language models (LLMs), RoboPearls automates the simulation\nproduction process in a user-friendly manner through flexible command\ninterpretation and execution. Furthermore, RoboPearls employs a vision-language\nmodel (VLM) to analyze robotic learning issues to close the simulation loop for\nperformance enhancement. To demonstrate the effectiveness of RoboPearls, we\nconduct extensive experiments on multiple datasets and scenes, including\nRLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which\ndemonstrate our satisfactory simulation performance.", "AI": {"tldr": "RoboPearls is a video simulation framework for robotic manipulation, using 3D Gaussian Splatting and LLMs to automate simulation production and enhance performance.", "motivation": "High cost and inefficiency of real-world demonstration data collection hinder scalability, and existing simulation platforms struggle with the sim-to-real gap.", "method": "RoboPearls uses 3DGS for photo-realistic simulations, ISD and 3D-NNFM for object manipulations, and LLMs/VLMs for automation and performance analysis.", "result": "Extensive experiments on datasets like RLBench and real-world robots show satisfactory simulation performance.", "conclusion": "RoboPearls effectively addresses scalability and sim-to-real challenges in robotic manipulation."}}
{"id": "2506.23056", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23056", "abs": "https://arxiv.org/abs/2506.23056", "authors": ["Xiang Zhuang", "Bin Wu", "Jiyu Cui", "Kehua Feng", "Xiaotong Li", "Huabin Xing", "Keyan Ding", "Qiang Zhang", "Huajun Chen"], "title": "Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning", "comment": "ACL 2025 Main", "summary": "Molecular structure elucidation involves deducing a molecule's structure from\nvarious types of spectral data, which is crucial in chemical experimental\nanalysis. While large language models (LLMs) have shown remarkable proficiency\nin analyzing and reasoning through complex tasks, they still encounter\nsubstantial challenges in molecular structure elucidation. We identify that\nthese challenges largely stem from LLMs' limited grasp of specialized chemical\nknowledge. In this work, we introduce a Knowledge-enhanced reasoning framework\nfor Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search\nfor test-time scaling as a plugin. Specifically, we construct an external\nmolecular substructure knowledge base to extend the LLMs' coverage of the\nchemical structure space. Furthermore, we design a specialized\nmolecule-spectrum scorer to act as a reward model for the reasoning process,\naddressing the issue of inaccurate solution evaluation in LLMs. Experimental\nresults show that our approach significantly boosts performance, particularly\ngaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is\navailable at https://github.com/HICAI-ZJU/K-MSE.", "AI": {"tldr": "The paper introduces K-MSE, a knowledge-enhanced framework using Monte Carlo Tree Search and a molecular substructure knowledge base to improve LLMs' performance in molecular structure elucidation.", "motivation": "LLMs struggle with molecular structure elucidation due to limited specialized chemical knowledge.", "method": "K-MSE integrates an external molecular substructure knowledge base and a molecule-spectrum scorer for accurate reasoning and evaluation.", "result": "The framework achieves over 20% improvement on GPT-4o-mini and GPT-4o.", "conclusion": "K-MSE effectively enhances LLMs' capability in molecular structure elucidation by addressing knowledge gaps and evaluation issues."}}
{"id": "2506.22845", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2506.22845", "abs": "https://arxiv.org/abs/2506.22845", "authors": ["Batuhan Hangun", "Oguz Altun", "Onder Eyecioglu"], "title": "Quantum Neural Networks for Wind Energy Forecasting: A Comparative Study of Performance and Scalability with Classical Models", "comment": null, "summary": "Quantum Neural Networks (QNNs), a prominent approach in Quantum Machine\nLearning (QML), are emerging as a powerful alternative to classical machine\nlearning methods. Recent studies have focused on the applicability of QNNs to\nvarious tasks, such as time-series forecasting, prediction, and classification,\nacross a wide range of applications, including cybersecurity and medical\nimaging. With the increased use of smart grids driven by the integration of\nrenewable energy systems, machine learning plays an important role in\npredicting power demand and detecting system disturbances. This study provides\nan in-depth investigation of QNNs for predicting the power output of a wind\nturbine. We assess the predictive performance and simulation time of six QNN\nconfigurations that are based on the Z Feature Map for data encoding and\nvarying ansatz structures. Through detailed cross-validation experiments and\ntests on an unseen hold-out dataset, we experimentally demonstrate that QNNs\ncan achieve predictive performance that is competitive with, and in some cases\nmarginally better than, the benchmarked classical approaches. Our results also\nreveal the effects of dataset size and circuit complexity on predictive\nperformance and simulation time. We believe our findings will offer valuable\ninsights for researchers in the energy domain who wish to incorporate quantum\nmachine learning into their work.", "AI": {"tldr": "QNNs are explored for wind turbine power output prediction, showing competitive or slightly better performance than classical methods, with insights on dataset size and circuit complexity.", "motivation": "To investigate QNNs as an alternative to classical ML for predicting wind turbine power output, given the growing role of ML in smart grids and renewable energy systems.", "method": "Six QNN configurations using Z Feature Map for data encoding and varying ansatz structures were tested via cross-validation and hold-out dataset experiments.", "result": "QNNs achieved competitive or marginally better predictive performance than classical benchmarks, with dataset size and circuit complexity impacting results.", "conclusion": "QNNs show promise for energy applications, offering insights for integrating quantum ML in the field."}}
{"id": "2506.22992", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22992", "abs": "https://arxiv.org/abs/2506.22992", "authors": ["Yulun Jiang", "Yekun Chai", "Maria Brbi\u0107", "Michael Moor"], "title": "MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning", "comment": null, "summary": "The ability to process information from multiple modalities and to reason\nthrough it step-by-step remains a critical challenge in advancing artificial\nintelligence. However, existing reasoning benchmarks focus on text-only\nreasoning, or employ multimodal questions that can be answered by directly\nretrieving information from a non-text modality. Thus, complex reasoning\nremains poorly understood in multimodal domains. Here, we present MARBLE, a\nchallenging multimodal reasoning benchmark that is designed to scrutinize\nmultimodal language models (MLLMs) in their ability to carefully reason\nstep-by-step through complex multimodal problems and environments. MARBLE is\ncomposed of two highly challenging tasks, M-Portal and M-Cube, that require the\ncrafting and understanding of multistep plans under spatial, visual, and\nphysical constraints. We find that current MLLMs perform poorly on MARBLE --\nall the 12 advanced models obtain near-random performance on M-Portal and 0%\naccuracy on M-Cube. Only in simplified subtasks some models outperform the\nrandom baseline, indicating that complex reasoning is still a challenge for\nexisting MLLMs. Moreover, we show that perception remains a bottleneck, where\nMLLMs occasionally fail to extract information from the visual inputs. By\nshedding a light on the limitations of MLLMs, we hope that MARBLE will spur the\ndevelopment of the next generation of models with the ability to reason and\nplan across many, multimodal reasoning steps.", "AI": {"tldr": "MARBLE is a multimodal reasoning benchmark designed to test MLLMs' ability to handle complex, step-by-step reasoning across modalities, revealing significant limitations in current models.", "motivation": "Existing benchmarks lack focus on complex multimodal reasoning, limiting understanding of MLLMs' capabilities in such tasks.", "method": "MARBLE introduces two tasks, M-Portal and M-Cube, requiring multistep planning under spatial, visual, and physical constraints.", "result": "Current MLLMs perform poorly, with near-random accuracy on M-Portal and 0% on M-Cube, highlighting reasoning and perception challenges.", "conclusion": "MARBLE exposes MLLMs' limitations, aiming to inspire advancements in multimodal reasoning and planning."}}
{"id": "2506.22762", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22762", "abs": "https://arxiv.org/abs/2506.22762", "authors": ["Dinh Phu Tran", "Dao Duy Hung", "Daeyoung Kim"], "title": "VSRM: A Robust Mamba-Based Framework for Video Super-Resolution", "comment": "Accepted by ICCV 2025", "summary": "Video super-resolution remains a major challenge in low-level vision tasks.\nTo date, CNN- and Transformer-based methods have delivered impressive results.\nHowever, CNNs are limited by local receptive fields, while Transformers\nstruggle with quadratic complexity, posing challenges for processing long\nsequences in VSR. Recently, Mamba has drawn attention for its long-sequence\nmodeling, linear complexity, and large receptive fields. In this work, we\npropose VSRM, a novel \\textbf{V}ideo \\textbf{S}uper-\\textbf{R}esolution\nframework that leverages the power of \\textbf{M}amba. VSRM introduces\nSpatial-to-Temporal Mamba and Temporal-to-Spatial Mamba blocks to extract\nlong-range spatio-temporal features and enhance receptive fields efficiently.\nTo better align adjacent frames, we propose Deformable Cross-Mamba Alignment\nmodule. This module utilizes a deformable cross-mamba mechanism to make the\ncompensation stage more dynamic and flexible, preventing feature distortions.\nFinally, we minimize the frequency domain gaps between reconstructed and\nground-truth frames by proposing a simple yet effective Frequency\nCharbonnier-like loss that better preserves high-frequency content and enhances\nvisual quality. Through extensive experiments, VSRM achieves state-of-the-art\nresults on diverse benchmarks, establishing itself as a solid foundation for\nfuture research.", "AI": {"tldr": "VSRM, a novel video super-resolution framework using Mamba, introduces Spatial-to-Temporal and Temporal-to-Spatial Mamba blocks for efficient long-range spatio-temporal feature extraction. It includes a Deformable Cross-Mamba Alignment module and a Frequency Charbonnier-like loss, achieving state-of-the-art results.", "motivation": "Video super-resolution (VSR) faces challenges with CNNs' limited receptive fields and Transformers' quadratic complexity. Mamba's linear complexity and long-sequence modeling capabilities offer a promising alternative.", "method": "VSRM employs Spatial-to-Temporal and Temporal-to-Spatial Mamba blocks for feature extraction, a Deformable Cross-Mamba Alignment module for dynamic frame alignment, and a Frequency Charbonnier-like loss for frequency domain optimization.", "result": "VSRM achieves state-of-the-art performance on diverse benchmarks, demonstrating superior efficiency and visual quality.", "conclusion": "VSRM sets a new standard for video super-resolution, leveraging Mamba's strengths for efficient, high-quality results, and provides a foundation for future research."}}
{"id": "2506.23071", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23071", "abs": "https://arxiv.org/abs/2506.23071", "authors": ["Zhengren Wang", "Bozhou Li", "Dongwen Yao", "Wentao Zhang"], "title": "Text2VectorSQL: Bridging Text-to-SQL and Vector Search for Unified Natural Language Queries", "comment": "Work in progess", "summary": "While Text-to-SQL enables natural language interaction with structured\ndatabases, its effectiveness diminishes with unstructured data or ambiguous\nqueries due to rigid syntax and limited expressiveness. Concurrently, vector\nsearch has emerged as a powerful paradigm for semantic retrieval, particularly\nfor unstructured data. However, existing VectorSQL implementations still rely\nheavily on manual crafting and lack tailored evaluation frameworks, leaving a\nsignificant gap between theoretical potential and practical deployment. To\nbridge these complementary paradigms, we introduces Text2VectorSQL, a novel\nframework unifying Text-to-SQL and vector search to overcome expressiveness\nconstraints and support more diverse and holistical natural language queries.\nSpecifically, Text2VectorSQL enables semantic filtering, multi-modal matching,\nand retrieval acceleration. For evaluation, we build vector index on\nappropriate columns, extend user queries with semantic search, and annotate\nground truths via an automatic pipeline with expert review. Furthermore, we\ndevelop dedicated Text2VectorSQL models with synthetic data, demonstrating\nsignificant performance improvements over baseline methods. Our work\nestablishes the foundation for the Text2VectorSQL task, paving the way for more\nversatile and intuitive database interfaces. The repository will be publicly\navailable at https://github.com/Open-DataFlow/Text2VectorSQL.", "AI": {"tldr": "Text2VectorSQL unifies Text-to-SQL and vector search to enhance natural language query handling for structured and unstructured data, outperforming baselines with semantic filtering and multi-modal matching.", "motivation": "Existing Text-to-SQL and VectorSQL methods struggle with unstructured data and ambiguous queries, lacking expressiveness and tailored evaluation frameworks.", "method": "Text2VectorSQL integrates semantic filtering, multi-modal matching, and retrieval acceleration, using synthetic data for model training and an automatic pipeline for evaluation.", "result": "The framework shows significant performance improvements over baseline methods, enabling more versatile database interactions.", "conclusion": "Text2VectorSQL bridges the gap between Text-to-SQL and vector search, laying the groundwork for intuitive database interfaces."}}
{"id": "2506.22848", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22848", "abs": "https://arxiv.org/abs/2506.22848", "authors": ["Shengcai Liu", "Hui Ou-yang", "Zhiyuan Wang", "Cheng Chen", "Qijun Cai", "Yew-Soon Ong", "Ke Tang"], "title": "Scalable Structure Learning of Bayesian Networks by Learning Algorithm Ensembles", "comment": null, "summary": "Learning the structure of Bayesian networks (BNs) from data is challenging,\nespecially for datasets involving a large number of variables. The recently\nproposed divide-and-conquer (D\\&D) strategies present a promising approach for\nlearning large BNs. However, they still face a main issue of unstable learning\naccuracy across subproblems. In this work, we introduce the idea of employing\nstructure learning ensemble (SLE), which combines multiple BN structure\nlearning algorithms, to consistently achieve high learning accuracy. We further\npropose an automatic approach called Auto-SLE for learning near-optimal SLEs,\naddressing the challenge of manually designing high-quality SLEs. The learned\nSLE is then integrated into a D\\&D method. Extensive experiments firmly show\nthe superiority of our method over D\\&D methods with single BN structure\nlearning algorithm in learning large BNs, achieving accuracy improvement\nusually by 30\\%$\\sim$225\\% on datasets involving 10,000 variables. Furthermore,\nour method generalizes well to datasets with many more (e.g., 30000) variables\nand different network characteristics than those present in the training data\nfor learning the SLE. These results indicate the significant potential of\nemploying (automatic learning of) SLEs for scalable BN structure learning.", "AI": {"tldr": "The paper introduces a structure learning ensemble (SLE) method to improve the accuracy of Bayesian network (BN) structure learning, especially for large datasets. It proposes Auto-SLE for automatic SLE design and integrates it into a divide-and-conquer approach, showing significant accuracy improvements.", "motivation": "The motivation is to address the instability in learning accuracy of divide-and-conquer methods for large BN structure learning.", "method": "The method involves using SLE to combine multiple BN structure learning algorithms and proposing Auto-SLE for automatic SLE design, integrated into a divide-and-conquer framework.", "result": "Experiments show accuracy improvements of 30% to 225% on datasets with 10,000 variables, with good generalization to larger datasets (e.g., 30,000 variables).", "conclusion": "The paper concludes that SLE and Auto-SLE significantly enhance scalable BN structure learning, demonstrating their potential for large datasets."}}
{"id": "2506.23049", "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS", "68T42, 68T50,", "I.2.7; I.2.11; H.5.5"], "pdf": "https://arxiv.org/pdf/2506.23049", "abs": "https://arxiv.org/abs/2506.23049", "authors": ["Leander Melroy Maben", "Gayathri Ganesh Lakshmy", "Srijith Radhakrishnan", "Siddhant Arora", "Shinji Watanabe"], "title": "AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks", "comment": null, "summary": "Despite advances in language and speech technologies, no open-source system\nenables full speech-to-speech, multi-turn dialogue with integrated tool use and\nagentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and\nAutomated Tool Use), the first open-source, speech-native assistant capable of\ncompleting complex, goal-driven tasks through dynamic tool invocation and\nmulti-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a\ncascaded pipeline and supports tools such as calendar booking, contact lookup,\nweb search, and email. Its modular design allows easy integration of new tools\nusing natural language prompts and action classes. On VoiceBench, AURA scores\n92.75% on OpenBookQA-outperforming all open-weight systems and nearing\nGPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.\nHuman evaluation shows 90% task success on complex, multi-turn speech tasks.", "AI": {"tldr": "AURA is the first open-source, speech-native assistant for multi-turn dialogue with tool use, outperforming other open-weight systems in benchmarks.", "motivation": "To address the lack of open-source systems for speech-to-speech, multi-turn dialogue with integrated tool use and agentic reasoning.", "method": "Combines open-weight ASR, TTS, and LLMs in a cascaded pipeline, supporting tools like calendar booking and web search via natural language prompts.", "result": "Scores 92.75% on VoiceBench (OpenBookQA) and 4.39 on AlpacaEval, with 90% task success in human evaluations.", "conclusion": "AURA demonstrates strong performance in complex, multi-turn speech tasks, setting a benchmark for open-source assistants."}}
{"id": "2506.22783", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22783", "abs": "https://arxiv.org/abs/2506.22783", "authors": ["Oguzhan Baser", "Ahmet Ege Tanriverdi", "Sriram Vishwanath", "Sandeep P. Chinchali"], "title": "PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection", "comment": "5 pages, 3 figures, Published at Proceedings of Interspeech 2025, for\n  the dataset see https://huggingface.co/datasets/phonemefake/PhonemeFakeV2,\n  for the code see https://github.com/UTAustin-SwarmLab/ PhonemeFake", "summary": "Deepfake (DF) attacks pose a growing threat as generative models become\nincreasingly advanced. However, our study reveals that existing DF datasets\nfail to deceive human perception, unlike real DF attacks that influence public\ndiscourse. It highlights the need for more realistic DF attack vectors. We\nintroduce PhonemeFake (PF), a DF attack that manipulates critical speech\nsegments using language reasoning, significantly reducing human perception by\nup to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF\ndataset on HuggingFace and open-source bilevel DF segment detection model that\nadaptively prioritizes compute on manipulated regions. Our extensive\nexperiments across three known DF datasets reveal that our detection model\nreduces EER by 91% while achieving up to 90% speed-up, with minimal compute\noverhead and precise localization beyond existing models as a scalable\nsolution.", "AI": {"tldr": "The paper introduces PhonemeFake (PF), a deepfake attack method that outperforms existing datasets in deceiving humans and benchmarks, and proposes a detection model for scalable defense.", "motivation": "Existing deepfake datasets fail to mimic real-world attack effectiveness, highlighting the need for more realistic attack vectors.", "method": "PhonemeFake manipulates critical speech segments using language reasoning, and a bilevel detection model is introduced to prioritize compute on manipulated regions.", "result": "PF reduces human perception by 42% and benchmark accuracies by 94%. The detection model cuts EER by 91% and speeds up by 90% with minimal overhead.", "conclusion": "PF and the detection model offer a scalable solution for realistic deepfake threats, outperforming existing methods."}}
{"id": "2506.23101", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23101", "abs": "https://arxiv.org/abs/2506.23101", "authors": ["Yue Xu", "Wenjie Wang"], "title": "From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship", "comment": null, "summary": "Multimodal large language models (MLLMs) have shown impressive capabilities\nacross tasks involving both visual and textual modalities. However, growing\nconcerns remain about their potential to encode and amplify gender bias,\nparticularly in socially sensitive applications. Existing benchmarks\npredominantly evaluate bias in isolated scenarios, overlooking how bias may\nemerge subtly through interpersonal interactions. We fill this gap by going\nbeyond single-entity evaluation and instead focusing on a deeper examination of\nrelational and contextual gender bias in dual-individual interactions. We\nintroduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs\nthrough the lens of social relationships in generated narratives. Genres\nassesses gender bias through a dual-character profile and narrative generation\ntask that captures rich interpersonal dynamics and supports a fine-grained bias\nevaluation suite across multiple dimensions. Experiments on both open- and\nclosed-source MLLMs reveal persistent, context-sensitive gender biases that are\nnot evident in single-character settings. Our findings underscore the\nimportance of relationship-aware benchmarks for diagnosing subtle,\ninteraction-driven gender bias in MLLMs and provide actionable insights for\nfuture bias mitigation.", "AI": {"tldr": "The paper introduces Genres, a benchmark for evaluating gender bias in multimodal large language models (MLLMs) through relational and contextual interactions, revealing persistent biases not evident in single-character settings.", "motivation": "Addressing concerns about gender bias in MLLMs, particularly in socially sensitive applications, by moving beyond isolated evaluations to study relational and contextual bias in interactions.", "method": "Developed Genres, a benchmark using dual-character profiles and narrative generation tasks to assess gender bias across multiple dimensions in MLLMs.", "result": "Experiments showed context-sensitive gender biases in MLLMs, highlighting the need for relationship-aware benchmarks.", "conclusion": "The study emphasizes the importance of relational bias evaluation and offers insights for future bias mitigation in MLLMs."}}
{"id": "2506.22871", "categories": ["cs.LG", "cs.MM", "I.2.6"], "pdf": "https://arxiv.org/pdf/2506.22871", "abs": "https://arxiv.org/abs/2506.22871", "authors": ["Homayun Afrabandpey", "Hamed Rezazadegan Tavakoli"], "title": "P$^2$U: Progressive Precision Update For Efficient Model Distribution", "comment": null, "summary": "Efficient model distribution is becoming increasingly critical in\nbandwidth-constrained environments. In this paper, we propose a simple yet\neffective approach called Progressive Precision Update (P$^2$U) to address this\nproblem. Instead of transmitting the original high-precision model, P$^2$U\ntransmits a lower-bit precision model, coupled with a model update representing\nthe difference between the original high-precision model and the transmitted\nlow precision version. With extensive experiments on various model\narchitectures, ranging from small models ($1 - 6$ million parameters) to a\nlarge model (more than $100$ million parameters) and using three different data\nsets, e.g., chest X-Ray, PASCAL-VOC, and CIFAR-100, we demonstrate that P$^2$U\nconsistently achieves better tradeoff between accuracy, bandwidth usage and\nlatency. Moreover, we show that when bandwidth or startup time is the priority,\naggressive quantization (e.g., 4-bit) can be used without severely compromising\nperformance. These results establish P$^2$U as an effective and practical\nsolution for scalable and efficient model distribution in low-resource\nsettings, including federated learning, edge computing, and IoT deployments.\nGiven that P$^2$U complements existing compression techniques and can be\nimplemented alongside any compression method, e.g., sparsification,\nquantization, pruning, etc., the potential for improvement is even greater.", "AI": {"tldr": "Progressive Precision Update (P\u00b2U) reduces bandwidth usage by transmitting low-bit precision models and updates, achieving a balance between accuracy, bandwidth, and latency.", "motivation": "Addressing the challenge of efficient model distribution in bandwidth-constrained environments like federated learning and edge computing.", "method": "Transmits a low-bit precision model and an update representing the difference to the high-precision model, tested on various architectures and datasets.", "result": "P\u00b2U consistently improves tradeoffs between accuracy, bandwidth, and latency, even with aggressive quantization (e.g., 4-bit).", "conclusion": "P\u00b2U is a practical solution for scalable model distribution in low-resource settings, compatible with existing compression techniques."}}
{"id": "2506.23080", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23080", "abs": "https://arxiv.org/abs/2506.23080", "authors": ["Xinmin Fang", "Lingfeng Tao", "Zhengxiong Li"], "title": "AI's Euclid's Elements Moment: From Language Models to Computable Thought", "comment": null, "summary": "This paper presents a comprehensive five-stage evolutionary framework for\nunderstanding the development of artificial intelligence, arguing that its\ntrajectory mirrors the historical progression of human cognitive technologies.\nWe posit that AI is advancing through distinct epochs, each defined by a\nrevolutionary shift in its capacity for representation and reasoning, analogous\nto the inventions of cuneiform, the alphabet, grammar and logic, mathematical\ncalculus, and formal logical systems. This \"Geometry of Cognition\" framework\nmoves beyond mere metaphor to provide a systematic, cross-disciplinary model\nthat not only explains AI's past architectural shifts-from expert systems to\nTransformers-but also charts a concrete and prescriptive path forward.\nCrucially, we demonstrate that this evolution is not merely linear but\nreflexive: as AI advances through these stages, the tools and insights it\ndevelops create a feedback loop that fundamentally reshapes its own underlying\narchitecture. We are currently transitioning into a \"Metalinguistic Moment,\"\ncharacterized by the emergence of self-reflective capabilities like\nChain-of-Thought prompting and Constitutional AI. The subsequent stages, the\n\"Mathematical Symbolism Moment\" and the \"Formal Logic System Moment,\" will be\ndefined by the development of a computable calculus of thought, likely through\nneuro-symbolic architectures and program synthesis, culminating in provably\naligned and reliable AI that reconstructs its own foundational representations.\nThis work serves as the methodological capstone to our trilogy, which\npreviously explored the economic drivers (\"why\") and cognitive nature (\"what\")\nof AI. Here, we address the \"how,\" providing a theoretical foundation for\nfuture research and offering concrete, actionable strategies for startups and\ndevelopers aiming to build the next generation of intelligent systems.", "AI": {"tldr": "The paper proposes a five-stage evolutionary framework for AI development, likening it to human cognitive advancements, and predicts future stages of AI growth.", "motivation": "To systematically explain AI's historical progression and predict its future evolution, offering actionable insights for developers.", "method": "Introduces the 'Geometry of Cognition' framework, analyzing AI's stages from expert systems to Transformers, and forecasts future reflexive advancements.", "result": "Identifies a 'Metalinguistic Moment' and predicts subsequent stages leading to provably aligned AI.", "conclusion": "Provides a theoretical foundation for future AI research and practical strategies for next-gen intelligent systems."}}
{"id": "2506.22784", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22784", "abs": "https://arxiv.org/abs/2506.22784", "authors": ["Yu Han", "Zhiwei Huang", "Yanting Zhang", "Fangjun Ding", "Shen Cai", "Rui Fan"], "title": "Single-Frame Point-Pixel Registration via Supervised Cross-Modal Feature Matching", "comment": null, "summary": "Point-pixel registration between LiDAR point clouds and camera images is a\nfundamental yet challenging task in autonomous driving and robotic perception.\nA key difficulty lies in the modality gap between unstructured point clouds and\nstructured images, especially under sparse single-frame LiDAR settings.\nExisting methods typically extract features separately from point clouds and\nimages, then rely on hand-crafted or learned matching strategies. This separate\nencoding fails to bridge the modality gap effectively, and more critically,\nthese methods struggle with the sparsity and noise of single-frame LiDAR, often\nrequiring point cloud accumulation or additional priors to improve reliability.\nInspired by recent progress in detector-free matching paradigms (e.g.\nMatchAnything), we revisit the projection-based approach and introduce the\ndetector-free framework for direct point-pixel matching between LiDAR and\ncamera views. Specifically, we project the LiDAR intensity map into a 2D view\nfrom the LiDAR perspective and feed it into an attention-based detector-free\nmatching network, enabling cross-modal correspondence estimation without\nrelying on multi-frame accumulation. To further enhance matching reliability,\nwe introduce a repeatability scoring mechanism that acts as a soft visibility\nprior. This guides the network to suppress unreliable matches in regions with\nlow intensity variation, improving robustness under sparse input. Extensive\nexperiments on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks demonstrate that\nour method achieves state-of-the-art performance, outperforming prior\napproaches on nuScenes (even those relying on accumulated point clouds),\ndespite using only single-frame LiDAR.", "AI": {"tldr": "A detector-free framework for direct point-pixel matching between LiDAR and camera views, addressing modality gaps and sparsity in single-frame LiDAR, outperforming prior methods.", "motivation": "The modality gap between unstructured LiDAR point clouds and structured images, especially under sparse single-frame LiDAR settings, challenges point-pixel registration. Existing methods struggle with sparsity and noise, often requiring multi-frame accumulation.", "method": "Projects LiDAR intensity maps into 2D views and uses an attention-based detector-free matching network for cross-modal correspondence. Introduces a repeatability scoring mechanism to enhance reliability.", "result": "Achieves state-of-the-art performance on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks, outperforming prior methods even with single-frame LiDAR.", "conclusion": "The proposed framework effectively bridges the modality gap and improves robustness under sparse input, setting a new benchmark for point-pixel registration."}}
{"id": "2506.23111", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23111", "abs": "https://arxiv.org/abs/2506.23111", "authors": ["Janki Atul Nawale", "Mohammed Safi Ur Rahman Khan", "Janani D", "Mansi Gupta", "Danish Pruthi", "Mitesh M. Khapra"], "title": "FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes", "comment": "Accepted in ACL 2025", "summary": "Existing studies on fairness are largely Western-focused, making them\ninadequate for culturally diverse countries such as India. To address this gap,\nwe introduce INDIC-BIAS, a comprehensive India-centric benchmark designed to\nevaluate fairness of LLMs across 85 identity groups encompassing diverse\ncastes, religions, regions, and tribes. We first consult domain experts to\ncurate over 1,800 socio-cultural topics spanning behaviors and situations,\nwhere biases and stereotypes are likely to emerge. Grounded in these topics, we\ngenerate and manually validate 20,000 real-world scenario templates to probe\nLLMs for fairness. We structure these templates into three evaluation tasks:\nplausibility, judgment, and generation. Our evaluation of 14 popular LLMs on\nthese tasks reveals strong negative biases against marginalized identities,\nwith models frequently reinforcing common stereotypes. Additionally, we find\nthat models struggle to mitigate bias even when explicitly asked to rationalize\ntheir decision. Our evaluation provides evidence of both allocative and\nrepresentational harms that current LLMs could cause towards Indian identities,\ncalling for a more cautious usage in practical applications. We release\nINDIC-BIAS as an open-source benchmark to advance research on benchmarking and\nmitigating biases and stereotypes in the Indian context.", "AI": {"tldr": "INDIC-BIAS is an India-centric benchmark to evaluate fairness in LLMs across 85 diverse identity groups, revealing strong biases against marginalized identities.", "motivation": "Existing fairness studies are Western-focused, lacking applicability to culturally diverse regions like India.", "method": "Curated 1,800 socio-cultural topics, generated 20,000 scenario templates, and structured evaluations into plausibility, judgment, and generation tasks.", "result": "LLMs exhibit strong negative biases against marginalized groups and struggle to mitigate bias even when prompted.", "conclusion": "The study highlights allocative and representational harms of LLMs in India, advocating cautious usage and releasing INDIC-BIAS for further research."}}
{"id": "2506.22895", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22895", "abs": "https://arxiv.org/abs/2506.22895", "authors": ["Xinyu Chen", "Vassilis Digalakis Jr", "Lijun Ding", "Dingyi Zhuang", "Jinhua Zhao"], "title": "Interpretable Time Series Autoregression for Periodicity Quantification", "comment": null, "summary": "Time series autoregression is a classical statistical model for capturing\nauto-correlations and identifying temporal patterns such as periodicity and\nseasonality. In this work, we propose a novel sparse autoregression framework\nfrom an interpretable machine learning perspective and the model\ninterpretability for periodicity quantification is reinforced by $\\ell_0$-norm\ninduced sparsity constraints. On the time-varying time series data, we\nreformulate the sparse autoregression and convert the involved optimization\nproblem into a mixed-integer optimization (MIO). To accelerate it, we develop a\nsubspace pursuit based decision variable pruning (DVP) strategy to reduce the\nsearch space. On the multidimensional time series that involves complicated\nspatial and temporal dimensions, we propose a spatially- and time-varying\nsparse autoregression model and resolve the corresponding MIO problem by\ndeveloping a two-stage optimization scheme. In particular, the proposed scheme\nmakes the model scalable to large problems even with millions of decision\nvariables. Empirically, we conduct extensive experiments to evaluate the\nproposed models on real-world time series data. First, we demonstrate that the\nMIO solver can be drastically accelerated through the DVP strategy, while\nmaintaining the same solution quality as a full MIO solver. Applying the\ntime-varying sparse autoregression model to ridesharing trip data, we uncover\nboth daily and weekly periodicities and reveal long-term changes in regularity\nof human mobility. Second, we demonstrate the spatial patterns of yearly\nseasonality in climate variable time series such as temperature and\nprecipitation across the past four decades, and our model allows to discover\ndynamic climate patterns and identify climate phenomena such as El Nino in sea\nsurface temperature.", "AI": {"tldr": "A novel sparse autoregression framework is proposed for interpretable time series analysis, using \u21130-norm sparsity and mixed-integer optimization, with applications in human mobility and climate patterns.", "motivation": "To enhance interpretability in time series autoregression models and quantify periodicity effectively, especially for time-varying and multidimensional data.", "method": "Proposes a sparse autoregression framework with \u21130-norm constraints, converts the problem into mixed-integer optimization (MIO), and introduces a subspace pursuit strategy (DVP) for acceleration. For multidimensional data, a two-stage optimization scheme is developed.", "result": "The DVP strategy accelerates MIO without compromising solution quality. Applications reveal periodicities in ridesharing data and dynamic climate patterns, including El Nino.", "conclusion": "The framework is scalable and interpretable, successfully uncovering temporal and spatial patterns in real-world time series data."}}
{"id": "2506.23107", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23107", "abs": "https://arxiv.org/abs/2506.23107", "authors": ["Bing Song", "Jianing Liu", "Sisi Jian", "Chenyang Wu", "Vinayak Dixit"], "title": "Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study", "comment": "20 pages, 1 figure", "summary": "Large language models (LLMs) have made significant strides, extending their\napplications to dialogue systems, automated content creation, and\ndomain-specific advisory tasks. However, as their use grows, concerns have\nemerged regarding their reliability in simulating complex decision-making\nbehavior, such as risky decision-making, where a single choice can lead to\nmultiple outcomes. This study investigates the ability of LLMs to simulate\nrisky decision-making scenarios. We compare model-generated decisions with\nactual human responses in a series of lottery-based tasks, using transportation\nstated preference survey data from participants in Sydney, Dhaka, Hong Kong,\nand Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and\nChatGPT o1-mini -- which were tasked with predicting individual choices. Risk\npreferences were analyzed using the Constant Relative Risk Aversion (CRRA)\nframework. Results show that both models exhibit more risk-averse behavior than\nhuman participants, with o1-mini aligning more closely with observed human\ndecisions. Further analysis of multilingual data from Nanjing and Hong Kong\nindicates that model predictions in Chinese deviate more from actual responses\ncompared to English, suggesting that prompt language may influence simulation\nperformance. These findings highlight both the promise and the current\nlimitations of LLMs in replicating human-like risk behavior, particularly in\nlinguistic and cultural settings.", "AI": {"tldr": "The study evaluates LLMs' ability to simulate risky decision-making, comparing ChatGPT 4o and ChatGPT o1-mini with human responses in lottery tasks. Results show models are more risk-averse, with o1-mini closer to human behavior. Performance varies by language, with Chinese prompts less accurate than English.", "motivation": "To assess LLMs' reliability in simulating complex decision-making, like risky choices, given their growing use in diverse applications.", "method": "Compared model-generated decisions with human responses in lottery tasks using CRRA framework, analyzing multilingual data from four cities.", "result": "Both models were more risk-averse than humans, with o1-mini closer to human behavior. Chinese prompts showed less accuracy than English.", "conclusion": "LLMs show promise but have limitations in replicating human-like risk behavior, especially in linguistic and cultural contexts."}}
{"id": "2506.22800", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22800", "abs": "https://arxiv.org/abs/2506.22800", "authors": ["Sicong Du", "Jiarun Liu", "Qifeng Chen", "Hao-Xiang Chen", "Tai-Jiang Mu", "Sheng Yang"], "title": "RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via Diffusion Priors", "comment": null, "summary": "A single-pass driving clip frequently results in incomplete scanning of the\nroad structure, making reconstructed scene expanding a critical requirement for\nsensor simulators to effectively regress driving actions. Although contemporary\n3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction\nquality, their direct extension through the integration of diffusion priors\noften introduces cumulative physical inconsistencies and compromises training\nefficiency. To address these limitations, we present RGE-GS, a novel expansive\nreconstruction framework that synergizes diffusion-based generation with\nreward-guided Gaussian integration. The RGE-GS framework incorporates two key\ninnovations: First, we propose a reward network that learns to identify and\nprioritize consistently generated patterns prior to reconstruction phases,\nthereby enabling selective retention of diffusion outputs for spatial\nstability. Second, during the reconstruction process, we devise a\ndifferentiated training strategy that automatically adjust Gaussian\noptimization progress according to scene converge metrics, which achieving\nbetter convergence than baseline methods. Extensive evaluations of publicly\navailable datasets demonstrate that RGE-GS achieves state-of-the-art\nperformance in reconstruction quality. Our source-code will be made publicly\navailable at https://github.com/CN-ADLab/RGE-GS. (Camera-ready version\nincorporating reviewer suggestions will be updated soon.)", "AI": {"tldr": "RGE-GS is a novel framework for expansive road scene reconstruction, combining diffusion-based generation with reward-guided Gaussian integration to improve quality and efficiency.", "motivation": "Incomplete road scans from single-pass driving clips necessitate better reconstruction methods for sensor simulators, but current 3DGS extensions with diffusion priors introduce physical inconsistencies and inefficiencies.", "method": "RGE-GS uses a reward network to prioritize stable diffusion outputs and a differentiated training strategy for adaptive Gaussian optimization.", "result": "RGE-GS achieves state-of-the-art reconstruction quality on public datasets.", "conclusion": "The framework effectively addresses prior limitations, offering improved performance and stability in scene reconstruction."}}
{"id": "2506.23122", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.23122", "abs": "https://arxiv.org/abs/2506.23122", "authors": ["Shivam Sharma", "Tanmoy Chakraborty"], "title": "Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This work investigates the challenging task of identifying narrative roles -\nHero, Villain, Victim, and Other - in Internet memes, across three diverse test\nsets spanning English and code-mixed (English-Hindi) languages. Building on an\nannotated dataset originally skewed toward the 'Other' class, we explore a more\nbalanced and linguistically diverse extension, originally introduced as part of\nthe CLEF 2024 shared task. Comprehensive lexical and structural analyses\nhighlight the nuanced, culture-specific, and context-rich language used in real\nmemes, in contrast to synthetically curated hateful content, which exhibits\nexplicit and repetitive lexical markers. To benchmark the role detection task,\nwe evaluate a wide spectrum of models, including fine-tuned multilingual\ntransformers, sentiment and abuse-aware classifiers, instruction-tuned LLMs,\nand multimodal vision-language models. Performance is assessed under zero-shot\nsettings using precision, recall, and F1 metrics. While larger models like\nDeBERTa-v3 and Qwen2.5-VL demonstrate notable gains, results reveal consistent\nchallenges in reliably identifying the 'Victim' class and generalising across\ncultural and code-mixed content. We also explore prompt design strategies to\nguide multimodal models and find that hybrid prompts incorporating structured\ninstructions and role definitions offer marginal yet consistent improvements.\nOur findings underscore the importance of cultural grounding, prompt\nengineering, and multimodal reasoning in modelling subtle narrative framings in\nvisual-textual content.", "AI": {"tldr": "The paper explores identifying narrative roles (Hero, Villain, Victim, Other) in memes across English and code-mixed languages, using a balanced dataset. It evaluates various models, highlighting challenges in detecting 'Victim' and generalizing across cultures.", "motivation": "To address the nuanced, culture-specific language in memes and improve role detection in visual-textual content, contrasting synthetic hateful content.", "method": "Evaluates multilingual transformers, sentiment/abuse classifiers, LLMs, and multimodal models under zero-shot settings, using precision, recall, and F1 metrics.", "result": "Larger models like DeBERTa-v3 and Qwen2.5-VL perform well, but 'Victim' detection and cross-cultural generalization remain challenging. Hybrid prompts improve results marginally.", "conclusion": "Cultural grounding, prompt engineering, and multimodal reasoning are crucial for modeling subtle narrative roles in memes."}}
{"id": "2506.22901", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2506.22901", "abs": "https://arxiv.org/abs/2506.22901", "authors": ["Sina Tabakhi", "Haiping Lu"], "title": "Missing-Modality-Aware Graph Neural Network for Cancer Classification", "comment": "15 pages, 7 figures", "summary": "A key challenge in learning from multimodal biological data is missing\nmodalities, where all data from some modalities are missing for some patients.\nCurrent fusion methods address this by excluding patients with missing\nmodalities, imputing missing modalities, or making predictions directly with\npartial modalities. However, they often struggle with diverse missing-modality\npatterns and the exponential growth of the number of such patterns as the\nnumber of modalities increases. To address these limitations, we propose MAGNET\n(Missing-modality-Aware Graph neural NETwork) for direct prediction with\npartial modalities, which introduces a patient-modality multi-head attention\nmechanism to fuse lower-dimensional modality embeddings based on their\nimportance and missingness. MAGNET's complexity increases linearly with the\nnumber of modalities while adapting to missing-pattern variability. To generate\npredictions, MAGNET further constructs a patient graph with fused multimodal\nembeddings as node features and the connectivity determined by the modality\nmissingness, followed by a conventional graph neural network. Experiments on\nthree public multiomics datasets for cancer classification, with real-world\ninstead of artificial missingness, show that MAGNET outperforms the\nstate-of-the-art fusion methods. The data and code are available at\nhttps://github.com/SinaTabakhi/MAGNET.", "AI": {"tldr": "MAGNET is a graph neural network designed for handling missing modalities in multimodal biological data, outperforming existing methods by using a patient-modality attention mechanism and a patient graph structure.", "motivation": "Addressing the challenge of missing modalities in multimodal biological data, which current methods struggle with due to diverse missing patterns and scalability issues.", "method": "Proposes MAGNET, which uses a patient-modality multi-head attention mechanism to fuse embeddings and constructs a patient graph for predictions, adapting to missing patterns with linear complexity.", "result": "MAGNET outperforms state-of-the-art fusion methods on three public multiomics datasets for cancer classification, even with real-world missingness.", "conclusion": "MAGNET effectively handles missing modalities in multimodal data, offering scalable and adaptable solutions for real-world applications."}}
{"id": "2506.23123", "categories": ["cs.AI", "cs.CY", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.23123", "abs": "https://arxiv.org/abs/2506.23123", "authors": ["Rishi Bommasani"], "title": "The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy", "comment": "Stanford University PhD Dissertation of Rishi Bommasani (Department\n  of Computer Science, 2025). Also available at\n  https://purl.stanford.edu/zf669yy0336", "summary": "Artificial intelligence is humanity's most promising technology because of\nthe remarkable capabilities offered by foundation models. Yet, the same\ntechnology brings confusion and consternation: foundation models are poorly\nunderstood and they may precipitate a wide array of harms. This dissertation\nexplains how technology and society coevolve in the age of AI, organized around\nthree themes. First, the conceptual framing: the capabilities, risks, and the\nsupply chain that grounds foundation models in the broader economy. Second, the\nempirical insights that enrich the conceptual foundations: transparency created\nvia evaluations at the model level and indexes at the organization level.\nFinally, the transition from understanding to action: superior understanding of\nthe societal impact of foundation models advances evidence-based AI policy.\nView together, this dissertation makes inroads into achieving better societal\noutcomes in the age of AI by building the scientific foundations and\nresearch-policy interface required for better AI governance.", "AI": {"tldr": "The dissertation explores the coevolution of technology and society in the AI era, focusing on foundation models' capabilities, risks, and societal impacts, aiming to improve AI governance.", "motivation": "To address the confusion and potential harms caused by poorly understood foundation models in AI, and to bridge the gap between technology and societal outcomes.", "method": "Organized into three themes: conceptual framing (capabilities, risks, supply chain), empirical insights (transparency via evaluations and indexes), and actionable understanding (evidence-based AI policy).", "result": "Advances scientific foundations and research-policy interfaces for better AI governance, aiming for improved societal outcomes.", "conclusion": "The dissertation contributes to better societal outcomes in the AI age by enhancing understanding and governance of foundation models."}}
{"id": "2506.22803", "categories": ["cs.CV", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22803", "abs": "https://arxiv.org/abs/2506.22803", "authors": ["Nuoye Xiong", "Anqi Dong", "Ning Wang", "Cong Hua", "Guangming Zhu", "Mei Lin", "Peiyi Shen", "Liang Zhang"], "title": "Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding", "comment": "Accepted by ICCV 2025", "summary": "Recent advances in deep learning have led to increasingly complex models with\ndeeper layers and more parameters, reducing interpretability and making their\ndecisions harder to understand. While many methods explain black-box reasoning,\nmost lack effective interventions or only operate at sample-level without\nmodifying the model itself. To address this, we propose the Concept Bottleneck\nModel for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU).\nCBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable\nframework to approximate black-box reasoning and communicate conceptual\nunderstanding. Detrimental concepts are automatically identified and refined\n(removed/replaced) based on global gradient contributions. The modified CBM\nthen distills corrected knowledge back into the black-box model, enhancing both\ninterpretability and accuracy. We evaluate CBM-HNMU on various CNN and\ntransformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft,\nand CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum\nincrease in average accuracy across 1.03%. Source code is available at:\nhttps://github.com/XiGuaBo/CBM-HNMU.", "AI": {"tldr": "The paper introduces CBM-HNMU, a method to improve interpretability and accuracy of deep learning models by refining concepts in the Concept Bottleneck Model and distilling corrected knowledge back into the model.", "motivation": "Deep learning models are becoming less interpretable due to complexity, and existing explanation methods lack effective interventions or model modifications.", "method": "Uses the Concept Bottleneck Model (CBM) to approximate black-box reasoning, identifies and refines detrimental concepts globally, and distills corrected knowledge back into the model.", "result": "Evaluated on multiple datasets, achieving up to 2.64% accuracy improvement and 1.03% average accuracy increase.", "conclusion": "CBM-HNMU enhances both interpretability and accuracy of deep learning models by refining and distilling conceptual knowledge."}}
{"id": "2506.23127", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23127", "abs": "https://arxiv.org/abs/2506.23127", "authors": ["Zhaoye Fei", "Li Ji", "Siyin Wang", "Junhao Shi", "Jingjing Gong", "Xipeng Qiu"], "title": "Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they face significant challenges in embodied task planning\nscenarios that require continuous environmental understanding and action\ngeneration. Existing approaches generate open-loop action scripts based on\nstatic knowledge, making it difficult to learn causal relationships between\nactions and environmental feedback, particularly in partially observable\nenvironments. We introduce Embodied Planner-R1, a novel outcome-driven\nreinforcement learning framework that enables LLMs to develop interactive\ncapabilities through autonomous exploration with minimal supervision. Our\nframework incorporates three key innovations: (1) Without human annotations, we\nemploy pure reinforcement learning with group rollout, incorporating\nin-environment interaction through parallel exploration; (2) completion-driven\nsparse reward; and (3) Interactive Policy Optimization (IPO) for efficient\nlearning from grouped trajectories. Across two challenging text-based Embodied\nplanning benchmarks, Embodied Planner-R1 achieves impressive completion rates\nof 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a\nlarge margin, and suffers only a -3.66% drop in previously unseen environments,\nevidencing strong generalization.", "AI": {"tldr": "Embodied Planner-R1 is a reinforcement learning framework for LLMs to improve interactive task planning, achieving high success rates on benchmarks with strong generalization.", "motivation": "LLMs struggle with embodied task planning due to static knowledge and lack of causal learning in dynamic environments.", "method": "Uses pure reinforcement learning with group rollout, sparse rewards, and Interactive Policy Optimization (IPO) for efficient learning.", "result": "Achieves 97.78% on ALFWorld and 79.92% on ScienceWorld, with minimal performance drop in unseen environments.", "conclusion": "The framework enhances LLMs' interactive capabilities and generalization in embodied planning tasks."}}
{"id": "2506.22927", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22927", "abs": "https://arxiv.org/abs/2506.22927", "authors": ["Jaeyun Woo", "Jiseok Lee", "Brian Kenji Iwana"], "title": "Towards Time Series Generation Conditioned on Unstructured Natural Language", "comment": null, "summary": "Generative Artificial Intelligence (AI) has rapidly become a powerful tool,\ncapable of generating various types of data, such as images and text. However,\ndespite the significant advancement of generative AI, time series generative AI\nremains underdeveloped, even though the application of time series is essential\nin finance, climate, and numerous fields. In this research, we propose a novel\nmethod of generating time series conditioned on unstructured natural language\ndescriptions. We use a diffusion model combined with a language model to\ngenerate time series from the text. Through the proposed method, we demonstrate\nthat time series generation based on natural language is possible. The proposed\nmethod can provide various applications such as custom forecasting, time series\nmanipulation, data augmentation, and transfer learning. Furthermore, we\nconstruct and propose a new public dataset for time series generation,\nconsisting of 63,010 time series-description pairs.", "AI": {"tldr": "A novel method using diffusion and language models enables time series generation from natural language descriptions, with applications in forecasting, data augmentation, and more. A new dataset of 63,010 time series-description pairs is introduced.", "motivation": "Time series generative AI is underdeveloped despite its importance in fields like finance and climate. The research aims to bridge this gap by leveraging natural language descriptions.", "method": "Combines a diffusion model with a language model to generate time series from text.", "result": "Demonstrates feasibility of time series generation from natural language, with potential applications in custom forecasting and data augmentation.", "conclusion": "The proposed method advances time series generative AI and introduces a valuable public dataset for future research."}}
{"id": "2506.23128", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23128", "abs": "https://arxiv.org/abs/2506.23128", "authors": ["Chi Chiu So", "Yueyue Sun", "Jun-Min Wang", "Siu Pang Yung", "Anthony Wai Keung Loh", "Chun Pong Chau"], "title": "Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons", "comment": "10 pages, 0 figures, accepted by 2025 IEEE international conference\n  on artificial intelligence testing (AITest)", "summary": "How far are Large Language Models (LLMs) in performing deep relational\nreasoning? In this paper, we evaluate and compare the reasoning capabilities of\nthree cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a\nsuite of carefully designed benchmark tasks in family tree and general graph\nreasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the\nhighest F1-scores across multiple tasks and problem sizes, demonstrating strong\naptitude in logical deduction and relational inference. However, all evaluated\nmodels, including DeepSeek-R1, struggle significantly as problem complexity\nincreases, largely due to token length limitations and incomplete output\nstructures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought\nresponses uncovers its unique planning and verification strategies, but also\nhighlights instances of incoherent or incomplete reasoning, calling attention\nto the need for deeper scrutiny into LLMs' internal inference dynamics. We\nfurther discuss key directions for future work, including the role of\nmultimodal reasoning and the systematic examination of reasoning failures. Our\nfindings provide both empirical insights and theoretical implications for\nadvancing LLMs' reasoning abilities, particularly in tasks that demand\nstructured, multi-step logical inference. Our code repository will be publicly\navailable at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.", "AI": {"tldr": "The paper evaluates the relational reasoning capabilities of three LLMs (DeepSeek-R1, DeepSeek-V3, GPT-4o) using benchmark tasks. DeepSeek-R1 outperforms others but struggles with complexity due to token limits and incomplete outputs.", "motivation": "To assess and compare the deep relational reasoning abilities of state-of-the-art LLMs, identifying strengths and limitations in logical deduction and relational inference.", "method": "Benchmark tasks in family tree and general graph reasoning were used to evaluate the models. DeepSeek-R1's Chain-of-Thought responses were analyzed for planning and verification strategies.", "result": "DeepSeek-R1 achieved the highest F1-scores but all models faltered with increased complexity. Incoherent reasoning in DeepSeek-R1 was noted.", "conclusion": "The study highlights the need for deeper scrutiny of LLMs' reasoning dynamics and suggests future work on multimodal reasoning and systematic failure analysis."}}
{"id": "2506.22806", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22806", "abs": "https://arxiv.org/abs/2506.22806", "authors": ["Byung Hyun Lee", "Sungjin Lim", "Seunggyu Lee", "Dong Un Kang", "Se Young Chun"], "title": "Concept Pinpoint Eraser for Text-to-image Diffusion Models via Residual Attention Gate", "comment": null, "summary": "Remarkable progress in text-to-image diffusion models has brought a major\nconcern about potentially generating images on inappropriate or trademarked\nconcepts. Concept erasing has been investigated with the goals of deleting\ntarget concepts in diffusion models while preserving other concepts with\nminimal distortion. To achieve these goals, recent concept erasing methods\nusually fine-tune the cross-attention layers of diffusion models. In this work,\nwe first show that merely updating the cross-attention layers in diffusion\nmodels, which is mathematically equivalent to adding \\emph{linear} modules to\nweights, may not be able to preserve diverse remaining concepts. Then, we\npropose a novel framework, dubbed Concept Pinpoint Eraser (CPE), by adding\n\\emph{nonlinear} Residual Attention Gates (ResAGs) that selectively erase (or\ncut) target concepts while safeguarding remaining concepts from broad\ndistributions by employing an attention anchoring loss to prevent the\nforgetting. Moreover, we adversarially train CPE with ResAG and learnable text\nembeddings in an iterative manner to maximize erasing performance and enhance\nrobustness against adversarial attacks. Extensive experiments on the erasure of\ncelebrities, artistic styles, and explicit contents demonstrated that the\nproposed CPE outperforms prior arts by keeping diverse remaining concepts while\ndeleting the target concepts with robustness against attack prompts. Code is\navailable at https://github.com/Hyun1A/CPE", "AI": {"tldr": "The paper introduces Concept Pinpoint Eraser (CPE), a method to selectively erase target concepts in diffusion models while preserving others, outperforming prior methods.", "motivation": "Addressing concerns about inappropriate or trademarked content in text-to-image diffusion models by improving concept erasure without distorting other concepts.", "method": "Proposes CPE with nonlinear Residual Attention Gates (ResAGs) and an attention anchoring loss, trained adversarially with learnable text embeddings for robustness.", "result": "CPE effectively erases target concepts (e.g., celebrities, styles, explicit content) while preserving diverse remaining concepts and resisting adversarial attacks.", "conclusion": "CPE advances concept erasure in diffusion models, offering better performance and robustness compared to existing methods."}}
{"id": "2506.23133", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23133", "abs": "https://arxiv.org/abs/2506.23133", "authors": ["Dingzirui Wang", "Xuanliang Zhang", "Rongyu Cao", "Longxu Dou", "Xianzhen Luo", "Yingwei Ma", "Qingfu Zhu", "Wanxiang Che", "Binhua Li", "Fei Huang", "Yongbin Li"], "title": "Format-Adapter: Improving Reasoning Capability of LLMs by Adapting Suitable Format", "comment": null, "summary": "Generating and voting multiple answers is an effective method to mitigate\nreasoning inconsistencies of large language models (LLMs). Prior works have\nshown that multiple reasoning formats outperform a single format when\ngenerating multiple answers. However, previous works using multiple formats\nrely on formats labeled by humans, which could be unsuitable for all tasks and\nhave high labeling costs. To address this issue, we adapt suitable formats to\nthe given tasks by generating and selecting formats. We first propose how to\nmeasure the reasoning error when generating multiple answers. Then, we\nintroduce Format-Adapter, which utilizes LLMs to generate and select suitable\nreasoning formats by minimizing the error measurement we present. We conduct\nexperiments on math and commonsense reasoning tasks, where Format-Adapter\nachieves a 4.3% performance improvement on average over previous works,\ndemonstrating the effectiveness.", "AI": {"tldr": "Format-Adapter uses LLMs to generate and select reasoning formats, improving performance by 4.3% over prior methods.", "motivation": "To address the limitations of human-labeled reasoning formats, which are costly and may not suit all tasks.", "method": "Proposes a method to measure reasoning errors and introduces Format-Adapter, which generates and selects formats to minimize these errors.", "result": "Achieves a 4.3% average performance improvement on math and commonsense reasoning tasks.", "conclusion": "Format-Adapter effectively adapts reasoning formats to tasks, outperforming previous methods."}}
{"id": "2506.22929", "categories": ["cs.LG", "cs.AI", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22929", "abs": "https://arxiv.org/abs/2506.22929", "authors": ["Chen Zhang"], "title": "Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration", "comment": null, "summary": "While deep learning excels in natural image and language processing, its\napplication to high-dimensional data faces computational challenges due to the\ndimensionality curse. Current large-scale data tools focus on business-oriented\ndescriptive statistics, lacking mathematical statistics support for advanced\nanalysis. We propose a parallel computation architecture based on space\ncompleteness, decomposing high-dimensional data into dimension-independent\nstructures for distributed processing. This framework enables seamless\nintegration of data mining and parallel-optimized machine learning methods,\nsupporting scientific computations across diverse data types like medical and\nnatural images within a unified system.", "AI": {"tldr": "A parallel computation architecture is proposed to handle high-dimensional data by decomposing it into dimension-independent structures, enabling efficient distributed processing and integration of advanced analysis methods.", "motivation": "Deep learning struggles with high-dimensional data due to computational challenges, and current tools lack support for advanced mathematical statistics.", "method": "A parallel computation framework based on space completeness decomposes high-dimensional data into dimension-independent structures for distributed processing.", "result": "The framework supports seamless integration of data mining and parallel-optimized machine learning, applicable to diverse data types like medical and natural images.", "conclusion": "The proposed architecture addresses the dimensionality curse and enhances computational efficiency for high-dimensional data analysis."}}
{"id": "2506.23141", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23141", "abs": "https://arxiv.org/abs/2506.23141", "authors": ["Siyuan Li", "Ruitong Liu", "Yan Wen", "Te Sun"], "title": "Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing", "comment": null, "summary": "Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge\nGraph Completion (KGC), providing vital cues for prediction. However,\ntraditional node-based message passing mechanisms, when applied to knowledge\ngraphs, often introduce noise and suffer from information dilution or\nover-smoothing by indiscriminately aggregating information from all neighboring\nedges. To address this challenge, we propose a semantic-aware relational\nmessage passing. A core innovation of this framework is the introduction of a\n\\textbf{semantic-aware Top-K neighbor selection strategy}. Specifically, this\nstrategy first evaluates the semantic relevance between a central node and its\nincident edges within a shared latent space, selecting only the Top-K most\npertinent ones. Subsequently, information from these selected edges is\neffectively fused with the central node's own representation using a\n\\textbf{multi-head attention aggregator} to generate a semantically focused\nnode message. In this manner, our model not only leverages the structure and\nfeatures of edges within the knowledge graph but also more accurately captures\nand propagates the contextual information most relevant to the specific link\nprediction task, thereby effectively mitigating interference from irrelevant\ninformation. Extensive experiments demonstrate that our method achieves\nsuperior performance compared to existing approaches on several established\nbenchmarks.", "AI": {"tldr": "Proposes a semantic-aware relational message passing framework for KGC, using a Top-K neighbor selection strategy and multi-head attention to improve prediction accuracy by focusing on relevant contextual information.", "motivation": "Traditional node-based message passing in KGC introduces noise and suffers from information dilution or over-smoothing by aggregating all neighboring edges indiscriminately.", "method": "Introduces a semantic-aware Top-K neighbor selection strategy to evaluate and select relevant edges, followed by a multi-head attention aggregator to fuse information.", "result": "Achieves superior performance on established benchmarks by mitigating interference from irrelevant information.", "conclusion": "The framework effectively leverages semantic context and edge features, improving KGC accuracy."}}
{"id": "2506.22807", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22807", "abs": "https://arxiv.org/abs/2506.22807", "authors": ["Yueyang Li", "Shengyu Gong", "Weiming Zeng", "Nizhuan Wang", "Wai Ting Siok"], "title": "FreqDGT: Frequency-Adaptive Dynamic Graph Networks with Transformer for Cross-subject EEG Emotion Recognition", "comment": null, "summary": "Electroencephalography (EEG) serves as a reliable and objective signal for\nemotion recognition in affective brain-computer interfaces, offering unique\nadvantages through its high temporal resolution and ability to capture\nauthentic emotional states that cannot be consciously controlled. However,\ncross-subject generalization remains a fundamental challenge due to individual\nvariability, cognitive traits, and emotional responses. We propose FreqDGT, a\nfrequency-adaptive dynamic graph transformer that systematically addresses\nthese limitations through an integrated framework. FreqDGT introduces\nfrequency-adaptive processing (FAP) to dynamically weight emotion-relevant\nfrequency bands based on neuroscientific evidence, employs adaptive dynamic\ngraph learning (ADGL) to learn input-specific brain connectivity patterns, and\nimplements multi-scale temporal disentanglement network (MTDN) that combines\nhierarchical temporal transformers with adversarial feature disentanglement to\ncapture both temporal dynamics and ensure cross-subject robustness.\nComprehensive experiments demonstrate that FreqDGT significantly improves\ncross-subject emotion recognition accuracy, confirming the effectiveness of\nintegrating frequency-adaptive, spatial-dynamic, and temporal-hierarchical\nmodeling while ensuring robustness to individual differences. The code is\navailable at https://github.com/NZWANG/FreqDGT.", "AI": {"tldr": "FreqDGT, a frequency-adaptive dynamic graph transformer, improves cross-subject EEG emotion recognition by integrating frequency-adaptive processing, adaptive dynamic graph learning, and multi-scale temporal disentanglement.", "motivation": "Cross-subject EEG emotion recognition is challenging due to individual variability. FreqDGT aims to address this by dynamically adapting to frequency bands and brain connectivity patterns.", "method": "FreqDGT combines frequency-adaptive processing (FAP), adaptive dynamic graph learning (ADGL), and a multi-scale temporal disentanglement network (MTDN) for robust modeling.", "result": "FreqDGT significantly enhances cross-subject emotion recognition accuracy, validating its integrated approach.", "conclusion": "FreqDGT effectively addresses individual variability in EEG emotion recognition through dynamic frequency, spatial, and temporal modeling."}}
{"id": "2506.23136", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23136", "abs": "https://arxiv.org/abs/2506.23136", "authors": ["Shadman Sobhan", "Mohammad Ariful Haque"], "title": "LLM-Assisted Question-Answering on Technical Documents Using Structured Data-Aware Retrieval Augmented Generation", "comment": "29 Pages, 11 Tables", "summary": "Large Language Models (LLMs) are capable of natural language understanding\nand generation. But they face challenges such as hallucination and outdated\nknowledge. Fine-tuning is one possible solution, but it is resource-intensive\nand must be repeated with every data update. Retrieval-Augmented Generation\n(RAG) offers an efficient solution by allowing LLMs to access external\nknowledge sources. However, traditional RAG pipelines struggle with retrieving\ninformation from complex technical documents with structured data such as\ntables and images. In this work, we propose a RAG pipeline, capable of handling\ntables and images in documents, for technical documents that support both\nscanned and searchable formats. Its retrieval process combines vector\nsimilarity search with a fine-tuned reranker based on Gemma-2-9b-it. The\nreranker is trained using RAFT (Retrieval-Augmented Fine-Tuning) on a custom\ndataset designed to improve context identification for question answering. Our\nevaluation demonstrates that the proposed pipeline achieves a high faithfulness\nscore of 94% (RAGas) and 96% (DeepEval), and an answer relevancy score of 87%\n(RAGas) and 93% (DeepEval). Comparative analysis demonstrates that the proposed\narchitecture is superior to general RAG pipelines in terms of table-based\nquestions and handling questions outside context.", "AI": {"tldr": "Proposes a RAG pipeline for technical documents, handling tables and images, with high faithfulness and relevancy scores.", "motivation": "Address challenges of LLMs like hallucination and outdated knowledge, and improve retrieval from complex technical documents.", "method": "Combines vector similarity search with a fine-tuned reranker (Gemma-2-9b-it) trained using RAFT on a custom dataset.", "result": "Achieves 94% faithfulness (RAGas) and 96% (DeepEval), with 87% (RAGas) and 93% (DeepEval) answer relevancy.", "conclusion": "The pipeline outperforms general RAG methods, especially for table-based and out-of-context questions."}}
{"id": "2506.22950", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22950", "abs": "https://arxiv.org/abs/2506.22950", "authors": ["Liangyu Wang", "Huanyi Xie", "Xinhai Wang", "Tianjin Huang", "Mengdi Li", "Di Wang"], "title": "Infinite Sampling: Efficient and Stable Grouped RL Training for Large Language Models", "comment": null, "summary": "Group-based reinforcement learning algorithms such as Group Reward Policy\nOptimization (GRPO) have proven effective for fine-tuning large language models\n(LLMs) with human feedback. However, generating and storing multiple responses\nper prompt incurs substantial memory overhead, especially as the sample group\nsize increases, limiting scalability under constrained hardware.\n  We propose Infinite Sampling, a framework that enables efficient and stable\nGRPO training by decoupling group size from GPU memory usage. It consists of:\n(1) micro sampling groups that decompose large groups into memory-feasible\nrounds; (2) continuous sampling that interleaves generation across groups to\nimprove utilization; and (3) a length-aware scheduler combining\ntoken-conditioned sequence length prediction with a two-stage plan: global\ngrouping via FPTAS and runtime refill via SJF.\n  Experiments show that our Micro Sampling Groups reduce peak memory usage by\nover 50% compared to full-group decoding (e.g., from 21.55 GB to 10.64 GB on\nQwen3-1.7B). Building on this, Infinite Sampling improves throughput by over\n25% compared to the naive micro sampling group method, reducing decoding steps\nwhile maintaining full-length completions and memory usage. Our hybrid\nscheduling ensures efficient and stable GRPO training with larger groups under\nrealistic GPU memory constraints.", "AI": {"tldr": "Infinite Sampling reduces memory overhead in GRPO for LLMs by decoupling group size from GPU memory, using micro sampling groups, continuous sampling, and a length-aware scheduler.", "motivation": "Addressing the high memory overhead and scalability issues in GRPO due to large sample group sizes.", "method": "Proposes micro sampling groups, continuous sampling, and a length-aware scheduler to optimize memory and throughput.", "result": "Reduces peak memory by 50% and improves throughput by 25% while maintaining performance.", "conclusion": "Infinite Sampling enables efficient and stable GRPO training under GPU memory constraints."}}
{"id": "2506.23168", "categories": ["cs.AI", "cs.DM", "math.CO", "math.RA", "06B99", "G.2.1"], "pdf": "https://arxiv.org/pdf/2506.23168", "abs": "https://arxiv.org/abs/2506.23168", "authors": ["Mohammad Abdulla", "Tobias Hille", "Dominik D\u00fcrrschnabel", "Gerd Stumme"], "title": "Rises for Measuring Local Distributivity in Lattices", "comment": "16 pages, 2 tables, 5 figures, International Joint Conference on\n  Conceptual Knowledge Structures", "summary": "Distributivity is a well-established and extensively studied notion in\nlattice theory. In the context of data analysis, particularly within Formal\nConcept Analysis (FCA), lattices are often observed to exhibit a high degree of\ndistributivity. However, no standardized measure exists to quantify this\nproperty. In this paper, we introduce the notion of rises in (concept) lattices\nas a means to assess distributivity. Rises capture how the number of attributes\nor objects in covering concepts change within the concept lattice. We show that\na lattice is distributive if and only if no non-unit rises occur. Furthermore,\nwe relate rises to the classical notion of meet- and join distributivity. We\nobserve that concept lattices from real-world data are to a high degree\njoin-distributive, but much less meet-distributive. We additionally study how\njoin-distributivity manifests on the level of ordered sets.", "AI": {"tldr": "The paper introduces 'rises' in concept lattices to measure distributivity, showing their link to classical distributivity notions and analyzing real-world data.", "motivation": "To quantify distributivity in Formal Concept Analysis (FCA) lattices, where no standardized measure exists.", "method": "Introduces 'rises' to assess distributivity, linking them to meet- and join-distributivity, and analyzes real-world concept lattices.", "result": "Concept lattices from real-world data are highly join-distributive but less meet-distributive.", "conclusion": "Rises effectively measure distributivity, revealing distinct distributive behaviors in real-world lattices."}}
{"id": "2506.22814", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22814", "abs": "https://arxiv.org/abs/2506.22814", "authors": ["Andrew Hamara", "Andrew C. Freeman"], "title": "Efficient Multi-Crop Saliency Partitioning for Automatic Image Cropping", "comment": null, "summary": "Automatic image cropping aims to extract the most visually salient regions\nwhile preserving essential composition elements. Traditional saliency-aware\ncropping methods optimize a single bounding box, making them ineffective for\napplications requiring multiple disjoint crops. In this work, we extend the\nFixed Aspect Ratio Cropping algorithm to efficiently extract multiple\nnon-overlapping crops in linear time. Our approach dynamically adjusts\nattention thresholds and removes selected crops from consideration without\nrecomputing the entire saliency map. We discuss qualitative results and\nintroduce the potential for future datasets and benchmarks.", "AI": {"tldr": "Extended Fixed Aspect Ratio Cropping algorithm to efficiently extract multiple non-overlapping crops in linear time, dynamically adjusting attention thresholds.", "motivation": "Traditional methods optimize a single bounding box, making them ineffective for applications requiring multiple disjoint crops.", "method": "Extends the Fixed Aspect Ratio Cropping algorithm to dynamically adjust attention thresholds and remove selected crops without recomputing the saliency map.", "result": "Efficient extraction of multiple non-overlapping crops in linear time.", "conclusion": "Qualitative results are discussed, with potential for future datasets and benchmarks."}}
{"id": "2506.23137", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23137", "abs": "https://arxiv.org/abs/2506.23137", "authors": ["Siyuan Li", "Ruitong Liu", "Yan Wen", "Te Sun"], "title": "Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion", "comment": "10 pages", "summary": "Effective modeling of multifaceted relations is pivotal for Knowledge Graph\nCompletion (KGC). However, a majority of existing approaches are predicated on\nstatic, embedding-based scoring, exhibiting inherent limitations in capturing\ncontextual dependencies and relational dynamics. Addressing this gap, we\npropose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal\ncomponents: (1) a semantic context learning module that encodes\ncontext-sensitive entity representations, and (2) a conditional flow-matching\nmodule designed to learn the dynamic transformation from a head to a tail\nembedding, governed by the aforementioned context. The resultant predictive\nvector field, representing the context-informed relational path, serves to\ndynamically refine the initial static score of an entity pair. Through this\nsynergy of context-aware static representations and conditioned dynamic\ninformation, FMS facilitates a more profound modeling of relational semantics.\nComprehensive evaluations on several standard benchmarks demonstrate that our\nproposed method surpasses prior state-of-the-art results.", "AI": {"tldr": "The paper introduces Flow-Modulated Scoring (FMS) for Knowledge Graph Completion, combining context-aware static embeddings with dynamic transformations to improve relational modeling, outperforming existing methods.", "motivation": "Existing embedding-based KGC methods lack contextual and dynamic relational understanding, limiting their effectiveness.", "method": "FMS uses a semantic context learning module and a conditional flow-matching module to dynamically refine static scores with context-informed relational paths.", "result": "FMS achieves superior performance on standard benchmarks compared to prior state-of-the-art methods.", "conclusion": "FMS enhances KGC by integrating static and dynamic relational information, offering a more profound semantic understanding."}}
{"id": "2506.22984", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22984", "abs": "https://arxiv.org/abs/2506.22984", "authors": ["Prathyush Kumar Reddy Lebaku", "Lu Gao", "Yunpeng Zhang", "Zhixia Li", "Yongxin Liu", "Tanvir Arafin"], "title": "Cybersecurity-Focused Anomaly Detection in Connected Autonomous Vehicles Using Machine Learning", "comment": null, "summary": "Anomaly detection in connected autonomous vehicles (CAVs) is crucial for\nmaintaining safe and reliable transportation networks, as CAVs can be\nsusceptible to sensor malfunctions, cyber-attacks, and unexpected environmental\ndisruptions. This study explores an anomaly detection approach by simulating\nvehicle behavior, generating a dataset that represents typical and atypical\nvehicular interactions. The dataset includes time-series data of position,\nspeed, and acceleration for multiple connected autonomous vehicles. We utilized\nmachine learning models to effectively identify abnormal driving patterns.\nFirst, we applied a stacked Long Short-Term Memory (LSTM) model to capture\ntemporal dependencies and sequence-based anomalies. The stacked LSTM model\nprocessed the sequential data to learn standard driving behaviors.\nAdditionally, we deployed a Random Forest model to support anomaly detection by\noffering ensemble-based predictions, which enhanced model interpretability and\nperformance. The Random Forest model achieved an R2 of 0.9830, MAE of 5.746,\nand a 95th percentile anomaly threshold of 14.18, while the stacked LSTM model\nattained an R2 of 0.9998, MAE of 82.425, and a 95th percentile anomaly\nthreshold of 265.63. These results demonstrate the models' effectiveness in\naccurately predicting vehicle trajectories and detecting anomalies in\nautonomous driving scenarios.", "AI": {"tldr": "The paper presents a machine learning approach for anomaly detection in connected autonomous vehicles (CAVs) using stacked LSTM and Random Forest models, achieving high accuracy in identifying abnormal driving patterns.", "motivation": "Anomaly detection is critical for CAVs due to risks like sensor malfunctions, cyber-attacks, and environmental disruptions, ensuring safe transportation networks.", "method": "Simulated vehicle behavior to create a dataset of typical and atypical interactions, then applied stacked LSTM for temporal dependencies and Random Forest for ensemble-based anomaly detection.", "result": "Random Forest achieved R2 of 0.9830 and MAE of 5.746, while stacked LSTM achieved R2 of 0.9998 and MAE of 82.425, demonstrating high accuracy in anomaly detection.", "conclusion": "The models effectively predict trajectories and detect anomalies, proving their utility for enhancing CAV safety and reliability."}}
{"id": "2506.23273", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23273", "abs": "https://arxiv.org/abs/2506.23273", "authors": ["Quang Hung Nguyen", "Phuong Anh Trinh", "Phan Quoc Hung Mai", "Tuan Phong Trinh"], "title": "FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis", "comment": null, "summary": "Despite the advancements of large language models, text2sql still faces many\nchallenges, particularly with complex and domain-specific queries. In finance,\ndatabase designs and financial reporting layouts vary widely between financial\nentities and countries, making text2sql even more challenging. We present\nFinStat2SQL, a lightweight text2sql pipeline enabling natural language queries\nover financial statements. Tailored to local standards like VAS, it combines\nlarge and small language models in a multi-agent setup for entity extraction,\nSQL generation, and self-correction. We build a domain-specific database and\nevaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves\n61.33\\% accuracy with sub-4-second response times on consumer hardware,\noutperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient\nsolution for financial analysis, making AI-powered querying accessible to\nVietnamese enterprises.", "AI": {"tldr": "FinStat2SQL is a lightweight text2sql pipeline for financial statements, combining large and small language models to handle domain-specific queries. It achieves 61.33% accuracy with fast response times, outperforming GPT-4o-mini.", "motivation": "Text2SQL faces challenges with complex, domain-specific queries, especially in finance due to varying database designs and reporting standards.", "method": "Uses a multi-agent setup with large and small language models for entity extraction, SQL generation, and self-correction. Tailored to local standards like VAS.", "result": "A fine-tuned 7B model achieves 61.33% accuracy with sub-4-second response times, outperforming GPT-4o-mini.", "conclusion": "FinStat2SQL provides a scalable, cost-efficient solution for financial analysis, making AI-powered querying accessible to Vietnamese enterprises."}}
{"id": "2506.22817", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22817", "abs": "https://arxiv.org/abs/2506.22817", "authors": ["Xingyilang Yin", "Jiale Wang", "Xi Yang", "Mutian Xu", "Xu Gu", "Nannan Wang"], "title": "Unleashing the Multi-View Fusion Potential: Noise Correction in VLM for Open-Vocabulary 3D Scene Understanding", "comment": null, "summary": "Recent open-vocabulary 3D scene understanding approaches mainly focus on\ntraining 3D networks through contrastive learning with point-text pairs or by\ndistilling 2D features into 3D models via point-pixel alignment. While these\nmethods show considerable performance in benchmarks with limited vocabularies,\nthey struggle to handle diverse object categories as the limited amount of 3D\ndata upbound training strong open-vocabulary 3d models. We observe that 2D\nmulti-view fusion methods take precedence in understanding diverse concepts in\n3D scenes. However, inherent noises in vision-language models lead multi-view\nfusion to sub-optimal performance. To this end, we introduce MVOV3D, a novel\napproach aimed at unleashing the potential of 2D multi-view fusion for\nopen-vocabulary 3D scene understanding. We focus on reducing the inherent\nnoises without training, thereby preserving the generalizability while\nenhancing open-world capabilities. Specifically, MVOV3D improves multi-view 2D\nfeatures by leveraging precise region-level image features and text features\nencoded by CLIP encoders and incorporates 3D geometric priors to optimize\nmulti-view fusion. Extensive experiments on various datasets demonstrate the\neffectiveness of our method. Notably, our MVOV3D achieves a new record with\n14.7% mIoU on ScanNet200 and 16.2% mIoU on Matterport160 for challenge\nopen-vocabulary semantic segmentation, outperforming current leading trained 3D\nnetworks by a significant margin.", "AI": {"tldr": "MVOV3D improves open-vocabulary 3D scene understanding by reducing noise in 2D multi-view fusion without training, leveraging CLIP encoders and 3D geometric priors.", "motivation": "Existing methods struggle with diverse object categories due to limited 3D data and noisy 2D multi-view fusion.", "method": "MVOV3D refines 2D multi-view features using CLIP encoders and 3D geometric priors, avoiding training.", "result": "Achieves 14.7% mIoU on ScanNet200 and 16.2% mIoU on Matterport160, outperforming trained 3D networks.", "conclusion": "MVOV3D effectively enhances open-vocabulary 3D scene understanding by optimizing multi-view fusion."}}
{"id": "2506.23139", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23139", "abs": "https://arxiv.org/abs/2506.23139", "authors": ["Prafulla Kumar Choubey", "Xiangyu Peng", "Shilpa Bhagavath", "Kung-Hsiang Huang", "Caiming Xiong", "Chien-Sheng Wu"], "title": "Benchmarking Deep Search over Heterogeneous Enterprise Data", "comment": null, "summary": "We present a new benchmark for evaluating Deep Search--a realistic and\ncomplex form of retrieval-augmented generation (RAG) that requires\nsource-aware, multi-hop reasoning over diverse, sparsed, but related sources.\nThese include documents, meeting transcripts, Slack messages, GitHub, and URLs,\nwhich vary in structure and often contain human-to-human interactions. We build\nit using a synthetic data pipeline that simulates business workflows across\nproduct planning, development, and support stages, generating interconnected\ncontent with realistic noise and multi-hop questions with guaranteed\nground-truth answers. We release our benchmark with both answerable and\nunanswerable queries, and retrieval pool of 39,190 enterprise artifacts,\nenabling fine-grained evaluation of long-context LLM and RAG systems. Our\nexperiments reveal that even the best-performing agentic RAG methods achieve an\naverage performance score of 32.96 on our benchmark. With further analysis, we\nhighlight retrieval as the main bottleneck: existing methods struggle to\nconduct deep searches and retrieve all necessary evidence. Consequently, they\noften reason over partial context, leading to significant performance\ndegradation.", "AI": {"tldr": "A new benchmark for Deep Search evaluates retrieval-augmented generation (RAG) using diverse, sparse sources like documents, Slack messages, and URLs. It includes synthetic data simulating business workflows, with 39,190 artifacts and answerable/unanswerable queries. Experiments show top RAG methods score only 32.96, with retrieval being the main bottleneck.", "motivation": "To address the challenge of evaluating RAG systems in realistic, complex scenarios requiring multi-hop reasoning over diverse, noisy sources.", "method": "A synthetic data pipeline simulates business workflows, generating interconnected content with noise and multi-hop questions. The benchmark includes 39,190 artifacts and varied queries.", "result": "Best RAG methods score 32.96, with retrieval identified as the primary bottleneck due to struggles in deep search and evidence retrieval.", "conclusion": "The benchmark highlights limitations in current RAG systems, emphasizing the need for improved retrieval methods to enhance performance in complex, real-world scenarios."}}
{"id": "2506.22994", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22994", "abs": "https://arxiv.org/abs/2506.22994", "authors": ["Can Hakan Da\u011f\u0131d\u0131r", "Mia Hubert", "Peter J. Rousseeuw"], "title": "Kernel Outlier Detection", "comment": null, "summary": "A new anomaly detection method called kernel outlier detection (KOD) is\nproposed. It is designed to address challenges of outlier detection in\nhigh-dimensional settings. The aim is to overcome limitations of existing\nmethods, such as dependence on distributional assumptions or on hyperparameters\nthat are hard to tune. KOD starts with a kernel transformation, followed by a\nprojection pursuit approach. Its novelties include a new ensemble of directions\nto search over, and a new way to combine results of different direction types.\nThis provides a flexible and lightweight approach for outlier detection. Our\nempirical evaluations illustrate the effectiveness of KOD on three small\ndatasets with challenging structures, and on four large benchmark datasets.", "AI": {"tldr": "Kernel Outlier Detection (KOD) is a new method for outlier detection in high-dimensional data, addressing limitations of existing methods with a kernel transformation and projection pursuit approach.", "motivation": "To overcome challenges in high-dimensional outlier detection, such as dependence on distributional assumptions or difficult-to-tune hyperparameters.", "method": "Uses a kernel transformation followed by projection pursuit, with a novel ensemble of search directions and combination of results.", "result": "Effective performance demonstrated on small datasets with challenging structures and large benchmark datasets.", "conclusion": "KOD provides a flexible and lightweight solution for outlier detection in high-dimensional settings."}}
{"id": "2506.23276", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23276", "abs": "https://arxiv.org/abs/2506.23276", "authors": ["David Guzman Piedrahita", "Yongjin Yang", "Mrinmaya Sachan", "Giorgia Ramponi", "Bernhard Sch\u00f6lkopf", "Zhijing Jin"], "title": "Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games", "comment": null, "summary": "As large language models (LLMs) are increasingly deployed as autonomous\nagents, understanding their cooperation and social mechanisms is becoming\nincreasingly important. In particular, how LLMs balance self-interest and\ncollective well-being is a critical challenge for ensuring alignment,\nrobustness, and safe deployment. In this paper, we examine the challenge of\ncostly sanctioning in multi-agent LLM systems, where an agent must decide\nwhether to invest its own resources to incentivize cooperation or penalize\ndefection. To study this, we adapt a public goods game with institutional\nchoice from behavioral economics, allowing us to observe how different LLMs\nnavigate social dilemmas over repeated interactions. Our analysis reveals four\ndistinct behavioral patterns among models: some consistently establish and\nsustain high levels of cooperation, others fluctuate between engagement and\ndisengagement, some gradually decline in cooperative behavior over time, and\nothers rigidly follow fixed strategies regardless of outcomes. Surprisingly, we\nfind that reasoning LLMs, such as the o1 series, struggle significantly with\ncooperation, whereas some traditional LLMs consistently achieve high levels of\ncooperation. These findings suggest that the current approach to improving\nLLMs, which focuses on enhancing their reasoning capabilities, does not\nnecessarily lead to cooperation, providing valuable insights for deploying LLM\nagents in environments that require sustained collaboration. Our code is\navailable at https://github.com/davidguzmanp/SanctSim", "AI": {"tldr": "The paper investigates how LLMs balance self-interest and collective well-being in multi-agent systems, focusing on costly sanctioning. It identifies four behavioral patterns and finds reasoning LLMs struggle with cooperation, unlike traditional LLMs.", "motivation": "Understanding LLMs' cooperation and social mechanisms is critical for alignment, robustness, and safe deployment in autonomous agents.", "method": "Adapts a public goods game with institutional choice from behavioral economics to observe LLMs' behavior in social dilemmas over repeated interactions.", "result": "Four behavioral patterns emerge: sustained cooperation, fluctuating engagement, declining cooperation, and rigid strategies. Reasoning LLMs struggle with cooperation, while traditional LLMs perform better.", "conclusion": "Enhancing reasoning capabilities in LLMs does not necessarily improve cooperation, offering insights for deploying LLMs in collaborative environments."}}
{"id": "2506.22819", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22819", "abs": "https://arxiv.org/abs/2506.22819", "authors": ["Ramya Hebbalaguppe", "Tamoghno Kandar", "Abhinav Nagpal", "Chetan Arora"], "title": "Prompting without Panic: Attribute-aware, Zero-shot, Test-Time Calibration", "comment": "26 pages", "summary": "Vision-language models (VLM) have demonstrated impressive performance in\nimage recognition by leveraging self-supervised training on large datasets.\nTheir performance can be further improved by adapting to the test sample using\ntest-time prompt tuning (TPT). Unfortunately, the singular focus of TPT\napproaches on improving the accuracy suffers from tunnel vision, and leads to\ndegradation in confidence calibration. This limits the applicability of TPT in\ncritical applications.\n  We make three contributions in this work. (1) We posit that random or naive\ninitialization of prompts leads to overfitting on a particular test sample, and\nis the main reason for miscalibration of the VLM after TPT. To mitigate the\nproblem, we propose careful initialization of test time prompt using prior\nknowledge about the target label attributes from a large language model (LLM);\n(2) To further maintain the quality of prompts during \\tpt, we propose a novel\nregularization loss to reduce intraclass distance, and increase inter-class\ndistance between the learnt\n  Through extensive experiments on different CLIP architectures and 15\ndatasets, we show that our approach can effectively improve the calibration\nafter TPT. We report an average expected calibration error (ECE) of 4.11 with\nour method, TCA, compared to 11.7 for vanilla TPT, 6.12 for C-TPT (ICLR'24),\n6.78 for DiffTPT (CVPR'23), and 8.43 for PromptAlign (NeurIPS'23). The code is\npublicly accessible at:\nhttps://github.com/rhebbalaguppe/TCA_PromptWithoutPanic.", "AI": {"tldr": "The paper addresses the issue of confidence calibration degradation in vision-language models (VLMs) during test-time prompt tuning (TPT), proposing a method to improve calibration by leveraging LLM knowledge and a novel regularization loss.", "motivation": "Current TPT methods focus on accuracy but degrade confidence calibration, limiting their use in critical applications. The paper aims to address this by improving calibration through better prompt initialization and regularization.", "method": "The proposed method, TCA, initializes prompts using prior knowledge from LLMs and introduces a regularization loss to reduce intraclass distance and increase inter-class distance during TPT.", "result": "Experiments show TCA reduces expected calibration error (ECE) to 4.11, outperforming other methods (vanilla TPT: 11.7, C-TPT: 6.12, DiffTPT: 6.78, PromptAlign: 8.43).", "conclusion": "TCA effectively improves calibration in VLMs during TPT, making it more reliable for critical applications."}}
{"id": "2506.23146", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23146", "abs": "https://arxiv.org/abs/2506.23146", "authors": ["Dingzriui Wang", "Xuanliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "title": "Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions", "comment": null, "summary": "In-context learning (ICL) has emerged as an effective approach to enhance the\nperformance of large language models (LLMs). However, its effectiveness varies\nsignificantly across models and tasks, posing challenges for practitioners to\ndetermine when ICL reliably improves performance. Current evaluation\napproaches, reliant on performance change after applying ICL, suffer from low\nreliability, poor attribution, and impracticality in data-insufficient\nscenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that\nquantifies ICL effectiveness by modeling the slope between learning gain (loss\ndecrease from demonstrations) and contextual relevance (demonstration-input\nrelevance). LCS addresses key limitations of performance-based metrics: (1) it\ncaptures continuous loss changes even when outputs are incorrect, improving\nreliability; (2) its formulation attributes ICL failures to weak contextual\nalignment (inability to adapt inputs to demonstrations) or strong output\ncalibration (self-verification of correctness); and (3) it minimizes reliance\non labeled data via synthetic evaluation. Extensive experiments demonstrate\nthat LCS strongly correlates with performance improvements in labeled settings\nand reliably reflects true effectiveness in biased or data-scarce scenarios.\nFurther analysis reveals actionable thresholds for LCS and identifies model\ncapabilities critical to ICL success.", "AI": {"tldr": "The paper introduces the Learning-to-Context Slope (LCS) metric to evaluate in-context learning (ICL) effectiveness in LLMs, addressing reliability, attribution, and data scarcity issues of current methods.", "motivation": "Current ICL evaluation methods are unreliable, poorly attributed, and impractical in data-limited scenarios, necessitating a better metric.", "method": "Proposes LCS, which models the slope between learning gain (loss decrease) and contextual relevance (demonstration-input relevance) to quantify ICL effectiveness.", "result": "LCS reliably correlates with performance improvements, works in biased/data-scarce settings, and identifies actionable thresholds and critical model capabilities.", "conclusion": "LCS is a robust metric for evaluating ICL effectiveness, offering practical insights for practitioners."}}
{"id": "2506.22995", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.22995", "abs": "https://arxiv.org/abs/2506.22995", "authors": ["Davide Salaorni", "Federico Bianchi", "Francesco Trov\u00f2", "Marcello Restelli"], "title": "A Reinforcement Learning Approach for Optimal Control in Microgrids", "comment": "8 pages, accepted to International Joint Conference on Neural\n  Networks 2025", "summary": "The increasing integration of renewable energy sources (RESs) is transforming\ntraditional power grid networks, which require new approaches for managing\ndecentralized energy production and consumption. Microgrids (MGs) provide a\npromising solution by enabling localized control over energy generation,\nstorage, and distribution. This paper presents a novel reinforcement learning\n(RL)-based methodology for optimizing microgrid energy management.\nSpecifically, we propose an RL agent that learns optimal energy trading and\nstorage policies by leveraging historical data on energy production,\nconsumption, and market prices. A digital twin (DT) is used to simulate the\nenergy storage system dynamics, incorporating degradation factors to ensure a\nrealistic emulation of the analysed setting. Our approach is validated through\nan experimental campaign using real-world data from a power grid located in the\nItalian territory. The results indicate that the proposed RL-based strategy\noutperforms rule-based methods and existing RL benchmarks, offering a robust\nsolution for intelligent microgrid management.", "AI": {"tldr": "A reinforcement learning (RL)-based method optimizes microgrid energy management, outperforming rule-based and existing RL methods, validated with real-world data.", "motivation": "The integration of renewable energy sources requires decentralized energy management solutions, with microgrids offering localized control.", "method": "Proposes an RL agent for energy trading and storage, using a digital twin to simulate storage dynamics and degradation.", "result": "The RL-based strategy outperforms rule-based and existing RL benchmarks in real-world tests.", "conclusion": "The approach provides a robust solution for intelligent microgrid management."}}
{"id": "2506.23306", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23306", "abs": "https://arxiv.org/abs/2506.23306", "authors": ["Qi Liu", "Can Li", "Wanjing Ma"], "title": "GATSim: Urban Mobility Simulation with Generative Agents", "comment": null, "summary": "Traditional agent-based urban mobility simulations rely on rigid rule-based\nsystems that fail to capture the complexity, adaptability, and behavioral\ndiversity characteristic of human travel decision-making. Recent advances in\nlarge language models and AI agent technology offer opportunities to create\nagents with reasoning capabilities, persistent memory, and adaptive learning\nmechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel\nframework that leverages these advances to create generative agents with rich\nbehavioral characteristics for urban mobility simulation. Unlike conventional\napproaches, GATSim agents possess diverse socioeconomic attributes, individual\nlifestyles, and evolving preferences that shape their mobility decisions\nthrough psychologically-informed memory systems, tool usage capabilities, and\nlifelong learning mechanisms. The main contributions of this study include: (1)\na comprehensive architecture combining an urban mobility foundation model with\nagent cognitive systems and transport simulation environment, (2) a fully\nfunctional prototype implementation, and (3) systematic validation\ndemonstrating that generative agents produce believable travel behaviors.\nThrough designed reflection processes, generative agents in this study can\ntransform specific travel experiences into generalized insights, enabling\nrealistic behavioral adaptation over time with specialized mechanisms for\nactivity planning and real-time reactive behaviors tailored to urban mobility\ncontexts. Experiments show that generative agents perform competitively with\nhuman annotators in mobility scenarios while naturally producing macroscopic\ntraffic evolution patterns. The code for the prototype system is shared at\nhttps://github.com/qiliuchn/gatsim.", "AI": {"tldr": "GATSim introduces generative agents with adaptive learning for urban mobility simulation, outperforming rule-based systems by capturing human-like decision-making.", "motivation": "Traditional rule-based urban mobility simulations lack adaptability and behavioral diversity, prompting the need for AI-driven generative agents.", "method": "GATSim combines an urban mobility foundation model, agent cognitive systems, and transport simulation, featuring agents with memory, learning, and tool usage.", "result": "Generative agents produce believable travel behaviors, match human annotators in mobility scenarios, and generate realistic traffic patterns.", "conclusion": "GATSim demonstrates the potential of generative agents for realistic urban mobility simulation, with a functional prototype and open-source code."}}
{"id": "2506.22832", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22832", "abs": "https://arxiv.org/abs/2506.22832", "authors": ["Alexander Gambashidze", "Li Pengyi", "Matvey Skripkin", "Andrey Galichin", "Anton Gusarov", "Konstantin Sobolev", "Andrey Kuznetsov", "Ivan Oseledets"], "title": "Listener-Rewarded Thinking in VLMs for Image Preferences", "comment": null, "summary": "Training robust and generalizable reward models for human visual preferences\nis essential for aligning text-to-image and text-to-video generative models\nwith human intent. However, current reward models often fail to generalize, and\nsupervised fine-tuning leads to memorization, demanding complex annotation\npipelines. While reinforcement learning (RL), specifically Group Relative\nPolicy Optimization (GRPO), improves generalization, we uncover a key failure\nmode: a significant drop in reasoning accuracy occurs when a model's reasoning\ntrace contradicts that of an independent, frozen vision-language model\n(\"listener\") evaluating the same output. To address this, we introduce a\nlistener-augmented GRPO framework. Here, the listener re-evaluates the\nreasoner's chain-of-thought to provide a dense, calibrated confidence score,\nshaping the RL reward signal. This encourages the reasoner not only to answer\ncorrectly, but to produce explanations that are persuasive to an independent\nmodel. Our listener-shaped reward scheme achieves best accuracy on the\nImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)\nperformance on a large-scale human preference dataset (1.2M votes, up to +6%\nover naive reasoner), and reduces reasoning contradictions compared to strong\nGRPO and SFT baselines. These results demonstrate that listener-based rewards\nprovide a scalable, data-efficient path to aligning vision-language models with\nnuanced human preferences. We will release our reasoning model here:\nhttps://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.", "AI": {"tldr": "The paper introduces a listener-augmented GRPO framework to improve reward models for human visual preferences by addressing reasoning contradictions and enhancing generalization.", "motivation": "Current reward models for human visual preferences often fail to generalize, and supervised fine-tuning leads to memorization, demanding complex annotation pipelines.", "method": "The proposed method uses a listener-augmented GRPO framework, where a frozen vision-language model re-evaluates the reasoner's chain-of-thought to provide a dense, calibrated confidence score, shaping the RL reward signal.", "result": "The listener-shaped reward scheme achieves 67.4% accuracy on the ImageReward benchmark, improves OOD performance by up to +6%, and reduces reasoning contradictions compared to baselines.", "conclusion": "Listener-based rewards offer a scalable, data-efficient way to align vision-language models with nuanced human preferences."}}
{"id": "2506.23149", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23149", "abs": "https://arxiv.org/abs/2506.23149", "authors": ["Dingzirui Wang", "Xuanliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "title": "V-SYNTHESIS: Task-Agnostic Synthesis of Consistent and Diverse In-Context Demonstrations from Scratch via V-Entropy", "comment": null, "summary": "High labeling cost for in-context learning (ICL) demonstrations motivates\nusing large language models (LLMs) for synthesis to reduce overhead. However,\nexisting synthesis methods are mainly task-specific or rely on pre-existing\ndemonstrations. So this paper focuses on synthesizing demonstrations from\nscratch for arbitrary tasks. A major challenge in synthesizing from scratch is\nensuring consistency with the target task, as the lack of labeling guidance\ncould lead to synthesis bias. We first propose a consistency metric called\nV-Score, which has higher performance and lower computation cost compared with\nthe metrics based on grams or embedding vectors. Furthermore, we introduce\nV-Synthesis, which leverages V-Score for proportional sampling to ensure both\nhigh consistency and diversity of synthesized demonstrations. Experimental\nresults demonstrate that V-Synthesis yields an average performance improvement\nof 2.0% compared to existing synthesis methods confirming the effectiveness of\nV-Synthesis.", "AI": {"tldr": "The paper introduces V-Synthesis, a method to synthesize demonstrations from scratch for arbitrary tasks in ICL, using a novel consistency metric called V-Score to improve performance and reduce bias.", "motivation": "High labeling costs for ICL demonstrations and limitations of existing synthesis methods (task-specific or reliant on pre-existing demonstrations) drive the need for a more flexible and efficient approach.", "method": "Proposes V-Score, a consistency metric, and V-Synthesis, which uses proportional sampling based on V-Score to ensure high consistency and diversity in synthesized demonstrations.", "result": "V-Synthesis improves performance by an average of 2.0% over existing methods, demonstrating its effectiveness.", "conclusion": "V-Synthesis offers a scalable and efficient solution for synthesizing demonstrations from scratch, addressing the challenges of consistency and diversity."}}
{"id": "2506.23024", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2506.23024", "abs": "https://arxiv.org/abs/2506.23024", "authors": ["Jerry Liu", "Yasa Baig", "Denise Hui Jean Lee", "Rajat Vadiraj Dwaraknath", "Atri Rudra", "Chris R\u00e9"], "title": "BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs", "comment": "Workshop for the Theory of AI for Scientific Computing @ COLT 2025\n  (Best Paper). 39 pages, 24 figures", "summary": "Physics-informed neural networks (PINNs) offer a flexible way to solve\npartial differential equations (PDEs) with machine learning, yet they still\nfall well short of the machine-precision accuracy many scientific tasks demand.\nIn this work, we investigate whether the precision ceiling comes from the\nill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP)\narchitecture. We introduce the Barycentric Weight Layer (BWLer), which models\nthe PDE solution through barycentric polynomial interpolation. A BWLer can be\nadded on top of an existing MLP (a BWLer-hat) or replace it completely\n(explicit BWLer), cleanly separating how we represent the solution from how we\ntake derivatives for the PDE loss. Using BWLer, we identify fundamental\nprecision limitations within the MLP: on a simple 1-D interpolation task, even\nMLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above\nfloat64 machine precision -- before any PDE terms are added. In PDE learning,\nadding a BWLer lifts this ceiling and exposes a tradeoff between achievable\naccuracy and the conditioning of the PDE loss. For linear PDEs we fully\ncharacterize this tradeoff with an explicit error decomposition and navigate it\nduring training with spectral derivatives and preconditioning. Across five\nbenchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for\nconvection, 10x for reaction, and 1800x for wave equations while remaining\ncompatible with first-order optimizers. Replacing the MLP entirely lets an\nexplicit BWLer reach near-machine-precision on convection, reaction, and wave\nproblems (up to 10 billion times better than prior results) and match the\nperformance of standard PINNs on stiff Burgers' and irregular-geometry Poisson\nproblems. Together, these findings point to a practical path for combining the\nflexibility of PINNs with the precision of classical spectral solvers.", "AI": {"tldr": "PINNs struggle with precision; BWLer improves accuracy by addressing MLP limitations, achieving near-machine-precision for some PDEs.", "motivation": "Overcome the precision limitations of PINNs in solving PDEs by investigating the root cause (MLP architecture or PDE ill-conditioning).", "method": "Introduce Barycentric Weight Layer (BWLer) for polynomial interpolation, used atop or replacing MLPs, with spectral derivatives and preconditioning.", "result": "BWLer improves RMSE significantly (up to 1800x) and achieves near-machine-precision for certain PDEs, outperforming standard PINNs.", "conclusion": "BWLer bridges the gap between PINNs' flexibility and classical solvers' precision, offering a practical solution for high-accuracy PDE solving."}}
{"id": "2506.23464", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23464", "abs": "https://arxiv.org/abs/2506.23464", "authors": ["Sahil Tripathi", "Md Tabrez Nafis", "Imran Hussain", "Jiechao Gao"], "title": "The Confidence Paradox: Can LLM Know When It's Wrong", "comment": null, "summary": "Document Visual Question Answering (DocVQA) systems are increasingly deployed\nin real world applications, yet they remain ethically opaque-often producing\noverconfident answers to ambiguous questions or failing to communicate\nuncertainty in a trustworthy manner. This misalignment between model confidence\nand actual knowledge poses significant risks, particularly in domains requiring\nethical accountability. Existing approaches such as LayoutLMv3, UDOP, and DONUT\nhave advanced SOTA performance by focusing on architectural sophistication and\naccuracy; however, they fall short in ethical responsiveness.\n  To address these limitations, we introduce HonestVQA, a self-supervised\nhonesty calibration framework for ethically aligned DocVQA. Our model-agnostic\nmethod quantifies uncertainty to identify knowledge gaps, aligns model\nconfidence with actual correctness using weighted loss functions, and enforces\nethical response behavior via contrastive learning. We further introduce two\nprincipled evaluation metrics--Honesty Score (H-Score) and Ethical Confidence\nIndex (ECI)--to benchmark alignment between confidence, accuracy, and ethical\ncommunication. Empirically, HonestVQA improves DocVQA accuracy by up to 4.3%\nand F1 by 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets. It reduces\noverconfidence, lowering H-Score and ECI by 0.072 and 0.078, respectively. In\ncross domain evaluation, it achieves up to 78.9% accuracy and 76.1% F1-score,\ndemonstrating strong generalization. Ablation shows a 3.8% drop in accuracy\nwithout alignment or contrastive loss.", "AI": {"tldr": "HonestVQA introduces a self-supervised honesty calibration framework for DocVQA, improving accuracy and ethical alignment by quantifying uncertainty and aligning confidence with correctness.", "motivation": "Existing DocVQA systems lack ethical responsiveness, often producing overconfident answers to ambiguous questions, posing risks in ethically accountable domains.", "method": "HonestVQA uses uncertainty quantification, weighted loss functions for confidence alignment, and contrastive learning for ethical response behavior. It introduces H-Score and ECI metrics for evaluation.", "result": "HonestVQA improves accuracy by up to 4.3% and F1 by 4.3%, reduces overconfidence, and demonstrates strong generalization in cross-domain evaluation.", "conclusion": "HonestVQA effectively addresses ethical and performance gaps in DocVQA, offering a robust framework for trustworthy AI systems."}}
{"id": "2506.22833", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22833", "abs": "https://arxiv.org/abs/2506.22833", "authors": ["Shashikant Verma", "Shanmuganathan Raman"], "title": "SemFaceEdit: Semantic Face Editing on Generative Radiance Manifolds", "comment": null, "summary": "Despite multiple view consistency offered by 3D-aware GAN techniques, the\nresulting images often lack the capacity for localized editing. In response,\ngenerative radiance manifolds emerge as an efficient approach for constrained\npoint sampling within volumes, effectively reducing computational demands and\nenabling the learning of fine details. This work introduces SemFaceEdit, a\nnovel method that streamlines the appearance and geometric editing process by\ngenerating semantic fields on generative radiance manifolds. Utilizing latent\ncodes, our method effectively disentangles the geometry and appearance\nassociated with different facial semantics within the generated image. In\ncontrast to existing methods that can change the appearance of the entire\nradiance field, our method enables the precise editing of particular facial\nsemantics while preserving the integrity of other regions. Our network\ncomprises two key modules: the Geometry module, which generates semantic\nradiance and occupancy fields, and the Appearance module, which is responsible\nfor predicting RGB radiance. We jointly train both modules in adversarial\nsettings to learn semantic-aware geometry and appearance descriptors. The\nappearance descriptors are then conditioned on their respective semantic latent\ncodes by the Appearance Module, facilitating disentanglement and enhanced\ncontrol. Our experiments highlight SemFaceEdit's superior performance in\nsemantic field-based editing, particularly in achieving improved radiance field\ndisentanglement.", "AI": {"tldr": "SemFaceEdit introduces a method for localized facial editing in 3D-aware GANs by generating semantic fields on generative radiance manifolds, enabling precise control over geometry and appearance.", "motivation": "Existing 3D-aware GANs lack localized editing capabilities, limiting their practicality for fine-grained facial modifications.", "method": "SemFaceEdit uses two modules: Geometry (for semantic radiance and occupancy fields) and Appearance (for RGB radiance), trained adversarially with latent codes for disentanglement.", "result": "The method achieves superior radiance field disentanglement and precise editing of facial semantics while preserving other regions.", "conclusion": "SemFaceEdit advances localized editing in 3D-aware GANs, offering improved control and disentanglement for facial semantics."}}
{"id": "2506.23192", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23192", "abs": "https://arxiv.org/abs/2506.23192", "authors": ["Gabriel Iturra-Bocaz", "Felipe Bravo-Marquez"], "title": "RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams", "comment": "Accepted at SIGIR'23", "summary": "Word embeddings have become essential components in various information\nretrieval and natural language processing tasks, such as ranking, document\nclassification, and question answering. However, despite their widespread use,\ntraditional word embedding models present a limitation in their static nature,\nwhich hampers their ability to adapt to the constantly evolving language\npatterns that emerge in sources such as social media and the web (e.g., new\nhashtags or brand names). To overcome this problem, incremental word embedding\nalgorithms are introduced, capable of dynamically updating word representations\nin response to new language patterns and processing continuous data streams.\n  This paper presents RiverText, a Python library for training and evaluating\nincremental word embeddings from text data streams. Our tool is a resource for\nthe information retrieval and natural language processing communities that work\nwith word embeddings in streaming scenarios, such as analyzing social media.\nThe library implements different incremental word embedding techniques, such as\nSkip-gram, Continuous Bag of Words, and Word Context Matrix, in a standardized\nframework. In addition, it uses PyTorch as its backend for neural network\ntraining. We have implemented a module that adapts existing intrinsic static\nword embedding evaluation tasks for word similarity and word categorization to\na streaming setting. Finally, we compare the implemented methods with different\nhyperparameter settings and discuss the results. Our open-source library is\navailable at https://github.com/dccuchile/rivertext.", "AI": {"tldr": "RiverText is a Python library for training and evaluating incremental word embeddings from text streams, addressing the static nature of traditional models.", "motivation": "Traditional word embeddings are static and struggle with evolving language patterns, especially in dynamic contexts like social media.", "method": "RiverText implements incremental techniques (Skip-gram, CBOW, Word Context Matrix) using PyTorch and adapts evaluation tasks for streaming.", "result": "The library provides a standardized framework for dynamic word embeddings and compares methods with various hyperparameters.", "conclusion": "RiverText is a valuable open-source tool for NLP and IR communities working with streaming text data."}}
{"id": "2506.23025", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23025", "abs": "https://arxiv.org/abs/2506.23025", "authors": ["Tejas Vaidhya", "Ayush Kaushal", "Vineet Jain", "Francis Couture Harpin", "Prashant Shishodia", "Majid Behbahani", "Yuriy Nevmyvaka", "Irina Rish"], "title": "Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models", "comment": null, "summary": "Large language models (LLMs) are increasingly used across research and\nindustry applications, yet their inference efficiency remains a significant\nchallenge. As the computational power of modern GPU architectures continuously\nimproves, their memory bandwidth and capacity have not scaled proportionally,\ncreating a critical bottleneck during inference. To address this, we\ninvestigate ternary language models (TriLMs) that employ quantization-aware\ntraining to significantly reduce memory requirements. We first analyze the\nscalability of TriLMs by conducting a scaling law analysis, revealing that\nTriLMs benefit more from increasing training data than from scaling model\nparameters. Based on this observation, we introduce Spectra-1.1, an open suite\nof TriLMs trained on up to 1.2 trillion tokens, demonstrating sustained\nperformance gains at scale. Furthermore, to improve inference efficiency, we\npropose novel 2-bit and 1.6-bit packing schemes for ternary weights, which\ndemonstrate accelerated inference across various CPU architectures. Also,\nbuilding on the 2-bit packing, we develop a GPU kernel called TriRun that\naccelerates end-to-end model inference by up to 5 times compared to\nfloating-point baselines. To encourage further exploration and development of\nTriLMs, we will release the Spectra-1.1 suite and TriRun inference kernels.\nOverall, our work lays the foundation for building and deploying efficient\nLLMs, providing a valuable resource for the research community.", "AI": {"tldr": "The paper introduces ternary language models (TriLMs) to address the memory bottleneck in LLM inference, proposing quantization-aware training, 2-bit/1.6-bit packing schemes, and a GPU kernel (TriRun) for faster inference.", "motivation": "The memory bandwidth and capacity of GPUs lag behind computational power, creating inefficiencies in LLM inference. TriLMs aim to reduce memory requirements and improve inference speed.", "method": "The study uses quantization-aware training for TriLMs, analyzes their scalability, and introduces Spectra-1.1 (a suite of TriLMs). It also proposes novel packing schemes and the TriRun GPU kernel.", "result": "TriLMs show better performance with increased training data rather than model size. Spectra-1.1 and TriRun achieve up to 5x faster inference compared to floating-point baselines.", "conclusion": "The work provides efficient LLM solutions, releasing Spectra-1.1 and TriRun to support further research in ternary language models."}}
{"id": "2506.23503", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23503", "abs": "https://arxiv.org/abs/2506.23503", "authors": ["Bosubabu Sambana", "Kondreddygari Archana", "Suram Indhra Sena Reddy", "Shaik Meethaigar Jameer Basha", "Shaik Karishma"], "title": "Data Augmentation for Cognitive Behavioral Therapy: Leveraging ERNIE Language Models using Artificial Intelligence", "comment": "6 Pages, 5 Figures, IEEE IDCIoT 2025", "summary": "Cognitive Behavioral Therapy (CBT) is a proven approach for addressing the\nirrational thought patterns associated with mental health disorders, but its\neffectiveness relies on accurately identifying cognitive pathways to provide\ntargeted treatment. In today's digital age, individuals often express negative\nemotions on social media, where they may reveal cognitive distortions, and in\nsevere cases, exhibit suicidal tendencies. However, there is a significant gap\nin methodologies designed to analyze these cognitive pathways, which could be\ncritical for psychotherapists aiming to deliver timely and effective\ninterventions in online environments. Cognitive Behavioral Therapy (CBT)\nframework leveraging acceptance, commitment and data augmentation to categorize\nand address both textual and visual content as positive or negative.\nSpecifically, the system employs BERT, RoBERTa for Sentiment Analysis and T5,\nPEGASUS for Text Summarization, mT5 for Text Translation in Multiple Languages\nfocusing on detecting negative emotions and cognitive distortions within social\nmedia data. While existing models are primarily designed to identify negative\nthoughts, the proposed system goes beyond this by predicting additional\nnegative side effects and other potential mental health disorders likes\nPhobias, Eating Disorders. This enhancement allows for a more comprehensive\nunderstanding and intervention strategy, offering psychotherapists a powerful\ntool for early detection and treatment of various psychological issues.", "AI": {"tldr": "The paper proposes a CBT-based system using NLP models (BERT, RoBERTa, T5, PEGASUS, mT5) to analyze social media content for cognitive distortions and negative emotions, aiding psychotherapists in early intervention.", "motivation": "To bridge the gap in methodologies for analyzing cognitive pathways in online environments, enabling timely mental health interventions.", "method": "Leverages NLP models for sentiment analysis, text summarization, and translation to detect negative emotions and cognitive distortions in social media data.", "result": "The system predicts negative side effects and potential mental health disorders (e.g., phobias, eating disorders), enhancing intervention strategies.", "conclusion": "The proposed system offers a comprehensive tool for early detection and treatment of psychological issues in digital spaces."}}
{"id": "2506.22836", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22836", "abs": "https://arxiv.org/abs/2506.22836", "authors": ["Hongyan An", "Kuan Zhu", "Xin He", "Haiyun Guo", "Chaoyang Zhao", "Ming Tang", "Jinqiao Wang"], "title": "FOCUS: Fine-grained Optimization with Semantic Guided Understanding for Pedestrian Attributes Recognition", "comment": "ICME 2025 Oral", "summary": "Pedestrian attribute recognition (PAR) is a fundamental perception task in\nintelligent transportation and security. To tackle this fine-grained task, most\nexisting methods focus on extracting regional features to enrich attribute\ninformation. However, a regional feature is typically used to predict a fixed\nset of pre-defined attributes in these methods, which limits the performance\nand practicality in two aspects: 1) Regional features may compromise\nfine-grained patterns unique to certain attributes in favor of capturing common\ncharacteristics shared across attributes. 2) Regional features cannot\ngeneralize to predict unseen attributes in the test time. In this paper, we\npropose the \\textbf{F}ine-grained \\textbf{O}ptimization with semanti\\textbf{C}\ng\\textbf{U}ided under\\textbf{S}tanding (FOCUS) approach for PAR, which\nadaptively extracts fine-grained attribute-level features for each attribute\nindividually, regardless of whether the attributes are seen or not during\ntraining. Specifically, we propose the Multi-Granularity Mix Tokens (MGMT) to\ncapture latent features at varying levels of visual granularity, thereby\nenriching the diversity of the extracted information. Next, we introduce the\nAttribute-guided Visual Feature Extraction (AVFE) module, which leverages\ntextual attributes as queries to retrieve their corresponding visual attribute\nfeatures from the Mix Tokens using a cross-attention mechanism. To ensure that\ntextual attributes focus on the appropriate Mix Tokens, we further incorporate\na Region-Aware Contrastive Learning (RACL) method, encouraging attributes\nwithin the same region to share consistent attention maps. Extensive\nexperiments on PA100K, PETA, and RAPv1 datasets demonstrate the effectiveness\nand strong generalization ability of our method.", "AI": {"tldr": "The paper introduces FOCUS, a method for Pedestrian Attribute Recognition (PAR) that adaptively extracts fine-grained attribute-level features using semantic guidance, improving performance and generalization to unseen attributes.", "motivation": "Existing PAR methods rely on regional features, which may compromise fine-grained patterns and fail to generalize to unseen attributes. The goal is to overcome these limitations.", "method": "Proposes FOCUS with Multi-Granularity Mix Tokens (MGMT) for diverse feature capture, Attribute-guided Visual Feature Extraction (AVFE) for semantic-guided retrieval, and Region-Aware Contrastive Learning (RACL) for consistent attention.", "result": "Demonstrates effectiveness and strong generalization on PA100K, PETA, and RAPv1 datasets.", "conclusion": "FOCUS advances PAR by adaptively extracting fine-grained features and generalizing to unseen attributes, outperforming existing methods."}}
{"id": "2506.23235", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23235", "abs": "https://arxiv.org/abs/2506.23235", "authors": ["Yi-Chen Li", "Tian Xu", "Yang Yu", "Xuqin Zhang", "Xiong-Hui Chen", "Zhongxiang Ling", "Ningjing Chao", "Lei Yuan", "Zhi-Hua Zhou"], "title": "Generalist Reward Models: Found Inside Large Language Models", "comment": null, "summary": "The alignment of Large Language Models (LLMs) is critically dependent on\nreward models trained on costly human preference data. While recent work\nexplores bypassing this cost with AI feedback, these methods often lack a\nrigorous theoretical foundation. In this paper, we discover that a powerful\ngeneralist reward model is already latently present within any LLM trained via\nstandard next-token prediction. We prove that this endogenous reward is not a\nheuristic, but is theoretically equivalent to a reward function learned through\noffline inverse reinforcement learning. This connection allows us to directly\nelicit a high-quality reward signal from a base (pre-trained or supervised\nfine-tuned) model without any further training. Critically, we also prove that\nsubsequent reinforcement learning using this endogenous reward leads to a\npolicy with a provably superior error bound compared to the base model. To our\nbest knowledge, this is the first theoretical proof of the effectiveness of\nreinforcement learning for LLMs. Our experiments validate this theory,\ndemonstrating that our method not only outperforms existing LLM-as-a-judge\napproaches but can also surpass explicitly trained reward models. These\nfindings suggest that the reward modeling stage can be replaced by a principled\nmethod of eliciting the knowledge already captured during pre-training,\nheralding a more efficient, powerful, and scalable paradigm for LLMs alignment\nas well as multi-modal models.", "AI": {"tldr": "The paper shows that LLMs inherently contain a latent reward model, equivalent to inverse reinforcement learning, eliminating the need for costly human preference data. This method outperforms existing approaches.", "motivation": "To bypass the high cost of human preference data for aligning LLMs and provide a rigorous theoretical foundation for AI feedback methods.", "method": "Proves that a latent reward model exists in LLMs trained via next-token prediction, equivalent to offline inverse reinforcement learning, and uses it for alignment without further training.", "result": "Demonstrates superior performance over existing methods, including trained reward models, with a provably better error bound.", "conclusion": "The findings suggest a more efficient and scalable paradigm for LLM alignment, leveraging pre-trained knowledge instead of explicit reward modeling."}}
{"id": "2506.23033", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.23033", "abs": "https://arxiv.org/abs/2506.23033", "authors": ["Yash Vardhan Tomar"], "title": "Feature-Wise Mixing for Mitigating Contextual Bias in Predictive Supervised Learning", "comment": null, "summary": "Bias in predictive machine learning (ML) models is a fundamental challenge\ndue to the skewed or unfair outcomes produced by biased models. Existing\nmitigation strategies rely on either post-hoc corrections or rigid constraints.\nHowever, emerging research claims that these techniques can limit scalability\nand reduce generalizability. To address this, this paper introduces a\nfeature-wise mixing framework to mitigate contextual bias. This was done by\nredistributing feature representations across multiple contextual datasets. To\nassess feature-wise mixing's effectiveness, four ML classifiers were trained\nusing cross-validation and evaluated with bias-sensitive loss functions,\nincluding disparity metrics and mean squared error (MSE), which served as a\nstandard measure of predictive performance. The proposed method achieved an\naverage bias reduction of 43.35% and a statistically significant decrease in\nMSE across all classifiers trained on mixed datasets. Additionally,\nbenchmarking against established bias mitigation techniques found that\nfeature-wise mixing consistently outperformed SMOTE oversampling and\ndemonstrated competitive effectiveness without requiring explicit bias\nattribute identification. Feature-wise mixing efficiently avoids the\ncomputational overhead typically associated with fairness-aware learning\nalgorithms. Future work could explore applying feature-wise mixing for\nreal-world fields where accurate predictions are necessary.", "AI": {"tldr": "The paper introduces a feature-wise mixing framework to mitigate bias in ML models, achieving significant bias reduction and improved predictive performance without explicit bias attribute identification.", "motivation": "Addressing the limitations of existing bias mitigation strategies, which are either post-hoc or rigid, and aiming for scalable and generalizable solutions.", "method": "Proposes feature-wise mixing by redistributing feature representations across contextual datasets, evaluated using bias-sensitive loss functions and cross-validation.", "result": "Achieved 43.35% average bias reduction and statistically significant MSE decrease, outperforming SMOTE oversampling.", "conclusion": "Feature-wise mixing is effective, computationally efficient, and scalable, with potential for real-world applications requiring accurate predictions."}}
{"id": "2506.23504", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23504", "abs": "https://arxiv.org/abs/2506.23504", "authors": ["Bosubabu Sambana", "Kotamsetty Geethika Devi", "Bandi Rajeswara Reddy", "Galeti Mohammad Hussain", "Gownivalla Siddartha"], "title": "Hybrid Approach for Electricity Price Forecasting using AlexNet and LSTM", "comment": "6 Pages, 7 Figures", "summary": "The recent development of advanced machine learning methods for hybrid models\nhas greatly addressed the need for the correct prediction of electrical prices.\nThis method combines AlexNet and LSTM algorithms, which are used to introduce a\nnew model with higher accuracy in price forecasting. Despite RNN and ANN being\neffective, they often fail to deal with forex time sequence data. The\ntraditional methods do not accurately forecast the prices. These traditional\nmethods only focus on demand and price which leads to insufficient analysis of\ndata. To address this issue, using the hybrid approach, which focuses on\nexternal variables that also effect the predicted prices. Nevertheless, due to\nAlexNet's excellent feature extraction and LSTM's learning sequential patterns,\nthe prediction accuracy is vastly increased. The model is built on the past\ndata, which has been supplied with the most significant elements like demand,\ntemperature, sunlight, and rain. For example, the model applies methods, such\nas minimum-maximum scaling and a time window, to predict the electricity prices\nof the future. The results show that this hybrid model is good than the\nstandalone ones in terms of accuracy. Although we got our accuracy rating of\n97.08, it shows higher accompaniments than remaining models RNN and ANN with\naccuracies of 96.64 and 96.63 respectively.", "AI": {"tldr": "A hybrid model combining AlexNet and LSTM improves electricity price forecasting accuracy by addressing limitations of traditional methods and standalone models like RNN and ANN.", "motivation": "Traditional methods fail to accurately forecast electricity prices due to insufficient analysis of external variables and sequential data.", "method": "The hybrid model integrates AlexNet for feature extraction and LSTM for learning sequential patterns, using data like demand, temperature, sunlight, and rain. Techniques like minimum-maximum scaling and time windows are applied.", "result": "The hybrid model achieves 97.08% accuracy, outperforming RNN (96.64%) and ANN (96.63%).", "conclusion": "The hybrid approach significantly enhances prediction accuracy, making it superior to standalone models for electricity price forecasting."}}
{"id": "2506.22843", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22843", "abs": "https://arxiv.org/abs/2506.22843", "authors": ["Kien Nguyen", "Clinton Fookes", "Sridha Sridharan", "Huy Nguyen", "Feng Liu", "Xiaoming Liu", "Arun Ross", "Dana Michalski", "Tam\u00e1s Endrei", "Ivan DeAndres-Tame", "Ruben Tolosana", "Ruben Vera-Rodriguez", "Aythami Morales", "Julian Fierrez", "Javier Ortega-Garcia", "Zijing Gong", "Yuhao Wang", "Xuehu Liu", "Pingping Zhang", "Md Rashidunnabi", "Hugo Proen\u00e7a", "Kailash A. Hambarde", "Saeid Rezaei"], "title": "AG-VPReID 2025: Aerial-Ground Video-based Person Re-identification Challenge Results", "comment": null, "summary": "Person re-identification (ReID) across aerial and ground vantage points has\nbecome crucial for large-scale surveillance and public safety applications.\nAlthough significant progress has been made in ground-only scenarios, bridging\nthe aerial-ground domain gap remains a formidable challenge due to extreme\nviewpoint differences, scale variations, and occlusions. Building upon the\nachievements of the AG-ReID 2023 Challenge, this paper introduces the AG-VPReID\n2025 Challenge - the first large-scale video-based competition focused on\nhigh-altitude (80-120m) aerial-ground ReID. Constructed on the new AG-VPReID\ndataset with 3,027 identities, over 13,500 tracklets, and approximately 3.7\nmillion frames captured from UAVs, CCTV, and wearable cameras, the challenge\nfeatured four international teams. These teams developed solutions ranging from\nmulti-stream architectures to transformer-based temporal reasoning and\nphysics-informed modeling. The leading approach, X-TFCLIP from UAM, attained\n72.28% Rank-1 accuracy in the aerial-to-ground ReID setting and 70.77% in the\nground-to-aerial ReID setting, surpassing existing baselines while highlighting\nthe dataset's complexity. For additional details, please refer to the official\nwebsite at https://agvpreid25.github.io.", "AI": {"tldr": "The AG-VPReID 2025 Challenge introduces a large-scale video-based competition for aerial-ground person re-identification (ReID), addressing challenges like viewpoint differences and scale variations. The dataset includes 3,027 identities and 3.7 million frames, with top-performing methods achieving over 70% Rank-1 accuracy.", "motivation": "Bridging the aerial-ground domain gap in ReID is critical for surveillance and public safety, but challenges like viewpoint differences and occlusions persist.", "method": "The challenge involved four teams using multi-stream architectures, transformer-based temporal reasoning, and physics-informed modeling.", "result": "The leading method, X-TFCLIP, achieved 72.28% aerial-to-ground and 70.77% ground-to-aerial Rank-1 accuracy.", "conclusion": "The AG-VPReID 2025 Challenge advances aerial-ground ReID, showcasing the dataset's complexity and the potential of innovative methods."}}
{"id": "2506.23288", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23288", "abs": "https://arxiv.org/abs/2506.23288", "authors": ["Miguel Domingo", "Francisco Casacuberta"], "title": "Two Spelling Normalization Approaches Based on Large Language Models", "comment": null, "summary": "The absence of standardized spelling conventions and the organic evolution of\nhuman language present an inherent linguistic challenge within historical\ndocuments, a longstanding concern for scholars in the humanities. Addressing\nthis issue, spelling normalization endeavors to align a document's orthography\nwith contemporary standards. In this study, we propose two new approaches based\non large language models: one of which has been trained without a supervised\ntraining, and a second one which has been trained for machine translation. Our\nevaluation spans multiple datasets encompassing diverse languages and\nhistorical periods, leading us to the conclusion that while both of them\nyielded encouraging results, statistical machine translation still seems to be\nthe most suitable technology for this task.", "AI": {"tldr": "The paper proposes two new spelling normalization methods using large language models, comparing unsupervised training and machine translation, and finds machine translation more suitable.", "motivation": "The lack of standardized spelling in historical documents poses challenges for scholars, prompting the need for effective normalization techniques.", "method": "Two approaches are introduced: one using unsupervised training and another leveraging machine translation with large language models.", "result": "Both methods show promise, but machine translation outperforms unsupervised training across diverse datasets.", "conclusion": "Statistical machine translation remains the most effective technology for spelling normalization in historical texts."}}
{"id": "2506.23036", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.23036", "abs": "https://arxiv.org/abs/2506.23036", "authors": ["Zain ul Abdeen", "Ming Jin"], "title": "Fragile, Robust, and Antifragile: A Perspective from Parameter Responses in Reinforcement Learning Under Stress", "comment": null, "summary": "This paper explores Reinforcement learning (RL) policy robustness by\nsystematically analyzing network parameters under internal and external\nstresses. Inspired by synaptic plasticity in neuroscience, synaptic filtering\nintroduces internal stress by selectively perturbing parameters, while\nadversarial attacks apply external stress through modified agent observations.\nThis dual approach enables the classification of parameters as fragile, robust,\nor antifragile, based on their influence on policy performance in clean and\nadversarial settings. Parameter scores are defined to quantify these\ncharacteristics, and the framework is validated on PPO-trained agents in Mujoco\ncontinuous control environments. The results highlight the presence of\nantifragile parameters that enhance policy performance under stress,\ndemonstrating the potential of targeted filtering techniques to improve RL\npolicy adaptability. These insights provide a foundation for future\nadvancements in the design of robust and antifragile RL systems.", "AI": {"tldr": "The paper analyzes RL policy robustness by testing parameters under internal (synaptic filtering) and external (adversarial attacks) stresses, classifying them as fragile, robust, or antifragile. It validates the framework on PPO-trained agents, finding antifragile parameters that improve performance under stress.", "motivation": "To understand and enhance RL policy robustness by examining how parameters react to internal and external stresses, inspired by synaptic plasticity.", "method": "Uses synaptic filtering for internal stress and adversarial attacks for external stress to classify parameters. Validates on PPO-trained agents in Mujoco environments.", "result": "Identifies antifragile parameters that boost policy performance under stress, showing potential for targeted filtering to improve adaptability.", "conclusion": "The framework offers insights for designing robust and antifragile RL systems, paving the way for future advancements."}}
{"id": "2506.23517", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23517", "abs": "https://arxiv.org/abs/2506.23517", "authors": ["Selin Dik", "Osman Erdem", "Mehmet Dik"], "title": "Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays", "comment": null, "summary": "As the use of AI tools by students has become more prevalent, instructors\nhave started using AI detection tools like GPTZero and QuillBot to detect AI\nwritten text. However, the reliability of these detectors remains uncertain. In\nour study, we focused mostly on the success rate of GPTZero, the most-used AI\ndetector, in identifying AI-generated texts based on different lengths of\nrandomly submitted essays: short (40-100 word count), medium (100-350 word\ncount), and long (350-800 word count). We gathered a data set consisting of\ntwenty-eight AI-generated papers and fifty human-written papers. With this\nrandomized essay data, papers were individually plugged into GPTZero and\nmeasured for percentage of AI generation and confidence. A vast majority of the\nAI-generated papers were detected accurately (ranging from 91-100% AI believed\ngeneration), while the human generated essays fluctuated; there were a handful\nof false positives. These findings suggest that although GPTZero is effective\nat detecting purely AI-generated content, its reliability in distinguishing\nhuman-authored texts is limited. Educators should therefore exercise caution\nwhen relying solely on AI detection tools.", "AI": {"tldr": "GPTZero detects AI-generated texts well but struggles with false positives for human-written essays.", "motivation": "To evaluate GPTZero's reliability in detecting AI-generated vs. human-written essays of varying lengths.", "method": "Tested GPTZero on 28 AI-generated and 50 human-written essays of short, medium, and long lengths, measuring AI generation percentage and confidence.", "result": "AI-generated texts were accurately detected (91-100%), but human-written essays had false positives.", "conclusion": "GPTZero is effective for AI detection but unreliable for human texts; educators should use it cautiously."}}
{"id": "2506.22850", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22850", "abs": "https://arxiv.org/abs/2506.22850", "authors": ["Aalok Gangopadhyay", "Shashikant Verma", "Shanmuganathan Raman"], "title": "DMD-Net: Deep Mesh Denoising Network", "comment": null, "summary": "We present Deep Mesh Denoising Network (DMD-Net), an end-to-end deep learning\nframework, for solving the mesh denoising problem. DMD-Net consists of a Graph\nConvolutional Neural Network in which aggregation is performed in both the\nprimal as well as the dual graph. This is realized in the form of an asymmetric\ntwo-stream network, which contains a primal-dual fusion block that enables\ncommunication between the primal-stream and the dual-stream. We develop a\nFeature Guided Transformer (FGT) paradigm, which consists of a feature\nextractor, a transformer, and a denoiser. The feature extractor estimates the\nlocal features, that guide the transformer to compute a transformation, which\nis applied to the noisy input mesh to obtain a useful intermediate\nrepresentation. This is further processed by the denoiser to obtain the\ndenoised mesh. Our network is trained on a large scale dataset of 3D objects.\nWe perform exhaustive ablation studies to demonstrate that each component in\nour network is essential for obtaining the best performance. We show that our\nmethod obtains competitive or better results when compared with the\nstate-of-the-art mesh denoising algorithms. We demonstrate that our method is\nrobust to various kinds of noise. We observe that even in the presence of\nextremely high noise, our method achieves excellent performance.", "AI": {"tldr": "DMD-Net is a deep learning framework for mesh denoising using a dual-stream GCN and a Feature Guided Transformer, achieving state-of-the-art performance even with high noise.", "motivation": "To address the mesh denoising problem effectively, leveraging deep learning for robust and high-quality results.", "method": "Uses a dual-stream Graph Convolutional Neural Network (primal and dual graphs) with a Feature Guided Transformer (feature extractor, transformer, denoiser) for denoising.", "result": "Competitive or superior performance compared to state-of-the-art methods, robust to various noise levels.", "conclusion": "DMD-Net is an effective and robust solution for mesh denoising, excelling even under extreme noise conditions."}}
{"id": "2506.23293", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2506.23293", "abs": "https://arxiv.org/abs/2506.23293", "authors": ["P. Myles Eugenio"], "title": "Objective-Free Local Learning and Emergent Language Structure in Thinking Machines", "comment": "22 pages, 7 figures", "summary": "We present a neuro-symbolic framework for generative language modeling based\non local, event-driven emergent learning. At its core is a hierarchical\nHopfield memory chain acting as a compositional short-term memory and dynamic\ntokenizer (retokenizer). Rather than relying on predefined tokens or\nsupervision, the model builds structure from scratch, learning symbol sequences\nas multi-scale representations. It constructs projection tensors that bind\nco-occurring features into hierarchical tokens, introducing redundancy (i.e an\nemergent gauge structure) and enabling compression of local activations into\nlong-range dependencies. Curiously, we find that the retokenizer can filter\nnatural language patterns from noise, generating synthetic languages with\ncoherent internal morphology -- quantifiably the same as human language.\nLanguage is learned in a local (Hebbian) fashion, where model constraints\ndictate allowed emergent structure, and new information is retained in\nalignment with this structure. The absence of a global objective enables a form\nof plasticity not found in conventional language models, allowing the system to\ngeneralize beyond its initial inference class -- even without explicit data. We\ndemonstrate that briefly activating a new neuron during inference binds\ndistributed multi-scale token features into a symbolic embedding. These\nemergent embedding neurons act as long-term memory and support a key-value\nmechanism for compositional inference and generalization. This architecture\nprovides a methodological foundation for studying how symbolic structure can\nemerge from local neural learning. It offers a new pathway for building\nscalable, interpretable neuro-symbolic systems -- where tokens, grammar, and\nreasoning arise as compressed memory traces within a Hopfield hierarchy. This\napproach advances the development of neuromorphic architectures for generative\nlanguage models.", "AI": {"tldr": "A neuro-symbolic framework for generative language modeling using emergent learning and hierarchical Hopfield memory, enabling unsupervised tokenization and coherent language generation.", "motivation": "To explore how symbolic structure can emerge from local neural learning without predefined tokens or supervision, advancing scalable and interpretable neuro-symbolic systems.", "method": "Uses a hierarchical Hopfield memory chain as a dynamic tokenizer, learning symbol sequences as multi-scale representations and binding co-occurring features into hierarchical tokens.", "result": "The model generates synthetic languages with human-like morphology, generalizes beyond initial inference class, and supports compositional inference via emergent embedding neurons.", "conclusion": "The framework provides a foundation for scalable, interpretable neuro-symbolic systems, advancing neuromorphic architectures for generative language models."}}
{"id": "2506.23041", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23041", "abs": "https://arxiv.org/abs/2506.23041", "authors": ["Chengyu Dong", "Huan Gui", "Noveen Sachdeva", "Long Jin", "Ke Yin", "Jingbo Shang", "Lichan Hong", "Ed H. Chi", "Zhe Zhao"], "title": "ReMem: Mutual Information-Aware Fine-tuning of Pretrained Vision Transformers for Effective Knowledge Distillation", "comment": null, "summary": "Knowledge distillation from pretrained visual representation models offers an\neffective approach to improve small, task-specific production models. However,\nthe effectiveness of such knowledge transfer drops significantly when\ndistilling from strong models that are pretrained in a large scale. In this\npaper, we address this challenge for pretrained Vision Transformers (ViTs) by\nexploring methods to fine-tune them for more effective knowledge transfer.\nMotivated by the connection between mutual information and distillation\neffectiveness, we propose to employ mutual information-aware optimization\nduring finetuning. For small or highly-imbalanced downstream datasets where\nsuch optimization becomes less effective, we introduce a simple yet effective\nheuristic of reweighting MLP blocks. This approach is inspired by our\nobservation that top MLP blocks are primarily responsible for mutual\ninformation loss. Our method enables small student models to benefit from those\npretrained models among the strongest.", "AI": {"tldr": "The paper addresses the challenge of effective knowledge distillation from large-scale pretrained Vision Transformers (ViTs) to smaller models by proposing mutual information-aware optimization and MLP block reweighting.", "motivation": "The effectiveness of knowledge transfer drops when distilling from large-scale pretrained models, especially for small or imbalanced datasets. The paper aims to improve this by leveraging mutual information insights.", "method": "The authors propose mutual information-aware optimization during finetuning and introduce a heuristic to reweight MLP blocks, which are key to mutual information loss.", "result": "The method enhances knowledge transfer, allowing small student models to benefit from strong pretrained ViTs.", "conclusion": "The proposed approach effectively improves distillation from large-scale pretrained models, particularly for challenging datasets."}}
{"id": "2506.23520", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23520", "abs": "https://arxiv.org/abs/2506.23520", "authors": ["Yu Zhang", "Ruijie Yu", "Jidong Tian", "Feng Zhu", "Jiapeng Liu", "Xiaokang Yang", "Yaohui Jin", "Yanyan Xu"], "title": "ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data", "comment": null, "summary": "With the increasing interest in robotic synthesis in the context of organic\nchemistry, the automated extraction of chemical procedures from literature is\ncritical. However, this task remains challenging due to the inherent ambiguity\nof chemical language and the high cost of human annotation required for\ndeveloping reliable computer-aided extraction protocols. Here, we present\nChemActor, a fully fine-tuned large language model (LLM), as a chemical\nexecutor to convert between unstructured experimental procedures and structured\naction sequences. We propose a sequential LLM-generated data framework to\naddress the challenges of insufficient and low-quality annotated data. This\nframework integrates a data selection module that selects data based on\ndistribution divergence, with a general-purpose LLM, to generate\nmachine-executable actions from a single molecule input. Additionally, we\nintroduce a novel multi-round LLMs circle review metric, which reflects the\nmodel's advanced understanding of chemical experimental procedures. Extensive\nexperiments on reaction-to-description (R2D) and description-to-action (D2A)\ntasks demonstrate that ChemActor, augmented by LLM-generated data, achieves\nstate-of-the-art performance, outperforming the baseline model by 10%. The code\nis available at: https://github.com/Zhanghahah/ChemActor.", "AI": {"tldr": "ChemActor, a fine-tuned LLM, converts chemical procedures into structured actions, using a sequential data framework and multi-round review to achieve state-of-the-art performance.", "motivation": "Automating chemical procedure extraction is challenging due to ambiguous language and costly human annotation. ChemActor addresses this gap.", "method": "Uses a sequential LLM-generated data framework with a data selection module and multi-round review to generate machine-executable actions.", "result": "Outperforms baseline by 10% in R2D and D2A tasks, demonstrating advanced understanding of chemical procedures.", "conclusion": "ChemActor, enhanced by LLM-generated data, sets a new standard for automated chemical procedure extraction."}}
{"id": "2506.22864", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22864", "abs": "https://arxiv.org/abs/2506.22864", "authors": ["Li-Cheng Shen", "Jih-Kang Hsieh", "Wei-Hua Li", "Chu-Song Chen"], "title": "Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval", "comment": "ICMR 2025", "summary": "Text-to-image retrieval (TIR) aims to find relevant images based on a textual\nquery, but existing approaches are primarily based on whole-image captions and\nlack interpretability. Meanwhile, referring expression segmentation (RES)\nenables precise object localization based on natural language descriptions but\nis computationally expensive when applied across large image collections. To\nbridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies\nTIR and RES, requiring both efficient image search and accurate object\nsegmentation. To address this task, we propose a two-stage framework,\ncomprising a first stage for segmentation-aware image retrieval and a second\nstage for reranking and object grounding with a multimodal large language model\n(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract\nregion-level embeddings offline at first, enabling effective and scalable\nonline retrieval. Secondly, MLLM is used to refine retrieval rankings and\ngenerate bounding boxes, which are matched to segmentation masks. We evaluate\nour approach on COCO and D$^3$ datasets, demonstrating significant improvements\nin both retrieval accuracy and segmentation quality over previous methods.", "AI": {"tldr": "MaTIR unifies text-to-image retrieval (TIR) and referring expression segmentation (RES) for efficient search and accurate segmentation, using a two-stage framework with SAM and Alpha-CLIP for offline mask generation and MLLM for reranking.", "motivation": "Existing TIR lacks interpretability, while RES is computationally expensive for large datasets. MaTIR bridges this gap.", "method": "Two-stage framework: 1) Segmentation-aware retrieval with SAM and Alpha-CLIP for offline mask/embedding generation. 2) MLLM for reranking and object grounding.", "result": "Improved retrieval accuracy and segmentation quality on COCO and D$^3$ datasets.", "conclusion": "MaTIR effectively combines TIR and RES, offering scalable and accurate results."}}
{"id": "2506.23315", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23315", "abs": "https://arxiv.org/abs/2506.23315", "authors": ["Shouvon Sarker", "Xishuang Dong", "Lijun Qian"], "title": "Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)", "comment": null, "summary": "Identification of key variables such as medications, diseases, relations from\nhealth records and clinical notes has a wide range of applications in the\nclinical domain. n2c2 2022 provided shared tasks on challenges in natural\nlanguage processing for clinical data analytics on electronic health records\n(EHR), where it built a comprehensive annotated clinical data Contextualized\nMedication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of\nthis challenge that is to detect and classify medication events from clinical\nnotes through building a novel BERT-based ensemble model. It started with\npretraining BERT models on different types of big data such as Wikipedia and\nMIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED\ntraining data. These fine-tuned BERT models were employed to accomplish\nmedication event classification on CMED testing data with multiple predictions.\nThese multiple predictions generated by these fine-tuned BERT models were\nintegrated to build final prediction with voting strategies. Experimental\nresults demonstrated that BERT-based ensemble models can effectively improve\nstrict Micro-F score by about 5% and strict Macro-F score by about 6%,\nrespectively.", "AI": {"tldr": "The paper presents a BERT-based ensemble model for detecting and classifying medication events in clinical notes, improving performance metrics by 5-6%.", "motivation": "To address the challenge of identifying key variables like medications from clinical data, leveraging the n2c2 2022 shared task and its annotated dataset (CMED).", "method": "Pretrained BERT models on Wikipedia and MIMIC data, fine-tuned them on CMED, and used ensemble voting for final predictions.", "result": "The ensemble model improved strict Micro-F score by ~5% and strict Macro-F score by ~6%.", "conclusion": "BERT-based ensemble models are effective for medication event classification in clinical notes."}}
{"id": "2506.23053", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23053", "abs": "https://arxiv.org/abs/2506.23053", "authors": ["Hanlin Dong", "Arian Prabowo", "Hao Xue", "Flora D. Salim"], "title": "Double-Diffusion: Diffusion Conditioned Diffusion Probabilistic Model For Air Quality Prediction", "comment": null, "summary": "Air quality prediction is a challenging forecasting task due to its\nspatio-temporal complexity and the inherent dynamics as well as uncertainty.\nMost of the current models handle these two challenges by applying Graph Neural\nNetworks or known physics principles, and quantifying stochasticity through\nprobabilistic networks like Diffusion models. Nevertheless, finding the right\nbalancing point between the certainties and uncertainties remains an open\nquestion. Therefore, we propose Double-Diffusion, a novel diffusion\nprobabilistic model that harnesses the power of known physics to guide air\nquality forecasting with stochasticity. To the best of our knowledge, while\nprecedents have been made of using conditional diffusion models to predict air\npollution, this is the first attempt to use physics as a conditional generative\napproach for air quality prediction. Along with a sampling strategy adopted\nfrom image restoration and a new denoiser architecture, Double-Diffusion ranks\nfirst in most evaluation scenarios across two real-life datasets compared with\nother probabilistic models, it also cuts inference time by 50% to 30% while\nenjoying an increase between 3-12% in Continuous Ranked Probabilistic Score\n(CRPS).", "AI": {"tldr": "Double-Diffusion is a novel physics-guided diffusion model for air quality prediction, outperforming other probabilistic models in accuracy and efficiency.", "motivation": "Addressing the challenge of balancing certainty and uncertainty in air quality prediction by integrating physics principles with stochasticity.", "method": "Proposes Double-Diffusion, a diffusion probabilistic model using physics as a conditional generative approach, with a new denoiser architecture and sampling strategy.", "result": "Ranks first in evaluations, reduces inference time by 30-50%, and improves CRPS by 3-12%.", "conclusion": "Double-Diffusion effectively combines physics and stochasticity for superior air quality forecasting."}}
{"id": "2506.23549", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23549", "abs": "https://arxiv.org/abs/2506.23549", "authors": ["Huai-Chih Wang", "Hsiang-Chun Chuang", "Hsi-Chun Cheng", "Dai-Jie Wu", "Shao-Hua Sun"], "title": "CooT: Learning to Coordinate In-Context with Coordination Transformers", "comment": "23 pages, 10 tables, 8 figures", "summary": "Effective coordination among artificial agents in dynamic and uncertain\nenvironments remains a significant challenge in multi-agent systems. Existing\napproaches, such as self-play and population-based methods, either generalize\npoorly to unseen partners or require extensive training. To overcome these\nlimitations, we propose Coordination Transformers (CooT), a novel in-context\ncoordination framework that uses recent interaction histories to adapt to\nunseen partners rapidly. Unlike previous approaches that primarily aim to\nincrease the diversity of training partners, CooT explicitly focuses on\nadapting to new partner behaviors by predicting actions aligned with observed\npartner interactions. Trained on interaction trajectories collected from\ndiverse pairs of agents with complementary behaviors, CooT quickly learns\neffective coordination strategies without explicit supervision or fine-tuning.\nEvaluations on the Overcooked benchmark demonstrate that CooT significantly\noutperforms baseline methods in coordination tasks involving previously unseen\npartners. Human evaluations further confirm CooT as the most effective\ncollaborative partner, while extensive ablations highlight its robustness,\nflexibility, and sensitivity to context in multi-agent scenarios.", "AI": {"tldr": "Coordination Transformers (CooT) is a novel framework for multi-agent systems that adapts to unseen partners using interaction histories, outperforming baselines in coordination tasks.", "motivation": "Addressing poor generalization and extensive training requirements in existing multi-agent coordination methods.", "method": "Uses in-context coordination, leveraging interaction histories to predict and align actions with partner behaviors without supervision.", "result": "Outperforms baselines in unseen partner scenarios on the Overcooked benchmark and excels in human evaluations.", "conclusion": "CooT is robust, flexible, and context-sensitive, making it an effective solution for multi-agent coordination."}}
{"id": "2506.22866", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22866", "abs": "https://arxiv.org/abs/2506.22866", "authors": ["Hang-Cheng Dong", "Lu Zou", "Bingguo Liu", "Dong Ye", "Guodong Liu"], "title": "Region-Aware CAM: High-Resolution Weakly-Supervised Defect Segmentation via Salient Region Perception", "comment": null, "summary": "Surface defect detection plays a critical role in industrial quality\ninspection. Recent advances in artificial intelligence have significantly\nenhanced the automation level of detection processes. However, conventional\nsemantic segmentation and object detection models heavily rely on large-scale\nannotated datasets, which conflicts with the practical requirements of defect\ndetection tasks. This paper proposes a novel weakly supervised semantic\nsegmentation framework comprising two key components: a region-aware class\nactivation map (CAM) and pseudo-label training. To address the limitations of\nexisting CAM methods, especially low-resolution thermal maps, and insufficient\ndetail preservation, we introduce filtering-guided backpropagation (FGBP),\nwhich refines target regions by filtering gradient magnitudes to identify areas\nwith higher relevance to defects. Building upon this, we further develop a\nregion-aware weighted module to enhance spatial precision. Finally,\npseudo-label segmentation is implemented to refine the model's performance\niteratively. Comprehensive experiments on industrial defect datasets\ndemonstrate the superiority of our method. The proposed framework effectively\nbridges the gap between weakly supervised learning and high-precision defect\nsegmentation, offering a practical solution for resource-constrained industrial\nscenarios.", "AI": {"tldr": "A novel weakly supervised semantic segmentation framework for industrial defect detection, using region-aware CAM and pseudo-label training, outperforms existing methods.", "motivation": "Addressing the reliance on large annotated datasets in defect detection by proposing a weakly supervised approach.", "method": "Combines filtering-guided backpropagation (FGBP) for refined target regions and a region-aware weighted module for spatial precision, followed by pseudo-label training.", "result": "Superior performance on industrial defect datasets, bridging the gap between weak supervision and high-precision segmentation.", "conclusion": "The framework offers a practical solution for defect detection in resource-constrained settings."}}
{"id": "2506.23340", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23340", "abs": "https://arxiv.org/abs/2506.23340", "authors": ["Yumeng Lin", "Xufeng Duan", "David Haslett", "Yige Chen", "Zhenguang G. Cai"], "title": "Information Loss in LLMs' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family", "comment": null, "summary": "Large language models have achieved impressive progress in multilingual\ntranslation, yet they continue to face challenges with certain language\npairs-particularly those with limited training data or significant linguistic\ndivergence from English. This study systematically investigates how training\ndata, language proximity, and language family affect information loss in\nmultilingual translation. We evaluate two large language models, GPT-4 and\nLlama 2, by performing round-trip translations. Translation quality was\nassessed using BLEU scores and BERT similarity metrics. Our results reveal a\nrobust interaction between training data size and language distance: while\nabundant training data can mitigate the effects of linguistic divergence,\nlanguages structurally closer to English consistently yield higher translation\nquality in low-resource conditions. Among various distance metrics,\northographic, phylogenetic, syntactic, and geographical distances emerge as\nstrong predictors of translation performance. Language family also exerts an\nindependent influence. These findings contribute to a deeper understanding of\nthe linguistic constraints shaping multilingual translation in large language\nmodels, emphasizing that translation quality is shaped not only by data volume\nbut also by structural and typological relationships between languages.", "AI": {"tldr": "The study examines how training data, language proximity, and family affect multilingual translation quality in GPT-4 and Llama 2, revealing data size and linguistic distance as key factors.", "motivation": "To understand challenges in multilingual translation, especially for low-resource or linguistically distant languages, and identify factors influencing translation quality.", "method": "Evaluated GPT-4 and Llama 2 using round-trip translations, assessed with BLEU scores and BERT similarity metrics.", "result": "Found that abundant training data mitigates linguistic divergence, but languages closer to English perform better in low-resource settings. Orthographic, phylogenetic, syntactic, and geographical distances predict performance.", "conclusion": "Translation quality depends on both data volume and structural/typological language relationships, highlighting linguistic constraints in multilingual models."}}
{"id": "2506.23055", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23055", "abs": "https://arxiv.org/abs/2506.23055", "authors": ["Hiro Taiyo Hamada", "Ippei Fujisawa", "Genji Kawakita", "Yuki Yamada"], "title": "Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis", "comment": null, "summary": "Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities\nin producing human-like text. However, it is unclear how accurately these\nmodels internalize concepts that shape human thought and behavior. Here, we\ndeveloped a quantitative framework to assess concept alignment between LLMs and\nhuman psychological dimensions using 43 standardized psychological\nquestionnaires, selected for their established validity in measuring distinct\npsychological constructs. Our method evaluates how accurately language models\nreconstruct and classify questionnaire items through pairwise similarity\nanalysis. We compared resulting cluster structures with the original\ncategorical labels using hierarchical clustering. A GPT-4 model achieved\nsuperior classification accuracy (66.2\\%), significantly outperforming GPT-3.5\n(55.9\\%) and BERT (48.1\\%), all exceeding random baseline performance (31.9\\%).\nWe also demonstrated that the estimated semantic similarity from GPT-4 is\nassociated with Pearson's correlation coefficients of human responses in\nmultiple psychological questionnaires. This framework provides a novel approach\nto evaluate the alignment of the human-LLM concept and identify potential\nrepresentational biases. Our findings demonstrate that modern LLMs can\napproximate human psychological constructs with measurable accuracy, offering\ninsights for developing more interpretable AI systems.", "AI": {"tldr": "The paper evaluates how well LLMs like GPT-4 align with human psychological concepts using standardized questionnaires, showing GPT-4 outperforms others in classification accuracy.", "motivation": "To assess whether LLMs internalize human psychological concepts accurately, given their human-like text generation capabilities.", "method": "A quantitative framework using 43 psychological questionnaires, analyzing pairwise similarity and hierarchical clustering to compare LLM outputs with human constructs.", "result": "GPT-4 achieved 66.2% classification accuracy, outperforming GPT-3.5 (55.9%) and BERT (48.1%), and showed correlation with human responses.", "conclusion": "Modern LLMs can approximate human psychological constructs, offering insights for more interpretable AI systems."}}
{"id": "2506.23563", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23563", "abs": "https://arxiv.org/abs/2506.23563", "authors": ["Huanjin Yao", "Jiaxing Huang", "Yawen Qiu", "Michael K. Chen", "Wenzheng Liu", "Wei Zhang", "Wenjie Zeng", "Xikun Zhang", "Jingyi Zhang", "Yuxin Song", "Wenhao Wu", "Dacheng Tao"], "title": "MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI", "comment": "Technical report", "summary": "Reasoning plays a crucial role in advancing Multimodal Large Language Models\n(MLLMs) toward Artificial General Intelligence. However, existing MLLM\nbenchmarks often fall short in precisely and comprehensively evaluating\nlong-chain reasoning abilities from three key aspects: (1) lack of difficulty\nand diversity, (2) susceptibility to guessability and memorization, (3)\ninadequate assessment of intermediate reasoning steps. To fill this gap, we\nintroduce MMReason, a new benchmark designed to precisely and comprehensively\nevaluate MLLM long-chain reasoning capability with diverse, open-ended,\nchallenging questions. First, we curate challenging questions requiring\nmulti-step reasoning from various fields (i.e., 6 disciplines) and multiple\ndifficulty levels (i.e., from pre-university to university, and from\nfoundational to competition tiers). Second, these questions are reformulated\ninto an open-ended format and filtered using a multi-model voting technique to\neliminate shortcut cases related to guessing and memorization, ensuring robust\nreasoning evaluations. Third, we annotate the questions with detailed\nstep-by-step solutions, and design a reference-based ternary scoring mechanism\nto reliably assess intermediate reasoning steps. With MMReason, we benchmark\npopular leading MLLMs and provide an in-depth analysis of their reasoning\ncapabilities. We hope MMReason will serve as a valuable resource for advancing\nMLLM reasoning research. Code will be available at\nhttps://github.com/HJYao00/MMReason.", "AI": {"tldr": "MMReason is a new benchmark for evaluating long-chain reasoning in Multimodal Large Language Models (MLLMs), addressing gaps in difficulty, diversity, guessability, and intermediate reasoning assessment.", "motivation": "Existing MLLM benchmarks lack precision in evaluating long-chain reasoning due to insufficient difficulty, diversity, and susceptibility to shortcuts like guessing or memorization.", "method": "MMReason curates diverse, challenging questions from 6 disciplines and multiple difficulty levels, reformulates them into open-ended formats, filters shortcuts via multi-model voting, and annotates step-by-step solutions with a ternary scoring mechanism.", "result": "The benchmark evaluates popular MLLMs, providing insights into their reasoning capabilities.", "conclusion": "MMReason aims to advance MLLM reasoning research by offering a robust evaluation tool."}}
{"id": "2506.22868", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22868", "abs": "https://arxiv.org/abs/2506.22868", "authors": ["Junsung Lee", "Junoh Kang", "Bohyung Han"], "title": "STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing", "comment": "15 pages, 9 figures, 3 tables", "summary": "Previous text-guided video editing methods often suffer from temporal\ninconsistency, motion distortion, and-most notably-limited domain\ntransformation. We attribute these limitations to insufficient modeling of\nspatiotemporal pixel relevance during the editing process. To address this, we\npropose STR-Match, a training-free video editing algorithm that produces\nvisually appealing and spatiotemporally coherent videos through latent\noptimization guided by our novel STR score. The score captures spatiotemporal\npixel relevance across adjacent frames by leveraging 2D spatial attention and\n1D temporal modules in text-to-video (T2V) diffusion models, without the\noverhead of computationally expensive 3D attention mechanisms. Integrated into\na latent optimization framework with a latent mask, STR-Match generates\ntemporally consistent and visually faithful videos, maintaining strong\nperformance even under significant domain transformations while preserving key\nvisual attributes of the source. Extensive experiments demonstrate that\nSTR-Match consistently outperforms existing methods in both visual quality and\nspatiotemporal consistency.", "AI": {"tldr": "STR-Match is a training-free video editing method using latent optimization guided by a novel STR score to improve spatiotemporal consistency and visual quality.", "motivation": "Addressing limitations like temporal inconsistency, motion distortion, and limited domain transformation in text-guided video editing by modeling spatiotemporal pixel relevance.", "method": "Proposes STR-Match, leveraging 2D spatial attention and 1D temporal modules in T2V diffusion models for latent optimization with a latent mask.", "result": "Outperforms existing methods in visual quality and spatiotemporal consistency, even under significant domain transformations.", "conclusion": "STR-Match effectively enhances video editing by ensuring coherence and preserving source attributes without expensive 3D attention."}}
{"id": "2506.23342", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23342", "abs": "https://arxiv.org/abs/2506.23342", "authors": ["Akim Tsvigun", "Daniil Vasilev", "Ivan Tsvigun", "Ivan Lysenko", "Talgat Bektleuov", "Aleksandr Medvedev", "Uliana Vinogradova", "Nikita Severin", "Mikhail Mozikov", "Andrey Savchenko", "Rostislav Grigorev", "Ramil Kuleev", "Fedor Zhdanov", "Artem Shelmanov", "Ilya Makarov"], "title": "ATGen: A Framework for Active Text Generation", "comment": "Accepted at ACL 2025 System Demonstrations", "summary": "Active learning (AL) has demonstrated remarkable potential in reducing the\nannotation effort required for training machine learning models. However,\ndespite the surging popularity of natural language generation (NLG) tasks in\nrecent years, the application of AL to NLG has been limited. In this paper, we\nintroduce Active Text Generation (ATGen) - a comprehensive framework that\nbridges AL with text generation tasks, enabling the application of\nstate-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered\nannotation in NLG tasks using both human annotators and automatic annotation\nagents based on large language models (LLMs). The framework supports LLMs\ndeployed as services, such as ChatGPT and Claude, or operated on-premises.\nFurthermore, ATGen provides a unified platform for smooth implementation and\nbenchmarking of novel AL strategies tailored to NLG tasks. Finally, we present\nevaluation results for state-of-the-art AL strategies across diverse settings\nand multiple text generation tasks. We show that ATGen reduces both the effort\nof human annotators and costs associated with API calls to LLM-based annotation\nagents. The code of the framework is available on GitHub under the MIT license.\nThe video presentation is available at http://atgen-video.nlpresearch.group", "AI": {"tldr": "ATGen bridges active learning (AL) with text generation tasks, reducing annotation effort and costs using human annotators and LLMs.", "motivation": "Limited application of AL to natural language generation (NLG) tasks despite their popularity.", "method": "ATGen framework integrates AL strategies with NLG, supporting human annotators and LLMs (e.g., ChatGPT, Claude).", "result": "Reduces human annotation effort and LLM API costs; provides a platform for benchmarking AL strategies in NLG.", "conclusion": "ATGen successfully applies AL to NLG, offering a practical and cost-effective solution."}}
{"id": "2506.23068", "categories": ["cs.LG", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2506.23068", "abs": "https://arxiv.org/abs/2506.23068", "authors": ["Zhiyu Zhao", "Haoxuan Li", "Haifeng Zhang", "Jun Wang", "Francesco Faccio", "J\u00fcrgen Schmidhuber", "Mengyue Yang"], "title": "Curious Causality-Seeking Agents Learn Meta Causal World", "comment": "33 pages", "summary": "When building a world model, a common assumption is that the environment has\na single, unchanging underlying causal rule, like applying Newton's laws to\nevery situation. In reality, what appears as a drifting causal mechanism is\noften the manifestation of a fixed underlying mechanism seen through a narrow\nobservational window. This brings about a problem that, when building a world\nmodel, even subtle shifts in policy or environment states can alter the very\nobserved causal mechanisms. In this work, we introduce the \\textbf{Meta-Causal\nGraph} as world models, a minimal unified representation that efficiently\nencodes the transformation rules governing how causal structures shift across\ndifferent latent world states. A single Meta-Causal Graph is composed of\nmultiple causal subgraphs, each triggered by meta state, which is in the latent\nstate space. Building on this representation, we introduce a\n\\textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta\nstates that trigger each subgraph, (2) discover the corresponding causal\nrelationships by agent curiosity-driven intervention policy, and (3)\niteratively refine the Meta-Causal Graph through ongoing curiosity-driven\nexploration and agent experiences. Experiments on both synthetic tasks and a\nchallenging robot arm manipulation task demonstrate that our method robustly\ncaptures shifts in causal dynamics and generalizes effectively to previously\nunseen contexts.", "AI": {"tldr": "The paper introduces a Meta-Causal Graph for world modeling, addressing shifts in observed causal mechanisms due to policy or environment changes. A Causality-Seeking Agent uses curiosity-driven exploration to refine this graph.", "motivation": "Real-world environments often exhibit shifting causal mechanisms, challenging traditional world models that assume fixed rules. This work aims to model and adapt to these shifts.", "method": "Proposes a Meta-Causal Graph with causal subgraphs triggered by latent meta states. A Causality-Seeking Agent identifies meta states, discovers causal relationships, and refines the graph through curiosity-driven interventions.", "result": "Experiments show the method effectively captures causal dynamics shifts and generalizes to unseen contexts in synthetic and robot arm tasks.", "conclusion": "The Meta-Causal Graph and Causality-Seeking Agent provide a robust framework for adaptive world modeling in dynamic environments."}}
{"id": "2506.23576", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23576", "abs": "https://arxiv.org/abs/2506.23576", "authors": ["Maria Carolina Cornelia Wit", "Jun Pang"], "title": "Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models", "comment": "26 pages, 1 figure", "summary": "Recent advances in large language models (LLMs) have raised concerns about\njailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper\ninvestigates the use of multi-agent LLM systems as a defence against such\nattacks. We evaluate three jailbreaking strategies, including the original\nAutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the\nAutoDefense framework, we compare single-agent setups with two- and three-agent\nconfigurations. Our results show that multi-agent systems enhance resistance to\njailbreaks, especially by reducing false negatives. However, its effectiveness\nvaries by attack type, and it introduces trade-offs such as increased false\npositives and computational overhead. These findings point to the limitations\nof current automated defences and suggest directions for improving alignment\nrobustness in future LLM systems.", "AI": {"tldr": "Multi-agent LLM systems improve resistance to jailbreaking attacks but come with trade-offs like higher false positives and computational costs.", "motivation": "Address concerns about jailbreaking attacks on LLMs by exploring multi-agent systems as a defence mechanism.", "method": "Evaluated three jailbreaking strategies (AutoDefense, BetterDan, JB) using single-agent, two-agent, and three-agent configurations.", "result": "Multi-agent systems reduce false negatives but increase false positives and computational overhead; effectiveness varies by attack type.", "conclusion": "Current automated defences have limitations; multi-agent systems show promise but need refinement for better alignment robustness."}}
{"id": "2506.22880", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22880", "abs": "https://arxiv.org/abs/2506.22880", "authors": ["Dang Jisheng", "Wu Xudong", "Wang Bimei", "Lv Ning", "Chen Jiayu", "Jingwen Zhao", "Yichu liu", "Jizhao Liu", "Juncheng Li", "Teng Wang"], "title": "Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and Grounder", "comment": null, "summary": "Existing video segmenter and grounder approaches, exemplified by Sa2VA,\ndirectly fuse features within segmentation models. This often results in an\nundesirable entanglement of dynamic visual information and static semantics,\nthereby degrading segmentation accuracy. To systematically mitigate this issue,\nwe propose DeSa2VA, a decoupling-enhanced prompting scheme integrating text\npre-training and a linear decoupling module to address the information\nprocessing limitations inherent in SAM-2. Specifically, first, we devise a\npre-training paradigm that converts textual ground-truth labels into\npoint-level prompts while generating corresponding text masks. These masks are\nrefined through a hybrid loss function to strengthen the model's semantic\ngrounding capabilities. Next, we employ linear projection to disentangle hidden\nstates that generated by a large language model into distinct textual and\nvisual feature subspaces. Finally, a dynamic mask fusion strategy\nsynergistically combines these decoupled features through triple supervision\nfrom predicted text/visual masks and ground-truth annotations. Extensive\nexperiments demonstrate state-of-the-art performance across diverse tasks,\nincluding image segmentation, image question answering, video segmentation, and\nvideo question answering. Our codes are available at\nhttps://github.com/longmalongma/DeSa2VA.", "AI": {"tldr": "DeSa2VA introduces a decoupling-enhanced prompting scheme to improve segmentation accuracy by disentangling visual and textual features, outperforming existing methods in multiple tasks.", "motivation": "Existing methods like Sa2VA entangle dynamic visual and static semantic features, degrading segmentation accuracy. DeSa2VA aims to systematically mitigate this issue.", "method": "DeSa2VA uses text pre-training, a linear decoupling module, and dynamic mask fusion to disentangle and synergistically combine textual and visual features.", "result": "The method achieves state-of-the-art performance in image/video segmentation and question answering tasks.", "conclusion": "DeSa2VA effectively addresses feature entanglement, enhancing segmentation accuracy and semantic grounding."}}
{"id": "2506.23377", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23377", "abs": "https://arxiv.org/abs/2506.23377", "authors": ["Taejin Kim", "Siun-Chuon Mau", "Konrad Vesey"], "title": "Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs", "comment": "7 pages, 5 main pages of text, 5 figures, 2 tables. Research work\n  performed at CACI INTL INC", "summary": "Large language models (LLMs) are used in a variety of mission-critical roles.\nDue to the rapidly developing nature of LLMs, there is a lack of quantifiable\nunderstanding of the bias and perspective associated with LLM output. Inspired\nby this need, this paper considers the broader issue of perspective or\nviewpoint of general text and perspective control of large-language model (LLM)\noutput. Perspective-Dial consists of two main components: a (1) metric space,\ndubbed Perspective Space, that enables quantitative measurements of different\nperspectives regarding a topic, and the use of (2) Systematic Prompt\nEngineering that utilizes greedy-coordinate descent to control LLM output\nperspective based on measurement feedback from the Perspective Space. The\nempirical nature of the approach allows progress to side step a principled\nunderstanding of perspective or bias -- effectively quantifying and adjusting\noutputs for a variety of topics. Potential applications include detection,\ntracking and mitigation of LLM bias, narrative detection, sense making and\ntracking in public discourse, and debate bot advocating given perspective.", "AI": {"tldr": "The paper introduces Perspective-Dial, a method to quantify and control bias and perspective in LLM outputs using a metric space and systematic prompt engineering.", "motivation": "There's a lack of quantifiable understanding of bias and perspective in LLM outputs, which are increasingly used in critical roles.", "method": "Uses Perspective Space for measurement and Systematic Prompt Engineering with greedy-coordinate descent to control LLM output perspective.", "result": "Empirically quantifies and adjusts LLM outputs for various topics, enabling applications like bias detection and narrative tracking.", "conclusion": "Perspective-Dial provides a practical approach to manage LLM output perspectives without needing a deep theoretical understanding of bias."}}
{"id": "2506.23145", "categories": ["cs.LG", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23145", "abs": "https://arxiv.org/abs/2506.23145", "authors": ["Shahad Hardan", "Darya Taratynova", "Abdelmajid Essofi", "Karthik Nandakumar", "Mohammad Yaqub"], "title": "Forget-MI: Machine Unlearning for Forgetting Multimodal Information in Healthcare Settings", "comment": null, "summary": "Privacy preservation in AI is crucial, especially in healthcare, where models\nrely on sensitive patient data. In the emerging field of machine unlearning,\nexisting methodologies struggle to remove patient data from trained multimodal\narchitectures, which are widely used in healthcare. We propose Forget-MI, a\nnovel machine unlearning method for multimodal medical data, by establishing\nloss functions and perturbation techniques. Our approach unlearns unimodal and\njoint representations of the data requested to be forgotten while preserving\nknowledge from the remaining data and maintaining comparable performance to the\noriginal model. We evaluate our results using performance on the forget\ndataset, performance on the test dataset, and Membership Inference Attack\n(MIA), which measures the attacker's ability to distinguish the forget dataset\nfrom the training dataset. Our model outperforms the existing approaches that\naim to reduce MIA and the performance on the forget dataset while keeping an\nequivalent performance on the test set. Specifically, our approach reduces MIA\nby 0.202 and decreases AUC and F1 scores on the forget set by 0.221 and 0.305,\nrespectively. Additionally, our performance on the test set matches that of the\nretrained model, while allowing forgetting. Code is available at\nhttps://github.com/BioMedIA-MBZUAI/Forget-MI.git", "AI": {"tldr": "Forget-MI is a novel machine unlearning method for multimodal medical data, improving privacy by removing sensitive data while maintaining model performance.", "motivation": "Privacy in AI, especially healthcare, is critical, but existing methods struggle to remove data from multimodal models.", "method": "Forget-MI uses loss functions and perturbation techniques to unlearn unimodal and joint representations of forgotten data.", "result": "Reduces MIA by 0.202, decreases AUC/F1 on forget set by 0.221/0.305, and matches test set performance of retrained models.", "conclusion": "Forget-MI effectively unlearns sensitive data while preserving model utility, outperforming existing approaches."}}
{"id": "2506.23626", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23626", "abs": "https://arxiv.org/abs/2506.23626", "authors": ["Ant\u00f3nio Afonso", "Iolanda Leite", "Alessandro Sestini", "Florian Fuchs", "Konrad Tollmar", "Linus Gissl\u00e9n"], "title": "Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games", "comment": "16 pages in total, 10 pages of main paper, 5 figures", "summary": "Reinforcement Learning (RL) in games has gained significant momentum in\nrecent years, enabling the creation of different agent behaviors that can\ntransform a player's gaming experience. However, deploying RL agents in\nproduction environments presents two key challenges: (1) designing an effective\nreward function typically requires an RL expert, and (2) when a game's content\nor mechanics are modified, previously tuned reward weights may no longer be\noptimal. Towards the latter challenge, we propose an automated approach for\niteratively fine-tuning an RL agent's reward function weights, based on a\nuser-defined language based behavioral goal. A Language Model (LM) proposes\nupdated weights at each iteration based on this target behavior and a summary\nof performance statistics from prior training rounds. This closed-loop process\nallows the LM to self-correct and refine its output over time, producing\nincreasingly aligned behavior without the need for manual reward engineering.\nWe evaluate our approach in a racing task and show that it consistently\nimproves agent performance across iterations. The LM-guided agents show a\nsignificant increase in performance from $9\\%$ to $74\\%$ success rate in just\none iteration. We compare our LM-guided tuning against a human expert's manual\nweight design in the racing task: by the final iteration, the LM-tuned agent\nachieved an $80\\%$ success rate, and completed laps in an average of $855$ time\nsteps, a competitive performance against the expert-tuned agent's peak $94\\%$\nsuccess, and $850$ time steps.", "AI": {"tldr": "An automated method using a Language Model (LM) to iteratively fine-tune RL agent reward functions, improving performance without manual engineering.", "motivation": "Addressing the challenge of RL reward function tuning when game mechanics change, reducing reliance on experts.", "method": "LM proposes updated reward weights based on behavioral goals and performance stats, enabling self-correction over iterations.", "result": "LM-guided agents improved from 9% to 74% success in one iteration, reaching 80% success and 855 time steps, competing with expert-tuned agents.", "conclusion": "The LM-based approach effectively automates reward tuning, achieving competitive performance without manual intervention."}}
{"id": "2506.22881", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22881", "abs": "https://arxiv.org/abs/2506.22881", "authors": ["Fumiya Uchiyama", "Rintaro Yanagi", "Shohei Taniguchi", "Shota Takashiro", "Masahiro Suzuki", "Hirokatsu Kataoka", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "How Semantically Informative is an Image?: Measuring the Covariance-Weighted Norm of Contrastive Learning Embeddings", "comment": null, "summary": "Contrastive learning has the capacity to model multimodal probability\ndistributions by embedding and aligning visual representations with semantics\nfrom captions. This approach enables the estimation of relational semantic\nsimilarity; however, it remains unclear whether it can also represent absolute\nsemantic informativeness. In this work, we introduce a semantic informativeness\nmetric for an image calculated from text samples via a contrastive learning\nmodel; similarly, the informativeness of a text is calculated from image\nsamples. We propose a redefinition of the concept of Information Gain, a\nconcept previously explored in natural language processing, extending its\napplication to the domains of vision and language. Our metric quantifies how\nconditioning on an image distorts the distribution of associated texts, and\nvice versa for text conditioning on image distributions. In OpenCLIP's\nempirical results, we observe that images with the lowest Information Gain\nscores often correspond to placeholder icons such as \"image not found.\"\nFurthermore, we propose to measure a norm-based metric of the embedding to\nestimate the Information Gain, following the theoretical results for Skip-Gram\nwith Negative Sampling (SGNS) word embedding. Information Gain can be measured\nusing either CLIP or SigLIP, and the results demonstrate a strong correlation\nwith a coefficient of determination ranging from 0.98 to 1.00. After obtaining\nthe mean and the covariance of the sample embedding, the computational cost of\nthis method is independent of the sample size, and it is compatible with\npublicly available, open-weight models.", "AI": {"tldr": "The paper introduces a metric for semantic informativeness in multimodal contrastive learning, extending Information Gain to vision and language, and validates it with strong empirical results.", "motivation": "To address the gap in representing absolute semantic informativeness in contrastive learning models, despite their success in relational similarity.", "method": "Proposes a semantic informativeness metric using contrastive learning, redefines Information Gain for vision and language, and validates it with OpenCLIP and SigLIP models.", "result": "Strong correlation (0.98-1.00) between the proposed metric and empirical results, with low computational cost and compatibility with open-weight models.", "conclusion": "The method effectively quantifies semantic informativeness, offering practical applications in multimodal learning with scalable and efficient computation."}}
{"id": "2506.23393", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23393", "abs": "https://arxiv.org/abs/2506.23393", "authors": ["Eugene J. Yu", "Dawei Zhu", "Yifan Song", "Xiangyu Wong", "Jiebin Zhang", "Wenxuan Shi", "Xiaoguang Li", "Qun Liu", "Sujian Li"], "title": "Hierarchical Memory Organization for Wikipedia Generation", "comment": "ACL 2025 Main Conference", "summary": "Generating Wikipedia articles autonomously is a challenging task requiring\nthe integration of accurate, comprehensive, and well-structured information\nfrom diverse sources. This paper introduces the Memory Organization-based\nGeneration (MOG) framework, a novel approach to address these challenges by\nleveraging a hierarchical memory architecture. MOG extracts fine-grained memory\nunits from web documents, recursively organizes them into a Wikipedia-style\nhierarchical structure, and uses this structure to guide the generation\nprocess. This ensures alignment between memory and the article outline,\nimproving both informativeness and verifiability while minimizing\nhallucinations. Additionally, a citation module is implemented to enhance\ntraceability by linking every generated sentence to specific memory units.\nEvaluations on our newly created WikiStart dataset demonstrate that MOG\noutperforms baseline methods in producing informative and reliable articles,\nmaking it particularly robust in real-world scenarios.", "AI": {"tldr": "The paper introduces the MOG framework for autonomously generating Wikipedia articles by organizing fine-grained memory units hierarchically, improving informativeness and verifiability.", "motivation": "To address the challenge of integrating accurate and comprehensive information from diverse sources for Wikipedia article generation.", "method": "Proposes the Memory Organization-based Generation (MOG) framework, which extracts and hierarchically organizes memory units from web documents to guide article generation, including a citation module for traceability.", "result": "MOG outperforms baseline methods on the WikiStart dataset, producing more informative and reliable articles with minimized hallucinations.", "conclusion": "MOG is effective for real-world Wikipedia article generation, ensuring alignment between memory and article structure while enhancing traceability."}}
{"id": "2506.23147", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23147", "abs": "https://arxiv.org/abs/2506.23147", "authors": ["Jonathan Schuster", "Fabian Transchel"], "title": "maneuverRecognition -- A Python package for Timeseries Classification in the domain of Vehicle Telematics", "comment": "6 pages, 2 figures", "summary": "In the domain of vehicle telematics the automated recognition of driving\nmaneuvers is used to classify and evaluate driving behaviour. This not only\nserves as a component to enhance the personalization of insurance policies, but\nalso to increase road safety, reduce accidents and the associated costs as well\nas to reduce fuel consumption and support environmentally friendly driving. In\nthis context maneuver recognition technically requires a continuous application\nof time series classification which poses special challenges to the transfer,\npreprocessing and storage of telematic sensor data, the training of predictive\nmodels, and the prediction itself. Although much research has been done in the\nfield of gathering relevant data or regarding the methods to build predictive\nmodels for the task of maneuver recognition, there is a practical need for\npython packages and functions that allow to quickly transform data into the\nrequired structure as well as to build and evaluate such models. The\nmaneuverRecognition package was therefore developed to provide the necessary\nfunctions for preprocessing, modelling and evaluation and also includes a ready\nto use LSTM based network structure that can be modified. The implementation of\nthe package is demonstrated using real driving data of three different persons\nrecorded via smartphone sensors.", "AI": {"tldr": "The paper introduces the maneuverRecognition package for automating driving maneuver recognition in vehicle telematics, addressing data preprocessing, modeling, and evaluation challenges.", "motivation": "To enhance road safety, reduce accidents and costs, and support eco-friendly driving by improving maneuver recognition in vehicle telematics.", "method": "Developed a Python package (maneuverRecognition) for preprocessing, modeling, and evaluating driving data, including an LSTM-based network structure.", "result": "Demonstrated the package's effectiveness using real driving data from smartphone sensors of three individuals.", "conclusion": "The maneuverRecognition package provides a practical solution for maneuver recognition, facilitating efficient data handling and model development."}}
{"id": "2506.23673", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23673", "abs": "https://arxiv.org/abs/2506.23673", "authors": ["Jingsong Liu", "Han Li", "Chen Yang", "Michael Deutges", "Ario Sadafi", "Xin You", "Katharina Breininger", "Nassir Navab", "Peter J. Sch\u00fcffler"], "title": "HASD: Hierarchical Adaption for pathology Slide-level Domain-shift", "comment": null, "summary": "Domain shift is a critical problem for pathology AI as pathology data is\nheavily influenced by center-specific conditions. Current pathology domain\nadaptation methods focus on image patches rather than WSI, thus failing to\ncapture global WSI features required in typical clinical scenarios. In this\nwork, we address the challenges of slide-level domain shift by proposing a\nHierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD\nachieves multi-scale feature consistency and computationally efficient\nslide-level domain adaptation through two key components: (1) a hierarchical\nadaptation framework that integrates a Domain-level Alignment Solver for\nfeature alignment, a Slide-level Geometric Invariance Regularization to\npreserve the morphological structure, and a Patch-level Attention Consistency\nRegularization to maintain local critical diagnostic cues; and (2) a prototype\nselection mechanism that reduces computational overhead. We validate our method\non two slide-level tasks across five datasets, achieving a 4.1\\% AUROC\nimprovement in a Breast Cancer HER2 Grading cohort and a 3.9\\% C-index gain in\na UCEC survival prediction cohort. Our method provides a practical and reliable\nslide-level domain adaption solution for pathology institutions, minimizing\nboth computational and annotation costs.", "AI": {"tldr": "Proposes HASD, a hierarchical framework for slide-level domain adaptation in pathology AI, improving performance and reducing computational costs.", "motivation": "Addresses the challenge of domain shift in pathology AI, focusing on whole slide images (WSI) rather than patches, which is critical for clinical applications.", "method": "Uses a hierarchical adaptation framework with domain-level alignment, slide-level geometric invariance, and patch-level attention consistency, along with a prototype selection mechanism.", "result": "Achieves 4.1% AUROC improvement in Breast Cancer HER2 Grading and 3.9% C-index gain in UCEC survival prediction.", "conclusion": "HASD provides a practical, efficient solution for slide-level domain adaptation in pathology, reducing computational and annotation costs."}}
{"id": "2506.22890", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.22890", "abs": "https://arxiv.org/abs/2506.22890", "authors": ["Senkang Hu", "Yihang Tao", "Guowen Xu", "Xinyuan Qian", "Yiqin Deng", "Xianhao Chen", "Sam Tak Wu Kwong", "Yuguang Fang"], "title": "CP-Guard: A Unified, Probability-Agnostic, and Adaptive Framework for Malicious Agent Detection and Defense in Multi-Agent Embodied Perception Systems", "comment": null, "summary": "Collaborative Perception (CP) has been shown to be a promising technique for\nmulti-agent autonomous driving and multi-agent robotic systems, where multiple\nagents share their perception information to enhance the overall perception\nperformance and expand the perception range. However, in CP, an ego agent needs\nto receive messages from its collaborators, which makes it vulnerable to\nattacks from malicious agents. To address this critical issue, we propose a\nunified, probability-agnostic, and adaptive framework, namely, CP-Guard, which\nis a tailored defense mechanism for CP deployed by each agent to accurately\ndetect and eliminate malicious agents in its collaboration network. Our key\nidea is to enable CP to reach a consensus rather than a conflict against an ego\nagent's perception results. Based on this idea, we first develop a\nprobability-agnostic sample consensus (PASAC) method to effectively sample a\nsubset of the collaborators and verify the consensus without prior\nprobabilities of malicious agents. Furthermore, we define collaborative\nconsistency loss (CCLoss) for object detection task and bird's eye view (BEV)\nsegmentation task to capture the discrepancy between an ego agent and its\ncollaborators, which is used as a verification criterion for consensus. In\naddition, we propose online adaptive threshold via dual sliding windows to\ndynamically adjust the threshold for consensus verification and ensure the\nreliability of the systems in dynamic environments. Finally, we conduct\nextensive experiments and demonstrate the effectiveness of our framework. Code\nwill be released at https://github.com/CP-Security/CP-Guard", "AI": {"tldr": "CP-Guard is a defense framework for Collaborative Perception (CP) that detects and eliminates malicious agents by ensuring consensus among collaborators, using PASAC, CCLoss, and adaptive thresholds.", "motivation": "CP is vulnerable to attacks from malicious agents, necessitating a robust defense mechanism to maintain system reliability.", "method": "Develops PASAC for sampling and consensus verification, defines CCLoss for discrepancy measurement, and uses adaptive thresholds for dynamic environments.", "result": "Demonstrates effectiveness in detecting and eliminating malicious agents, ensuring reliable CP performance.", "conclusion": "CP-Guard provides a unified, adaptive solution to secure CP systems against malicious attacks."}}
{"id": "2506.23411", "categories": ["cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23411", "abs": "https://arxiv.org/abs/2506.23411", "authors": ["Jiale Zhang", "Zichong Wang", "Avash Palikhe", "Zhipeng Yin", "Wenbin Zhang"], "title": "Datasets for Fairness in Language Models: An In-Depth Survey", "comment": null, "summary": "Fairness benchmarks play a central role in shaping how we evaluate language\nmodels, yet surprisingly little attention has been given to examining the\ndatasets that these benchmarks rely on. This survey addresses that gap by\npresenting a broad and careful review of the most widely used fairness datasets\nin current language model research, characterizing them along several key\ndimensions including their origin, scope, content, and intended use to help\nresearchers better appreciate the assumptions and limitations embedded in these\nresources. To support more meaningful comparisons and analyses, we introduce a\nunified evaluation framework that reveals consistent patterns of demographic\ndisparities across datasets and scoring methods. Applying this framework to\ntwenty four common benchmarks, we highlight the often overlooked biases that\ncan influence conclusions about model fairness and offer practical guidance for\nselecting, combining, and interpreting these datasets. We also point to\nopportunities for creating new fairness benchmarks that reflect more diverse\nsocial contexts and encourage more thoughtful use of these tools going forward.\nAll code, data, and detailed results are publicly available at\nhttps://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets\nto promote transparency and reproducibility across the research community.", "AI": {"tldr": "A survey reviewing fairness datasets in language model research, introducing a unified evaluation framework to highlight biases and guide dataset selection.", "motivation": "Address the lack of attention to fairness datasets in benchmarks, which shape evaluations of language models.", "method": "Review and characterize widely used fairness datasets, introduce a unified evaluation framework, and apply it to 24 benchmarks.", "result": "Reveals demographic disparities and overlooked biases, offering guidance for dataset use and interpretation.", "conclusion": "Encourages more thoughtful use of fairness datasets and creation of diverse benchmarks; provides open resources for transparency."}}
{"id": "2506.23165", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2506.23165", "abs": "https://arxiv.org/abs/2506.23165", "authors": ["David Bossens", "Atsushi Nitanda"], "title": "Mirror Descent Policy Optimisation for Robust Constrained Markov Decision Processes", "comment": null, "summary": "Safety is an essential requirement for reinforcement learning systems. The\nnewly emerging framework of robust constrained Markov decision processes allows\nlearning policies that satisfy long-term constraints while providing guarantees\nunder epistemic uncertainty. This paper presents mirror descent policy\noptimisation for robust constrained Markov decision processes (RCMDPs), making\nuse of policy gradient techniques to optimise both the policy (as a maximiser)\nand the transition kernel (as an adversarial minimiser) on the Lagrangian\nrepresenting a constrained MDP. In the oracle-based RCMDP setting, we obtain an\n$\\mathcal{O}\\left(\\frac{1}{T}\\right)$ convergence rate for the squared distance\nas a Bregman divergence, and an $\\mathcal{O}\\left(e^{-T}\\right)$ convergence\nrate for entropy-regularised objectives. In the sample-based RCMDP setting, we\nobtain an $\\tilde{\\mathcal{O}}\\left(\\frac{1}{T^{1/3}}\\right)$ convergence rate.\nExperiments confirm the benefits of mirror descent policy optimisation in\nconstrained and unconstrained optimisation, and significant improvements are\nobserved in robustness tests when compared to baseline policy optimisation\nalgorithms.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.23689", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.23689", "abs": "https://arxiv.org/abs/2506.23689", "authors": ["Zihao Liu", "Xinhang Sui", "Yueran Song", "Siwen Wang"], "title": "Pok\u00e9AI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red", "comment": null, "summary": "We introduce Pok\\'eAI, the first text-based, multi-agent large language model\n(LLM) framework designed to autonomously play and progress through Pok\\'emon\nRed. Our system consists of three specialized agents-Planning, Execution, and\nCritique-each with its own memory bank, role, and skill set. The Planning Agent\nfunctions as the central brain, generating tasks to progress through the game.\nThese tasks are then delegated to the Execution Agent, which carries them out\nwithin the game environment. Upon task completion, the Critique Agent evaluates\nthe outcome to determine whether the objective was successfully achieved. Once\nverification is complete, control returns to the Planning Agent, forming a\nclosed-loop decision-making system.\n  As a preliminary step, we developed a battle module within the Execution\nAgent. Our results show that the battle AI achieves an average win rate of\n80.8% across 50 wild encounters, only 6% lower than the performance of an\nexperienced human player. Furthermore, we find that a model's battle\nperformance correlates strongly with its LLM Arena score on language-related\ntasks, indicating a meaningful link between linguistic ability and strategic\nreasoning. Finally, our analysis of gameplay logs reveals that each LLM\nexhibits a unique playstyle, suggesting that individual models develop distinct\nstrategic behaviors.", "AI": {"tldr": "Pok\u00e9AI is a multi-agent LLM framework for autonomously playing Pok\u00e9mon Red, featuring Planning, Execution, and Critique agents. It achieves an 80.8% win rate in battles, with performance linked to linguistic ability.", "motivation": "To create an autonomous system for playing Pok\u00e9mon Red using specialized LLM agents, exploring the link between language models and strategic reasoning.", "method": "Three agents (Planning, Execution, Critique) work in a closed-loop system. The Execution Agent includes a battle module tested in wild encounters.", "result": "80.8% average win rate in battles, close to human performance. Battle performance correlates with LLM Arena scores, and models show unique playstyles.", "conclusion": "Pok\u00e9AI demonstrates effective autonomous gameplay, linking linguistic and strategic abilities, and reveals distinct model behaviors."}}
{"id": "2506.22899", "categories": ["cs.CV", "cs.GR", "cs.LG", "cs.MA", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.22899", "abs": "https://arxiv.org/abs/2506.22899", "authors": ["Ehsan Pajouheshgar", "Yitao Xu", "Ali Abbasi", "Alexander Mordvintsev", "Wenzel Jakob", "Sabine S\u00fcsstrunk"], "title": "Neural Cellular Automata: From Cells to Pixels", "comment": "6 pages, 5 figures, first draft", "summary": "Neural Cellular Automata (NCAs) are bio-inspired systems in which identical\ncells self-organize to form complex and coherent patterns by repeatedly\napplying simple local rules. NCAs display striking emergent behaviors including\nself-regeneration, generalization and robustness to unseen situations, and\nspontaneous motion. Despite their success in texture synthesis and\nmorphogenesis, NCAs remain largely confined to low-resolution grids. This\nlimitation stems from (1) training time and memory requirements that grow\nquadratically with grid size, (2) the strictly local propagation of information\nwhich impedes long-range cell communication, and (3) the heavy compute demands\nof real-time inference at high resolution. In this work, we overcome this\nlimitation by pairing NCA with a tiny, shared implicit decoder, inspired by\nrecent advances in implicit neural representations. Following NCA evolution on\na coarse grid, a lightweight decoder renders output images at arbitrary\nresolution. We also propose novel loss functions for both morphogenesis and\ntexture synthesis tasks, specifically tailored for high-resolution output with\nminimal memory and computation overhead. Combining our proposed architecture\nand loss functions brings substantial improvement in quality, efficiency, and\nperformance. NCAs equipped with our implicit decoder can generate full-HD\noutputs in real time while preserving their self-organizing, emergent\nproperties. Moreover, because each MLP processes cell states independently,\ninference remains highly parallelizable and efficient. We demonstrate the\napplicability of our approach across multiple NCA variants (on 2D, 3D grids,\nand 3D meshes) and multiple tasks, including texture generation and\nmorphogenesis (growing patterns from a seed), showing that with our proposed\nframework, NCAs seamlessly scale to high-resolution outputs with minimal\ncomputational overhead.", "AI": {"tldr": "The paper introduces an implicit decoder to scale Neural Cellular Automata (NCAs) to high-resolution grids, addressing training, memory, and computational limitations while preserving emergent properties.", "motivation": "NCAs are limited to low-resolution grids due to quadratic growth in training time, local information propagation, and high compute demands. The goal is to enable high-resolution outputs efficiently.", "method": "Pair NCA with a tiny shared implicit decoder for rendering high-resolution outputs. Introduce novel loss functions for morphogenesis and texture synthesis tailored for minimal overhead.", "result": "The proposed framework allows NCAs to generate full-HD outputs in real time, maintaining self-organizing properties and efficiency across 2D, 3D grids, and meshes.", "conclusion": "The implicit decoder and tailored loss functions enable NCAs to scale to high-resolution outputs with minimal computational overhead, enhancing quality and performance."}}
{"id": "2506.23423", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23423", "abs": "https://arxiv.org/abs/2506.23423", "authors": ["Felipe Nuti", "Tim Franzmeyer", "Jo\u00e3o Henriques"], "title": "TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs", "comment": "ICML 2025", "summary": "Past work has studied the effects of fine-tuning on large language models'\n(LLMs) overall performance on certain tasks. However, a quantitative and\nsystematic method for analyzing its effect on individual outputs is still\nlacking. Here, we propose a new method for measuring the contribution that\nfine-tuning makes to individual LLM responses, assuming access to the original\npre-trained model. Our method tracks the model's intermediate hidden states,\nproviding a more fine-grained insight into the effects of fine-tuning than a\nsimple comparison of final outputs from pre-trained and fine-tuned models. We\nintroduce and theoretically analyze an exact decomposition of any fine-tuned\nLLM into a pre-training component and a fine-tuning component. Empirically, we\nfind that model behavior and performance can be steered by up- or down-scaling\nthe fine-tuning component during the forward pass. Motivated by this finding\nand our theoretical analysis, we define the Tuning Contribution (TuCo) as the\nratio of the magnitudes of the fine-tuning component to the pre-training\ncomponent. We observe that three prominent adversarial attacks on LLMs\ncircumvent safety measures in a way that reduces TuCo, and that TuCo is\nconsistently lower on prompts where these attacks succeed compared to those\nwhere they do not. This suggests that attenuating the effect of fine-tuning on\nmodel outputs plays a role in the success of such attacks. In summary, TuCo\nenables the quantitative study of how fine-tuning influences model behavior and\nsafety, and vice versa.", "AI": {"tldr": "The paper introduces Tuning Contribution (TuCo), a method to measure fine-tuning's impact on individual LLM outputs by analyzing hidden states and decomposing models into pre-training and fine-tuning components.", "motivation": "Existing work lacks a systematic way to quantify fine-tuning's effect on individual LLM outputs, limiting understanding of its influence on model behavior and safety.", "method": "Proposes TuCo, which tracks hidden states and decomposes models into pre-training and fine-tuning components, allowing scaling of the fine-tuning effect during inference.", "result": "TuCo reveals that adversarial attacks reduce fine-tuning's influence, and successful attacks correlate with lower TuCo values.", "conclusion": "TuCo provides a quantitative tool to study fine-tuning's role in model behavior and safety, highlighting its importance in adversarial scenarios."}}
{"id": "2506.23174", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23174", "abs": "https://arxiv.org/abs/2506.23174", "authors": ["Chen Gong", "Bo Liang", "Wei Gao", "Chenren Xu"], "title": "Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data", "comment": "Published in MobiSys 2025", "summary": "Generative models have gained significant attention for their ability to\nproduce realistic synthetic data that supplements the quantity of real-world\ndatasets. While recent studies show performance improvements in wireless\nsensing tasks by incorporating all synthetic data into training sets, the\nquality of synthetic data remains unpredictable and the resulting performance\ngains are not guaranteed. To address this gap, we propose tractable and\ngeneralizable metrics to quantify quality attributes of synthetic data -\naffinity and diversity. Our assessment reveals prevalent affinity limitation in\ncurrent wireless synthetic data, leading to mislabeled data and degraded task\nperformance. We attribute the quality limitation to generative models' lack of\nawareness of untrained conditions and domain-specific processing. To mitigate\nthese issues, we introduce SynCheck, a quality-guided synthetic data\nutilization scheme that refines synthetic data quality during task model\ntraining. Our evaluation demonstrates that SynCheck consistently outperforms\nquality-oblivious utilization of synthetic data, and achieves 4.3% performance\nimprovement even when the previous utilization degrades performance by 13.4%.", "AI": {"tldr": "The paper proposes SynCheck, a quality-guided scheme to improve synthetic data utilization in wireless sensing by addressing affinity and diversity limitations.", "motivation": "Current synthetic data in wireless sensing lacks predictable quality, leading to performance degradation. The paper aims to quantify and improve synthetic data quality.", "method": "Introduces metrics for affinity and diversity, and SynCheck, a scheme to refine synthetic data quality during training.", "result": "SynCheck outperforms traditional methods, achieving a 4.3% performance improvement even when others degrade by 13.4%.", "conclusion": "SynCheck effectively mitigates synthetic data quality issues, enhancing wireless sensing task performance."}}
{"id": "2506.23692", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23692", "abs": "https://arxiv.org/abs/2506.23692", "authors": ["Boyuan Zheng", "Zerui Fang", "Zhe Xu", "Rui Wang", "Yiwen Chen", "Cunshi Wang", "Mengwei Qu", "Lei Lei", "Zhen Feng", "Yan Liu", "Yuyang Li", "Mingzhou Tan", "Jiaji Wu", "Jianwei Shuai", "Jia Li", "Fangfu Ye"], "title": "Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models", "comment": null, "summary": "While AI for Science (AI4S) serves as an analytical tool in the current\nresearch paradigm, it doesn't solve its core inefficiency. We propose \"Agent\nfor Science\" (Agent4S)-the use of LLM-driven agents to automate the entire\nresearch workflow-as the true Fifth Scientific Paradigm. This paper introduces\na five-level classification for Agent4S, outlining a clear roadmap from simple\ntask automation to fully autonomous, collaborative \"AI Scientists.\" This\nframework defines the next revolutionary step in scientific discovery.", "AI": {"tldr": "The paper proposes 'Agent for Science' (Agent4S) as a transformative paradigm using LLM-driven agents to automate research workflows, surpassing AI for Science (AI4S). It introduces a five-level classification for Agent4S, aiming for fully autonomous AI Scientists.", "motivation": "Current AI4S is inefficient for core research workflows. The paper aims to revolutionize scientific discovery by automating the entire process with Agent4S.", "method": "Introduces a five-level classification framework for Agent4S, progressing from task automation to autonomous AI Scientists.", "result": "A roadmap for Agent4S is outlined, defining its potential to transform scientific research.", "conclusion": "Agent4S represents the Fifth Scientific Paradigm, enabling fully autonomous and collaborative scientific discovery."}}
{"id": "2506.22900", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22900", "abs": "https://arxiv.org/abs/2506.22900", "authors": ["Mai A. Shaaban", "Tausifa Jan Saleem", "Vijay Ram Papineni", "Mohammad Yaqub"], "title": "MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering", "comment": null, "summary": "Medical visual question answering (MedVQA) plays a vital role in clinical\ndecision-making by providing contextually rich answers to image-based queries.\nAlthough vision-language models (VLMs) are widely used for this task, they\noften generate factually incorrect answers. Retrieval-augmented generation\naddresses this challenge by providing information from external sources, but\nrisks retrieving irrelevant context, which can degrade the reasoning\ncapabilities of VLMs. Re-ranking retrievals, as introduced in existing\napproaches, enhances retrieval relevance by focusing on query-text alignment.\nHowever, these approaches neglect the visual or multimodal context, which is\nparticularly crucial for medical diagnosis. We propose MOTOR, a novel\nmultimodal retrieval and re-ranking approach that leverages grounded captions\nand optimal transport. It captures the underlying relationships between the\nquery and the retrieved context based on textual and visual information.\nConsequently, our approach identifies more clinically relevant contexts to\naugment the VLM input. Empirical analysis and human expert evaluation\ndemonstrate that MOTOR achieves higher accuracy on MedVQA datasets,\noutperforming state-of-the-art methods by an average of 6.45%. Code is\navailable at https://github.com/BioMedIA-MBZUAI/MOTOR.", "AI": {"tldr": "MOTOR improves MedVQA accuracy by 6.45% using multimodal retrieval and re-ranking with grounded captions and optimal transport.", "motivation": "Existing VLMs for MedVQA often produce incorrect answers, and retrieval-augmented methods risk irrelevant context, ignoring visual/multimodal cues crucial for medical diagnosis.", "method": "MOTOR combines grounded captions and optimal transport to align query and context using textual and visual information.", "result": "MOTOR outperforms state-of-the-art methods by 6.45% on MedVQA datasets.", "conclusion": "MOTOR enhances clinical relevance in MedVQA by integrating multimodal context, improving accuracy significantly."}}
{"id": "2506.23431", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23431", "abs": "https://arxiv.org/abs/2506.23431", "authors": ["Zixian Huang", "Chenxu Niu", "Yu Gu", "Gengyang Xiao", "Xinwei Huang", "Gong Cheng"], "title": "Pipelined Decoder for Efficient Context-Aware Text Generation", "comment": null, "summary": "As the basis of generative AI, an autoregressive model requires the\ngeneration of a new token depending on all the previously generated tokens,\nwhich brings high quality but also restricts the model to generate tokens one\nby one, forming a bottleneck limiting the generation speed. In this paper, we\npropose a new decoder architecture that efficiently generates text in parallel\nfor context-aware generation tasks. Our proposed pipelined decoder initiates\nthe generation of multiple subsequences simultaneously, and, at each time-step,\nit generates a new token for each subsequence to realize parallelism.\nExperiments on multiple text generation tasks, including question answering,\ntext summarization, and keyphrase generation, show that our pipelined decoder\nsignificantly improves the generation speed without a significant loss of\ngeneration quality or additional memory consumption.", "AI": {"tldr": "Proposes a pipelined decoder for parallel text generation, improving speed without compromising quality or memory.", "motivation": "Autoregressive models are slow due to sequential token generation, creating a bottleneck.", "method": "Introduces a pipelined decoder that generates multiple subsequences in parallel at each time-step.", "result": "Significantly improves generation speed in tasks like QA, summarization, and keyphrase generation, with minimal quality loss.", "conclusion": "The pipelined decoder effectively addresses speed limitations while maintaining performance and efficiency."}}
{"id": "2506.23182", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2506.23182", "abs": "https://arxiv.org/abs/2506.23182", "authors": ["Robert Frank", "Michael Widrich", "Rahmad Akbar", "G\u00fcnter Klambauer", "Geir Kjetil Sandve", "Philippe A. Robert", "Victor Greiff"], "title": "Attribution assignment for deep-generative sequence models enables interpretability analysis using positive-only data", "comment": null, "summary": "Generative machine learning models offer a powerful framework for therapeutic\ndesign by efficiently exploring large spaces of biological sequences enriched\nfor desirable properties. Unlike supervised learning methods, which require\nboth positive and negative labeled data, generative models such as LSTMs can be\ntrained solely on positively labeled sequences, for example, high-affinity\nantibodies. This is particularly advantageous in biological settings where\nnegative data are scarce, unreliable, or biologically ill-defined. However, the\nlack of attribution methods for generative models has hindered the ability to\nextract interpretable biological insights from such models. To address this\ngap, we developed Generative Attribution Metric Analysis (GAMA), an attribution\nmethod for autoregressive generative models based on Integrated Gradients. We\nassessed GAMA using synthetic datasets with known ground truths to characterize\nits statistical behavior and validate its ability to recover biologically\nrelevant features. We further demonstrated the utility of GAMA by applying it\nto experimental antibody-antigen binding data. GAMA enables model\ninterpretability and the validation of generative sequence design strategies\nwithout the need for negative training data.", "AI": {"tldr": "GAMA is a new attribution method for generative models like LSTMs, enabling interpretable biological insights without requiring negative training data.", "motivation": "Generative models lack interpretability, hindering biological insights. Negative data is often scarce or unreliable in biological settings.", "method": "Developed GAMA, an attribution method based on Integrated Gradients, tested on synthetic and experimental antibody-antigen binding data.", "result": "GAMA successfully recovers biologically relevant features and validates generative sequence designs without negative data.", "conclusion": "GAMA enhances interpretability and utility of generative models in therapeutic design."}}
{"id": "2506.23703", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23703", "abs": "https://arxiv.org/abs/2506.23703", "authors": ["Lars Ullrich", "Walter Zimmer", "Ross Greer", "Knut Graichen", "Alois C. Knoll", "Mohan Trivedi"], "title": "A New Perspective On AI Safety Through Control Theory Methodologies", "comment": "Accepted to be published as part of the 2025 IEEE Open Journal of\n  Intelligent Transportation Systems (OJ-ITS)", "summary": "While artificial intelligence (AI) is advancing rapidly and mastering\nincreasingly complex problems with astonishing performance, the safety\nassurance of such systems is a major concern. Particularly in the context of\nsafety-critical, real-world cyber-physical systems, AI promises to achieve a\nnew level of autonomy but is hampered by a lack of safety assurance. While\ndata-driven control takes up recent developments in AI to improve control\nsystems, control theory in general could be leveraged to improve AI safety.\nTherefore, this article outlines a new perspective on AI safety based on an\ninterdisciplinary interpretation of the underlying data-generation process and\nthe respective abstraction by AI systems in a system theory-inspired and system\nanalysis-driven manner. In this context, the new perspective, also referred to\nas data control, aims to stimulate AI engineering to take advantage of existing\nsafety analysis and assurance in an interdisciplinary way to drive the paradigm\nof data control. Following a top-down approach, a generic foundation for safety\nanalysis and assurance is outlined at an abstract level that can be refined for\nspecific AI systems and applications and is prepared for future innovation.", "AI": {"tldr": "The paper proposes a new interdisciplinary approach, 'data control,' to enhance AI safety by integrating control theory and AI, addressing safety concerns in cyber-physical systems.", "motivation": "AI's rapid advancement lacks safety assurance, especially in safety-critical systems, prompting the need for an interdisciplinary solution.", "method": "The paper introduces 'data control,' leveraging control theory and system analysis to improve AI safety through a top-down approach.", "result": "A generic foundation for safety analysis and assurance is outlined, adaptable for specific AI systems and future innovations.", "conclusion": "The interdisciplinary perspective of data control aims to bridge AI and control theory, fostering safer AI engineering practices."}}
{"id": "2506.22902", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.22902", "abs": "https://arxiv.org/abs/2506.22902", "authors": ["Yiling Xu", "Yujie Zhang", "Shuting Xia", "Kaifa Yang", "He Huang", "Ziyu Shan", "Wenjie Huang", "Qi Yang", "Le Yang"], "title": "Point Cloud Compression and Objective Quality Assessment: A Survey", "comment": null, "summary": "The rapid growth of 3D point cloud data, driven by applications in autonomous\ndriving, robotics, and immersive environments, has led to criticals demand for\nefficient compression and quality assessment techniques. Unlike traditional 2D\nmedia, point clouds present unique challenges due to their irregular structure,\nhigh data volume, and complex attributes. This paper provides a comprehensive\nsurvey of recent advances in point cloud compression (PCC) and point cloud\nquality assessment (PCQA), emphasizing their significance for real-time and\nperceptually relevant applications. We analyze a wide range of handcrafted and\nlearning-based PCC algorithms, along with objective PCQA metrics. By\nbenchmarking representative methods on emerging datasets, we offer detailed\ncomparisons and practical insights into their strengths and limitations.\nDespite notable progress, challenges such as enhancing visual fidelity,\nreducing latency, and supporting multimodal data remain. This survey outlines\nfuture directions, including hybrid compression frameworks and advanced feature\nextraction strategies, to enable more efficient, immersive, and intelligent 3D\napplications.", "AI": {"tldr": "A survey on point cloud compression (PCC) and quality assessment (PCQA), highlighting challenges, methods, and future directions for efficient 3D applications.", "motivation": "The surge in 3D point cloud data necessitates efficient compression and quality assessment due to its irregular structure and high volume, especially for real-time and perceptually relevant applications.", "method": "Analyzes handcrafted and learning-based PCC algorithms and objective PCQA metrics, benchmarking them on emerging datasets for detailed comparisons.", "result": "Identifies strengths and limitations of current methods, noting challenges like visual fidelity, latency, and multimodal data support.", "conclusion": "Proposes future directions, such as hybrid compression frameworks and advanced feature extraction, to improve 3D applications."}}
{"id": "2506.23463", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.23463", "abs": "https://arxiv.org/abs/2506.23463", "authors": ["Jang Won June"], "title": "What to Keep and What to Drop: Adaptive Table Filtering Framework", "comment": "26 pages, 9 figures", "summary": "Large language models (LLMs) for table-based reasoning often struggle with\nlarge tables due to input length limits. We propose ATF (Adaptive Table\nFiltering Framework), a modular and question-aware filtering pipeline that\nprunes uninformative columns and rows using LLM-generated column descriptions,\nclustering, and sparse-dense alignment scores. ATF integrates seamlessly with\nexisting models (e.g., TAPAS, TAPEX) without retraining. Experiments show that\nATF reduces table cells by ~70\\%, boosting performance on out-of-domain TableQA\ntasks while causing slight performance drops on Table Fact Verification, where\nfull-table context is more critical. These results highlight ATF's ability to\nadaptively balance informativeness and minimalism across tasks.", "AI": {"tldr": "ATF (Adaptive Table Filtering Framework) prunes uninformative table columns and rows to improve LLM performance on large tables, reducing cells by ~70% and boosting TableQA tasks.", "motivation": "Large tables exceed input limits for LLMs, hindering table-based reasoning.", "method": "ATF uses LLM-generated column descriptions, clustering, and sparse-dense alignment scores for question-aware filtering.", "result": "ATF reduces table cells by ~70%, improving TableQA performance but slightly dropping Table Fact Verification accuracy.", "conclusion": "ATF adaptively balances informativeness and minimalism, enhancing LLM efficiency for table-based tasks."}}
{"id": "2506.23186", "categories": ["cs.LG", "cs.DM", "math.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.23186", "abs": "https://arxiv.org/abs/2506.23186", "authors": ["Marco Bressan", "Victor Chepoi", "Emmanuel Esposito", "Maximilian Thiessen"], "title": "Efficient Algorithms for Learning and Compressing Monophonic Halfspaces in Graphs", "comment": null, "summary": "Abstract notions of convexity over the vertices of a graph, and corresponding\nnotions of halfspaces, have recently gained attention from the machine learning\ncommunity. In this work we study monophonic halfspaces, a notion of graph\nhalfspaces defined through closure under induced paths. Our main result is a\n$2$-satisfiability based decomposition theorem, which allows one to represent\nmonophonic halfspaces as a disjoint union of certain vertex subsets. Using this\ndecomposition, we achieve efficient and (nearly) optimal algorithms for various\nlearning problems, such as teaching, active, and online learning. Most notably,\nwe obtain a polynomial-time algorithm for empirical risk minimization.\nIndependently of the decomposition theorem, we obtain an efficient, stable, and\nproper sample compression scheme. This makes monophonic halfspaces efficiently\nlearnable with proper learners and linear error rate $1/\\varepsilon$ in the\nrealizable PAC setting. Our results answer open questions from the literature,\nand show a stark contrast with geodesic halfspaces, for which most of the said\nlearning problems are NP-hard.", "AI": {"tldr": "The paper studies monophonic halfspaces in graphs, introduces a decomposition theorem, and provides efficient learning algorithms, contrasting with NP-hard geodesic halfspaces.", "motivation": "To explore graph-based convexity and halfspaces, addressing open questions in machine learning about efficient learning methods for such structures.", "method": "A $2$-satisfiability based decomposition theorem is used to represent monophonic halfspaces as disjoint vertex subsets, enabling efficient algorithms for learning tasks.", "result": "Efficient algorithms for teaching, active, online learning, and empirical risk minimization, along with a proper sample compression scheme.", "conclusion": "Monophonic halfspaces are efficiently learnable, unlike geodesic halfspaces, resolving open questions and highlighting computational advantages."}}
{"id": "2506.23706", "categories": ["cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.23706", "abs": "https://arxiv.org/abs/2506.23706", "authors": ["Christoph Schnabl", "Daniel Hugenroth", "Bill Marino", "Alastair R. Beresford"], "title": "Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments", "comment": "ICML 2024 Workshop TAIG", "summary": "Benchmarks are important measures to evaluate safety and compliance of AI\nmodels at scale. However, they typically do not offer verifiable results and\nlack confidentiality for model IP and benchmark datasets. We propose Attestable\nAudits, which run inside Trusted Execution Environments and enable users to\nverify interaction with a compliant AI model. Our work protects sensitive data\neven when model provider and auditor do not trust each other. This addresses\nverification challenges raised in recent AI governance frameworks. We build a\nprototype demonstrating feasibility on typical audit benchmarks against\nLlama-3.1.", "AI": {"tldr": "Attestable Audits use Trusted Execution Environments to verify AI model compliance while protecting sensitive data, addressing governance challenges.", "motivation": "Current benchmarks lack verifiable results and confidentiality for AI model IP and datasets, raising governance concerns.", "method": "Proposes Attestable Audits using Trusted Execution Environments to ensure verifiable and confidential interactions with compliant AI models.", "result": "A prototype demonstrates feasibility on audit benchmarks against Llama-3.1, protecting data even with untrusted parties.", "conclusion": "Attestable Audits offer a solution for verifiable, confidential AI model compliance, aligning with governance needs."}}
{"id": "2506.22907", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2506.22907", "abs": "https://arxiv.org/abs/2506.22907", "authors": ["Yunzhe Shao", "Xinyu Yi", "Lu Yin", "Shihui Guo", "Junhai Yong", "Feng Xu"], "title": "MagShield: Towards Better Robustness in Sparse Inertial Motion Capture Under Magnetic Disturbances", "comment": null, "summary": "This paper proposes a novel method called MagShield, designed to address the\nissue of magnetic interference in sparse inertial motion capture (MoCap)\nsystems. Existing Inertial Measurement Unit (IMU) systems are prone to\norientation estimation errors in magnetically disturbed environments, limiting\ntheir practical application in real-world scenarios. To address this problem,\nMagShield employs a \"detect-then-correct\" strategy, first detecting magnetic\ndisturbances through multi-IMU joint analysis, and then correcting orientation\nerrors using human motion priors. MagShield can be integrated with most\nexisting sparse inertial MoCap systems, improving their performance in\nmagnetically disturbed environments. Experimental results demonstrate that\nMagShield significantly enhances the accuracy of motion capture under magnetic\ninterference and exhibits good compatibility across different sparse inertial\nMoCap systems.", "AI": {"tldr": "MagShield is a novel method to mitigate magnetic interference in sparse inertial MoCap systems by detecting and correcting disturbances using multi-IMU analysis and motion priors.", "motivation": "Existing IMU systems suffer from orientation errors in magnetically disturbed environments, limiting real-world usability.", "method": "MagShield uses a 'detect-then-correct' approach: detecting disturbances via multi-IMU joint analysis and correcting errors with human motion priors.", "result": "MagShield improves motion capture accuracy under interference and is compatible with various sparse inertial MoCap systems.", "conclusion": "MagShield effectively addresses magnetic interference, enhancing the practicality of sparse inertial MoCap systems."}}
{"id": "2506.23485", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.23485", "abs": "https://arxiv.org/abs/2506.23485", "authors": ["Haocheng Yu", "Yaxiong Wu", "Hao Wang", "Wei Guo", "Yong Liu", "Yawen Li", "Yuyang Ye", "Junping Du", "Enhong Chen"], "title": "Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent", "comment": null, "summary": "Interactive recommendation is a typical information-seeking task that allows\nusers to interactively express their needs through natural language and obtain\npersonalized recommendations. Large language model-powered (LLM-powered) agents\nhave become a new paradigm in interactive recommendations, effectively\ncapturing users' real-time needs and enhancing personalized experiences.\nHowever, due to limited planning and generalization capabilities, existing\nformulations of LLM-powered interactive recommender agents struggle to\neffectively address diverse and complex user intents, such as intuitive,\nunrefined, or occasionally ambiguous requests. To tackle this challenge, we\npropose a novel thought-augmented interactive recommender agent system (TAIRA)\nthat addresses complex user intents through distilled thought patterns.\nSpecifically, TAIRA is designed as an LLM-powered multi-agent system featuring\na manager agent that orchestrates recommendation tasks by decomposing user\nneeds and planning subtasks, with its planning capacity strengthened through\nThought Pattern Distillation (TPD), a thought-augmentation method that extracts\nhigh-level thoughts from the agent's and human experts' experiences. Moreover,\nwe designed a set of user simulation schemes to generate personalized queries\nof different difficulties and evaluate the recommendations based on specific\ndatasets. Through comprehensive experiments conducted across multiple datasets,\nTAIRA exhibits significantly enhanced performance compared to existing methods.\nNotably, TAIRA shows a greater advantage on more challenging tasks while\ngeneralizing effectively on novel tasks, further validating its superiority in\nmanaging complex user intents within interactive recommendation systems. The\ncode is publicly available at:https://github.com/Alcein/TAIRA.", "AI": {"tldr": "TAIRA is a thought-augmented LLM-powered interactive recommender agent system that improves handling of complex user intents through distilled thought patterns, outperforming existing methods.", "motivation": "Existing LLM-powered interactive recommender agents struggle with diverse and complex user intents due to limited planning and generalization capabilities.", "method": "TAIRA uses a multi-agent system with a manager agent for task decomposition and planning, enhanced by Thought Pattern Distillation (TPD). User simulation schemes evaluate performance.", "result": "TAIRA outperforms existing methods, especially on challenging tasks, and generalizes well on novel tasks.", "conclusion": "TAIRA effectively manages complex user intents in interactive recommendation systems, validated by experiments and user simulations."}}
{"id": "2506.23201", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.23201", "abs": "https://arxiv.org/abs/2506.23201", "authors": ["Haoran Li", "Muhao Guo", "Marija Ilic", "Yang Weng", "Guangchun Ruan"], "title": "External Data-Enhanced Meta-Representation for Adaptive Probabilistic Load Forecasting", "comment": "10 pages", "summary": "Accurate residential load forecasting is critical for power system\nreliability with rising renewable integration and demand-side flexibility.\nHowever, most statistical and machine learning models treat external factors,\nsuch as weather, calendar effects, and pricing, as extra input, ignoring their\nheterogeneity, and thus limiting the extraction of useful external information.\nWe propose a paradigm shift: external data should serve as meta-knowledge to\ndynamically adapt the forecasting model itself. Based on this idea, we design a\nmeta-representation framework using hypernetworks that modulate selected\nparameters of a base Deep Learning (DL) model in response to external\nconditions. This provides both expressivity and adaptability. We further\nintegrate a Mixture-of-Experts (MoE) mechanism to enhance efficiency through\nselective expert activation, while improving robustness by filtering redundant\nexternal inputs. The resulting model, dubbed as a Meta Mixture of Experts for\nExternal data (M2oE2), achieves substantial improvements in accuracy and\nrobustness with limited additional overhead, outperforming existing\nstate-of-the-art methods in diverse load datasets. The dataset and source code\nare publicly available at\nhttps://github.com/haorandd/M2oE2\\_load\\_forecast.git.", "AI": {"tldr": "The paper proposes a meta-representation framework, M2oE2, using hypernetworks and Mixture-of-Experts to dynamically adapt load forecasting models to external conditions, improving accuracy and robustness.", "motivation": "Accurate residential load forecasting is crucial for power system reliability, but existing models inadequately handle external factors like weather and pricing.", "method": "The framework uses hypernetworks to modulate a base DL model based on external data and integrates MoE for selective expert activation and input filtering.", "result": "M2oE2 outperforms state-of-the-art methods in accuracy and robustness across diverse datasets with minimal overhead.", "conclusion": "The approach effectively leverages external data as meta-knowledge, enhancing forecasting performance while maintaining efficiency."}}
{"id": "2506.23773", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2506.23773", "abs": "https://arxiv.org/abs/2506.23773", "authors": ["Stefano M. Nicoletti", "Mari\u00eblle Stoelinga"], "title": "BayesL: Towards a Logical Framework for Bayesian Networks", "comment": null, "summary": "We introduce BayesL, a novel logical framework for specifying, querying, and\nverifying the behaviour of Bayesian networks (BNs). BayesL (pronounced \"Basil\")\nis a structured language that allows for the creation of queries over BNs. It\nfacilitates versatile reasoning concerning causal and evidence-based\nrelationships, and permits comprehensive what-if scenario evaluations without\nthe need for manual modifications to the model.", "AI": {"tldr": "BayesL is a new logical framework for querying and verifying Bayesian networks, enabling versatile reasoning and what-if evaluations without manual model changes.", "motivation": "To simplify and enhance the process of querying and verifying Bayesian networks by providing a structured language.", "method": "Developed BayesL, a logical framework for creating queries over Bayesian networks, supporting causal and evidence-based reasoning.", "result": "BayesL allows comprehensive what-if scenario evaluations and versatile reasoning without manual model adjustments.", "conclusion": "BayesL offers a powerful and flexible tool for working with Bayesian networks, streamlining complex analyses."}}
{"id": "2506.22908", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22908", "abs": "https://arxiv.org/abs/2506.22908", "authors": ["Yuzhu Wang", "Manni Duan", "Shu Kong"], "title": "Attention to Burstiness: Low-Rank Bilinear Prompt Tuning", "comment": "ICCV 2025", "summary": "Visual Prompt Tuning (VPT) is a parameter-efficient fune-tuning technique\nthat adapts a pre-trained vision Transformer (ViT) by learning a small set of\nparameters in the input space, known as prompts. In VPT, we uncover\n``burstiness'' in the values arising from the interaction of image patch\nembeddings, and the key and query projectors within Transformer's\nself-attention module. Furthermore, the values of patch embeddings and the key\nand query projectors exhibit Laplacian and hyper-Laplacian distribution,\nrespectively. Intuitively, these non-Gaussian distributions pose challenges for\nlearning prompts. To address this, we propose whitening these data,\nde-correlating them and equalizing their variance towards more Gaussian before\nlearning prompts. We derive the whitening matrix over random image patch\nembeddings and ViT's key and query projectors, and multiply it with the prompt\nto be learned in a bilinear manner. Surprisingly, this method significantly\naccelerates prompt tuning and boosts accuracy, e.g., $>$25 accuracy points on\nthe CUB dataset; interestingly, it learns ``bursty prompts''. Extending the\nbilinear model which is known to introduce burstiness, we present a compact,\nlow-rank version by learning two smaller matrices whose multiplication yields\nthe final prompts. We call the proposed methods Bilinear Prompt Tuning (BPT).\nExtensive experiments across multiple benchmark datasets demonstrate that BPT\nmethods not only outperform various VPT methods but also reduce parameter count\nand computation overhead.", "AI": {"tldr": "Visual Prompt Tuning (VPT) is enhanced by whitening data to address non-Gaussian distributions, leading to Bilinear Prompt Tuning (BPT), which improves accuracy and efficiency.", "motivation": "The non-Gaussian distributions in patch embeddings and key/query projectors hinder prompt learning in VPT, motivating the need for whitening.", "method": "Proposes whitening data and a bilinear model (BPT) to de-correlate and equalize variance, improving prompt tuning.", "result": "BPT significantly boosts accuracy (e.g., +25 points on CUB) and reduces parameters/computation.", "conclusion": "BPT outperforms VPT methods, offering a more efficient and effective approach to prompt tuning."}}
{"id": "2506.23508", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23508", "abs": "https://arxiv.org/abs/2506.23508", "authors": ["Zhihao Zhang", "Qiaole Dong", "Qi Zhang", "Jun Zhao", "Enyu Zhou", "Zhiheng Xi", "Senjie Jin", "Xiaoran Fan", "Yuhao Zhou", "Yanwei Fu", "Tao Ji", "Tao Gui", "Xuanjing Huang"], "title": "Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably", "comment": "18 pages (Preprint. Work in progress)", "summary": "Post-training algorithms such as Supervised Fine-Tuning (SFT) and\nReinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large\nlanguage models to downstream tasks. While effective at task adaptation, their\nimpact on prior knowledge remains unclear. In this paper, we introduce jigsaw\npuzzles as a novel task absent from existing pretraining corpora and\nsystematically study the behavior of SFT and RFT on an open-source multimodal\nmodel, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid\ntask acquisition but leads to catastrophic forgetting, whereas RFT learns more\nslowly on novel tasks but maintains prior knowledge. We analyze this phenomenon\nthrough the lens of learning dynamics, showing that RFT reinforces correct\nsamples that are naturally aligned with the base model's probability landscape,\nmitigating interference with prior knowledge. Moreover, supervised training on\ncorrect RFT-simulated rollouts allows SFT to preserve knowledge while rapidly\nlearning new tasks. These findings suggest that data distribution, rather than\nalgorithmic differences, plays a central role in forgetting, and highlight\nRFT's potential for stable continual learning in multimodal large language\nmodels.", "AI": {"tldr": "SFT and RFT adapt multimodal models to tasks but differ in knowledge retention. SFT learns fast but forgets prior knowledge, while RFT learns slower but retains knowledge. RFT's alignment with the base model's probability landscape reduces interference.", "motivation": "To understand how SFT and RFT impact prior knowledge in multimodal models when adapting to novel tasks.", "method": "Used jigsaw puzzles as a novel task with Qwen2.5-VL, comparing SFT and RFT's learning dynamics and knowledge retention.", "result": "SFT causes catastrophic forgetting, while RFT maintains prior knowledge by reinforcing aligned samples. Supervised training on RFT rollouts helps SFT preserve knowledge.", "conclusion": "Data distribution, not algorithmic differences, drives forgetting. RFT is promising for stable continual learning in multimodal models."}}
{"id": "2506.23210", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.23210", "abs": "https://arxiv.org/abs/2506.23210", "authors": ["Taehwan Yoon", "Bongjun Choi"], "title": "FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model", "comment": "6 pages,14 equation", "summary": "Federated learning(FL) is used for distributed scenarios to train artificial\nintelligence(AI) models while ensuring users' privacy. In federated learning\nscenario, the server generally never knows about users' data. This type of\nconcept makes the AI training process efficient in terms of data privacy.\nHowever, regarding model performance, federated AI models may not sufficiently\nsatisfy AI users' expectations. Furthermore, AI users have a wide range of\ndifferent needs. It is not easy to satisfy the whole users needs. These types\nof issues can be addressed through AI model optimization, fine-tuning, or\npersonalization to achieve optimal model performance. To address model\noptimization challenges, we propose reference model-based federated learning\nfor optimal fine-tuning, which overcomes catastrophic forgetting in each round.\nThis method is derived from Bayesian parameter-efficient transfer learning,\nwhich includes an optimal proximal term and enables overcoming the catastrophic\nforgetting issue in each round by utilizing a reference model that incorporates\nprevious model parameters. As a result, this method achieves both high model\nperformance and low computing cost.", "AI": {"tldr": "Proposes a reference model-based federated learning method for optimal fine-tuning, addressing catastrophic forgetting and improving model performance with low computing cost.", "motivation": "Federated learning ensures privacy but often lacks in model performance and personalization, prompting the need for optimization methods.", "method": "Uses Bayesian parameter-efficient transfer learning with an optimal proximal term and a reference model to avoid catastrophic forgetting.", "result": "Achieves high model performance and low computing cost.", "conclusion": "The proposed method effectively balances privacy, performance, and efficiency in federated learning."}}
{"id": "2506.23784", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23784", "abs": "https://arxiv.org/abs/2506.23784", "authors": ["Parosh Aziz Abdulla", "Mohamed Faouzi Atig", "Julie Cailler", "Chencheng Liang", "Philipp R\u00fcmmer"], "title": "When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)", "comment": null, "summary": "Nielsen transformation is a standard approach for solving word equations: by\nrepeatedly splitting equations and applying simplification steps, equations are\nrewritten until a solution is reached. When solving a conjunction of word\nequations in this way, the performance of the solver will depend considerably\non the order in which equations are processed. In this work, the use of Graph\nNeural Networks (GNNs) for ranking word equations before and during the solving\nprocess is explored. For this, a novel graph-based representation for word\nequations is presented, preserving global information across conjuncts,\nenabling the GNN to have a holistic view during ranking. To handle the variable\nnumber of conjuncts, three approaches to adapt a multi-classification task to\nthe problem of ranking equations are proposed. The training of the GNN is done\nwith the help of minimum unsatisfiable subsets (MUSes) of word equations. The\nexperimental results show that, compared to state-of-the-art string solvers,\nthe new framework solves more problems in benchmarks where each variable\nappears at most once in each equation.", "AI": {"tldr": "The paper explores using Graph Neural Networks (GNNs) to rank word equations for solving conjunctions more efficiently, outperforming state-of-the-art solvers in certain benchmarks.", "motivation": "The performance of solvers for word equations depends heavily on the order of processing, motivating the use of GNNs for better ranking.", "method": "A novel graph-based representation for word equations is introduced, and three approaches for ranking equations are proposed, trained using minimum unsatisfiable subsets (MUSes).", "result": "The GNN-based framework solves more problems in benchmarks where variables appear at most once per equation compared to existing solvers.", "conclusion": "GNNs offer an effective way to improve the solving of word equations by optimizing the processing order."}}
{"id": "2506.22930", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22930", "abs": "https://arxiv.org/abs/2506.22930", "authors": ["Yiwei He", "Xiangtai Li", "Zhenglin Huang", "Yi Dong", "Hao Fei", "Jiangning Zhang", "Baoyuan Wu", "Guangliang Cheng"], "title": "Towards Explainable Bilingual Multimodal Misinformation Detection and Localization", "comment": null, "summary": "The increasing realism of multimodal content has made misinformation more\nsubtle and harder to detect, especially in news media where images are\nfrequently paired with bilingual (e.g., Chinese-English) subtitles. Such\ncontent often includes localized image edits and cross-lingual inconsistencies\nthat jointly distort meaning while remaining superficially plausible. We\nintroduce BiMi, a bilingual multimodal framework that jointly performs\nregion-level localization, cross-modal and cross-lingual consistency detection,\nand natural language explanation for misinformation analysis. To support\ngeneralization, BiMi integrates an online retrieval module that supplements\nmodel reasoning with up-to-date external context. We further release BiMiBench,\na large-scale and comprehensive benchmark constructed by systematically editing\nreal news images and subtitles, comprising 104,000 samples with realistic\nmanipulations across visual and linguistic modalities. To enhance\ninterpretability, we apply Group Relative Policy Optimization (GRPO) to improve\nexplanation quality, marking the first use of GRPO in this domain. Extensive\nexperiments demonstrate that BiMi outperforms strong baselines by up to +8.9 in\nclassification accuracy, +15.9 in localization accuracy, and +2.5 in\nexplanation BERTScore, advancing state-of-the-art performance in realistic,\nmultilingual misinformation detection. Code, models, and datasets will be\nreleased.", "AI": {"tldr": "BiMi is a bilingual multimodal framework for detecting misinformation in news media by analyzing region-level edits and cross-lingual inconsistencies, outperforming baselines in accuracy and explanation quality.", "motivation": "Misinformation in bilingual news media is harder to detect due to subtle edits and cross-lingual inconsistencies, requiring advanced multimodal analysis.", "method": "BiMi combines region-level localization, cross-modal/lingual consistency detection, and natural language explanation, enhanced by GRPO for better interpretability.", "result": "BiMi outperforms baselines by +8.9 in classification accuracy, +15.9 in localization accuracy, and +2.5 in explanation BERTScore.", "conclusion": "BiMi advances multilingual misinformation detection with superior performance and interpretability, supported by the BiMiBench dataset."}}
{"id": "2506.23524", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23524", "abs": "https://arxiv.org/abs/2506.23524", "authors": ["Phan Quoc Hung Mai", "Quang Hung Nguyen", "Phuong Giang Duong", "Hong Hanh Nguyen", "Nguyen Tuan Long"], "title": "NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning", "comment": null, "summary": "In the field of education, understanding students' opinions through their\ncomments is crucial, especially in the Vietnamese language, where resources\nremain limited. Existing educational datasets often lack domain relevance and\nstudent slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese\ndataset for Educational Sentiment Classification and Topic Classification,\ncurated from university forums, which offers more samples, richer class\ndiversity, longer texts, and broader vocabulary. In addition, we explore\nmultitask learning using encoder-only language models (BERT), in which we\nshowed that it achieves performance up to 83.7% and 79.8% accuracy for\nsentiment and topic classification tasks. We also benchmark our dataset and\nmodel with other datasets and models, including Large Language Models, and\ndiscuss these benchmarks. The dataset is publicly available at:\nhttps://huggingface.co/datasets/hung20gg/NEU-ESC.", "AI": {"tldr": "NEU-ESC is a new Vietnamese dataset for sentiment and topic classification in education, addressing gaps in existing datasets. It achieves high accuracy with multitask BERT models and is publicly available.", "motivation": "Existing educational datasets lack domain relevance and student slang, especially in Vietnamese. NEU-ESC fills this gap by providing a richer, more diverse dataset.", "method": "The dataset is curated from university forums. Multitask learning with BERT models is explored for sentiment and topic classification.", "result": "Performance reaches 83.7% and 79.8% accuracy for sentiment and topic classification, respectively. Benchmarks compare NEU-ESC with other datasets and models.", "conclusion": "NEU-ESC improves Vietnamese educational sentiment and topic analysis, with strong model performance and public availability for further research."}}
{"id": "2506.23221", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23221", "abs": "https://arxiv.org/abs/2506.23221", "authors": ["B\u00e1lint Horv\u00e1th", "Bal\u00e1zs Csan\u00e1d Cs\u00e1ji"], "title": "Single Image Inpainting and Super-Resolution with Simultaneous Uncertainty Guarantees by Universal Reproducing Kernels", "comment": "23 pages, 8 figures, 6 tables", "summary": "The paper proposes a statistical learning approach to the problem of\nestimating missing pixels of images, crucial for image inpainting and\nsuper-resolution problems. One of the main novelties of the method is that it\nalso provides uncertainty quantifications together with the estimated values.\nOur core assumption is that the underlying data-generating function comes from\na Reproducing Kernel Hilbert Space (RKHS). A special emphasis is put on\nband-limited functions, central to signal processing, which form Paley-Wiener\ntype RKHSs. The proposed method, which we call Simultaneously Guaranteed Kernel\nInterpolation (SGKI), is an extension and refinement of a recently developed\nkernel method. An advantage of SGKI is that it not only estimates the missing\npixels, but also builds non-asymptotic confidence bands for the unobserved\nvalues, which are simultaneously guaranteed for all missing pixels. We also\nshow how to compute these bands efficiently using Schur complements, we discuss\na generalization to vector-valued functions, and we present a series of\nnumerical experiments on various datasets containing synthetically generated\nand benchmark images, as well.", "AI": {"tldr": "The paper introduces SGKI, a kernel-based method for image inpainting and super-resolution, providing pixel estimates with uncertainty quantification.", "motivation": "Addressing the need for accurate missing pixel estimation in images, especially with uncertainty measures, for tasks like inpainting and super-resolution.", "method": "SGKI extends kernel methods, leveraging RKHS and band-limited functions, and uses Schur complements for efficient confidence band computation.", "result": "SGKI successfully estimates missing pixels and provides non-asymptotic confidence bands, validated on synthetic and benchmark datasets.", "conclusion": "SGKI is an effective tool for image restoration with built-in uncertainty quantification, applicable to vector-valued functions."}}
{"id": "2506.23793", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.23793", "abs": "https://arxiv.org/abs/2506.23793", "authors": ["Anton Andreychuk", "Konstantin Yakovlev", "Aleksandr Panov", "Alexey Skrynnik"], "title": "Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning", "comment": null, "summary": "Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot\ntrajectory planning problems, where multiple homogeneous robots simultaneously\nmove in the shared environment. While solving MAPF optimally has been proven to\nbe NP-hard, scalable, and efficient, solvers are vital for real-world\napplications like logistics, search-and-rescue, etc. To this end, decentralized\nsuboptimal MAPF solvers that leverage machine learning have come on stage.\nBuilding on the success of the recently introduced MAPF-GPT, a pure imitation\nlearning solver, we introduce MAPF-GPT-DDG. This novel approach effectively\nfine-tunes the pre-trained MAPF model using centralized expert data. Leveraging\na novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training\nwhile significantly improving performance at test time. Our experiments\ndemonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF\nsolvers, including the original MAPF-GPT, regarding solution quality across\nmany testing scenarios. Remarkably, it can work with MAPF instances involving\nup to 1 million agents in a single environment, setting a new milestone for\nscalability in MAPF domains.", "AI": {"tldr": "MAPF-GPT-DDG, a decentralized MAPF solver, improves on MAPF-GPT by fine-tuning with centralized expert data and a delta-data generation mechanism, achieving superior scalability and performance.", "motivation": "To enhance the performance and scalability of learning-based MAPF solvers for real-world applications like logistics and search-and-rescue.", "method": "Fine-tunes the pre-trained MAPF-GPT model using centralized expert data and a novel delta-data generation mechanism.", "result": "Outperforms existing learning-based MAPF solvers, including MAPF-GPT, and scales to 1 million agents.", "conclusion": "MAPF-GPT-DDG sets a new benchmark for scalable and efficient MAPF solvers."}}
{"id": "2506.22939", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22939", "abs": "https://arxiv.org/abs/2506.22939", "authors": ["Ghufran A. Omran", "Wassan Saad Abduljabbar Hayale", "Ahmad AbdulQadir AlRababah", "Israa Ibraheem Al-Barazanchi", "Ravi Sekhar", "Pritesh Shah", "Sushma Parihar", "Harshavardhan Reddy Penubadi"], "title": "Utilizing a Novel Deep Learning Method for Scene Categorization in Remote Sensing Data", "comment": null, "summary": "Scene categorization (SC) in remotely acquired images is an important subject\nwith broad consequences in different fields, including catastrophe control,\necological observation, architecture for cities, and more. Nevertheless, its\nseveral apps, reaching a high degree of accuracy in SC from distant observation\ndata has demonstrated to be difficult. This is because traditional conventional\ndeep learning models require large databases with high variety and high levels\nof noise to capture important visual features. To address these problems, this\ninvestigation file introduces an innovative technique referred to as the\nCuttlefish Optimized Bidirectional Recurrent Neural Network (CO- BRNN) for type\nof scenes in remote sensing data. The investigation compares the execution of\nCO-BRNN with current techniques, including Multilayer Perceptron- Convolutional\nNeural Network (MLP-CNN), Convolutional Neural Network-Long Short Term Memory\n(CNN-LSTM), and Long Short Term Memory-Conditional Random Field (LSTM-CRF),\nGraph-Based (GB), Multilabel Image Retrieval Model (MIRM-CF), Convolutional\nNeural Networks Data Augmentation (CNN-DA). The results demonstrate that\nCO-BRNN attained the maximum accuracy of 97%, followed by LSTM-CRF with 90%,\nMLP-CNN with 85%, and CNN-LSTM with 80%. The study highlights the significance\nof physical confirmation to ensure the efficiency of satellite data.", "AI": {"tldr": "The paper introduces CO-BRNN for scene categorization in remote sensing, achieving 97% accuracy, outperforming existing methods.", "motivation": "High accuracy in scene categorization from remote sensing is challenging due to noise and data variety.", "method": "Proposes Cuttlefish Optimized Bidirectional Recurrent Neural Network (CO-BRNN) and compares it with MLP-CNN, CNN-LSTM, LSTM-CRF, GB, MIRM-CF, and CNN-DA.", "result": "CO-BRNN achieved 97% accuracy, surpassing LSTM-CRF (90%), MLP-CNN (85%), and CNN-LSTM (80%).", "conclusion": "CO-BRNN is effective for remote sensing scene categorization, emphasizing the need for physical validation of satellite data."}}
{"id": "2506.23527", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23527", "abs": "https://arxiv.org/abs/2506.23527", "authors": ["Jan Kvapil", "Martin Fajcik"], "title": "On Recipe Memorization and Creativity in Large Language Models: Is Your Model a Creative Cook, a Bad Cook, or Merely a Plagiator?", "comment": "13 pages, 5 figures", "summary": "This work-in-progress investigates the memorization, creativity, and nonsense\nfound in cooking recipes generated from Large Language Models (LLMs).\nPrecisely, we aim (i) to analyze memorization, creativity, and non-sense in\nLLMs using a small, high-quality set of human judgments and (ii) to evaluate\npotential approaches to automate such a human annotation in order to scale our\nstudy to hundreds of recipes. To achieve (i), we conduct a detailed human\nannotation on 20 preselected recipes generated by LLM (Mixtral), extracting\neach recipe's ingredients and step-by-step actions to assess which elements are\nmemorized--i.e., directly traceable to online sources possibly seen during\ntraining--and which arise from genuine creative synthesis or outright nonsense.\nWe find that Mixtral consistently reuses ingredients that can be found in\nonline documents, potentially seen during model training, suggesting strong\nreliance on memorized content. To achieve aim (ii) and scale our analysis\nbeyond small sample sizes and single LLM validation, we design an\n``LLM-as-judge'' pipeline that automates recipe generation, nonsense detection,\nparsing ingredients and recipe steps, and their annotation. For instance,\ncomparing its output against human annotations, the best ingredient extractor\nand annotator is Llama 3.1+Gemma 2 9B, achieving up to 78% accuracy on\ningredient matching. This automated framework enables large-scale\nquantification of memorization, creativity, and nonsense in generated recipes,\nproviding rigorous evidence of the models' creative capacities.", "AI": {"tldr": "The paper investigates memorization, creativity, and nonsense in LLM-generated recipes, using human annotations and an automated 'LLM-as-judge' pipeline for scalability.", "motivation": "To analyze and quantify memorization, creativity, and nonsense in LLM-generated recipes, and to automate human annotation for scalability.", "method": "Human annotation of 20 recipes (ingredients and steps) to assess memorization, creativity, and nonsense, followed by an automated 'LLM-as-judge' pipeline for large-scale analysis.", "result": "Mixtral relies heavily on memorized content. The automated pipeline, using Llama 3.1+Gemma 2 9B, achieves 78% accuracy in ingredient matching.", "conclusion": "The study provides a scalable framework to rigorously evaluate LLMs' creative capacities in recipe generation."}}
{"id": "2506.23225", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23225", "abs": "https://arxiv.org/abs/2506.23225", "authors": ["Yukito Tajima", "Nakamasa Inoue", "Yusuke Sekikawa", "Ikuro Sato", "Rio Yokota"], "title": "Masked Gated Linear Unit", "comment": null, "summary": "Gated Linear Units (GLUs) have become essential components in the\nfeed-forward networks of state-of-the-art Large Language Models (LLMs).\nHowever, they require twice as many memory reads compared to feed-forward\nlayers without gating, due to the use of separate weight matrices for the gate\nand value streams. To address this bottleneck, we introduce Masked Gated Linear\nUnits (MGLUs), a novel family of GLUs with an efficient kernel implementation.\nThe core contribution of MGLUs include: (1) the Mixture of Element-wise Gating\n(MoEG) architecture that learns multiple binary masks, each determining gate or\nvalue assignments at the element level on a single shared weight matrix\nresulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly\nkernel that yields up to a 19.7 $\\times$ inference-time speed-up over a naive\nPyTorch MGLU and is 47% more memory-efficient and 34% faster than standard GLUs\ndespite added architectural complexity on an RTX5090 GPU. In LLM experiments,\nthe Swish-activated variant SwiMGLU preserves its memory advantages while\nmatching - or even surpassing - the downstream accuracy of the SwiGLU baseline.", "AI": {"tldr": "MGLUs improve GLUs by reducing memory reads and enhancing efficiency with shared weight matrices and hardware-friendly kernels, achieving faster inference and better memory usage.", "motivation": "GLUs in LLMs require high memory reads due to separate weight matrices for gate and value streams, creating a bottleneck.", "method": "Introduces MGLUs with MoEG architecture for shared weight matrices and FlashMGLU kernel for efficient implementation.", "result": "MGLUs achieve 19.7x speed-up, 47% memory efficiency, and 34% faster inference than GLUs, with SwiMGLU matching or surpassing SwiGLU accuracy.", "conclusion": "MGLUs offer a scalable and efficient alternative to GLUs in LLMs, balancing performance and resource usage."}}
{"id": "2506.23844", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23844", "abs": "https://arxiv.org/abs/2506.23844", "authors": ["Hang Su", "Jun Luo", "Chang Liu", "Xiao Yang", "Yichi Zhang", "Yinpeng Dong", "Jun Zhu"], "title": "A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents", "comment": "18 pages", "summary": "Recent advances in large language models (LLMs) have catalyzed the rise of\nautonomous AI agents capable of perceiving, reasoning, and acting in dynamic,\nopen-ended environments. These large-model agents mark a paradigm shift from\nstatic inference systems to interactive, memory-augmented entities. While these\ncapabilities significantly expand the functional scope of AI, they also\nintroduce qualitatively novel security risks - such as memory poisoning, tool\nmisuse, reward hacking, and emergent misalignment - that extend beyond the\nthreat models of conventional systems or standalone LLMs. In this survey, we\nfirst examine the structural foundations and key capabilities that underpin\nincreasing levels of agent autonomy, including long-term memory retention,\nmodular tool use, recursive planning, and reflective reasoning. We then analyze\nthe corresponding security vulnerabilities across the agent stack, identifying\nfailure modes such as deferred decision hazards, irreversible tool chains, and\ndeceptive behaviors arising from internal state drift or value misalignment.\nThese risks are traced to architectural fragilities that emerge across\nperception, cognition, memory, and action modules. To address these challenges,\nwe systematically review recent defense strategies deployed at different\nautonomy layers, including input sanitization, memory lifecycle control,\nconstrained decision-making, structured tool invocation, and introspective\nreflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a\nunified cognitive framework grounded in Constrained Markov Decision Processes\n(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,\nand joint reward-risk optimization to enable principled, proactive safety\nacross the agent's decision-making loop.", "AI": {"tldr": "The paper surveys autonomous AI agents powered by large language models (LLMs), highlighting their capabilities and novel security risks, and proposes the Reflective Risk-Aware Agent Architecture (R2A2) for proactive safety.", "motivation": "To address the emerging security risks in autonomous AI agents, which extend beyond conventional systems, by analyzing vulnerabilities and proposing defensive strategies.", "method": "Examines structural foundations of agent autonomy, identifies security vulnerabilities, reviews defense strategies, and introduces the R2A2 framework based on Constrained Markov Decision Processes (CMDPs).", "result": "Identifies key risks (e.g., memory poisoning, tool misuse) and proposes R2A2 for risk-aware decision-making and proactive safety.", "conclusion": "The R2A2 framework offers a principled approach to mitigate security risks in autonomous AI agents, balancing functionality and safety."}}
{"id": "2506.22955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22955", "abs": "https://arxiv.org/abs/2506.22955", "authors": ["Haniyeh Nikkhah", "Jafar Tanha", "Mahdi Zarrin", "SeyedEhsan Roshan", "Amin Kazempour"], "title": "YM-WML: A new Yolo-based segmentation Model with Weighted Multi-class Loss for medical imaging", "comment": "Accepted at The 7th International conference on Pattern Recognition\n  and Image Analysis (IPRIA 2025)", "summary": "Medical image segmentation poses significant challenges due to class\nimbalance and the complex structure of medical images. To address these\nchallenges, this study proposes YM-WML, a novel model for cardiac image\nsegmentation. The model integrates a robust backbone for effective feature\nextraction, a YOLOv11 neck for multi-scale feature aggregation, and an\nattention-based segmentation head for precise and accurate segmentation. To\naddress class imbalance, we introduce the Weighted Multi-class Exponential\n(WME) loss function. On the ACDC dataset, YM-WML achieves a Dice Similarity\nCoefficient of 91.02, outperforming state-of-the-art methods. The model\ndemonstrates stable training, accurate segmentation, and strong generalization,\nsetting a new benchmark in cardiac segmentation tasks.", "AI": {"tldr": "YM-WML, a novel model for cardiac image segmentation, integrates a robust backbone, YOLOv11 neck, and attention-based head, achieving superior performance with a Dice score of 91.02.", "motivation": "Addressing challenges like class imbalance and complex structures in medical image segmentation.", "method": "Proposes YM-WML with a robust backbone, YOLOv11 neck, and attention-based segmentation head, plus the WME loss function for class imbalance.", "result": "Achieves a Dice Similarity Coefficient of 91.02 on the ACDC dataset, outperforming state-of-the-art methods.", "conclusion": "YM-WML sets a new benchmark in cardiac segmentation with stable training, accuracy, and strong generalization."}}
{"id": "2506.23601", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23601", "abs": "https://arxiv.org/abs/2506.23601", "authors": ["Weijie Shi", "Yue Cui", "Yaguang Wu", "Jingzhi Fang", "Shibo Zhang", "Mengze Li", "Sirui Han", "Jia Zhu", "Jiajie Xu", "Xiaofang Zhou"], "title": "Semantic-guided Diverse Decoding for Large Language Model", "comment": null, "summary": "Diverse decoding of large language models is crucial for applications\nrequiring multiple semantically distinct responses, yet existing methods\nprimarily achieve lexical rather than semantic diversity. This limitation\nsignificantly constrains Best-of-N strategies, group-based reinforcement\nlearning, and data synthesis. While temperature sampling and diverse beam\nsearch modify token distributions or apply n-gram penalties, they fail to\nensure meaningful semantic differentiation. We introduce Semantic-guided\nDiverse Decoding (SemDiD), operating directly in embedding space that balances\nquality with diversity through three complementary mechanisms: orthogonal\ndirectional guidance, dynamic inter-group repulsion, and position-debiased\nprobability assessment. SemDiD harmonizes these competing objectives using\nadaptive gain functions and constraint optimization, ensuring both quality\nthresholds and maximal semantic differentiation. Experiments show SemDiD\nconsistently outperforms existing methods, improving Best-of-N coverage by\n1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15%\nwhile increasing accuracy by up to 2.1%.", "AI": {"tldr": "SemDiD improves semantic diversity in language model outputs, outperforming existing methods in quality and diversity.", "motivation": "Existing methods lack semantic diversity, limiting applications like Best-of-N strategies and reinforcement learning.", "method": "SemDiD uses orthogonal directional guidance, dynamic inter-group repulsion, and position-debiased probability assessment in embedding space.", "result": "SemDiD improves Best-of-N coverage by 1.4-5.2%, accelerates RLHF training by 15%, and boosts accuracy by up to 2.1%.", "conclusion": "SemDiD effectively balances quality and semantic diversity, outperforming current approaches."}}
{"id": "2506.23266", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23266", "abs": "https://arxiv.org/abs/2506.23266", "authors": ["Lujun Li", "Zhu Qiyuan", "Jiacheng Wang", "Wei Li", "Hao Gu", "Sirui Han", "Yike Guo"], "title": "Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert Merging", "comment": "Work in progress, revisions ongoing", "summary": "Mixture of Experts (MoE) LLMs face significant obstacles due to their massive\nparameter scale, which imposes memory, storage, and deployment challenges.\nAlthough recent expert merging methods promise greater efficiency by\nconsolidating multiple experts, they are fundamentally hindered by parameter\nconflicts arising from expert specialization. In this paper, we present\nSub-MoE, a novel MoE compression framework via Subspace Expert Merging. Our key\ninsight is to perform joint Singular Value Decomposition (SVD) on concatenated\nexpert weights, reducing conflicting parameters by extracting shared\n$U$-matrices while enabling effective merging of the expert-specific $V$\ncomponents. Specifically, Sub-MoE consists of two innovative phases: (1)\nAdaptive Expert Clustering, which groups functionally coherent experts via\nK-means clustering based on cosine similarity of expert outputs; and (2)\nSubspace Expert Merging, which first enforces Experts Union Decomposition to\nderive the shared $U$-matrix across experts in the same group, then pursues\nfrequency-based merging for individual $V$-matrices, and finalizes expert\nreconstruction using the merged $V$-matrix. In this way, we align and fuse\nexperts in a shared subspace, and can be extended with intra-expert compression\nfor further inference optimization. Extensive experiments on Mixtral, DeepSeek,\nand Qwen-1.5|3 MoE LLMs demonstrate that our Sub-MoE significantly outperforms\nexisting expert pruning and merging methods. Notably, our Sub-MoE maintains\n96\\%|86\\% of original performance with 25\\%|50\\% expert reduction on\nMixtral-8x7B in zero-shot benchmarks. Code will be released at\nhttps://github.com/lliai/MoERazor.", "AI": {"tldr": "Sub-MoE is a compression framework for Mixture of Experts (MoE) LLMs, addressing parameter conflicts via subspace expert merging and adaptive clustering.", "motivation": "MoE LLMs face memory and deployment challenges due to large parameter scales, and existing merging methods suffer from parameter conflicts.", "method": "Sub-MoE uses joint SVD on expert weights, adaptive clustering, and subspace merging to align and fuse experts.", "result": "Sub-MoE maintains 96%|86% performance with 25%|50% expert reduction on Mixtral-8x7B.", "conclusion": "Sub-MoE outperforms existing methods, offering efficient compression without significant performance loss."}}
{"id": "2506.23908", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23908", "abs": "https://arxiv.org/abs/2506.23908", "authors": ["Andr\u00e1s Gy\u00f6rgy", "Tor Lattimore", "Nevena Lazi\u0107", "Csaba Szepesv\u00e1ri"], "title": "Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence", "comment": null, "summary": "Sound deductive reasoning -- the ability to derive new knowledge from\nexisting facts and rules -- is an indisputably desirable aspect of general\nintelligence. Despite the major advances of AI systems in areas such as math\nand science, especially since the introduction of transformer architectures, it\nis well-documented that even the most advanced frontier systems regularly and\nconsistently falter on easily-solvable deductive reasoning tasks. Hence, these\nsystems are unfit to fulfill the dream of achieving artificial general\nintelligence capable of sound deductive reasoning. We argue that their unsound\nbehavior is a consequence of the statistical learning approach powering their\ndevelopment. To overcome this, we contend that to achieve reliable deductive\nreasoning in learning-based AI systems, researchers must fundamentally shift\nfrom optimizing for statistical performance against distributions on reasoning\nproblems and algorithmic tasks to embracing the more ambitious exact learning\nparadigm, which demands correctness on all inputs. We argue that exact learning\nis both essential and possible, and that this ambitious objective should guide\nalgorithm design.", "AI": {"tldr": "The paper critiques AI's reliance on statistical learning for deductive reasoning, advocating for a shift to exact learning to achieve reliable general intelligence.", "motivation": "Current AI systems, despite advances, fail at simple deductive reasoning tasks, making them unsuitable for artificial general intelligence.", "method": "Proposes a shift from statistical learning to exact learning, ensuring correctness on all inputs.", "result": "Statistical learning is inadequate for sound deductive reasoning; exact learning is essential.", "conclusion": "Exact learning should guide future AI algorithm design to achieve reliable deductive reasoning."}}
{"id": "2506.22960", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22960", "abs": "https://arxiv.org/abs/2506.22960", "authors": ["Shreyas Dixit", "Ashhar Aziz", "Shashwat Bajpai", "Vasu Sharma", "Aman Chadha", "Vinija Jain", "Amitava Das"], "title": "Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image Watermarking Technique for AI-Generated Images", "comment": null, "summary": "A report by the European Union Law Enforcement Agency predicts that by 2026,\nup to 90 percent of online content could be synthetically generated, raising\nconcerns among policymakers, who cautioned that \"Generative AI could act as a\nforce multiplier for political disinformation. The combined effect of\ngenerative text, images, videos, and audio may surpass the influence of any\nsingle modality.\" In response, California's Bill AB 3211 mandates the\nwatermarking of AI-generated images, videos, and audio. However, concerns\nremain regarding the vulnerability of invisible watermarking techniques to\ntampering and the potential for malicious actors to bypass them entirely.\nGenerative AI-powered de-watermarking attacks, especially the newly introduced\nvisual paraphrase attack, have shown an ability to fully remove watermarks,\nresulting in a paraphrase of the original image. This paper introduces PECCAVI,\nthe first visual paraphrase attack-safe and distortion-free image watermarking\ntechnique. In visual paraphrase attacks, an image is altered while preserving\nits core semantic regions, termed Non-Melting Points (NMPs). PECCAVI\nstrategically embeds watermarks within these NMPs and employs multi-channel\nfrequency domain watermarking. It also incorporates noisy burnishing to counter\nreverse-engineering efforts aimed at locating NMPs to disrupt the embedded\nwatermark, thereby enhancing durability. PECCAVI is model-agnostic. All\nrelevant resources and codes will be open-sourced.", "AI": {"tldr": "PECCAVI is a new watermarking technique for AI-generated content that resists visual paraphrase attacks by embedding watermarks in Non-Melting Points (NMPs) and using multi-channel frequency domain methods.", "motivation": "The rise of synthetic content and its potential for disinformation has led to regulatory measures like watermarking, but existing methods are vulnerable to attacks like visual paraphrasing.", "method": "PECCAVI embeds watermarks in NMPs (core semantic regions) and uses multi-channel frequency domain watermarking with noisy burnishing to prevent reverse-engineering.", "result": "PECCAVI is the first technique to resist visual paraphrase attacks while maintaining image quality.", "conclusion": "PECCAVI offers a robust, model-agnostic solution for watermarking AI-generated content, addressing vulnerabilities in current methods."}}
{"id": "2506.23610", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.23610", "abs": "https://arxiv.org/abs/2506.23610", "authors": ["Manuel Pratelli", "Marinella Petrocchi"], "title": "Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs", "comment": "pre-print version - paper actually under submission", "summary": "Large language models (LLMs) make it possible to generate synthetic\nbehavioural data at scale, offering an ethical and low-cost alternative to\nhuman experiments. Whether such data can faithfully capture psychological\ndifferences driven by personality traits, however, remains an open question. We\nevaluate the capacity of LLM agents, conditioned on Big-Five profiles, to\nreproduce personality-based variation in susceptibility to misinformation,\nfocusing on news discernment, the ability to judge true headlines as true and\nfalse headlines as false. Leveraging published datasets in which human\nparticipants with known personality profiles rated headline accuracy, we create\nmatching LLM agents and compare their responses to the original human patterns.\nCertain trait-misinformation associations, notably those involving\nAgreeableness and Conscientiousness, are reliably replicated, whereas others\ndiverge, revealing systematic biases in how LLMs internalize and express\npersonality. The results underscore both the promise and the limits of\npersonality-aligned LLMs for behavioral simulation, and offer new insight into\nmodeling cognitive diversity in artificial agents.", "AI": {"tldr": "LLMs can generate synthetic behavioral data, but their ability to replicate personality-driven psychological differences is uncertain. This study tests LLM agents conditioned on Big-Five traits to mimic human susceptibility to misinformation, finding some traits replicated while others diverge.", "motivation": "To assess whether LLMs can ethically and accurately simulate human personality-driven behaviors, particularly in discerning misinformation.", "method": "LLM agents were conditioned on Big-Five personality profiles and compared to human responses in judging headline accuracy using published datasets.", "result": "Some traits (Agreeableness, Conscientiousness) were reliably replicated, while others showed biases in how LLMs internalize personality.", "conclusion": "LLMs show promise but have limits in simulating personality-driven behaviors, offering insights into modeling cognitive diversity in AI."}}
{"id": "2506.23274", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23274", "abs": "https://arxiv.org/abs/2506.23274", "authors": ["Hans Peter Lynsg\u00f8e Raaschou-jensen", "Constanza Fierro", "Anders S\u00f8gaard"], "title": "Predicting thinking time in Reasoning models", "comment": null, "summary": "Reasoning models that produce long, hidden chains of thought have emerged as\npowerful tools for complex, reasoning-intensive\ntasks\\citep{deepseekai2025deepseekr1incentivizingreasoningcapability,\nopenai2024openaio1card}. However, this paradigm introduces a new user\nexperience challenge: users have little insight into how much time the model\nwill spend reasoning before returning an answer. This unpredictability, can\nlead to user frustration and is likely to compound as LLMs can produce\nincreasingly long tasks asynchronously\n\\citep{kwa2025measuringaiabilitycomplete}. In this paper, we introduce and\nevaluate methods for both online and offline prediction of model \"thinking\ntime,\" aiming to develop a practical \"progress bar for reasoning.\" We discuss\nthe implications for user interaction and future research directions.", "AI": {"tldr": "The paper addresses the unpredictability of reasoning time in models with hidden chains of thought and proposes methods to predict and display this time, akin to a \"progress bar for reasoning.\"", "motivation": "Users lack insight into how long models will spend reasoning before providing answers, leading to frustration, especially as models handle longer tasks asynchronously.", "method": "The paper introduces and evaluates methods for online and offline prediction of model \"thinking time.\"", "result": "The proposed methods aim to provide a practical solution for predicting and displaying reasoning time.", "conclusion": "The work highlights the importance of predictable reasoning time for user interaction and suggests future research directions."}}
{"id": "2506.23924", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23924", "abs": "https://arxiv.org/abs/2506.23924", "authors": ["Akshit Kumar", "Tianyi Peng", "Yuhang Wu", "Assaf Zeevi"], "title": "Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice", "comment": null, "summary": "Large language models (LLMs) have exhibited expert-level capabilities across\nvarious domains. However, their abilities to solve problems in Operations\nResearch (OR) -- the analysis and optimization of mathematical models derived\nfrom real-world problems or their verbal descriptions -- remain underexplored.\nIn this work, we take a first step toward evaluating LLMs' abilities to solve\nstochastic modeling problems, a core class of OR problems characterized by\nuncertainty and typically involving tools from probability, statistics, and\nstochastic processes. We manually procure a representative set of\ngraduate-level homework and doctoral qualification-exam problems and test LLMs'\nabilities to solve them. We further leverage SimOpt, an open-source library of\nsimulation-optimization problems and solvers, to investigate LLMs' abilities to\nmake real-world decisions under uncertainty. Our results show that, though a\nnontrivial amount of work is still needed to reliably automate the stochastic\nmodeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on\npar with human experts in both classroom and practical settings. These findings\nhighlight the potential of building AI agents that assist OR researchers and\namplify the real-world impact of OR through automation.", "AI": {"tldr": "LLMs show promise in solving stochastic OR problems, matching human experts in some cases, but further work is needed for reliable automation.", "motivation": "To explore LLMs' capabilities in solving stochastic modeling problems in Operations Research, an underexplored area.", "method": "Manually curated graduate-level problems and SimOpt library tests to evaluate LLMs' performance in stochastic modeling and decision-making under uncertainty.", "result": "LLMs perform on par with human experts in classroom and practical settings, though more work is needed for reliable automation.", "conclusion": "LLMs have potential to assist OR researchers and enhance real-world OR impact through automation."}}
{"id": "2506.22967", "categories": ["cs.CV", "cs.LG", "cs.MM", "I.2.10; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.22967", "abs": "https://arxiv.org/abs/2506.22967", "authors": ["Amir Aghdam", "Vincent Tao Hu"], "title": "ActAlign: Zero-Shot Fine-Grained Video Classification via Language-Guided Sequence Alignment", "comment": "Preprint manuscript - Project page:\n  https://github.com/aghdamamir/act-align", "summary": "We address the task of zero-shot fine-grained video classification, where no\nvideo examples or temporal annotations are available for unseen action classes.\nWhile contrastive vision-language models such as SigLIP demonstrate strong\nopen-set recognition via mean-pooled image-text similarity, they fail to\ncapture the temporal structure critical for distinguishing fine-grained\nactivities. We introduce ActAlign, a zero-shot framework that formulates video\nclassification as sequence alignment. For each class, a large language model\ngenerates an ordered sub-action sequence, which is aligned with video frames\nusing Dynamic Time Warping (DTW) in a shared embedding space. Without any\nvideo-text supervision or fine-tuning, ActAlign achieves 30.5% accuracy on the\nextremely challenging ActionAtlas benchmark, where human accuracy is only\n61.6%. ActAlign outperforms billion-parameter video-language models while using\napproximately 8x less parameters. These results demonstrate that structured\nlanguage priors, combined with classical alignment techniques, offer a scalable\nand general approach to unlocking the open-set recognition potential of\nvision-language models for fine-grained video understanding.", "AI": {"tldr": "ActAlign, a zero-shot framework, uses sequence alignment and language priors for fine-grained video classification without video-text supervision, outperforming larger models.", "motivation": "Existing contrastive vision-language models lack temporal structure understanding for fine-grained video classification.", "method": "ActAlign aligns sub-action sequences (generated by a large language model) with video frames using Dynamic Time Warping (DTW) in a shared embedding space.", "result": "Achieves 30.5% accuracy on ActionAtlas (human accuracy: 61.6%) with 8x fewer parameters than larger models.", "conclusion": "Structured language priors and classical alignment techniques enhance vision-language models for fine-grained video understanding."}}
{"id": "2506.23661", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23661", "abs": "https://arxiv.org/abs/2506.23661", "authors": ["Arnisa Fazla", "Lucas Krauter", "David Guzman Piedrahita", "Andrianos Michail"], "title": "Robustness of Misinformation Classification Systems to Adversarial Examples Through BeamAttack", "comment": "12 pages main text, 27 pages total including references and\n  appendices. 13 figures, 10 tables. Accepted for publication in the LNCS\n  proceedings of CLEF 2025 (Best-of-Labs track)", "summary": "We extend BeamAttack, an adversarial attack algorithm designed to evaluate\nthe robustness of text classification systems through word-level modifications\nguided by beam search. Our extensions include support for word deletions and\nthe option to skip substitutions, enabling the discovery of minimal\nmodifications that alter model predictions. We also integrate LIME to better\nprioritize word replacements. Evaluated across multiple datasets and victim\nmodels (BiLSTM, BERT, and adversarially trained RoBERTa) within the BODEGA\nframework, our approach achieves over a 99\\% attack success rate while\npreserving the semantic and lexical similarity of the original texts. Through\nboth quantitative and qualitative analysis, we highlight BeamAttack's\neffectiveness and its limitations. Our implementation is available at\nhttps://github.com/LucK1Y/BeamAttack", "AI": {"tldr": "BeamAttack is extended to include word deletions and skip substitutions, achieving high attack success rates while preserving text similarity.", "motivation": "To enhance adversarial attack methods for evaluating text classification robustness by discovering minimal modifications.", "method": "Extends BeamAttack with word deletions, skip substitutions, and LIME integration for prioritizing word replacements. Evaluated on BiLSTM, BERT, and RoBERTa.", "result": "Over 99% attack success rate while maintaining semantic and lexical similarity.", "conclusion": "BeamAttack is effective but has limitations; implementation is publicly available."}}
{"id": "2506.23280", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23280", "abs": "https://arxiv.org/abs/2506.23280", "authors": ["Chaoqun Du", "Yulin Wang", "Shiji Song", "Gao Huang"], "title": "BAPE: Learning an Explicit Bayes Classifier for Long-tailed Visual Recognition", "comment": null, "summary": "Bayesian decision theory advocates the Bayes classifier as the optimal\napproach for minimizing the risk in machine learning problems. Current deep\nlearning algorithms usually solve for the optimal classifier by\n\\emph{implicitly} estimating the posterior probabilities, \\emph{e.g.}, by\nminimizing the Softmax cross-entropy loss. This simple methodology has been\nproven effective for meticulously balanced academic benchmark datasets.\nHowever, it is not applicable to the long-tailed data distributions in the real\nworld, where it leads to the gradient imbalance issue and fails to ensure the\nBayes optimal decision rule. To address these challenges, this paper presents a\nnovel approach (BAPE) that provides a more precise theoretical estimation of\nthe data distributions by \\emph{explicitly} modeling the parameters of the\nposterior probabilities and solving them with point estimation. Consequently,\nour method directly learns the Bayes classifier without gradient descent based\non Bayes' theorem, simultaneously alleviating the gradient imbalance and\nensuring the Bayes optimal decision rule. Furthermore, we propose a\nstraightforward yet effective \\emph{distribution adjustment} technique. This\nmethod enables the Bayes classifier trained from the long-tailed training set\nto effectively adapt to the test data distribution with an arbitrary imbalance\nfactor, thereby enhancing performance without incurring additional\ncomputational costs. In addition, we demonstrate the gains of our method are\northogonal to existing learning approaches for long-tailed scenarios, as they\nare mostly designed under the principle of \\emph{implicitly} estimating the\nposterior probabilities. Extensive empirical evaluations on CIFAR-10-LT,\nCIFAR-100-LT, ImageNet-LT, and iNaturalist demonstrate that our method\nsignificantly improves the generalization performance of popular deep networks,\ndespite its simplicity.", "AI": {"tldr": "The paper introduces BAPE, a method for explicitly modeling posterior probabilities to address gradient imbalance in long-tailed data, ensuring Bayes optimal decisions without gradient descent.", "motivation": "Current deep learning methods implicitly estimate posterior probabilities, failing in long-tailed data distributions due to gradient imbalance and suboptimal decisions.", "method": "BAPE explicitly models posterior probabilities using point estimation, directly learning the Bayes classifier. It includes a distribution adjustment technique for adapting to test data.", "result": "BAPE improves generalization on CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and iNaturalist, outperforming existing methods.", "conclusion": "BAPE provides a theoretically sound and effective solution for long-tailed data, orthogonal to existing implicit estimation approaches."}}
{"id": "2506.23926", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23926", "abs": "https://arxiv.org/abs/2506.23926", "authors": ["Junping Wang", "Bicheng Wang", "Yibo Xuea", "Yuan Xie"], "title": "Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system", "comment": null, "summary": "Resilience non-equilibrium measurement, the ability to maintain fundamental\nfunctionality amidst failures and errors, is crucial for scientific management\nand engineering applications of industrial chain. The problem is particularly\nchallenging when the number or types of multiple co-evolution of resilience\n(for example, randomly placed) are extremely chaos. Existing end-to-end deep\nlearning ordinarily do not generalize well to unseen full-feld reconstruction\nof spatiotemporal co-evolution structure, and predict resilience of network\ntopology, especially in multiple chaos data regimes typically seen in\nreal-world applications. To address this challenge, here we propose industrial\nbrain, a human-like autonomous cognitive decision-making and planning framework\nintegrating higher-order activity-driven neuro network and CT-OODA symbolic\nreasoning to autonomous plan resilience directly from observational data of\nglobal variable. The industrial brain not only understands and model structure\nof node activity dynamics and network co-evolution topology without simplifying\nassumptions, and reveal the underlying laws hidden behind complex networks, but\nalso enabling accurate resilience prediction, inference, and planning.\nExperimental results show that industrial brain significantly outperforms\nresilience prediction and planning methods, with an accurate improvement of up\nto 10.8\\% over GoT and OlaGPT framework and 11.03\\% over spectral dimension\nreduction. It also generalizes to unseen topologies and dynamics and maintains\nrobust performance despite observational disturbances. Our findings suggest\nthat industrial brain addresses an important gap in resilience prediction and\nplanning for industrial chain.", "AI": {"tldr": "The paper introduces 'industrial brain,' a framework combining neuro networks and symbolic reasoning to predict and plan resilience in industrial chains, outperforming existing methods by up to 11.03%.", "motivation": "Existing deep learning methods struggle with resilience prediction in chaotic, real-world industrial chain scenarios, necessitating a more robust solution.", "method": "Proposes 'industrial brain,' integrating higher-order neuro networks and CT-OODA symbolic reasoning to model node dynamics and network co-evolution without simplifications.", "result": "Industrial brain improves resilience prediction accuracy by up to 10.8% over GoT/OlaGPT and 11.03% over spectral dimension reduction, generalizing well to unseen data.", "conclusion": "The industrial brain effectively fills a critical gap in resilience prediction and planning for industrial chains, demonstrating superior performance and robustness."}}
{"id": "2506.22979", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22979", "abs": "https://arxiv.org/abs/2506.22979", "authors": ["Jie Liu", "Jiayi Shen", "Pan Zhou", "Jan-Jakob Sonke", "Efstratios Gavves"], "title": "Probabilistic Prototype Calibration of Vision-Language Models for Generalized Few-shot Semantic Segmentation", "comment": "ICCV2025 Proceeding", "summary": "Generalized Few-Shot Semantic Segmentation (GFSS) aims to extend a\nsegmentation model to novel classes with only a few annotated examples while\nmaintaining performance on base classes. Recently, pretrained vision-language\nmodels (VLMs) such as CLIP have been leveraged in GFSS to improve\ngeneralization on novel classes through multi-modal prototypes learning.\nHowever, existing prototype-based methods are inherently deterministic,\nlimiting the adaptability of learned prototypes to diverse samples,\nparticularly for novel classes with scarce annotations. To address this, we\npropose FewCLIP, a probabilistic prototype calibration framework over\nmulti-modal prototypes from the pretrained CLIP, thus providing more adaptive\nprototype learning for GFSS. Specifically, FewCLIP first introduces a prototype\ncalibration mechanism, which refines frozen textual prototypes with learnable\nvisual calibration prototypes, leading to a more discriminative and adaptive\nrepresentation. Furthermore, unlike deterministic prototype learning\ntechniques, FewCLIP introduces distribution regularization over these\ncalibration prototypes. This probabilistic formulation ensures structured and\nuncertainty-aware prototype learning, effectively mitigating overfitting to\nlimited novel class data while enhancing generalization. Extensive experimental\nresults on PASCAL-5$^i$ and COCO-20$^i$ datasets demonstrate that our proposed\nFewCLIP significantly outperforms state-of-the-art approaches across both GFSS\nand class-incremental setting. The code is available at\nhttps://github.com/jliu4ai/FewCLIP.", "AI": {"tldr": "FewCLIP introduces a probabilistic prototype calibration framework for GFSS, improving adaptability and generalization over deterministic methods.", "motivation": "Existing prototype-based methods in GFSS are deterministic, limiting adaptability to diverse samples, especially for novel classes with few annotations.", "method": "FewCLIP refines frozen textual prototypes with learnable visual calibration prototypes and introduces distribution regularization for uncertainty-aware learning.", "result": "FewCLIP outperforms state-of-the-art methods on PASCAL-5$^i$ and COCO-20$^i$ datasets in GFSS and class-incremental settings.", "conclusion": "The probabilistic approach of FewCLIP enhances prototype adaptability and generalization, addressing limitations of deterministic methods."}}
{"id": "2506.23662", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.23662", "abs": "https://arxiv.org/abs/2506.23662", "authors": ["Philip Lippmann", "Jie Yang"], "title": "Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation", "comment": null, "summary": "Context-aware embedding methods boost retrieval accuracy by conditioning on\ncorpus statistics (e.g., term co-occurrence and topical patterns) extracted\nfrom neighboring documents. However, this context-aware approach requires\naccess to the target corpus or requires domain-specific finetuning, posing\npractical barriers in privacy-sensitive or resource-constrained settings. We\npresent ZEST, a zero-shot contextual adaptation framework that replaces real\ncorpus access with a one-time offline synthesis of a compact proxy. Given only\na handful exemplar documents representative of the general target domain, we\nuse a multi-step hierarchical procedure to generate a synthetic context corpus\nof several hundred documents that aims to emulate key domain-specific\ndistributions. At inference, the frozen context-aware encoder uses this proxy\ncorpus -- without any finetuning or target corpus access -- to produce\ndomain-adapted embeddings. Across the MTEB benchmark, ZEST's zero-shot\nsynthetic context adaptation using only five example documents performs within\n0.5% of models leveraging full target corpus access -- demonstrating remarkable\nefficacy without any retraining. ZEST thus provides a practical method for\ndeploying high-performance, adaptable embeddings in constrained environments.", "AI": {"tldr": "ZEST is a zero-shot framework for contextual embedding adaptation using synthetic proxy corpora, achieving near-full-corpus performance without retraining or real corpus access.", "motivation": "Current context-aware embedding methods require target corpus access or domain-specific finetuning, which is impractical in privacy-sensitive or resource-limited settings.", "method": "ZEST synthesizes a compact proxy corpus from a few exemplar documents, emulating domain-specific distributions, and uses it for zero-shot adaptation of frozen encoders.", "result": "On the MTEB benchmark, ZEST performs within 0.5% of models with full corpus access, using only five example documents.", "conclusion": "ZEST offers a practical solution for deploying adaptable, high-performance embeddings in constrained environments without retraining or real corpus access."}}
{"id": "2506.23286", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.23286", "abs": "https://arxiv.org/abs/2506.23286", "authors": ["Alan Jeffares", "Mihaela van der Schaar"], "title": "Not All Explanations for Deep Learning Phenomena Are Equally Valuable", "comment": "Accepted at ICML 2025 for oral presentation", "summary": "Developing a better understanding of surprising or counterintuitive phenomena\nhas constituted a significant portion of deep learning research in recent\nyears. These include double descent, grokking, and the lottery ticket\nhypothesis -- among many others. Works in this area often develop ad hoc\nhypotheses attempting to explain these observed phenomena on an isolated,\ncase-by-case basis. This position paper asserts that, in many prominent cases,\nthere is little evidence to suggest that these phenomena appear in real-world\napplications and these efforts may be inefficient in driving progress in the\nbroader field. Consequently, we argue against viewing them as isolated puzzles\nthat require bespoke resolutions or explanations. However, despite this, we\nsuggest that deep learning phenomena do still offer research value by providing\nunique settings in which we can refine our broad explanatory theories of more\ngeneral deep learning principles. This position is reinforced by analyzing the\nresearch outcomes of several prominent examples of these phenomena from the\nrecent literature. We revisit the current norms in the research community in\napproaching these problems and propose practical recommendations for future\nresearch, aiming to ensure that progress on deep learning phenomena is well\naligned with the ultimate pragmatic goal of progress in the broader field of\ndeep learning.", "AI": {"tldr": "The paper critiques the focus on isolated, counterintuitive deep learning phenomena, arguing they lack real-world relevance but can still refine broader theories.", "motivation": "To challenge the efficiency of researching isolated deep learning phenomena without clear real-world applicability.", "method": "Analyzes prominent examples of such phenomena and revisits research norms.", "result": "Finds little evidence of real-world impact but suggests value in refining general theories.", "conclusion": "Recommends aligning research on deep learning phenomena with broader field progress."}}
{"id": "2506.23949", "categories": ["cs.AI", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.23949", "abs": "https://arxiv.org/abs/2506.23949", "authors": ["Anthony M. Barrett", "Jessica Newman", "Brandie Nonnecke", "Nada Madkour", "Dan Hendrycks", "Evan R. Murphy", "Krystal Jackson", "Deepika Raman"], "title": "AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models", "comment": null, "summary": "Increasingly multi-purpose AI models, such as cutting-edge large language\nmodels or other 'general-purpose AI' (GPAI) models, 'foundation models,'\ngenerative AI models, and 'frontier models' (typically all referred to\nhereafter with the umbrella term 'GPAI/foundation models' except where greater\nspecificity is needed), can provide many beneficial capabilities but also risks\nof adverse events with profound consequences. This document provides\nrisk-management practices or controls for identifying, analyzing, and\nmitigating risks of GPAI/foundation models. We intend this document primarily\nfor developers of large-scale, state-of-the-art GPAI/foundation models; others\nthat can benefit from this guidance include downstream developers of end-use\napplications that build on a GPAI/foundation model. This document facilitates\nconformity with or use of leading AI risk management-related standards,\nadapting and building on the generic voluntary guidance in the NIST AI Risk\nManagement Framework and ISO/IEC 23894, with a focus on the unique issues faced\nby developers of GPAI/foundation models.", "AI": {"tldr": "The paper outlines risk-management practices for general-purpose AI (GPAI) and foundation models, targeting developers to mitigate risks and align with standards like NIST and ISO/IEC.", "motivation": "Address the dual nature of GPAI/foundation models, which offer benefits but also pose significant risks, necessitating structured risk management.", "method": "Proposes risk-management controls and practices tailored for GPAI/foundation model developers, adapting existing frameworks like NIST AI Risk Management Framework and ISO/IEC 23894.", "result": "Provides actionable guidance for identifying, analyzing, and mitigating risks associated with GPAI/foundation models.", "conclusion": "The document serves as a resource for developers to manage risks effectively while leveraging the capabilities of GPAI/foundation models."}}
{"id": "2506.22982", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22982", "abs": "https://arxiv.org/abs/2506.22982", "authors": ["Atharv Mittal", "Agam Pandey", "Amritanshu Tiwari", "Sukrit Jindal", "Swadesh Swain"], "title": "Revisiting CroPA: A Reproducibility Study and Enhancements for Cross-Prompt Adversarial Transferability in Vision-Language Models", "comment": "Accepted to MLRC 2025", "summary": "Large Vision-Language Models (VLMs) have revolutionized computer vision,\nenabling tasks such as image classification, captioning, and visual question\nanswering. However, they remain highly vulnerable to adversarial attacks,\nparticularly in scenarios where both visual and textual modalities can be\nmanipulated. In this study, we conduct a comprehensive reproducibility study of\n\"An Image is Worth 1000 Lies: Adversarial Transferability Across Prompts on\nVision-Language Models\" validating the Cross-Prompt Attack (CroPA) and\nconfirming its superior cross-prompt transferability compared to existing\nbaselines. Beyond replication we propose several key improvements: (1) A novel\ninitialization strategy that significantly improves Attack Success Rate (ASR).\n(2) Investigate cross-image transferability by learning universal\nperturbations. (3) A novel loss function targeting vision encoder attention\nmechanisms to improve generalization. Our evaluation across prominent VLMs --\nincluding Flamingo, BLIP-2, and InstructBLIP as well as extended experiments on\nLLaVA validates the original results and demonstrates that our improvements\nconsistently boost adversarial effectiveness. Our work reinforces the\nimportance of studying adversarial vulnerabilities in VLMs and provides a more\nrobust framework for generating transferable adversarial examples, with\nsignificant implications for understanding the security of VLMs in real-world\napplications.", "AI": {"tldr": "The study validates and improves the Cross-Prompt Attack (CroPA) on Vision-Language Models (VLMs), enhancing adversarial transferability and attack success rates.", "motivation": "VLMs are vulnerable to adversarial attacks, especially when both visual and textual inputs are manipulated. This work aims to reproduce and improve existing adversarial methods.", "method": "The study replicates CroPA and introduces three improvements: a novel initialization strategy, investigation of cross-image transferability, and a new loss function targeting vision encoder attention.", "result": "The improvements consistently boost adversarial effectiveness across VLMs like Flamingo, BLIP-2, InstructBLIP, and LLaVA.", "conclusion": "The work highlights the need to study VLM vulnerabilities and provides a robust framework for generating transferable adversarial examples, impacting real-world VLM security."}}
{"id": "2506.23667", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23667", "abs": "https://arxiv.org/abs/2506.23667", "authors": ["Junjie Zhang", "Jingyi Xi", "Zhuoyang Song", "Junyu Lu", "Yuhua Ke", "Ting Sun", "Yukun Yang", "Jiaxing Zhang", "Songxin Zhang", "Zejian Xie"], "title": "L0: Reinforcement Learning to Become General Agents", "comment": null, "summary": "Training large language models (LLMs) to act as autonomous agents for\nmulti-turn, long-horizon tasks remains significant challenges in scalability\nand training efficiency. To address this, we introduce L-Zero (L0), a scalable,\nend-to-end training pipeline for general-purpose agents. Featuring a low-cost,\nextensible, and sandboxed concurrent agent worker pool, L0 lowers the barrier\nfor applying reinforcement learning in complex environments. We also introduce\nNB-Agent, the agent scaffold within L0, which operates in a \"code-as-action\"\nfashion via a Read-Eval-Print-Loop (REPL). We evaluate L0 on factuality\nquestion-answering benchmarks. Our experiments demonstrate that a base model\ncan develop robust problem-solving skills using solely Reinforcement Learning\nwith Verifiable Rewards (RLVR). On the Qwen2.5-7B-Instruct model, our method\nboosts accuracy on SimpleQA from 30 % to 80 % and on HotpotQA from 22 % to 41\n%. We have open-sourced the entire L0 system, including our L0 series models,\nthe NB-Agent, a complete training pipeline, and the corresponding training\nrecipes on (https://github.com/cmriat/l0).", "AI": {"tldr": "L-Zero (L0) is a scalable, end-to-end training pipeline for general-purpose agents, improving LLM performance on multi-turn tasks via reinforcement learning.", "motivation": "Addressing scalability and training efficiency challenges in training LLMs for autonomous, multi-turn tasks.", "method": "Introduces L0 with a low-cost, sandboxed agent worker pool and NB-Agent, a code-as-action scaffold. Uses Reinforcement Learning with Verifiable Rewards (RLVR).", "result": "Boosts accuracy on SimpleQA from 30% to 80% and on HotpotQA from 22% to 41%.", "conclusion": "L0 effectively enhances LLM problem-solving skills and is open-sourced for broader use."}}
{"id": "2506.23287", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2506.23287", "abs": "https://arxiv.org/abs/2506.23287", "authors": ["Zelin Zang", "WenZhe Li", "Fei Chen", "Yongjie Xu", "Chang Yu", "Zhen Lei", "Stan Z. Li"], "title": "Hierarchical Quantized Diffusion Based Tree Generation Method for Hierarchical Representation and Lineage Analysis", "comment": "9 pages, 6 figures, under review", "summary": "In single-cell research, tracing and analyzing high-throughput single-cell\ndifferentiation trajectories is crucial for understanding complex biological\nprocesses. Key to this is the modeling and generation of hierarchical data that\nrepresents the intrinsic structure within datasets. Traditional methods face\nlimitations in terms of computational cost, performance, generative capacity,\nand stability. Recent VAEs based approaches have made strides in addressing\nthese challenges but still require specialized network modules for each tree\nbranch, limiting their stability and ability to capture deep hierarchical\nrelationships. To overcome these challenges, we introduce diffusion-based\napproach called HDTree. HDTree captures tree relationships within a\nhierarchical latent space using a unified hierarchical codebook and quantized\ndiffusion processes to model tree node transitions. This method improves\nstability by eliminating branch-specific modules and enhancing generative\ncapacity through gradual hierarchical changes simulated by the diffusion\nprocess. HDTree's effectiveness is demonstrated through comparisons on both\ngeneral-purpose and single-cell datasets, where it outperforms existing methods\nin terms of accuracy and performance. These contributions provide a new tool\nfor hierarchical lineage analysis, enabling more accurate and efficient\nmodeling of cellular differentiation paths and offering insights for downstream\nbiological tasks. The code of HDTree is available at anonymous link\nhttps://anonymous.4open.science/r/code_HDTree_review-A8DB.", "AI": {"tldr": "HDTree, a diffusion-based approach, improves hierarchical lineage analysis in single-cell research by using a unified hierarchical codebook and quantized diffusion processes, outperforming existing methods in accuracy and performance.", "motivation": "Traditional methods for modeling hierarchical single-cell data face computational and stability limitations. Recent VAEs-based approaches still struggle with capturing deep hierarchical relationships due to branch-specific modules.", "method": "HDTree employs a unified hierarchical codebook and quantized diffusion processes to model tree node transitions, eliminating branch-specific modules and enhancing generative capacity.", "result": "HDTree outperforms existing methods in accuracy and performance on general-purpose and single-cell datasets.", "conclusion": "HDTree provides a more accurate and efficient tool for hierarchical lineage analysis, aiding in modeling cellular differentiation paths and supporting downstream biological tasks."}}
{"id": "2506.23992", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.23992", "abs": "https://arxiv.org/abs/2506.23992", "authors": ["Aditya Shrivastava", "Komal Gupta", "Shraddha Arora"], "title": "Harnessing AI Agents to Advance Research on Refugee Child Mental Health", "comment": "14 page , 2 image , 2 tables , accepted under 5th International\n  Conference on Innovations in Computational Intelligence and Computer Vision\n  (ICICV-2025)", "summary": "The international refugee crisis deepens, exposing millions of dis placed\nchildren to extreme psychological trauma. This research suggests a com pact,\nAI-based framework for processing unstructured refugee health data and\ndistilling knowledge on child mental health. We compare two Retrieval-Aug\nmented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to\ndetermine how well they process challenging humanitarian datasets while avoid\ning hallucination hazards. By combining cutting-edge AI methods with migration\nresearch and child psychology, this study presents a scalable strategy to\nassist policymakers, mental health practitioners, and humanitarian agencies to\nbetter assist displaced children and recognize their mental wellbeing. In\ntotal, both the models worked properly but significantly Deepseek R1 is\nsuperior to Zephyr with an accuracy of answer relevance 0.91", "AI": {"tldr": "AI-based framework for analyzing refugee child mental health data, comparing RAG models Zephyr-7B-beta and DeepSeek R1-7B, with DeepSeek R1 showing superior accuracy.", "motivation": "Addressing the psychological trauma of displaced children by leveraging AI to process unstructured health data.", "method": "Comparison of two RAG pipelines (Zephyr-7B-beta and DeepSeek R1-7B) on humanitarian datasets to avoid hallucination.", "result": "DeepSeek R1 outperformed Zephyr with an accuracy of 0.91 in answer relevance.", "conclusion": "The study offers a scalable AI strategy to aid policymakers and practitioners in improving mental health support for displaced children."}}
{"id": "2506.23004", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.23004", "abs": "https://arxiv.org/abs/2506.23004", "authors": ["Vaigai Nayaki Yokar", "Hoa Le-Minh", "Xicong Li", "Wai Lok Woo", "Luis Nero Alves", "Stanislav Zvanovec", "Tran The Son", "Zabih Ghassemlooy"], "title": "A Novel Frame Identification and Synchronization Technique for Smartphone Visible Light Communication Systems Based on Convolutional Neural Networks", "comment": null, "summary": "This paper proposes a novel, robust, and lightweight supervised Convolutional\nNeural Network (CNN)-based technique for frame identification and\nsynchronization, designed to enhance short-link communication performance in a\nscreen-to-camera (S2C) based visible light communication (VLC) system.\nDeveloped using Python and the TensorFlow Keras framework, the proposed CNN\nmodel was trained through three real-time experimental investigations conducted\nin Jupyter Notebook. These experiments incorporated a dataset created from\nscratch to address various real-time challenges in S2C communication, including\nblurring, cropping, and rotated images in mobility scenarios. Overhead frames\nwere introduced for synchronization, which leads to enhanced system\nperformance. The experimental results demonstrate that the proposed model\nachieves an overall accuracy of approximately 98.74%, highlighting its\neffectiveness in identifying and synchronizing frames in S2C VLC systems.", "AI": {"tldr": "A lightweight CNN-based method for frame identification and synchronization in S2C VLC systems achieves 98.74% accuracy.", "motivation": "To improve short-link communication performance in S2C VLC systems by addressing challenges like blurring, cropping, and rotated images.", "method": "A supervised CNN model developed in Python/TensorFlow Keras, trained on a custom dataset via real-time experiments.", "result": "98.74% accuracy in frame identification and synchronization.", "conclusion": "The proposed CNN model is effective for enhancing S2C VLC system performance."}}
{"id": "2506.23735", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23735", "abs": "https://arxiv.org/abs/2506.23735", "authors": ["JiaRu Wu", "Mingwei Liu"], "title": "AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data", "comment": null, "summary": "Large language models (LLMs) have shown remarkable performance on various\ntasks, but existing evaluation benchmarks are often static and insufficient to\nfully assess their robustness and generalization in realistic scenarios. Prior\nwork using evolutionary or adversarial data augmentation has improved\nevaluation diversity but lacks systematic control over perturbation types and\nmulti-step complexity, limiting comprehensive robustness analysis. To address\nthese gaps, we propose AutoEvoEval, an evolution-based evaluation framework for\nclose-ended tasks such as multi-choice question answering. AutoEvoEval\nintroduces 22 interpretable atomic evolution operations and supports\nmulti-round compositions, enabling controlled generation of diverse,\nchallenging, and realistic test samples. We conduct extensive experiments\naddressing four research questions on a broad set of open- and closed-source\nLLMs. Our results show that atomic operations cause an average accuracy drop of\n7.283\\%, with structure-disrupting or misleading semantic edits causing the\nlargest declines. Model sensitivities vary significantly for the same\nperturbation, and combining multiple evolution steps amplifies adversarial\neffects by up to 52.932\\%. These findings suggest current benchmarks may\noverestimate true model generalization and emphasize the need for\nevolution-aware robustness evaluation. Code and resources are available at:\nhttps://github.com/SYSUSELab/AutoEvoEval.", "AI": {"tldr": "AutoEvoEval is a framework for evaluating LLMs using controlled, diverse, and realistic test samples via interpretable atomic evolution operations. It reveals significant accuracy drops and model sensitivities, highlighting the limitations of current benchmarks.", "motivation": "Existing evaluation benchmarks for LLMs are static and lack systematic control over perturbation types and complexity, limiting robustness assessment.", "method": "Proposes AutoEvoEval, a framework with 22 atomic evolution operations and multi-round compositions to generate diverse test samples for close-ended tasks.", "result": "Atomic operations cause a 7.283% accuracy drop, with structure-disrupting edits being most harmful. Combining steps amplifies adversarial effects by up to 52.932%.", "conclusion": "Current benchmarks may overestimate model generalization; evolution-aware evaluation is needed for robust assessment."}}
{"id": "2506.23339", "categories": ["cs.LG", "cs.AI", "physics.chem-ph", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2506.23339", "abs": "https://arxiv.org/abs/2506.23339", "authors": ["Malikussaid", "Hilal Hudan Nuha"], "title": "VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular Design", "comment": "16 pages, 1 figure, 5 algorithms, 7 tables, to be published in ICSECS\n  Conference 2025, unabridged version", "summary": "Large Language Models (LLMs) demonstrate remarkable potential for scientific\ndiscovery, but their application in domains requiring factual accuracy and\ndomain-specific constraints remains challenging. In molecular design for drug\ndiscovery, LLMs can suggest creative molecular modifications but often produce\nchemically invalid or impractical structures. We present VALID-Mol, a\nsystematic framework for integrating chemical validation with LLM-driven\nmolecular design that increases the rate of generating valid chemical\nstructures from 3% to 83%. Our approach combines methodical prompt engineering,\nautomated chemical validation, and a fine-tuned domain-adapted LLM to ensure\nreliable generation of synthesizable molecules with improved properties. Beyond\nthe specific implementation, we contribute a generalizable methodology for\nscientifically-constrained LLM applications, with quantifiable reliability\nimprovements. Computational predictions suggest our framework can generate\npromising candidates for synthesis with up to 17-fold computationally predicted\nimprovements in target affinity while maintaining synthetic accessibility. We\nprovide a detailed analysis of our prompt engineering process, validation\narchitecture, and fine-tuning approach, offering a reproducible blueprint for\napplying LLMs to other scientific domains where domain-specific validation is\nessential.", "AI": {"tldr": "VALID-Mol improves LLM-driven molecular design by integrating chemical validation, increasing valid structure generation from 3% to 83%.", "motivation": "LLMs struggle with factual accuracy and domain-specific constraints in scientific applications like drug discovery, often producing invalid molecular structures.", "method": "Combines prompt engineering, automated chemical validation, and a fine-tuned domain-adapted LLM to ensure reliable molecule generation.", "result": "Achieves up to 17-fold predicted improvements in target affinity while maintaining synthetic accessibility.", "conclusion": "Offers a generalizable framework for scientifically-constrained LLM applications, with reproducible methods for other domains."}}
{"id": "2506.24026", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.24026", "abs": "https://arxiv.org/abs/2506.24026", "authors": ["Yongyi Wang", "Wenxin Li"], "title": "Constructing Non-Markovian Decision Process via History Aggregator", "comment": null, "summary": "In the domain of algorithmic decision-making, non-Markovian dynamics manifest\nas a significant impediment, especially for paradigms such as Reinforcement\nLearning (RL), thereby exerting far-reaching consequences on the advancement\nand effectiveness of the associated systems. Nevertheless, the existing\nbenchmarks are deficient in comprehensively assessing the capacity of decision\nalgorithms to handle non-Markovian dynamics. To address this deficiency, we\nhave devised a generalized methodology grounded in category theory. Notably, we\nestablished the category of Markov Decision Processes (MDP) and the category of\nnon-Markovian Decision Processes (NMDP), and proved the equivalence\nrelationship between them. This theoretical foundation provides a novel\nperspective for understanding and addressing non-Markovian dynamics. We further\nintroduced non-Markovianity into decision-making problem settings via the\nHistory Aggregator for State (HAS). With HAS, we can precisely control the\nstate dependency structure of decision-making problems in the time series. Our\nanalysis demonstrates the effectiveness of our method in representing a broad\nrange of non-Markovian dynamics. This approach facilitates a more rigorous and\nflexible evaluation of decision algorithms by testing them in problem settings\nwhere non-Markovian dynamics are explicitly constructed.", "AI": {"tldr": "The paper introduces a category theory-based method to address non-Markovian dynamics in decision-making, proving equivalence between MDP and NMDP categories and proposing HAS for precise state dependency control.", "motivation": "Existing benchmarks fail to assess decision algorithms' handling of non-Markovian dynamics, limiting progress in fields like Reinforcement Learning.", "method": "Developed a generalized methodology using category theory, defining MDP and NMDP categories and proving their equivalence. Introduced HAS to control state dependency.", "result": "Demonstrated effectiveness in representing non-Markovian dynamics, enabling rigorous evaluation of decision algorithms.", "conclusion": "The approach provides a theoretical foundation and practical tool for understanding and testing non-Markovian dynamics in decision-making."}}
{"id": "2506.23009", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23009", "abs": "https://arxiv.org/abs/2506.23009", "authors": ["Jian Chen", "Wenye Ma", "Penghang Liu", "Wei Wang", "Tengwei Song", "Ming Li", "Chenguang Wang", "Ruiyi Zhang", "Changyou Chen"], "title": "MusiXQA: Advancing Visual Music Understanding in Multimodal Large Language Models", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable visual\nreasoning abilities in natural images, text-rich documents, and graphic\ndesigns. However, their ability to interpret music sheets remains\nunderexplored. To bridge this gap, we introduce MusiXQA, the first\ncomprehensive dataset for evaluating and advancing MLLMs in music sheet\nunderstanding. MusiXQA features high-quality synthetic music sheets generated\nvia MusiXTeX, with structured annotations covering note pitch and duration,\nchords, clefs, key/time signatures, and text, enabling diverse visual QA tasks.\nThrough extensive evaluations, we reveal significant limitations of current\nstate-of-the-art MLLMs in this domain. Beyond benchmarking, we developed\nPhi-3-MusiX, an MLLM fine-tuned on our dataset, achieving significant\nperformance gains over GPT-based methods. The proposed dataset and model\nestablish a foundation for future advances in MLLMs for music sheet\nunderstanding. Code, data, and model will be released upon acceptance.", "AI": {"tldr": "The paper introduces MusiXQA, a dataset for evaluating MLLMs in music sheet understanding, and presents Phi-3-MusiX, a fine-tuned model outperforming GPT-based methods.", "motivation": "Current MLLMs lack exploration in music sheet interpretation, despite their success in other visual reasoning tasks.", "method": "The authors create MusiXQA, a synthetic dataset with structured annotations, and develop Phi-3-MusiX, a fine-tuned MLLM.", "result": "Evaluations show current MLLMs struggle with music sheets, while Phi-3-MusiX achieves significant performance improvements.", "conclusion": "MusiXQA and Phi-3-MusiX provide a foundation for advancing MLLMs in music sheet understanding."}}
{"id": "2506.23743", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23743", "abs": "https://arxiv.org/abs/2506.23743", "authors": ["Tiziano Labruna", "Simone Gallo", "Giovanni Da San Martino"], "title": "Positional Bias in Binary Question Answering: How Uncertainty Shapes Model Preferences", "comment": null, "summary": "Positional bias in binary question answering occurs when a model\nsystematically favors one choice over another based solely on the ordering of\npresented options. In this study, we quantify and analyze positional bias\nacross five large language models under varying degrees of answer uncertainty.\nWe re-adapted the SQuAD-it dataset by adding an extra incorrect answer option\nand then created multiple versions with progressively less context and more\nout-of-context answers, yielding datasets that range from low to high\nuncertainty. Additionally, we evaluate two naturally higher-uncertainty\nbenchmarks: (1) WebGPT - question pairs with unequal human-assigned quality\nscores, and (2) Winning Arguments - where models predict the more persuasive\nargument in Reddit's r/ChangeMyView exchanges. Across each dataset, the order\nof the \"correct\" (or higher-quality/persuasive) option is systematically\nflipped (first placed in position 1, then in position 2) to compute both\nPreference Fairness and Position Consistency. We observe that positional bias\nis nearly absent under low-uncertainty conditions, but grows exponentially when\nit becomes doubtful to decide which option is correct.", "AI": {"tldr": "The paper investigates positional bias in binary question answering across five large language models, showing bias grows with answer uncertainty.", "motivation": "To quantify and analyze positional bias in models when answer uncertainty varies.", "method": "Adapted SQuAD-it dataset with incorrect options and tested on WebGPT and Winning Arguments benchmarks, flipping answer order to measure bias.", "result": "Positional bias is minimal in low-uncertainty conditions but increases exponentially with uncertainty.", "conclusion": "Answer uncertainty significantly influences positional bias in models, highlighting a need for bias mitigation in uncertain scenarios."}}
{"id": "2506.23349", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23349", "abs": "https://arxiv.org/abs/2506.23349", "authors": ["Keziah Naggita", "Julienne LaChance"], "title": "A case for data valuation transparency via DValCards", "comment": null, "summary": "Following the rise in popularity of data-centric machine learning (ML),\nvarious data valuation methods have been proposed to quantify the contribution\nof each datapoint to desired ML model performance metrics (e.g., accuracy).\nBeyond the technical applications of data valuation methods (e.g., data\ncleaning, data acquisition, etc.), it has been suggested that within the\ncontext of data markets, data buyers might utilize such methods to fairly\ncompensate data owners. Here we demonstrate that data valuation metrics are\ninherently biased and unstable under simple algorithmic design choices,\nresulting in both technical and ethical implications. By analyzing 9 tabular\nclassification datasets and 6 data valuation methods, we illustrate how (1)\ncommon and inexpensive data pre-processing techniques can drastically alter\nestimated data values; (2) subsampling via data valuation metrics may increase\nclass imbalance; and (3) data valuation metrics may undervalue underrepresented\ngroup data. Consequently, we argue in favor of increased transparency\nassociated with data valuation in-the-wild and introduce the novel Data\nValuation Cards (DValCards) framework towards this aim. The proliferation of\nDValCards will reduce misuse of data valuation metrics, including in data\npricing, and build trust in responsible ML systems.", "AI": {"tldr": "Data valuation methods in ML are biased and unstable, affected by pre-processing and algorithmic choices, leading to ethical and technical issues. A new framework, DValCards, is proposed for transparency.", "motivation": "To highlight biases and instability in data valuation methods and their ethical implications, advocating for transparency.", "method": "Analysis of 9 tabular datasets and 6 valuation methods, examining effects of pre-processing, subsampling, and bias.", "result": "Pre-processing alters data values, subsampling worsens imbalance, and underrepresented data is undervalued.", "conclusion": "DValCards framework is introduced to promote transparency and responsible use of data valuation metrics."}}
{"id": "2506.24119", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.24119", "abs": "https://arxiv.org/abs/2506.24119", "authors": ["Bo Liu", "Leon Guertler", "Simon Yu", "Zichen Liu", "Penghui Qi", "Daniel Balcells", "Mickel Liu", "Cheston Tan", "Weiyan Shi", "Min Lin", "Wee Sun Lee", "Natasha Jaques"], "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning", "comment": "Work in Progress", "summary": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.", "AI": {"tldr": "SPIRAL is a self-play framework for language models to learn reasoning through zero-sum games without human supervision, achieving transferable improvements in reasoning tasks.", "motivation": "To eliminate dependency on human-curated data and domain-specific rewards in reinforcement learning for reasoning tasks.", "method": "SPIRAL uses self-play in multi-turn, zero-sum games with role-conditioned advantage estimation (RAE) for stable multi-agent training.", "result": "Training on Kuhn Poker improved math and general reasoning by 8.6% and 8.4%, outperforming supervised fine-tuning. Multi-game training further enhanced performance.", "conclusion": "Zero-sum games naturally develop transferable reasoning, offering a promising path for autonomous reasoning development."}}
{"id": "2506.23030", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23030", "abs": "https://arxiv.org/abs/2506.23030", "authors": ["Alejandro Romero Amezcua", "Mariano Jos\u00e9 Juan Rivera Meraz"], "title": "VisionScores -- A system-segmented image score dataset for deep learning tasks", "comment": "Comments: 5 pages, 3 figures. Accepted for presentation at the 2025\n  IEEE International Conference on Image Processing (ICIP). \\c{opyright} 2025\n  IEEE. Personal use of this material is permitted. Permission from IEEE must\n  be obtained for any other use", "summary": "VisionScores presents a novel proposal being the first system-segmented image\nscore dataset, aiming to offer structure-rich, high information-density images\nfor machine and deep learning tasks. Delimited to two-handed piano pieces, it\nwas built to consider not only certain graphic similarity but also composition\npatterns, as this creative process is highly instrument-dependent. It provides\ntwo scenarios in relation to composer and composition type. The first, formed\nby 14k samples, considers works from different authors but the same composition\ntype, specifically, Sonatinas. The latter, consisting of 10.8K samples,\npresents the opposite case, various composition types from the same author,\nbeing the one selected Franz Liszt. All of the 24.8k samples are formatted as\ngrayscale jpg images of $128 \\times 512$ pixels. VisionScores supplies the\nusers not only the formatted samples but the systems' order and pieces'\nmetadata. Moreover, unsegmented full-page scores and the pre-formatted images\nare included for further analysis.", "AI": {"tldr": "VisionScores introduces the first system-segmented image score dataset for machine learning, focusing on two-handed piano pieces with 24.8k samples in two scenarios: same composition type (Sonatinas) from different composers and different composition types from Franz Liszt.", "motivation": "To provide a structured, high-density image dataset for machine/deep learning, capturing graphic similarity and composition patterns specific to piano pieces.", "method": "Dataset includes 24.8k grayscale images (128x512 pixels) of two-handed piano scores, segmented by system. Two scenarios: 14k samples of Sonatinas from various composers and 10.8k samples of diverse compositions by Liszt. Metadata and full-page scores are also provided.", "result": "A comprehensive dataset (VisionScores) with structured, segmented piano scores, supporting machine learning tasks with rich metadata and additional resources.", "conclusion": "VisionScores fills a gap in structured musical score datasets, offering valuable resources for ML/DL applications in music analysis."}}
{"id": "2506.23840", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23840", "abs": "https://arxiv.org/abs/2506.23840", "authors": ["Bowen Ding", "Yuhan Chen", "Futing Wang", "Lingfeng Ming", "Tao Lin"], "title": "Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model", "comment": "13 pages, 5 figures", "summary": "Large Reasoning Models (LRMs) excel at solving complex problems but face an\noverthinking dilemma. When handling simple tasks, they often produce verbose\nresponses overloaded with thinking tokens (e.g., wait, however). These tokens\ntrigger unnecessary high-level reasoning behaviors like reflection and\nbacktracking, reducing efficiency. In this work, our pilot study reveals that\nthese thinking-token-induced behaviors are not essential for effective\nproblem-solving and may even hinder correct reasoning within constrained token\nbudgets. We identify this phenomenon as the thinking trap. To mitigate this\nissue, we propose Dual Policy Preference Optimization (DuP-PO), a novel\nalgorithm featuring: (1) A rollout sampling strategy that guarantees balanced\nexposure to responses with and without thinking tokens; (2) A fine-grained\nadvantage control technique to dynamically regulate the prediction of target\ntokens; (3) A policy shaping method ensuring stable gradient contributions from\nthinking tokens. Experimental results on five popular math reasoning benchmarks\nshow that DuP-PO performs well on the popular LRM, which significantly improves\ntheir token efficiency during reasoning, while achieving superior performance\nof the base model.", "AI": {"tldr": "DuP-PO improves token efficiency in Large Reasoning Models by reducing unnecessary thinking tokens, enhancing performance on simple tasks.", "motivation": "LRMs waste tokens on verbose responses for simple tasks due to thinking tokens, reducing efficiency.", "method": "Proposes DuP-PO with rollout sampling, advantage control, and policy shaping to regulate thinking tokens.", "result": "DuP-PO boosts token efficiency and maintains or improves performance on math reasoning benchmarks.", "conclusion": "DuP-PO effectively mitigates the thinking trap in LRMs, optimizing token usage without sacrificing accuracy."}}
{"id": "2506.23358", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23358", "abs": "https://arxiv.org/abs/2506.23358", "authors": ["Pawel Renc", "Michal K. Grzeszczyk", "Linglong Qian", "Nassim Oufattole", "Jeff Rasley", "Arkadiusz Sitek"], "title": "Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment", "comment": "conference paper", "summary": "We present Federated Timeline Synthesis (FTS), a novel framework for training\ngenerative foundation models across distributed timeseries data applied to\nelectronic health records (EHR). At its core, FTS represents patient history as\ntokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding\ntemporal, categorical, and continuous clinical information. Each institution\ntrains an autoregressive transformer on its local PHTs and transmits only model\nweights to a central server. The server uses the generators to synthesize a\nlarge corpus of trajectories and train a Global Generator (GG), enabling\nzero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS\non five clinically meaningful prediction tasks using MIMIC-IV data, showing\nthat models trained on synthetic data generated by GG perform comparably to\nthose trained on real data. FTS offers strong privacy guarantees, scalability\nacross institutions, and extensibility to diverse prediction and simulation\ntasks especially in healthcare, including counterfactual inference, early\nwarning detection, and synthetic trial design.", "AI": {"tldr": "FTS is a federated framework for training generative models on distributed EHR data, using tokenized patient timelines and federated learning to ensure privacy and scalability.", "motivation": "To enable generative modeling of EHR data across institutions without sharing raw data, addressing privacy and scalability challenges.", "method": "Tokenizes patient histories into PHTs, trains local autoregressive transformers, aggregates model weights centrally, and synthesizes data for a global generator.", "result": "GG-trained models perform comparably to real-data models on clinical prediction tasks.", "conclusion": "FTS provides privacy, scalability, and extensibility for healthcare applications like counterfactual inference and synthetic trials."}}
{"id": "2402.09146", "categories": ["cs.LG", "cs.AI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2402.09146", "abs": "https://arxiv.org/abs/2402.09146", "authors": ["Muhammad Kashif", "Muhammad Shafique"], "title": "ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks", "comment": null, "summary": "In this paper, we present a novel framework for enhancing the performance of\nQuanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional\nlayers and addressing the critical challenges associated with them. Traditional\nquanvolutional layers, although beneficial for feature extraction, have largely\nbeen static, offering limited adaptability. Unlike state-of-the-art, our\nresearch overcomes this limitation by enabling training within these layers,\nsignificantly increasing the flexibility and potential of QuNNs. However, the\nintroduction of multiple trainable quanvolutional layers induces complexities\nin gradient-based optimization, primarily due to the difficulty in accessing\ngradients across these layers. To resolve this, we propose a novel\narchitecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging\nthe concept of residual learning, which facilitates the flow of gradients by\nadding skip connections between layers. By inserting residual blocks between\nquanvolutional layers, we ensure enhanced gradient access throughout the\nnetwork, leading to improved training performance. Moreover, we provide\nempirical evidence on the strategic placement of these residual blocks within\nQuNNs. Through extensive experimentation, we identify an efficient\nconfiguration of residual blocks, which enables gradients across all the layers\nin the network that eventually results in efficient training. Our findings\nsuggest that the precise location of residual blocks plays a crucial role in\nmaximizing the performance gains in QuNNs. Our results mark a substantial step\nforward in the evolution of quantum deep learning, offering new avenues for\nboth theoretical development and practical quantum computing applications.", "AI": {"tldr": "A novel framework, Residual Quanvolutional Neural Networks (ResQuNNs), enhances QuNNs by introducing trainable quanvolutional layers and residual learning to improve gradient flow and training performance.", "motivation": "Traditional quanvolutional layers are static and lack adaptability, limiting QuNN performance. This research aims to overcome this by enabling training within these layers.", "method": "Proposes ResQuNNs with residual blocks between quanvolutional layers to facilitate gradient flow. Empirical evidence identifies optimal residual block placement.", "result": "Improved training performance and gradient access in QuNNs, with strategic residual block placement maximizing gains.", "conclusion": "ResQuNNs advance quantum deep learning, offering theoretical and practical benefits for quantum computing."}}
{"id": "2506.23038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23038", "abs": "https://arxiv.org/abs/2506.23038", "authors": ["Xinrong Hu", "Yiyu Shi"], "title": "Inpainting is All You Need: A Diffusion-based Augmentation Method for Semi-supervised Medical Image Segmentation", "comment": null, "summary": "Collecting pixel-level labels for medical datasets can be a laborious and\nexpensive process, and enhancing segmentation performance with a scarcity of\nlabeled data is a crucial challenge. This work introduces AugPaint, a data\naugmentation framework that utilizes inpainting to generate image-label pairs\nfrom limited labeled data. AugPaint leverages latent diffusion models, known\nfor their ability to generate high-quality in-domain images with low overhead,\nand adapts the sampling process for the inpainting task without need for\nretraining. Specifically, given a pair of image and label mask, we crop the\narea labeled with the foreground and condition on it during reversed denoising\nprocess for every noise level. Masked background area would gradually be filled\nin, and all generated images are paired with the label mask. This approach\nensures the accuracy of match between synthetic images and label masks, setting\nit apart from existing dataset generation methods. The generated images serve\nas valuable supervision for training downstream segmentation models,\neffectively addressing the challenge of limited annotations. We conducted\nextensive evaluations of our data augmentation method on four public medical\nimage segmentation datasets, including CT, MRI, and skin imaging. Results\nacross all datasets demonstrate that AugPaint outperforms state-of-the-art\nlabel-efficient methodologies, significantly improving segmentation\nperformance.", "AI": {"tldr": "AugPaint is a data augmentation framework using inpainting to generate synthetic medical image-label pairs from limited labeled data, improving segmentation performance.", "motivation": "Pixel-level labeling in medical datasets is costly and time-consuming, and enhancing segmentation with scarce labeled data is a challenge.", "method": "AugPaint uses latent diffusion models for inpainting, conditioning on cropped foreground areas during denoising to generate accurate image-label pairs.", "result": "AugPaint outperforms state-of-the-art methods on four medical datasets (CT, MRI, skin imaging), significantly boosting segmentation performance.", "conclusion": "AugPaint effectively addresses limited annotations by generating high-quality synthetic data, enhancing segmentation model training."}}
{"id": "2506.23864", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23864", "abs": "https://arxiv.org/abs/2506.23864", "authors": ["Seyed Mahed Mousavi", "Edoardo Cecchinato", "Lucia Hornikova", "Giuseppe Riccardi"], "title": "Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What to Do About It", "comment": null, "summary": "We conduct a systematic audit of three widely used reasoning benchmarks,\nSocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark\nitems and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and\nLLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic\nissues in benchmark design (e.g., duplicated items, ambiguous wording, and\nimplausible answers), as well as scoring procedures that prioritize output form\nover reasoning process. Through systematic human annotation and re-evaluation\non cleaned benchmark subsets, we find that model scores often improve not due\nto due to erratic surface wording variations and not to improved reasoning.\nInfact, further analyses show that model performance is highly sensitive to\nminor input variations such as context availability and phrasing, revealing\nthat high scores may reflect alignment with format-specific cues rather than\nconsistent inference based on the input. These findings challenge the validity\nof current benchmark-based claims about reasoning in LLMs, and highlight the\nneed for evaluation protocols that assess reasoning as a process of drawing\ninference from available information, rather than as static output selection.\nWe release audited data and evaluation tools to support more interpretable and\ndiagnostic assessments of model reasoning.", "AI": {"tldr": "The paper audits three reasoning benchmarks (SocialIQa, FauxPas-EAI, ToMi) and finds flaws in items and evaluation methods. Using LLMs, it identifies design and scoring issues, showing model performance is sensitive to input variations, not reasoning. It calls for better evaluation protocols.", "motivation": "To assess the validity of reasoning benchmarks and LLM evaluations, uncovering flaws that may misrepresent model capabilities.", "method": "Systematic audit using five LLMs (GPT-3, 3.5, 4, o1, LLaMA 3.1) and human annotation to identify benchmark issues and re-evaluate cleaned subsets.", "result": "Found structural, semantic, and pragmatic flaws in benchmarks. Model scores improve due to input variations, not reasoning. Performance is sensitive to minor changes.", "conclusion": "Current benchmarks may misrepresent LLM reasoning. Need for evaluation protocols focusing on inference processes, not static outputs. Audited data and tools released for better assessments."}}
{"id": "2506.23374", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23374", "abs": "https://arxiv.org/abs/2506.23374", "authors": ["Dominik Meier", "Sujai Hiremath", "Promit Ghosal", "Kyra Gan"], "title": "When Additive Noise Meets Unobserved Mediators: Bivariate Denoising Diffusion for Causal Discovery", "comment": null, "summary": "Distinguishing cause and effect from bivariate observational data is a\nfoundational problem in many disciplines, but challenging without additional\nassumptions. Additive noise models (ANMs) are widely used to enable\nsample-efficient bivariate causal discovery. However, conventional ANM-based\nmethods fail when unobserved mediators corrupt the causal relationship between\nvariables. This paper makes three key contributions: first, we rigorously\ncharacterize why standard ANM approaches break down in the presence of\nunmeasured mediators. Second, we demonstrate that prior solutions for hidden\nmediation are brittle in finite sample settings, limiting their practical\nutility. To address these gaps, we propose Bivariate Denoising Diffusion (BiDD)\nfor causal discovery, a method designed to handle latent noise introduced by\nunmeasured mediators. Unlike prior methods that infer directionality through\nmean squared error loss comparisons, our approach introduces a novel\nindependence test statistic: during the noising and denoising processes for\neach variable, we condition on the other variable as input and evaluate the\nindependence of the predicted noise relative to this input. We prove asymptotic\nconsistency of BiDD under the ANM, and conjecture that it performs well under\nhidden mediation. Experiments on synthetic and real-world data demonstrate\nconsistent performance, outperforming existing methods in mediator-corrupted\nsettings while maintaining strong performance in mediator-free settings.", "AI": {"tldr": "The paper addresses the challenge of causal discovery in bivariate data with unobserved mediators, proposing a new method (BiDD) that outperforms existing approaches.", "motivation": "Standard additive noise models (ANMs) fail when unmeasured mediators corrupt causal relationships, and prior solutions are brittle in finite samples.", "method": "Proposes Bivariate Denoising Diffusion (BiDD), using a novel independence test during noising/denoising processes to handle latent noise.", "result": "BiDD shows consistent performance in synthetic and real-world data, excelling in mediator-corrupted settings.", "conclusion": "BiDD is a robust solution for causal discovery under hidden mediation, outperforming existing methods."}}
{"id": "2506.23042", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23042", "abs": "https://arxiv.org/abs/2506.23042", "authors": ["Hung Nguyen", "An Le", "Runfa Li", "Truong Nguyen"], "title": "From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient 3D Gaussian Splatting", "comment": "Accepted to ICCV Workshop", "summary": "3D Gaussian Splatting has emerged as a powerful approach in novel view\nsynthesis, delivering rapid training and rendering but at the cost of an\never-growing set of Gaussian primitives that strains memory and bandwidth. We\nintroduce AutoOpti3DGS, a training-time framework that automatically restrains\nGaussian proliferation without sacrificing visual fidelity. The key idea is to\nfeed the input images to a sequence of learnable Forward and Inverse Discrete\nWavelet Transforms, where low-pass filters are kept fixed, high-pass filters\nare learnable and initialized to zero, and an auxiliary orthogonality loss\ngradually activates fine frequencies. This wavelet-driven, coarse-to-fine\nprocess delays the formation of redundant fine Gaussians, allowing 3DGS to\ncapture global structure first and refine detail only when necessary. Through\nextensive experiments, AutoOpti3DGS requires just a single filter learning-rate\nhyper-parameter, integrates seamlessly with existing efficient 3DGS frameworks,\nand consistently produces sparser scene representations more compatible with\nmemory or storage-constrained hardware.", "AI": {"tldr": "AutoOpti3DGS reduces Gaussian proliferation in 3D Gaussian Splatting by using learnable wavelet transforms, maintaining visual quality while optimizing memory usage.", "motivation": "The growing set of Gaussian primitives in 3D Gaussian Splatting strains memory and bandwidth, necessitating a method to control proliferation without losing fidelity.", "method": "Uses learnable Forward and Inverse Discrete Wavelet Transforms with fixed low-pass and learnable high-pass filters, activated by an orthogonality loss for coarse-to-fine refinement.", "result": "Produces sparser scene representations, compatible with memory-constrained hardware, with minimal hyper-parameter tuning.", "conclusion": "AutoOpti3DGS effectively balances detail and efficiency in 3DGS, enhancing practicality for constrained hardware."}}
{"id": "2506.23888", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23888", "abs": "https://arxiv.org/abs/2506.23888", "authors": ["Andr\u00e9 de Souza Loureiro", "Jorge Valverde-Rebaza", "Julieta Noguez", "David Escarcega", "Ricardo Marcacini"], "title": "Advancing Multi-Step Mathematical Reasoning in Large Language Models through Multi-Layered Self-Reflection with Auto-Prompting", "comment": "Accepted for publication in: European Conference on Machine Learning\n  and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD\n  2025). Research Track", "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their problem-solving capabilities. However, these models still\nstruggle when faced with complex multi-step reasoning tasks. In this paper, we\npropose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework,\na novel approach designed to enhance multi-step mathematical reasoning in LLMs\nby integrating techniques such as Chain of Thought (CoT), Self-Reflection, and\nAuto-Prompting. Unlike traditional static prompting methods, MAPS employs an\niterative refinement process. Initially, the model generates a solution using\nCoT prompting. When errors are detected, an adaptive self-reflection mechanism\nidentifies and analyzes them, generating tailored prompts to guide corrections.\nThese dynamically adjusted prompts enable the model to iteratively refine its\nreasoning. Experiments on four well-established benchmarks across multiple LLMs\nshow that MAPS significantly outperforms standard CoT and achieves competitive\nresults with reasoning-optimized models. In addition, MAPS enables\ngeneral-purpose LLMs to reach performance levels comparable to specialized\nreasoning models. While deeper reflection layers improve accuracy, they also\nincrease token usage and costs. To balance this trade-off, MAPS strategically\nlimits reflection depth, ensuring an optimal balance between cost and reasoning\nperformance.", "AI": {"tldr": "The MAPS framework enhances multi-step reasoning in LLMs by combining CoT, Self-Reflection, and Auto-Prompting, outperforming standard methods and matching specialized models.", "motivation": "LLMs struggle with complex multi-step reasoning tasks, necessitating a more adaptive and iterative approach.", "method": "MAPS integrates CoT, Self-Reflection, and Auto-Prompting to iteratively refine reasoning through dynamic prompts.", "result": "MAPS outperforms standard CoT and achieves competitive results with reasoning-optimized models on benchmarks.", "conclusion": "MAPS balances accuracy and cost by limiting reflection depth, enabling general-purpose LLMs to perform like specialized models."}}
{"id": "2506.23408", "categories": ["cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2506.23408", "abs": "https://arxiv.org/abs/2506.23408", "authors": ["Claudionor Coelho Jr", "Yanen Li", "Philip Tee"], "title": "Do LLMs Dream of Discrete Algorithms?", "comment": null, "summary": "Large Language Models (LLMs) have rapidly transformed the landscape of\nartificial intelligence, enabling natural language interfaces and dynamic\norchestration of software components. However, their reliance on probabilistic\ninference limits their effectiveness in domains requiring strict logical\nreasoning, discrete decision-making, and robust interpretability. This paper\ninvestigates these limitations and proposes a neurosymbolic approach that\naugments LLMs with logic-based reasoning modules, particularly leveraging\nProlog predicates and composable toolsets. By integrating first-order logic and\nexplicit rule systems, our framework enables LLMs to decompose complex queries\ninto verifiable sub-tasks, orchestrate reliable solutions, and mitigate common\nfailure modes such as hallucination and incorrect step decomposition. We\ndemonstrate the practical benefits of this hybrid architecture through\nexperiments on the DABStep benchmark, showing improved precision, coverage, and\nsystem documentation in multi-step reasoning tasks. Our results indicate that\ncombining LLMs with modular logic reasoning restores engineering rigor,\nenhances system reliability, and offers a scalable path toward trustworthy,\ninterpretable AI agents across complex domains.", "AI": {"tldr": "The paper proposes a neurosymbolic approach to enhance LLMs by integrating logic-based reasoning (e.g., Prolog) to address limitations in logical reasoning and interpretability, demonstrating improved performance on multi-step tasks.", "motivation": "LLMs lack effectiveness in domains requiring strict logical reasoning and interpretability, prompting the need for a hybrid approach.", "method": "Augments LLMs with logic-based reasoning modules (Prolog predicates) and composable toolsets to decompose queries into verifiable sub-tasks.", "result": "Experiments on the DABStep benchmark show improved precision, coverage, and documentation in multi-step reasoning.", "conclusion": "Combining LLMs with logic reasoning enhances reliability, interpretability, and scalability for trustworthy AI agents."}}
{"id": "2506.23044", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23044", "abs": "https://arxiv.org/abs/2506.23044", "authors": ["Guo-Hua Wang", "Shanshan Zhao", "Xinjie Zhang", "Liangfu Cao", "Pengxin Zhan", "Lunhao Duan", "Shiyin Lu", "Minghao Fu", "Xiaohao Chen", "Jianshan Zhao", "Yang Li", "Qing-Guo Chen"], "title": "Ovis-U1 Technical Report", "comment": "A unified model for multimodal understanding, text-to-image\n  generation, and image editing. GitHub: https://github.com/AIDC-AI/Ovis-U1", "summary": "In this report, we introduce Ovis-U1, a 3-billion-parameter unified model\nthat integrates multimodal understanding, text-to-image generation, and image\nediting capabilities. Building on the foundation of the Ovis series, Ovis-U1\nincorporates a diffusion-based visual decoder paired with a bidirectional token\nrefiner, enabling image generation tasks comparable to leading models like\nGPT-4o. Unlike some previous models that use a frozen MLLM for generation\ntasks, Ovis-U1 utilizes a new unified training approach starting from a\nlanguage model. Compared to training solely on understanding or generation\ntasks, unified training yields better performance, demonstrating the\nenhancement achieved by integrating these two tasks. Ovis-U1 achieves a score\nof 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent\nstate-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In\ntext-to-image generation, it excels with scores of 83.72 and 0.89 on the\nDPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves\n4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the\ninitial version of the Ovis unified model series, Ovis-U1 pushes the boundaries\nof multimodal understanding, generation, and editing.", "AI": {"tldr": "Ovis-U1 is a 3B-parameter unified model excelling in multimodal understanding, text-to-image generation, and image editing, outperforming state-of-the-art models.", "motivation": "To integrate multimodal understanding, generation, and editing into a single model, improving performance over specialized models.", "method": "Uses a diffusion-based visual decoder and bidirectional token refiner, trained with a unified approach from a language model.", "result": "Achieves top scores on benchmarks like OpenCompass (69.6), DPG-Bench (83.72), and ImgEdit-Bench (4.00).", "conclusion": "Ovis-U1 advances multimodal capabilities, setting a new standard for unified models."}}
{"id": "2506.23921", "categories": ["cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.23921", "abs": "https://arxiv.org/abs/2506.23921", "authors": ["Germans Savcisens", "Tina Eliassi-Rad"], "title": "The Trilemma of Truth in Large Language Models", "comment": null, "summary": "We often attribute human characteristics to large language models (LLMs) and\nclaim that they \"know\" certain things. LLMs have an internal probabilistic\nknowledge that represents information retained during training. How can we\nassess the veracity of this knowledge? We examine two common methods for\nprobing the veracity of LLMs and discover several assumptions that are flawed.\nTo address these flawed assumptions, we introduce sAwMIL (short for Sparse\nAware Multiple-Instance Learning), a probing method that utilizes the internal\nactivations of LLMs to separate statements into true, false, and neither.\nsAwMIL is based on multiple-instance learning and conformal prediction. We\nevaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including\nboth default and chat-based variants, as well as on 3 new datasets. Among the\ninsights we provide are: (1) the veracity signal is often concentrated in the\nthird quarter of an LLM's depth; (2) truth and falsehood signals are not always\nsymmetric; (3) linear probes perform better on chat models than on default\nmodels; (4) nonlinear probes may be required to capture veracity signals for\nsome LLMs with reinforcement learning from human feedback or knowledge\ndistillation; and (5) LLMs capture a third type of signal that is distinct from\ntrue and false and is neither true nor false. These findings provide a reliable\nmethod for verifying what LLMs \"know\" and how certain they are of their\nprobabilistic internal knowledge.", "AI": {"tldr": "The paper introduces sAwMIL, a method to assess the veracity of LLMs' internal knowledge, revealing key insights about truth signals and probe performance.", "motivation": "To address flawed assumptions in existing methods for probing LLM knowledge and provide a reliable way to verify what LLMs 'know.'", "method": "Introduces sAwMIL, a probing method using internal activations of LLMs, based on multiple-instance learning and conformal prediction.", "result": "Evaluated on 16 LLMs and 3 datasets, findings include truth signal concentration, asymmetry in truth/falsehood signals, and probe performance differences.", "conclusion": "sAwMIL offers a reliable method for verifying LLM knowledge, with insights into truth signals and probe effectiveness."}}
{"id": "2506.23419", "categories": ["cs.LG", "cs.AI", "cs.DL", "62G09", "J.1"], "pdf": "https://arxiv.org/pdf/2506.23419", "abs": "https://arxiv.org/abs/2506.23419", "authors": ["Amanda S Barnard"], "title": "BenchMake: Turn any scientific data set into a reproducible benchmark", "comment": "10 pages, 15 pages in Appendix, 15 figures, 5 tables, 57 references", "summary": "Benchmark data sets are a cornerstone of machine learning development and\napplications, ensuring new methods are robust, reliable and competitive. The\nrelative rarity of benchmark sets in computational science, due to the\nuniqueness of the problems and the pace of change in the associated domains,\nmakes evaluating new innovations difficult for computational scientists. In\nthis paper a new tool is developed and tested to potentially turn any of the\nincreasing numbers of scientific data sets made openly available into a\nbenchmark accessible to the community. BenchMake uses non-negative matrix\nfactorisation to deterministically identify and isolate challenging edge cases\non the convex hull (the smallest convex set that contains all existing data\ninstances) and partitions a required fraction of matched data instances into a\ntesting set that maximises divergence and statistical significance, across\ntabular, graph, image, signal and textual modalities. BenchMake splits are\ncompared to establish splits and random splits using ten publicly available\nbenchmark sets from different areas of science, with different sizes, shapes,\ndistributions.", "AI": {"tldr": "A tool called BenchMake is introduced to transform scientific datasets into benchmarks using non-negative matrix factorization to identify edge cases and partition data for testing.", "motivation": "The rarity of benchmark sets in computational science makes evaluating new innovations difficult. BenchMake aims to address this by leveraging openly available scientific datasets.", "method": "BenchMake uses non-negative matrix factorization to isolate challenging edge cases on the convex hull and partitions data into testing sets to maximize divergence and statistical significance.", "result": "BenchMake's splits are compared to established and random splits across ten diverse benchmark sets, demonstrating its effectiveness.", "conclusion": "BenchMake provides a robust method to create benchmarks from scientific datasets, enhancing evaluation of computational innovations."}}
{"id": "2506.23061", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23061", "abs": "https://arxiv.org/abs/2506.23061", "authors": ["Jiazhen Liu", "Yuchuan Deng", "Long Chen"], "title": "Empowering Small VLMs to Think with Dynamic Memorization and Exploration", "comment": null, "summary": "Empowering Small-scale Vision-Language Models (SVLMs) with reliable thinking\ncapabilities remains fundamentally challenging due to their limited parameter\ncapacity and weak instruction-following abilities. Existing training paradigms,\nincluding Supervised Fine-Tuning (SFT) and Reinforcement Learning with\nVerifiable Reward (RLVR), impose substantial demands on the base VLM, exceeding\nthe capabilities of SVLMs. Consequently, directly applying these paradigms to\nSVLMs often suffers from severe pseudo thinking traces and advantage collapse,\nultimately undermining both thinking reliability and task performance. A\nnatural solution is to combine SFT and RLVR, leveraging their complementarity\nto reduce the dependence on model capacity. However, the widely adopted\ntwo-stage training paradigm still performs poorly on SVLMs, as their tendency\ntoward sub-optimal convergence hinders the trade-off and limits the benefits of\nthe combination. To address this, we propose DyME, a novel training paradigm\nthat Dynamically selects between Memorization (via SFT) and Exploration (via\nRLVR) modes at each optimization step, ensuring that every update contributes\nto the trade-off. Extensive experiments across diverse domains demonstrate that\nDyME consistently achieves this balance, and thus delivers substantial\nperformance improvements. These results establish DyME as a practical and\neffective solution for empowering SVLMs with reliable thinking capabilities.\nGitHub: https://github.com/HKUST-LongGroup/DyME", "AI": {"tldr": "DyME is a novel training paradigm for Small-scale Vision-Language Models (SVLMs) that dynamically switches between Memorization (SFT) and Exploration (RLVR) modes to enhance thinking reliability and performance.", "motivation": "Existing training paradigms (SFT and RLVR) are too demanding for SVLMs, leading to pseudo thinking traces and advantage collapse. A combined approach is needed but struggles with sub-optimal convergence.", "method": "DyME dynamically selects between SFT (Memorization) and RLVR (Exploration) modes at each optimization step to balance trade-offs and improve performance.", "result": "DyME consistently balances the trade-off and delivers substantial performance improvements across diverse domains.", "conclusion": "DyME is a practical and effective solution for enhancing SVLMs' thinking capabilities."}}
{"id": "2506.23929", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23929", "abs": "https://arxiv.org/abs/2506.23929", "authors": ["Mohammed J. Saeed", "Tommi Vehvilainen", "Evgeny Fedoseev", "Sevil Caliskan", "Tatiana Vodolazova"], "title": "IMPACT: Inflectional Morphology Probes Across Complex Typologies", "comment": null, "summary": "Large Language Models (LLMs) have shown significant progress on various\nmultilingual benchmarks and are increasingly used to generate and evaluate text\nin non-English languages. However, while they may produce fluent outputs, it\nremains unclear to what extent these models truly grasp the underlying\nlinguistic complexity of those languages, particularly in morphology. To\ninvestigate this, we introduce IMPACT, a synthetically generated evaluation\nframework focused on inflectional morphology, which we publicly release,\ndesigned to evaluate LLM performance across five morphologically rich\nlanguages: Arabic, Russian, Finnish, Turkish, and Hebrew. IMPACT includes\nunit-test-style cases covering both shared and language-specific phenomena,\nfrom basic verb inflections (e.g., tense, number, gender) to unique features\nlike Arabic's reverse gender agreement and vowel harmony in Finnish and\nTurkish. We assess eight multilingual LLMs that, despite strong English\nperformance, struggle with other languages and uncommon morphological patterns,\nespecially when judging ungrammatical examples. We also show that Chain of\nThought and Thinking Models can degrade performance. Our work exposes gaps in\nLLMs' handling of linguistic complexity, pointing to clear room for\nimprovement. To support further research, we publicly release the IMPACT\nframework.", "AI": {"tldr": "IMPACT is a synthetic evaluation framework for assessing LLMs' understanding of inflectional morphology in five morphologically rich languages, revealing gaps in their linguistic complexity handling.", "motivation": "To determine if LLMs truly grasp linguistic complexity, especially in morphology, beyond fluent outputs in non-English languages.", "method": "Introduce IMPACT, a synthetic framework with unit-test-style cases for evaluating LLMs across Arabic, Russian, Finnish, Turkish, and Hebrew.", "result": "LLMs struggle with morphologically rich languages and uncommon patterns, especially in judging ungrammatical examples, and some techniques degrade performance.", "conclusion": "LLMs have significant gaps in handling linguistic complexity, highlighting the need for improvement; IMPACT is released to aid further research."}}
{"id": "2506.23424", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23424", "abs": "https://arxiv.org/abs/2506.23424", "authors": ["Heitor R. Medeiros", "Hossein Sharifi-Noghabi", "Gabriel L. Oliveira", "Saghar Irandoust"], "title": "Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting", "comment": "Second Workshop on Test-Time Adaptation: Putting Updates to the Test!\n  at ICML 2025, Vancouver, Canada. 2025", "summary": "Real-world time series often exhibit a non-stationary nature, degrading the\nperformance of pre-trained forecasting models. Test-Time Adaptation (TTA)\naddresses this by adjusting models during inference, but existing methods\ntypically update the full model, increasing memory and compute costs. We\npropose PETSA, a parameter-efficient method that adapts forecasters at test\ntime by only updating small calibration modules on the input and output. PETSA\nuses low-rank adapters and dynamic gating to adjust representations without\nretraining. To maintain accuracy despite limited adaptation capacity, we\nintroduce a specialized loss combining three components: (1) a robust term, (2)\na frequency-domain term to preserve periodicity, and (3) a patch-wise\nstructural term for structural alignment. PETSA improves the adaptability of\nvarious forecasting backbones while requiring fewer parameters than baselines.\nExperimental results on benchmark datasets show that PETSA achieves competitive\nor better performance across all horizons. Our code is available at:\nhttps://github.com/BorealisAI/PETSA", "AI": {"tldr": "PETSA is a parameter-efficient test-time adaptation method for time series forecasting, using small calibration modules and a specialized loss to improve adaptability without full model updates.", "motivation": "Real-world time series are non-stationary, degrading pre-trained models. Existing TTA methods update the full model, increasing costs.", "method": "PETSA updates only small calibration modules using low-rank adapters and dynamic gating, with a specialized loss combining robust, frequency-domain, and patch-wise structural terms.", "result": "PETSA improves adaptability of forecasting backbones with fewer parameters, achieving competitive or better performance on benchmark datasets.", "conclusion": "PETSA offers an efficient and effective solution for test-time adaptation in time series forecasting."}}
{"id": "2506.23066", "categories": ["cs.CV", "cs.CR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.23066", "abs": "https://arxiv.org/abs/2506.23066", "authors": ["Jiale Meng", "Yiming Li", "Zheming Lu", "Zewei He", "Hao Luo", "Tianwei Zhang"], "title": "CoreMark: Toward Robust and Universal Text Watermarking Technique", "comment": "10 pages, 16 figures", "summary": "Text watermarking schemes have gained considerable attention in recent years,\nyet still face critical challenges in achieving simultaneous robustness,\ngeneralizability, and imperceptibility. This paper introduces a new embedding\nparadigm,termed CORE, which comprises several consecutively aligned black pixel\nsegments. Its key innovation lies in its inherent noise resistance during\ntransmission and broad applicability across languages and fonts. Based on the\nCORE, we present a text watermarking framework named CoreMark. Specifically,\nCoreMark first dynamically extracts COREs from characters. Then, the characters\nwith stronger robustness are selected according to the lengths of COREs. By\nmodifying the thickness of the CORE, the hidden data is embedded into the\nselected characters without causing significant visual distortions. Moreover, a\ngeneral plug-and-play embedding strength modulator is proposed, which can\nadaptively enhance the robustness for small font sizes by adjusting the\nembedding strength according to the font size. Experimental evaluation\nindicates that CoreMark demonstrates outstanding generalizability across\nmultiple languages and fonts. Compared to existing methods, CoreMark achieves\nsignificant improvements in resisting screenshot, print-scan, and print camera\nattacks, while maintaining satisfactory imperceptibility.", "AI": {"tldr": "CoreMark introduces a text watermarking framework using CORE segments for robust, generalizable, and imperceptible data embedding across languages and fonts.", "motivation": "Addressing challenges in text watermarking like robustness, generalizability, and imperceptibility.", "method": "Uses CORE segments for embedding, selects robust characters, adjusts thickness for data embedding, and employs an adaptive strength modulator.", "result": "Outperforms existing methods in resisting attacks (screenshot, print-scan, print-camera) while maintaining imperceptibility.", "conclusion": "CoreMark is a highly effective and adaptable text watermarking solution."}}
{"id": "2506.23930", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23930", "abs": "https://arxiv.org/abs/2506.23930", "authors": ["Ruhina Tabasshum Prome", "Tarikul Islam Tamiti", "Anomadarshi Barua"], "title": "Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages", "comment": null, "summary": "The rapid expansion of social media leads to a marked increase in hate\nspeech, which threatens personal lives and results in numerous hate crimes.\nDetecting hate speech presents several challenges: diverse dialects, frequent\ncode-mixing, and the prevalence of misspelled words in user-generated content\non social media platforms. Recent progress in hate speech detection is\ntypically concentrated on high-resource languages. However, low-resource\nlanguages still face significant challenges due to the lack of large-scale,\nhigh-quality datasets. This paper investigates how we can overcome this\nlimitation via prompt engineering on large language models (LLMs) focusing on\nlow-resource Bengali language. We investigate six prompting strategies -\nzero-shot prompting, refusal suppression, flattering the classifier, multi-shot\nprompting, role prompting, and finally our innovative metaphor prompting to\ndetect hate speech effectively in low-resource languages. We pioneer the\nmetaphor prompting to circumvent the built-in safety mechanisms of LLMs that\nmarks a significant departure from existing jailbreaking methods. We\ninvestigate all six different prompting strategies on the Llama2-7B model and\ncompare the results extensively with three pre-trained word embeddings - GloVe,\nWord2Vec, and FastText for three different deep learning models - multilayer\nperceptron (MLP), convolutional neural network (CNN), and bidirectional gated\nrecurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in\nthe low-resource Bengali language, we also evaluate it in another low-resource\nlanguage - Hindi, and two high-resource languages - English and German. The\nperformance of all prompting techniques is evaluated using the F1 score, and\nenvironmental impact factor (IF), which measures CO$_2$ emissions, electricity\nusage, and computational time.", "AI": {"tldr": "The paper explores prompt engineering on LLMs for hate speech detection in low-resource languages, introducing metaphor prompting and evaluating six strategies on Llama2-7B.", "motivation": "Addressing the lack of hate speech detection in low-resource languages due to limited datasets, focusing on Bengali and extending to Hindi, English, and German.", "method": "Six prompting strategies (e.g., zero-shot, metaphor prompting) tested on Llama2-7B, compared with pre-trained embeddings (GloVe, Word2Vec, FastText) and deep learning models (MLP, CNN, BiGRU).", "result": "Performance evaluated via F1 score and environmental impact (CO2 emissions, electricity, computational time), demonstrating metaphor prompting's effectiveness.", "conclusion": "Metaphor prompting is innovative and effective for hate speech detection in low-resource languages, outperforming traditional methods while considering environmental impact."}}
{"id": "2506.23446", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23446", "abs": "https://arxiv.org/abs/2506.23446", "authors": ["Mohamed Elbasheer", "Adewale Akinfaderin"], "title": "Enhancing Insider Threat Detection Using User-Based Sequencing and Transformer Encoders", "comment": null, "summary": "Insider threat detection presents unique challenges due to the authorized\nstatus of malicious actors and the subtlety of anomalous behaviors. Existing\nmachine learning methods often treat user activity as isolated events, thereby\nfailing to leverage sequential dependencies in user behavior. In this study, we\npropose a User-Based Sequencing (UBS) methodology, transforming the CERT\ninsider threat dataset into structured temporal sequences suitable for deep\nsequential modeling. We deploy a Transformer Encoder architecture to model\nbenign user activity and employ its reconstruction errors as anomaly scores.\nThese scores are subsequently evaluated using three unsupervised outlier\ndetection algorithms: One-Class SVM (OCSVM), Local Outlier Factor (LOF), and\nIsolation Forest (iForest). Across four rigorously designed test sets,\nincluding combinations of multiple CERT dataset releases, our UBS-Transformer\npipeline consistently achieves state-of-the-art performance - notably 96.61%\naccuracy, 99.43% recall, 96.38% F1-score, 95.00% AUROC, and exceptionally low\nfalse negative (0.0057) and false positive (0.0571) rates. Comparative analyses\ndemonstrate that our approach substantially outperforms tabular and\nconventional autoencoder baselines, underscoring the efficacy of sequential\nuser modeling and advanced anomaly detection in the insider threat domain.", "AI": {"tldr": "The paper proposes a User-Based Sequencing (UBS) method with a Transformer Encoder for insider threat detection, achieving high accuracy and low error rates.", "motivation": "Existing methods fail to capture sequential dependencies in user behavior, limiting their effectiveness in detecting subtle insider threats.", "method": "UBS transforms user activity into temporal sequences, uses a Transformer Encoder for modeling, and evaluates anomalies with unsupervised outlier detection algorithms (OCSVM, LOF, iForest).", "result": "The UBS-Transformer pipeline achieves 96.61% accuracy, 99.43% recall, and exceptionally low false negative/positive rates, outperforming baselines.", "conclusion": "Sequential user modeling and advanced anomaly detection significantly improve insider threat detection performance."}}
{"id": "2506.23072", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23072", "abs": "https://arxiv.org/abs/2506.23072", "authors": ["Jing Gao"], "title": "Unsupervised 3D Braided Hair Reconstruction from a Single-View Image", "comment": "6 pages, 3 figures, accepted to the 2025 International Conference on\n  Machine Vision Applications (MVA 2025)", "summary": "Reconstructing 3D braided hairstyles from single-view images remains a\nchallenging task due to the intricate interwoven structure and complex\ntopologies of braids. Existing strand-based hair reconstruction methods\ntypically focus on loose hairstyles and often struggle to capture the\nfine-grained geometry of braided hair. In this paper, we propose a novel\nunsupervised pipeline for efficiently reconstructing 3D braided hair from\nsingle-view RGB images. Leveraging a synthetic braid model inspired by braid\ntheory, our approach effectively captures the complex intertwined structures of\nbraids. Extensive experiments demonstrate that our method outperforms\nstate-of-the-art approaches, providing superior accuracy, realism, and\nefficiency in reconstructing 3D braided hairstyles, supporting expressive\nhairstyle modeling in digital humans.", "AI": {"tldr": "A novel unsupervised pipeline for 3D braided hair reconstruction from single-view images, outperforming existing methods in accuracy and realism.", "motivation": "Existing methods struggle with braided hair due to its complex structure; this work aims to address this gap.", "method": "Uses a synthetic braid model inspired by braid theory to capture intricate braid structures.", "result": "Outperforms state-of-the-art methods in accuracy, realism, and efficiency.", "conclusion": "Supports expressive hairstyle modeling in digital humans."}}
{"id": "2506.23940", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23940", "abs": "https://arxiv.org/abs/2506.23940", "authors": ["Yang Dai", "Jianxiang An", "Tianwei Lin", "Hongyang He", "Hongzhe Huang", "Wenqiao Zhang", "Zheqi Lv", "Siliang Tang", "Yueting Zhuang"], "title": "Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved success across various\ndomains. However, their applicability tends to degrade when confronted with\ndifferent types of data inputs, especially for MLLMs that have been fine-tuned\nfor specific tasks. Despite its importance, the study of knowledge sharing\namong domain-specific MLLMs--such as those trained for mathematics or\ncode--remains largely underexplored. To address the fragmentation of knowledge\nacross domain-specialized MLLMs, we propose a unified parameter integration\nframework that enables modular composition of expert capabilities. Our method\nis grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy,\nwhich leverages both local functional attribution and global\ninformation-theoretic signals to guide selective parameter fusion. By extending\nthis mechanism to the low-rank adaptation layer granularity, we ensure\nefficient integration with minimal inference overhead. Furthermore, we\nintroduce a domain compatibility scoring mechanism that quantifies inter-expert\nalignment at the activation level and correlates with downstream task utility.\nThis principled fusion protocol allows the final model to synergize\nheterogeneous expertise while preserving structural modularity. Extensive\nevaluations across diverse multimodal benchmarks validate the effectiveness of\nour framework, offering a scalable path toward compositional, domain-adaptive\nMLLMs.", "AI": {"tldr": "A framework for unifying domain-specific MLLMs via parameter integration, using CAPS for efficient fusion and domain compatibility scoring.", "motivation": "Addressing the fragmentation of knowledge in domain-specific MLLMs and enabling modular composition of expert capabilities.", "method": "Compatibility-Aware Parameter Splicing (CAPS) strategy for selective parameter fusion, extended to low-rank adaptation layers, with domain compatibility scoring.", "result": "Effective integration of heterogeneous expertise across diverse multimodal benchmarks, with minimal inference overhead.", "conclusion": "The framework offers a scalable solution for compositional, domain-adaptive MLLMs."}}
{"id": "2506.23462", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23462", "abs": "https://arxiv.org/abs/2506.23462", "authors": ["Manaswi Kulahara", "Gautam Siddharth Kashyap", "Nipun Joshi", "Arpita Soni"], "title": "Can We Predict the Unpredictable? Leveraging DisasterNet-LLM for Multimodal Disaster Classification", "comment": "Accepted in the 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025), scheduled for 3 - 8 August 2025 in Brisbane,\n  Australia", "summary": "Effective disaster management requires timely and accurate insights, yet\ntraditional methods struggle to integrate multimodal data such as images,\nweather records, and textual reports. To address this, we propose\nDisasterNet-LLM, a specialized Large Language Model (LLM) designed for\ncomprehensive disaster analysis. By leveraging advanced pretraining,\ncross-modal attention mechanisms, and adaptive transformers, DisasterNet-LLM\nexcels in disaster classification. Experimental results demonstrate its\nsuperiority over state-of-the-art models, achieving higher accuracy of 89.5%,\nan F1 score of 88.0%, AUC of 0.92%, and BERTScore of 0.88% in multimodal\ndisaster classification tasks.", "AI": {"tldr": "DisasterNet-LLM, a specialized LLM, outperforms state-of-the-art models in disaster classification by integrating multimodal data, achieving high accuracy and scores.", "motivation": "Traditional disaster management methods fail to effectively integrate multimodal data like images, weather records, and textual reports, necessitating a more robust solution.", "method": "DisasterNet-LLM uses advanced pretraining, cross-modal attention mechanisms, and adaptive transformers for comprehensive disaster analysis.", "result": "The model achieves 89.5% accuracy, 88.0% F1 score, 0.92% AUC, and 0.88% BERTScore in multimodal disaster classification.", "conclusion": "DisasterNet-LLM is a superior solution for disaster management, offering high performance in integrating and analyzing multimodal data."}}
{"id": "2506.23074", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23074", "abs": "https://arxiv.org/abs/2506.23074", "authors": ["Yu Zheng", "Boyang Gong", "Fanye Kong", "Yueqi Duan", "Bingyao Yu", "Wenzhao Zheng", "Lei Chen", "Jiwen Lu", "Jie Zhou"], "title": "Learning Counterfactually Decoupled Attention for Open-World Model Attribution", "comment": "Accepted by ICCV 2025. Code: \\url{https://github.com/yzheng97/CDAL}", "summary": "In this paper, we propose a Counterfactually Decoupled Attention Learning\n(CDAL) method for open-world model attribution. Existing methods rely on\nhandcrafted design of region partitioning or feature space, which could be\nconfounded by the spurious statistical correlations and struggle with novel\nattacks in open-world scenarios. To address this, CDAL explicitly models the\ncausal relationships between the attentional visual traces and source model\nattribution, and counterfactually decouples the discriminative model-specific\nartifacts from confounding source biases for comparison. In this way, the\nresulting causal effect provides a quantification on the quality of learned\nattention maps, thus encouraging the network to capture essential generation\npatterns that generalize to unseen source models by maximizing the effect.\nExtensive experiments on existing open-world model attribution benchmarks show\nthat with minimal computational overhead, our method consistently improves\nstate-of-the-art models by large margins, particularly for unseen novel\nattacks. Source code: https://github.com/yzheng97/CDAL.", "AI": {"tldr": "Proposes CDAL for open-world model attribution, addressing spurious correlations and novel attacks by modeling causal relationships and decoupling artifacts.", "motivation": "Existing methods struggle with spurious correlations and novel attacks due to handcrafted designs.", "method": "CDAL models causal relationships between attentional traces and attribution, decoupling artifacts from biases.", "result": "Improves state-of-the-art models significantly, especially for unseen attacks, with minimal overhead.", "conclusion": "CDAL effectively generalizes to unseen models by focusing on essential generation patterns."}}
{"id": "2506.23951", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23951", "abs": "https://arxiv.org/abs/2506.23951", "authors": ["Mathis Le Bail", "J\u00e9r\u00e9mie Dentan", "Davide Buscaldi", "Sonia Vanier"], "title": "Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders", "comment": null, "summary": "Sparse Autoencoders (SAEs) have been successfully used to probe Large\nLanguage Models (LLMs) and extract interpretable concepts from their internal\nrepresentations. These concepts are linear combinations of neuron activations\nthat correspond to human-interpretable features. In this paper, we investigate\nthe effectiveness of SAE-based explainability approaches for sentence\nclassification, a domain where such methods have not been extensively explored.\nWe present a novel SAE-based architecture tailored for text classification,\nleveraging a specialized classifier head and incorporating an activation rate\nsparsity loss. We benchmark this architecture against established methods such\nas ConceptShap, Independent Component Analysis, and other SAE-based concept\nextraction techniques. Our evaluation covers two classification benchmarks and\nfour fine-tuned LLMs from the Pythia family. We further enrich our analysis\nwith two novel metrics for measuring the precision of concept-based\nexplanations, using an external sentence encoder. Our empirical results show\nthat our architecture improves both the causality and interpretability of the\nextracted features.", "AI": {"tldr": "SAEs are adapted for sentence classification, improving interpretability and causality of extracted features compared to existing methods.", "motivation": "To explore SAE-based explainability in sentence classification, a less-studied domain, and enhance interpretability.", "method": "A novel SAE-based architecture with a specialized classifier head and activation rate sparsity loss, benchmarked against ConceptShap, ICA, and other SAE techniques.", "result": "Improved causality and interpretability of features, validated on two benchmarks and four Pythia LLMs.", "conclusion": "The proposed SAE architecture is effective for sentence classification, offering better interpretability and causality."}}
{"id": "2506.23469", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2506.23469", "abs": "https://arxiv.org/abs/2506.23469", "authors": ["Chunjing Xiao", "Jiahui Lu", "Xovee Xu", "Fan Zhou", "Tianshu Xie", "Wei Lu", "Lifeng Xu"], "title": "Reconciling Attribute and Structural Anomalies for Improved Graph Anomaly Detection", "comment": "Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS); DOI: https://doi.org/10.1109/TNNLS.2025.3561172", "summary": "Graph anomaly detection is critical in domains such as healthcare and\neconomics, where identifying deviations can prevent substantial losses.\nExisting unsupervised approaches strive to learn a single model capable of\ndetecting both attribute and structural anomalies. However, they confront the\ntug-of-war problem between two distinct types of anomalies, resulting in\nsuboptimal performance. This work presents TripleAD, a mutual\ndistillation-based triple-channel graph anomaly detection framework. It\nincludes three estimation modules to identify the attribute, structural, and\nmixed anomalies while mitigating the interference between different types of\nanomalies. In the first channel, we design a multiscale attribute estimation\nmodule to capture extensive node interactions and ameliorate the over-smoothing\nissue. To better identify structural anomalies, we introduce a link-enhanced\nstructure estimation module in the second channel that facilitates information\nflow to topologically isolated nodes. The third channel is powered by an\nattribute-mixed curvature, a new indicator that encapsulates both attribute and\nstructural information for discriminating mixed anomalies. Moreover, a mutual\ndistillation strategy is introduced to encourage communication and\ncollaboration between the three channels. Extensive experiments demonstrate the\neffectiveness of the proposed TripleAD model against strong baselines.", "AI": {"tldr": "TripleAD is a triple-channel framework for graph anomaly detection, addressing the tug-of-war problem in existing methods by separately handling attribute, structural, and mixed anomalies through mutual distillation.", "motivation": "Existing unsupervised methods struggle with detecting both attribute and structural anomalies simultaneously due to conflicting objectives, leading to suboptimal performance.", "method": "TripleAD uses three modules: multiscale attribute estimation, link-enhanced structure estimation, and attribute-mixed curvature, with mutual distillation for collaboration.", "result": "The framework outperforms baselines in detecting anomalies by mitigating interference between anomaly types.", "conclusion": "TripleAD effectively addresses the limitations of single-model approaches by leveraging mutual distillation and specialized modules for different anomaly types."}}
{"id": "2506.23077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23077", "abs": "https://arxiv.org/abs/2506.23077", "authors": ["Suofei Zhang", "Xinxin Wang", "Xiaofu Wu", "Quan Zhou", "Haifeng Hu"], "title": "Dynamic Contrastive Learning for Hierarchical Retrieval: A Case Study of Distance-Aware Cross-View Geo-Localization", "comment": null, "summary": "Existing deep learning-based cross-view geo-localization methods primarily\nfocus on improving the accuracy of cross-domain image matching, rather than\nenabling models to comprehensively capture contextual information around the\ntarget and minimize the cost of localization errors. To support systematic\nresearch into this Distance-Aware Cross-View Geo-Localization (DACVGL) problem,\nwe construct Distance-Aware Campus (DA-Campus), the first benchmark that pairs\nmulti-view imagery with precise distance annotations across three spatial\nresolutions. Based on DA-Campus, we formulate DACVGL as a hierarchical\nretrieval problem across different domains. Our study further reveals that, due\nto the inherent complexity of spatial relationships among buildings, this\nproblem can only be addressed via a contrastive learning paradigm, rather than\nconventional metric learning. To tackle this challenge, we propose Dynamic\nContrastive Learning (DyCL), a novel framework that progressively aligns\nfeature representations according to hierarchical spatial margins. Extensive\nexperiments demonstrate that DyCL is highly complementary to existing\nmulti-scale metric learning methods and yields substantial improvements in both\nhierarchical retrieval performance and overall cross-view geo-localization\naccuracy. Our code and benchmark are publicly available at\nhttps://github.com/anocodetest1/DyCL.", "AI": {"tldr": "The paper introduces Distance-Aware Cross-View Geo-Localization (DACVGL) and a benchmark (DA-Campus) with precise distance annotations. It proposes Dynamic Contrastive Learning (DyCL) for hierarchical retrieval, outperforming existing methods.", "motivation": "Existing methods focus on cross-domain image matching but lack contextual understanding and error minimization in geo-localization.", "method": "Proposes DyCL, a contrastive learning framework aligning features hierarchically, tested on the DA-Campus benchmark.", "result": "DyCL improves hierarchical retrieval and geo-localization accuracy, complementing multi-scale metric learning.", "conclusion": "DyCL effectively addresses DACVGL, offering a robust solution for cross-view geo-localization."}}
{"id": "2506.23979", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23979", "abs": "https://arxiv.org/abs/2506.23979", "authors": ["Renren Jin", "Tianhao Shen", "Xinwei Wu", "Dan Shi", "Haoran Sun", "Wuwei Huang", "Quandong Wang", "Wei Liu", "Jian Luan", "Bin Wang", "Deyi Xiong"], "title": "TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation", "comment": "33 pages, 15 tables, 11 figures", "summary": "Conducting supervised fine-tuning and preference fine-tuning on large\nlanguage models (LLMs) requires high-quality datasets to improve their ability\nto follow instructions and align with human preferences and values. However,\nconstructing such datasets is resource-intensive, and most available datasets\nfor supervised and preference fine-tuning are in English. To address these\nchallenges, we propose the \\underline{\\textbf{Ta}}xonomy-Guided\n\\underline{\\textbf{P}}reference Data Generation (TaP) framework, which\nfacilitates automated and scalable construction of preference datasets across\nvarious languages. TaP is grounded in a structured taxonomy that allows\nfine-grained control over dataset composition, thereby ensuring both diversity\nand comprehensive coverage. We employ TaP-generated datasets to perform\nsupervised and preference fine-tuning on various LLMs. Experimental results\ndemonstrate that LLMs trained on TaP-generated datasets outperform those\ntrained on existing open-source datasets. Remarkably, LLMs trained on\nTaP-generated datasets surpass the performance of those trained on an\nopen-source dataset that is 180 times larger.", "AI": {"tldr": "The TaP framework automates scalable preference dataset generation for multilingual LLM fine-tuning, outperforming larger datasets.", "motivation": "High-quality datasets for LLM fine-tuning are resource-intensive and mostly English-only, necessitating a scalable multilingual solution.", "method": "TaP uses a structured taxonomy to automate diverse and comprehensive preference dataset generation for supervised and preference fine-tuning.", "result": "LLMs trained on TaP-generated datasets outperform those using larger open-source datasets.", "conclusion": "TaP provides an efficient, scalable solution for multilingual preference dataset generation, enhancing LLM performance."}}
{"id": "2506.23492", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23492", "abs": "https://arxiv.org/abs/2506.23492", "authors": ["Haolan Guo", "Linwei Tao", "Haoyang Luo", "Minjing Dong", "Chang Xu"], "title": "Sample Margin-Aware Recalibration of Temperature Scaling", "comment": null, "summary": "Recent advances in deep learning have significantly improved predictive\naccuracy. However, modern neural networks remain systematically overconfident,\nposing risks for deployment in safety-critical scenarios. Current post-hoc\ncalibration methods face a fundamental dilemma: global approaches like\nTemperature Scaling apply uniform adjustments across all samples, introducing\nhigh bias despite computational efficiency, while more expressive methods that\noperate on full logit distributions suffer from high variance due to noisy\nhigh-dimensional inputs and insufficient validation data. To address these\nchallenges, we propose Sample Margin-Aware Recalibration of Temperature\n(SMART), a lightweight, data-efficient recalibration method that precisely\nscales logits based on the margin between the top two logits -- termed the\nlogit gap. Specifically, the logit gap serves as a denoised, scalar signal\ndirectly tied to decision boundary uncertainty, providing a robust indicator\nthat avoids the noise inherent in high-dimensional logit spaces while\npreserving model prediction invariance. Meanwhile, SMART employs a novel\nsoft-binned Expected Calibration Error (SoftECE) objective that balances model\nbias and variance through adaptive binning, enabling stable parameter updates\neven with extremely limited calibration data. Extensive evaluations across\ndiverse datasets and architectures demonstrate that SMART achieves\nstate-of-the-art calibration performance even with substantially fewer\nparameters compared to existing parametric methods, offering a principled,\nrobust, and highly efficient solution for practical uncertainty quantification\nin neural network predictions. The source code is available at:\nhttps://anonymous.4open.science/r/SMART-8B11.", "AI": {"tldr": "SMART introduces a lightweight, data-efficient recalibration method for neural networks, using the logit gap and a novel SoftECE objective to improve calibration performance with fewer parameters.", "motivation": "Modern neural networks are systematically overconfident, posing risks in safety-critical scenarios, and current calibration methods struggle with bias-variance trade-offs.", "method": "SMART scales logits based on the logit gap (margin between top two logits) and uses a SoftECE objective with adaptive binning for stable parameter updates.", "result": "SMART achieves state-of-the-art calibration performance across diverse datasets and architectures, even with limited calibration data.", "conclusion": "SMART provides a robust, efficient solution for uncertainty quantification in neural networks, outperforming existing methods."}}
{"id": "2506.23086", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23086", "abs": "https://arxiv.org/abs/2506.23086", "authors": ["Jian Shi", "Tianqi You", "Pingping Zhang", "Hongli Zhang", "Rui Xu", "Haojie Li"], "title": "Frequency-enhanced Multi-granularity Context Network for Efficient Vertebrae Segmentation", "comment": "Accepted by MICCAI2025. More modifications my be performed", "summary": "Automated and accurate segmentation of individual vertebra in 3D CT and MRI\nimages is essential for various clinical applications. Due to the limitations\nof current imaging techniques and the complexity of spinal structures, existing\nmethods still struggle with reducing the impact of image blurring and\ndistinguishing similar vertebrae. To alleviate these issues, we introduce a\nFrequency-enhanced Multi-granularity Context Network (FMC-Net) to improve the\naccuracy of vertebrae segmentation. Specifically, we first apply wavelet\ntransform for lossless downsampling to reduce the feature distortion in blurred\nimages. The decomposed high and low-frequency components are then processed\nseparately. For the high-frequency components, we apply a High-frequency\nFeature Refinement (HFR) to amplify the prominence of key features and filter\nout noises, restoring fine-grained details in blurred images. For the\nlow-frequency components, we use a Multi-granularity State Space Model (MG-SSM)\nto aggregate feature representations with different receptive fields,\nextracting spatially-varying contexts while capturing long-range dependencies\nwith linear complexity. The utilization of multi-granularity contexts is\nessential for distinguishing similar vertebrae and improving segmentation\naccuracy. Extensive experiments demonstrate that our method outperforms\nstate-of-the-art approaches on both CT and MRI vertebrae segmentation datasets.\nThe source code is publicly available at https://github.com/anaanaa/FMCNet.", "AI": {"tldr": "FMC-Net improves vertebrae segmentation in 3D CT/MRI by using wavelet transform and multi-granularity context modeling to handle blurring and similar vertebrae.", "motivation": "Current methods struggle with image blurring and distinguishing similar vertebrae in spinal imaging, limiting clinical applications.", "method": "Uses wavelet transform for lossless downsampling, processes high/low-frequency components separately (HFR for high-frequency, MG-SSM for low-frequency), and aggregates multi-granularity contexts.", "result": "Outperforms state-of-the-art methods on CT/MRI datasets, with publicly available source code.", "conclusion": "FMC-Net effectively addresses blurring and vertebrae similarity, enhancing segmentation accuracy for clinical use."}}
{"id": "2506.23990", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23990", "abs": "https://arxiv.org/abs/2506.23990", "authors": ["Dustin Wright"], "title": "Machine Understanding of Scientific Language", "comment": "PhD Thesis, 210 pages", "summary": "Scientific information expresses human understanding of nature. This\nknowledge is largely disseminated in different forms of text, including\nscientific papers, news articles, and discourse among people on social media.\nWhile important for accelerating our pursuit of knowledge, not all scientific\ntext is faithful to the underlying science. As the volume of this text has\nburgeoned online in recent years, it has become a problem of societal\nimportance to be able to identify the faithfulness of a given piece of\nscientific text automatically. This thesis is concerned with the cultivation of\ndatasets, methods, and tools for machine understanding of scientific language,\nin order to analyze and understand science communication at scale. To arrive at\nthis, I present several contributions in three areas of natural language\nprocessing and machine learning: automatic fact checking, learning with limited\ndata, and scientific text processing. These contributions include new methods\nand resources for identifying check-worthy claims, adversarial claim\ngeneration, multi-source domain adaptation, learning from crowd-sourced labels,\ncite-worthiness detection, zero-shot scientific fact checking, detecting\nexaggerated scientific claims, and modeling degrees of information change in\nscience communication. Critically, I demonstrate how the research outputs of\nthis thesis are useful for effectively learning from limited amounts of\nscientific text in order to identify misinformative scientific statements and\ngenerate new insights into the science communication process", "AI": {"tldr": "The paper focuses on developing tools and methods for machine understanding of scientific language to identify faithfulness in scientific texts, addressing challenges like limited data and misinformation.", "motivation": "The motivation is to address the societal problem of identifying the faithfulness of scientific texts automatically due to the increasing volume of potentially misleading information online.", "method": "The thesis contributes to natural language processing and machine learning through methods like automatic fact-checking, adversarial claim generation, domain adaptation, and zero-shot scientific fact-checking.", "result": "The research provides new methods and resources for detecting exaggerated claims, modeling information change, and learning from limited data, demonstrating their effectiveness in identifying misinformation.", "conclusion": "The thesis concludes that its outputs are valuable for analyzing science communication at scale and generating insights into the process, particularly in handling limited data and misinformation."}}
{"id": "2506.23516", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23516", "abs": "https://arxiv.org/abs/2506.23516", "authors": ["Seung-Wook Kim", "Seongyeol Kim", "Jiah Kim", "Seowon Ji", "Se-Ho Lee"], "title": "FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization", "comment": null, "summary": "Federated learning (FL) often suffers from performance degradation due to key\nchallenges such as data heterogeneity and communication constraints. To address\nthese limitations, we present a novel FL framework called FedWSQ, which\nintegrates weight standardization (WS) and the proposed distribution-aware\nnon-uniform quantization (DANUQ). WS enhances FL performance by filtering out\nbiased components in local updates during training, thereby improving the\nrobustness of the model against data heterogeneity and unstable client\nparticipation. In addition, DANUQ minimizes quantization errors by leveraging\nthe statistical properties of local model updates. As a result, FedWSQ\nsignificantly reduces communication overhead while maintaining superior model\naccuracy. Extensive experiments on FL benchmark datasets demonstrate that\nFedWSQ consistently outperforms existing FL methods across various challenging\nFL settings, including extreme data heterogeneity and ultra-low-bit\ncommunication scenarios.", "AI": {"tldr": "FedWSQ improves federated learning by combining weight standardization and distribution-aware non-uniform quantization, enhancing robustness and reducing communication costs.", "motivation": "Address performance degradation in FL due to data heterogeneity and communication constraints.", "method": "Integrates weight standardization (WS) to filter biased updates and distribution-aware non-uniform quantization (DANUQ) to minimize quantization errors.", "result": "FedWSQ reduces communication overhead while maintaining high model accuracy, outperforming existing methods in challenging FL settings.", "conclusion": "FedWSQ is a robust and efficient solution for federated learning under data heterogeneity and communication constraints."}}
{"id": "2506.23088", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23088", "abs": "https://arxiv.org/abs/2506.23088", "authors": ["Yuchen Zhou", "Jiayu Tang", "Xiaoyan Xiao", "Yueyao Lin", "Linkai Liu", "Zipeng Guo", "Hao Fei", "Xiaobo Xia", "Chao Gou"], "title": "Where, What, Why: Towards Explainable Driver Attention Prediction", "comment": "Accepted by ICCV 2025", "summary": "Modeling task-driven attention in driving is a fundamental challenge for both\nautonomous vehicles and cognitive science. Existing methods primarily predict\nwhere drivers look by generating spatial heatmaps, but fail to capture the\ncognitive motivations behind attention allocation in specific contexts, which\nlimits deeper understanding of attention mechanisms. To bridge this gap, we\nintroduce Explainable Driver Attention Prediction, a novel task paradigm that\njointly predicts spatial attention regions (where), parses attended semantics\n(what), and provides cognitive reasoning for attention allocation (why). To\nsupport this, we present W3DA, the first large-scale explainable driver\nattention dataset. It enriches existing benchmarks with detailed semantic and\ncausal annotations across diverse driving scenarios, including normal\nconditions, safety-critical situations, and traffic accidents. We further\npropose LLada, a Large Language model-driven framework for driver attention\nprediction, which unifies pixel modeling, semantic parsing, and cognitive\nreasoning within an end-to-end architecture. Extensive experiments demonstrate\nthe effectiveness of LLada, exhibiting robust generalization across datasets\nand driving conditions. This work serves as a key step toward a deeper\nunderstanding of driver attention mechanisms, with significant implications for\nautonomous driving, intelligent driver training, and human-computer\ninteraction.", "AI": {"tldr": "The paper introduces Explainable Driver Attention Prediction, a task paradigm predicting spatial attention, semantics, and reasoning in driving. It presents W3DA, a dataset with semantic and causal annotations, and LLada, a framework for unified attention modeling.", "motivation": "Existing methods predict spatial attention but lack cognitive reasoning, limiting understanding of attention mechanisms in driving.", "method": "Proposes W3DA dataset with detailed annotations and LLada, a Large Language model-driven framework for end-to-end attention prediction.", "result": "LLada shows robust generalization across datasets and driving conditions.", "conclusion": "This work advances understanding of driver attention, benefiting autonomous driving and human-computer interaction."}}
{"id": "2506.23998", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23998", "abs": "https://arxiv.org/abs/2506.23998", "authors": ["Seungjun Yi", "Joakim Nguyen", "Huimin Xu", "Terence Lim", "Andrew Well", "Mia Markey", "Ying Ding"], "title": "Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning", "comment": "Presented at ACL 2025 SRW", "summary": "Congenital heart disease (CHD) presents complex, lifelong challenges often\nunderrepresented in traditional clinical metrics. While unstructured narratives\noffer rich insights into patient and caregiver experiences, manual thematic\nanalysis (TA) remains labor-intensive and unscalable. We propose a fully\nautomated large language model (LLM) pipeline that performs end-to-end TA on\nclinical narratives, which eliminates the need for manual coding or full\ntranscript review. Our system employs a novel multi-agent framework, where\nspecialized LLM agents assume roles to enhance theme quality and alignment with\nhuman analysis. To further improve thematic relevance, we optionally integrate\nreinforcement learning from human feedback (RLHF). This supports scalable,\npatient-centered analysis of large qualitative datasets and allows LLMs to be\nfine-tuned for specific clinical contexts.", "AI": {"tldr": "An automated LLM pipeline for thematic analysis of clinical narratives in CHD, reducing manual effort and improving scalability.", "motivation": "Traditional thematic analysis of CHD narratives is labor-intensive and unscalable, necessitating an automated solution.", "method": "A multi-agent LLM framework with optional RLHF for end-to-end thematic analysis of clinical narratives.", "result": "Enables scalable, patient-centered analysis of qualitative datasets, aligning with human analysis.", "conclusion": "The proposed system offers a scalable, automated alternative to manual thematic analysis in CHD research."}}
{"id": "2506.23544", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2506.23544", "abs": "https://arxiv.org/abs/2506.23544", "authors": ["Kento Imaizumi", "Hideaki Iiduka"], "title": "Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size", "comment": null, "summary": "Momentum methods were originally introduced for their superiority to\nstochastic gradient descent (SGD) in deterministic settings with convex\nobjective functions. However, despite their widespread application to deep\nneural networks -- a representative case of stochastic nonconvex optimization\n-- the theoretical justification for their effectiveness in such settings\nremains limited. Quasi-hyperbolic momentum (QHM) is an algorithm that\ngeneralizes various momentum methods and has been studied to better understand\nthe class of momentum-based algorithms as a whole. In this paper, we provide\nboth asymptotic and non-asymptotic convergence results for mini-batch QHM with\nan increasing batch size. We show that achieving asymptotic convergence\nrequires either a decaying learning rate or an increasing batch size. Since a\ndecaying learning rate adversely affects non-asymptotic convergence, we\ndemonstrate that using mini-batch QHM with an increasing batch size -- without\ndecaying the learning rate -- can be a more effective strategy. Our experiments\nshow that even a finite increase in batch size can provide benefits for\ntraining neural networks.", "AI": {"tldr": "The paper analyzes the effectiveness of Quasi-hyperbolic momentum (QHM) in stochastic nonconvex optimization, showing that increasing batch size, without decaying learning rates, improves convergence for neural networks.", "motivation": "The theoretical justification for momentum methods in stochastic nonconvex settings, like deep neural networks, is limited. QHM is studied to generalize momentum-based algorithms.", "method": "The paper provides asymptotic and non-asymptotic convergence results for mini-batch QHM with increasing batch size, comparing it to decaying learning rates.", "result": "Achieving asymptotic convergence requires decaying learning rates or increasing batch sizes. The latter is more effective for non-asymptotic convergence, as shown in experiments.", "conclusion": "Increasing batch size in mini-batch QHM, without decaying learning rates, is a viable strategy for training neural networks, offering practical benefits."}}
{"id": "2506.23104", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23104", "abs": "https://arxiv.org/abs/2506.23104", "authors": ["Jihun Kim", "Hoyong Kwon", "Hyeokjun Kweon", "Wooseong Jeong", "Kuk-Jin Yoon"], "title": "DC-TTA: Divide-and-Conquer Framework for Test-Time Adaptation of Interactive Segmentation", "comment": null, "summary": "Interactive segmentation (IS) allows users to iteratively refine object\nboundaries with minimal cues, such as positive and negative clicks. While the\nSegment Anything Model (SAM) has garnered attention in the IS community for its\npromptable segmentation capabilities, it often struggles in specialized domains\nor when handling complex scenarios (e.g., camouflaged or multi-part objects).\nTo overcome these challenges, we propose DC-TTA, a novel test-time adaptation\n(TTA) framework that adapts SAM on a per-sample basis by leveraging user\ninteractions as supervision. Instead of forcing a single model to incorporate\nall user clicks at once, DC-TTA partitions the clicks into more coherent\nsubsets, each processed independently via TTA with a separated model. This\nDivide-and-Conquer strategy reduces conflicts among diverse cues and enables\nmore localized updates. Finally, we merge the adapted models to form a unified\npredictor that integrates the specialized knowledge from each subset.\nExperimental results across various benchmarks demonstrate that DC-TTA\nsignificantly outperforms SAM's zero-shot results and conventional TTA methods,\neffectively handling complex tasks such as camouflaged object segmentation with\nfewer interactions and improved accuracy.", "AI": {"tldr": "DC-TTA is a test-time adaptation framework that improves SAM's interactive segmentation by dividing user clicks into subsets for localized updates, enhancing performance in complex scenarios.", "motivation": "SAM struggles in specialized domains and complex scenarios like camouflaged objects, prompting the need for a more adaptive approach.", "method": "DC-TTA partitions user clicks into coherent subsets, processes each independently via TTA, and merges adapted models for unified predictions.", "result": "DC-TTA outperforms SAM's zero-shot results and conventional TTA methods, achieving better accuracy with fewer interactions.", "conclusion": "DC-TTA effectively addresses SAM's limitations, offering a robust solution for complex interactive segmentation tasks."}}
{"id": "2506.24006", "categories": ["cs.CL", "math.HO"], "pdf": "https://arxiv.org/pdf/2506.24006", "abs": "https://arxiv.org/abs/2506.24006", "authors": ["Anselm R. Strohmaier", "Wim Van Dooren", "Kathrin Se\u00dfler", "Brian Greer", "Lieven Verschaffel"], "title": "Large Language Models Don't Make Sense of Word Problems. A Scoping Review from a Mathematics Education Perspective", "comment": null, "summary": "The progress of Large Language Models (LLMs) like ChatGPT raises the question\nof how they can be integrated into education. One hope is that they can support\nmathematics learning, including word-problem solving. Since LLMs can handle\ntextual input with ease, they appear well-suited for solving mathematical word\nproblems. Yet their real competence, whether they can make sense of the\nreal-world context, and the implications for classrooms remain unclear. We\nconducted a scoping review from a mathematics-education perspective, including\nthree parts: a technical overview, a systematic review of word problems used in\nresearch, and a state-of-the-art empirical evaluation of LLMs on mathematical\nword problems. First, in the technical overview, we contrast the\nconceptualization of word problems and their solution processes between LLMs\nand students. In computer-science research this is typically labeled\nmathematical reasoning, a term that does not align with usage in mathematics\neducation. Second, our literature review of 213 studies shows that the most\npopular word-problem corpora are dominated by s-problems, which do not require\na consideration of realities of their real-world context. Finally, our\nevaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, and o3 on 287 word problems\nshows that most recent LLMs solve these s-problems with near-perfect accuracy,\nincluding a perfect score on 20 problems from PISA. LLMs still showed\nweaknesses in tackling problems where the real-world context is problematic or\nnon-sensical. In sum, we argue based on all three aspects that LLMs have\nmastered a superficial solution process but do not make sense of word problems,\nwhich potentially limits their value as instructional tools in mathematics\nclassrooms.", "AI": {"tldr": "LLMs excel at solving word problems but lack deep understanding of real-world context, limiting their educational value.", "motivation": "To assess LLMs' ability to support math learning by solving word problems and their implications for education.", "method": "Conducted a scoping review with a technical overview, systematic literature review, and empirical evaluation of LLMs on word problems.", "result": "LLMs solve s-problems with high accuracy but struggle with real-world context, showing superficial understanding.", "conclusion": "LLMs' lack of deep comprehension limits their effectiveness as instructional tools in math classrooms."}}
{"id": "2506.23551", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23551", "abs": "https://arxiv.org/abs/2506.23551", "authors": ["Jingpu Cheng", "Qianxiao Li", "Ting Lin", "Zuowei Shen"], "title": "A unified framework on the universal approximation of transformer-type architectures", "comment": null, "summary": "We investigate the universal approximation property (UAP) of transformer-type\narchitectures, providing a unified theoretical framework that extends prior\nresults on residual networks to models incorporating attention mechanisms. Our\nwork identifies token distinguishability as a fundamental requirement for UAP\nand introduces a general sufficient condition that applies to a broad class of\narchitectures. Leveraging an analyticity assumption on the attention layer, we\ncan significantly simplify the verification of this condition, providing a\nnon-constructive approach in establishing UAP for such architectures. We\ndemonstrate the applicability of our framework by proving UAP for transformers\nwith various attention mechanisms, including kernel-based and sparse attention\nmechanisms. The corollaries of our results either generalize prior works or\nestablish UAP for architectures not previously covered. Furthermore, our\nframework offers a principled foundation for designing novel transformer\narchitectures with inherent UAP guarantees, including those with specific\nfunctional symmetries. We propose examples to illustrate these insights.", "AI": {"tldr": "The paper explores the universal approximation property (UAP) in transformer architectures, introducing a unified framework that extends prior work on residual networks to attention-based models. It identifies token distinguishability as key for UAP and simplifies verification under analyticity assumptions. The framework is applied to various attention mechanisms, generalizing prior results and enabling new architecture designs with UAP guarantees.", "motivation": "To extend theoretical understanding of UAP to transformer architectures, bridging gaps in prior work and providing a unified framework for analyzing attention-based models.", "method": "The study introduces a general sufficient condition for UAP, leveraging analyticity assumptions on attention layers to simplify verification. It applies this framework to transformers with diverse attention mechanisms (e.g., kernel-based, sparse).", "result": "The framework proves UAP for various transformer architectures, generalizing prior results and covering new cases. It also enables principled design of novel architectures with UAP guarantees.", "conclusion": "The work provides a foundational framework for UAP in transformers, simplifying analysis and enabling new designs with guaranteed approximation properties."}}
{"id": "2506.23106", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23106", "abs": "https://arxiv.org/abs/2506.23106", "authors": ["Ryo Ishiyama", "Shinnosuke Matsuo", "Seiichi Uchida"], "title": "Computer-Aided Multi-Stroke Character Simplification by Stroke Removal", "comment": "ICDAR2025 (Oral)", "summary": "Multi-stroke characters in scripts such as Chinese and Japanese can be highly\ncomplex, posing significant challenges for both native speakers and,\nespecially, non-native learners. If these characters can be simplified without\ndegrading their legibility, it could reduce learning barriers for non-native\nspeakers, facilitate simpler and legible font designs, and contribute to\nefficient character-based communication systems. In this paper, we propose a\nframework to systematically simplify multi-stroke characters by selectively\nremoving strokes while preserving their overall legibility. More specifically,\nwe use a highly accurate character recognition model to assess legibility and\nremove those strokes that minimally impact it. Experimental results on 1,256\ncharacter classes with 5, 10, 15, and 20 strokes reveal several key findings,\nincluding the observation that even after removing multiple strokes, many\ncharacters remain distinguishable. These findings suggest the potential for\nmore formalized simplification strategies.", "AI": {"tldr": "A framework simplifies multi-stroke Chinese/Japanese characters by removing strokes while maintaining legibility, aiding learners and font design.", "motivation": "Simplify complex characters to reduce learning barriers, improve font design, and enhance communication systems.", "method": "Uses a character recognition model to assess and remove strokes with minimal impact on legibility.", "result": "Tests on 1,256 characters show many remain distinguishable even after stroke removal.", "conclusion": "Suggests potential for formalized simplification strategies in character design."}}
{"id": "2506.24016", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24016", "abs": "https://arxiv.org/abs/2506.24016", "authors": ["Hyunjong Kim", "Sangyeop Kim", "Jongheon Jeong", "Yeongjae Cho", "Sungzoon Cho"], "title": "EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations", "comment": "Accepted at ACL 2025 Findings", "summary": "Recent advances in large language models and vision-language models have led\nto growing interest in explainable evaluation metrics for image captioning.\nHowever, these metrics generate explanations without standardized criteria, and\nthe overall quality of the generated explanations remains unverified. In this\npaper, we propose EXPERT, a reference-free evaluation metric that provides\nstructured explanations based on three fundamental criteria: fluency,\nrelevance, and descriptiveness. By constructing large-scale datasets of\nhigh-quality structured explanations, we develop a two-stage evaluation\ntemplate to effectively supervise a vision-language model for both scoring and\nexplanation generation. EXPERT achieves state-of-the-art results on benchmark\ndatasets while providing significantly higher-quality explanations than\nexisting metrics, as validated through comprehensive human evaluation. Our code\nand datasets are available at https://github.com/hjkim811/EXPERT.", "AI": {"tldr": "EXPERT is a reference-free evaluation metric for image captioning, providing structured explanations based on fluency, relevance, and descriptiveness, achieving state-of-the-art results.", "motivation": "Current explainable metrics lack standardized criteria and verified explanation quality, prompting the need for a structured, high-quality evaluation method.", "method": "EXPERT uses a two-stage evaluation template to supervise a vision-language model for scoring and explanation generation, leveraging large-scale datasets of structured explanations.", "result": "EXPERT outperforms existing metrics on benchmarks and provides higher-quality explanations, validated by human evaluation.", "conclusion": "EXPERT sets a new standard for explainable evaluation in image captioning, with publicly available code and datasets."}}
{"id": "2506.23589", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23589", "abs": "https://arxiv.org/abs/2506.23589", "authors": ["Neta Shaul", "Uriel Singer", "Itai Gat", "Yaron Lipman"], "title": "Transition Matching: Scalable and Flexible Generative Modeling", "comment": null, "summary": "Diffusion and flow matching models have significantly advanced media\ngeneration, yet their design space is well-explored, somewhat limiting further\nimprovements. Concurrently, autoregressive (AR) models, particularly those\ngenerating continuous tokens, have emerged as a promising direction for\nunifying text and media generation. This paper introduces Transition Matching\n(TM), a novel discrete-time, continuous-state generative paradigm that unifies\nand advances both diffusion/flow models and continuous AR generation. TM\ndecomposes complex generation tasks into simpler Markov transitions, allowing\nfor expressive non-deterministic probability transition kernels and arbitrary\nnon-continuous supervision processes, thereby unlocking new flexible design\navenues. We explore these choices through three TM variants: (i) Difference\nTransition Matching (DTM), which generalizes flow matching to discrete-time by\ndirectly learning transition probabilities, yielding state-of-the-art image\nquality and text adherence as well as improved sampling efficiency. (ii)\nAutoregressive Transition Matching (ARTM) and (iii) Full History Transition\nMatching (FHTM) are partially and fully causal models, respectively, that\ngeneralize continuous AR methods. They achieve continuous causal AR generation\nquality comparable to non-causal approaches and potentially enable seamless\nintegration with existing AR text generation techniques. Notably, FHTM is the\nfirst fully causal model to match or surpass the performance of flow-based\nmethods on text-to-image task in continuous domains. We demonstrate these\ncontributions through a rigorous large-scale comparison of TM variants and\nrelevant baselines, maintaining a fixed architecture, training data, and\nhyperparameters.", "AI": {"tldr": "The paper introduces Transition Matching (TM), a new generative paradigm unifying diffusion/flow models and continuous autoregressive (AR) generation, offering flexibility and improved performance in media generation.", "motivation": "Current diffusion/flow models have limited design space for further improvements, while continuous AR models show promise for unifying text and media generation. TM aims to bridge and advance both approaches.", "method": "TM decomposes generation tasks into Markov transitions, allowing expressive transition kernels and flexible supervision. Three variants are explored: DTM (generalizes flow matching), ARTM (partially causal), and FHTM (fully causal).", "result": "DTM achieves state-of-the-art image quality and text adherence with efficient sampling. ARTM and FHTM match or surpass non-causal AR and flow-based methods, with FHTM excelling in text-to-image tasks.", "conclusion": "TM offers a unified, flexible framework for generative tasks, outperforming existing methods in quality and efficiency, and enabling seamless integration with AR text generation."}}
{"id": "2506.23108", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23108", "abs": "https://arxiv.org/abs/2506.23108", "authors": ["Zhiyuan Zhu", "Jian Wang", "Yong Jiang", "Tong Han", "Yuhao Huang", "Ang Zhang", "Kaiwen Yang", "Mingyuan Luo", "Zhe Liu", "Yaofei Duan", "Dong Ni", "Tianhong Tang", "Xin Yang"], "title": "Hierarchical Corpus-View-Category Refinement for Carotid Plaque Risk Grading in Ultrasound", "comment": "Accepted at MICCAI 2025", "summary": "Accurate carotid plaque grading (CPG) is vital to assess the risk of\ncardiovascular and cerebrovascular diseases. Due to the small size and high\nintra-class variability of plaque, CPG is commonly evaluated using a\ncombination of transverse and longitudinal ultrasound views in clinical\npractice. However, most existing deep learning-based multi-view classification\nmethods focus on feature fusion across different views, neglecting the\nimportance of representation learning and the difference in class features. To\naddress these issues, we propose a novel Corpus-View-Category Refinement\nFramework (CVC-RF) that processes information from Corpus-, View-, and\nCategory-levels, enhancing model performance. Our contribution is four-fold.\nFirst, to the best of our knowledge, we are the foremost deep learning-based\nmethod for CPG according to the latest Carotid Plaque-RADS guidelines. Second,\nwe propose a novel center-memory contrastive loss, which enhances the network's\nglobal modeling capability by comparing with representative cluster centers and\ndiverse negative samples at the Corpus level. Third, we design a cascaded\ndown-sampling attention module to fuse multi-scale information and achieve\nimplicit feature interaction at the View level. Finally, a parameter-free\nmixture-of-experts weighting strategy is introduced to leverage class\nclustering knowledge to weight different experts, enabling feature decoupling\nat the Category level. Experimental results indicate that CVC-RF effectively\nmodels global features via multi-level refinement, achieving state-of-the-art\nperformance in the challenging CPG task.", "AI": {"tldr": "A novel Corpus-View-Category Refinement Framework (CVC-RF) is proposed for accurate carotid plaque grading (CPG), addressing multi-view classification challenges by enhancing representation learning and feature differences.", "motivation": "Accurate CPG is crucial for assessing cardiovascular and cerebrovascular risks, but existing deep learning methods neglect representation learning and class feature differences.", "method": "CVC-RF processes information at Corpus-, View-, and Category-levels, using center-memory contrastive loss, cascaded down-sampling attention, and a mixture-of-experts weighting strategy.", "result": "CVC-RF achieves state-of-the-art performance in CPG by effectively modeling global features through multi-level refinement.", "conclusion": "The proposed framework advances CPG by addressing key limitations in existing methods, demonstrating superior performance."}}
{"id": "2506.24068", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.24068", "abs": "https://arxiv.org/abs/2506.24068", "authors": ["Ian R. McKenzie", "Oskar J. Hollinsworth", "Tom Tseng", "Xander Davies", "Stephen Casper", "Aaron D. Tucker", "Robert Kirk", "Adam Gleave"], "title": "STACK: Adversarial Attacks on LLM Safeguard Pipelines", "comment": null, "summary": "Frontier AI developers are relying on layers of safeguards to protect against\ncatastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus\nmodel using one such defense pipeline, and other frontier developers including\nGoogle DeepMind and OpenAI pledge to soon deploy similar defenses. However, the\nsecurity of such pipelines is unclear, with limited prior work evaluating or\nattacking these pipelines. We address this gap by developing and red-teaming an\nopen-source defense pipeline. First, we find that a novel few-shot-prompted\ninput and output classifier outperforms state-of-the-art open-weight safeguard\nmodel ShieldGemma across three attacks and two datasets, reducing the attack\nsuccess rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,\nwe introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on\nClearHarm in a black-box attack against the few-shot-prompted classifier\npipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%\nASR, providing initial evidence that it is feasible to design attacks with no\naccess to the target pipeline. We conclude by suggesting specific mitigations\nthat developers could use to thwart staged attacks.", "AI": {"tldr": "The paper evaluates AI defense pipelines, proposing a novel classifier and a staged attack (STACK) that exposes vulnerabilities, suggesting mitigations.", "motivation": "To assess the security of AI defense pipelines, which lack prior evaluation, and identify vulnerabilities.", "method": "Developed an open-source defense pipeline, tested a few-shot-prompted classifier, and introduced the STACK attack procedure.", "result": "The classifier outperformed ShieldGemma (0% ASR on ClearHarm), but STACK achieved 71% ASR in black-box and 33% in transfer attacks.", "conclusion": "AI defense pipelines are vulnerable to staged attacks; specific mitigations are needed to enhance security."}}
{"id": "2506.23596", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23596", "abs": "https://arxiv.org/abs/2506.23596", "authors": ["Min-Yeong Park", "Won-Jeong Lee", "Seong Tae Kim", "Gyeong-Moon Park"], "title": "When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series", "comment": "18 pages, 10 figures, 12 tables, ICML 2025", "summary": "Recently, forecasting future abnormal events has emerged as an important\nscenario to tackle real-world necessities. However, the solution of predicting\nspecific future time points when anomalies will occur, known as Anomaly\nPrediction (AP), remains under-explored. Existing methods dealing with time\nseries data fail in AP, focusing only on immediate anomalies or failing to\nprovide precise predictions for future anomalies. To address the AP task, we\npropose a novel framework called Anomaly to Prompt (A2P), comprised of\nAnomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To\nenable the forecasting model to forecast abnormal time points, we adopt a\nstrategy to learn the relationships of anomalies. For the robust detection of\nanomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP)\nthat simulates diverse anomaly patterns using signal adaptive prompt.\nComprehensive experiments on multiple real-world datasets demonstrate the\nsuperiority of A2P over state-of-the-art methods, showcasing its ability to\npredict future anomalies. Our implementation code is available at\nhttps://github.com/KU-VGI/AP.", "AI": {"tldr": "The paper introduces A2P, a framework for Anomaly Prediction (AP) in time series data, combining Anomaly-Aware Forecasting and Synthetic Anomaly Prompting to predict future anomalies.", "motivation": "Existing methods for time series data fail to predict future anomalies, focusing only on immediate ones. The paper addresses this gap.", "method": "Proposes A2P with Anomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP), including a learnable Anomaly Prompt Pool (APP) for robust anomaly detection.", "result": "A2P outperforms state-of-the-art methods in predicting future anomalies, validated on multiple real-world datasets.", "conclusion": "A2P effectively addresses the AP task, offering a novel solution for predicting future anomalies in time series data."}}
{"id": "2506.23115", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23115", "abs": "https://arxiv.org/abs/2506.23115", "authors": ["Haonan Chen", "Hong Liu", "Yuping Luo", "Liang Wang", "Nan Yang", "Furu Wei", "Zhicheng Dou"], "title": "MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings", "comment": "Homepage: https://haon-chen.github.io/MoCa/", "summary": "Multimodal embedding models, built upon causal Vision Language Models (VLMs),\nhave shown promise in various tasks. However, current approaches face three key\nlimitations: the use of causal attention in VLM backbones is suboptimal for\nembedding tasks; scalability issues due to reliance on high-quality labeled\npaired data for contrastive learning; and limited diversity in training\nobjectives and data. To address these issues, we propose MoCa, a two-stage\nframework for transforming pre-trained VLMs into effective bidirectional\nmultimodal embedding models. The first stage, Modality-aware Continual\nPre-training, introduces a joint reconstruction objective that simultaneously\ndenoises interleaved text and image inputs, enhancing bidirectional\ncontext-aware reasoning. The second stage, Heterogeneous Contrastive\nFine-tuning, leverages diverse, semantically rich multimodal data beyond simple\nimage-caption pairs to enhance generalization and alignment. Our method\naddresses the stated limitations by introducing bidirectional attention through\ncontinual pre-training, scaling effectively with massive unlabeled datasets via\njoint reconstruction objectives, and utilizing diverse multimodal data for\nenhanced representation robustness. Experiments demonstrate that MoCa\nconsistently improves performance across MMEB and ViDoRe-v2 benchmarks,\nachieving new state-of-the-art results, and exhibits strong scalability with\nboth model size and training data on MMEB.", "AI": {"tldr": "MoCa transforms causal VLMs into bidirectional multimodal embedding models via two stages: continual pre-training with joint reconstruction and contrastive fine-tuning with diverse data, achieving SOTA results.", "motivation": "Current VLMs have suboptimal causal attention for embeddings, scalability issues, and limited training diversity.", "method": "Two-stage framework: Modality-aware Continual Pre-training (joint reconstruction) and Heterogeneous Contrastive Fine-tuning (diverse data).", "result": "MoCa improves performance on MMEB and ViDoRe-v2 benchmarks, showing scalability with model size and data.", "conclusion": "MoCa effectively addresses VLM limitations, enhancing bidirectional reasoning and representation robustness."}}
{"id": "2506.24106", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.24106", "abs": "https://arxiv.org/abs/2506.24106", "authors": ["Yanhong Li", "Ming Li", "Karen Livescu", "Jiawei Zhou"], "title": "On the Predictive Power of Representation Dispersion in Language Models", "comment": null, "summary": "We show that a language model's ability to predict text is tightly linked to\nthe breadth of its embedding space: models that spread their contextual\nrepresentations more widely tend to achieve lower perplexity. Concretely, we\nfind that representation dispersion - the average pairwise cosine distance\namong hidden vectors - strongly and negatively correlates with perplexity\nacross diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia,\nnews, scientific abstracts). Beyond illustrating this link, we show how\ndispersion can be leveraged for a range of practical tasks without requiring\nlabeled data. First, measuring dispersion on unlabeled text allows us to\npredict downstream accuracy in new domains, offering a data-efficient tool for\nmodel selection. Next, we find that identifying layers with higher dispersion\npinpoints the best representations for retrieval-based methods such as kNN-LM,\nbypassing exhaustive layer-by-layer searches. Finally, we integrate a simple\npush-away objective into training, which increases dispersion in both\nsingle-domain and cross-domain scenarios and directly improves perplexity in\neach.", "AI": {"tldr": "Language models with wider embedding spaces (higher representation dispersion) achieve lower perplexity, and this dispersion can be used for practical tasks like model selection and improving retrieval-based methods.", "motivation": "To explore the relationship between a model's embedding space breadth (representation dispersion) and its text prediction performance (perplexity).", "method": "Analyze representation dispersion (average pairwise cosine distance) across models (LLaMA, Qwen) and domains (Wikipedia, news, scientific abstracts). Use dispersion for tasks like model selection and improving retrieval methods. Introduce a push-away objective to increase dispersion.", "result": "Higher dispersion strongly correlates with lower perplexity. Dispersion aids in model selection, identifies optimal layers for retrieval, and improves perplexity when increased via training.", "conclusion": "Representation dispersion is a key factor in model performance and can be leveraged for practical improvements without labeled data."}}
{"id": "2506.23629", "categories": ["cs.LG", "cs.AI", "68T07(Primary) 62M10, 65C60 (Secondary)", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.23629", "abs": "https://arxiv.org/abs/2506.23629", "authors": ["Xin Liao", "Bing Yang", "Cai Yu"], "title": "A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data", "comment": "7 pages, 2 figures, conference", "summary": "The integrity of Water Quality Data (WQD) is critical in environmental\nmonitoring for scientific decision-making and ecological protection. However,\nwater quality monitoring systems are often challenged by large amounts of\nmissing data due to unavoidable problems such as sensor failures and\ncommunication delays, which further lead to water quality data becoming\nHigh-Dimensional and Sparse (HDS). Traditional data imputation methods are\ndifficult to depict the potential dynamics and fail to capture the deep data\nfeatures, resulting in unsatisfactory imputation performance. To effectively\naddress the above issues, this paper proposes a Nonlinear Low-rank\nRepresentation model (NLR) with Convolutional Neural Networks (CNN) for\nimputing missing WQD, which utilizes CNNs to implement two ideas: a) fusing\ntemporal features to model the temporal dependence of data between time slots,\nand b) Extracting nonlinear interactions and local patterns to mine\nhigher-order relationships features and achieve deep fusion of multidimensional\ninformation. Experimental studies on three real water quality datasets\ndemonstrate that the proposed model significantly outperforms existing\nstate-of-the-art data imputation models in terms of estimation accuracy. It\nprovides an effective approach for handling water quality monitoring data in\ncomplex dynamic environments.", "AI": {"tldr": "The paper proposes a Nonlinear Low-rank Representation model (NLR) with CNNs to impute missing Water Quality Data (WQD), outperforming traditional methods by capturing temporal and nonlinear features.", "motivation": "Missing WQD due to sensor failures and communication delays leads to High-Dimensional and Sparse data, which traditional methods fail to handle effectively.", "method": "The NLR model uses CNNs to fuse temporal features and extract nonlinear interactions for deep data fusion.", "result": "Experiments on real datasets show NLR significantly improves imputation accuracy over state-of-the-art methods.", "conclusion": "The NLR model provides an effective solution for WQD imputation in dynamic environments."}}
{"id": "2506.23120", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23120", "abs": "https://arxiv.org/abs/2506.23120", "authors": ["Zhenhua Ning", "Zhuotao Tian", "Shaoshuai Shi", "Guangming Lu", "Daojing He", "Wenjie Pei", "Li Jiang"], "title": "Enhancing Spatial Reasoning in Multimodal Large Language Models through Reasoning-based Segmentation", "comment": null, "summary": "Recent advances in point cloud perception have demonstrated remarkable\nprogress in scene understanding through vision-language alignment leveraging\nlarge language models (LLMs). However, existing methods may still encounter\nchallenges in handling complex instructions that require accurate spatial\nreasoning, even if the 3D point cloud data provides detailed spatial cues such\nas size and position for identifying the targets. To tackle this issue, we\npropose Relevant Reasoning Segmentation (R$^2$S), a reasoning-based\nsegmentation framework. The framework emulates human cognitive processes by\ndecomposing spatial reasoning into two sequential stages: first identifying\nrelevant elements, then processing instructions guided by their associated\nvisual priors. Furthermore, acknowledging the inadequacy of existing datasets\nin complex reasoning tasks, we introduce 3D ReasonSeg, a reasoning-based\nsegmentation dataset comprising 25,185 training samples and 3,966 validation\nsamples with precise annotations. Both quantitative and qualitative experiments\ndemonstrate that the R$^2$S and 3D ReasonSeg effectively endow 3D point cloud\nperception with stronger spatial reasoning capabilities, and we hope that they\ncan serve as a new baseline and benchmark for future work.", "AI": {"tldr": "The paper introduces R$^2$S, a reasoning-based segmentation framework for 3D point clouds, and a new dataset, 3D ReasonSeg, to address challenges in spatial reasoning tasks.", "motivation": "Existing methods struggle with complex spatial reasoning in 3D point cloud perception despite detailed spatial cues.", "method": "R$^2$S decomposes spatial reasoning into two stages: identifying relevant elements and processing instructions using visual priors.", "result": "R$^2$S and 3D ReasonSeg improve spatial reasoning in 3D point cloud perception, validated by experiments.", "conclusion": "The framework and dataset serve as a new baseline for future work in 3D point cloud perception."}}
{"id": "2506.24117", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.24117", "abs": "https://arxiv.org/abs/2506.24117", "authors": ["David M. Smiley"], "title": "Computational Detection of Intertextual Parallels in Biblical Hebrew: A Benchmark Study Using Transformer-Based Language Models", "comment": null, "summary": "Identifying parallel passages in biblical Hebrew is foundational in biblical\nscholarship for uncovering intertextual relationships. Traditional methods rely\non manual comparison, which is labor-intensive and prone to human error. This\nstudy evaluates the potential of pre-trained transformer-based language models,\nincluding E5, AlephBERT, MPNet, and LaBSE, for detecting textual parallels in\nthe Hebrew Bible. Focusing on known parallels between the books of Samuel/Kings\nand Chronicles, I assessed each model's capability to generate word embeddings\nthat delineate parallel from non-parallel passages. Utilizing cosine similarity\nand Wasserstein Distance measures, I found that E5 and AlephBERT show\nsignificant promise, with E5 excelling in parallel detection and AlephBERT\ndemonstrating stronger non-parallel differentiation. These findings indicate\nthat pre-trained models can enhance the efficiency and accuracy of detecting\nintertextual parallels in ancient texts, suggesting broader applications for\nancient language studies.", "AI": {"tldr": "The study evaluates transformer-based models (E5, AlephBERT, MPNet, LaBSE) for detecting parallel passages in the Hebrew Bible, finding E5 and AlephBERT most effective.", "motivation": "Traditional manual comparison of biblical Hebrew passages is labor-intensive and error-prone, prompting the need for automated methods.", "method": "Pre-trained models generate word embeddings for passages; cosine similarity and Wasserstein Distance measure parallels.", "result": "E5 excels in detecting parallels, while AlephBERT better differentiates non-parallel passages.", "conclusion": "Pre-trained models improve efficiency and accuracy in identifying intertextual parallels, with potential for broader ancient language studies."}}
{"id": "2506.23679", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.23679", "abs": "https://arxiv.org/abs/2506.23679", "authors": ["David Demitri Africa", "Sara M. Kapoor", "Theo Simon Sorg"], "title": "Learning Modular Exponentiation with Transformers", "comment": null, "summary": "Modular exponentiation is crucial to number theory and cryptography, yet\nremains largely unexplored from a mechanistic interpretability standpoint. We\ntrain a 4-layer encoder-decoder Transformer model to perform this operation and\ninvestigate the emergence of numerical reasoning during training. Utilizing\nprincipled sampling strategies, PCA-based embedding analysis, and activation\npatching, we examine how number-theoretic properties are encoded within the\nmodel. We find that reciprocal operand training leads to strong performance\ngains, with sudden generalization across related moduli. These synchronized\naccuracy surges reflect grokking-like dynamics, suggesting the model\ninternalizes shared arithmetic structure. We also find a subgraph consisting\nentirely of attention heads in the final layer sufficient to achieve full\nperformance on the task of regular exponentiation. These results suggest that\ntransformer models learn modular arithmetic through specialized computational\ncircuits, paving the way for more interpretable and efficient neural approaches\nto modular exponentiation.", "AI": {"tldr": "The paper explores modular exponentiation in Transformers, revealing specialized circuits and grokking-like dynamics for arithmetic tasks.", "motivation": "To understand how Transformer models learn and perform modular exponentiation, a key operation in cryptography and number theory.", "method": "Train a 4-layer encoder-decoder Transformer, analyze embeddings with PCA, use activation patching, and study reciprocal operand training.", "result": "Reciprocal operand training boosts performance, showing grokking-like generalization. A subgraph of attention heads in the final layer suffices for regular exponentiation.", "conclusion": "Transformers learn modular arithmetic via specialized circuits, offering insights for interpretable and efficient neural approaches."}}
{"id": "2506.23132", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23132", "abs": "https://arxiv.org/abs/2506.23132", "authors": ["Sophie Zhou", "Shu Kong"], "title": "Dare to Plagiarize? Plagiarized Painting Recognition and Retrieval", "comment": "to appear at AVSS'25", "summary": "Art plagiarism detection plays a crucial role in protecting artists'\ncopyrights and intellectual property, yet it remains a challenging problem in\nforensic analysis. In this paper, we address the task of recognizing\nplagiarized paintings and explaining the detected plagarisms by retrieving\nvisually similar authentic artworks. To support this study, we construct a\ndataset by collecting painting photos and synthesizing plagiarized versions\nusing generative AI, tailored to specific artists' styles. We first establish a\nbaseline approach using off-the-shelf features from the visual foundation model\nDINOv2 to retrieve the most similar images in the database and classify\nplagiarism based on a similarity threshold. Surprisingly, this non-learned\nmethod achieves a high recognition accuracy of 97.2\\% but suffers from low\nretrieval precision 29.0\\% average precision (AP). To improve retrieval\nquality, we finetune DINOv2 with a metric learning loss using positive and\nnegative sample pairs sampled in the database. The finetuned model greatly\nimproves retrieval performance by 12\\% AP over the baseline, though it\nunexpectedly results in a lower recognition accuracy (92.7\\%). We conclude with\ninsightful discussions and outline directions for future research.", "AI": {"tldr": "The paper proposes a method for detecting art plagiarism by retrieving visually similar authentic artworks, using a dataset of paintings and AI-synthesized plagiarized versions. A baseline method with DINOv2 achieves high recognition accuracy but low retrieval precision, while finetuning improves retrieval but reduces accuracy.", "motivation": "To protect artists' copyrights by detecting and explaining plagiarized paintings through retrieval of similar authentic artworks.", "method": "Construct a dataset of paintings and AI-synthesized plagiarized versions. Use DINOv2 for baseline retrieval and classification, then finetune with metric learning to improve retrieval.", "result": "Baseline: 97.2% recognition accuracy, 29.0% AP retrieval. Finetuned model: 12% AP improvement but 92.7% accuracy.", "conclusion": "Finetuning improves retrieval but reduces accuracy. Future research directions are outlined."}}
{"id": "2506.23719", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23719", "abs": "https://arxiv.org/abs/2506.23719", "authors": ["Alex Egg", "Martin Iglesias Goyanes", "Friso Kingma", "Andreu Mora", "Leandro von Werra", "Thomas Wolf"], "title": "DABstep: Data Agent Benchmark for Multi-step Reasoning", "comment": "13 pages, 5 figures", "summary": "We introduce DABstep, a novel benchmark for evaluating AI agents on realistic\nmulti-step data analysis tasks. DABstep comprises over 450 real-world\nchallenges derived from a financial analytics platform, requiring models to\ncombine code-based data processing with contextual reasoning over heterogeneous\ndocumentation. Each task demands an iterative, multi-step problem-solving\napproach, testing capabilities in data manipulation, cross-referencing multiple\nsources, and precise result reporting. The benchmark provides a factoid-style\nanswer format with automatic correctness checks for objective scoring at scale.\nWe evaluate leading LLM-based agents, revealing a substantial performance gap:\neven the best agent achieves only 14.55% accuracy on the hardest tasks. We\ndetail our benchmark's design, dataset composition, task formulation,\nevaluation protocol, report baseline results and analyze failure modes. DABstep\nis released with a public leaderboard and toolkit to accelerate research in\nautonomous data analysis.", "AI": {"tldr": "DABstep is a new benchmark for evaluating AI agents on multi-step data analysis tasks, featuring 450 real-world challenges. It tests capabilities like data manipulation and contextual reasoning, with automatic scoring. Leading LLMs perform poorly, with only 14.55% accuracy on hard tasks.", "motivation": "To create a realistic benchmark for assessing AI agents' abilities in multi-step data analysis, addressing gaps in current evaluation methods.", "method": "DABstep includes 450 tasks from financial analytics, requiring code-based processing and contextual reasoning. Tasks are scored automatically with factoid-style answers.", "result": "Leading LLM-based agents perform poorly, with the best achieving only 14.55% accuracy on the hardest tasks.", "conclusion": "DABstep provides a valuable tool for advancing research in autonomous data analysis, highlighting significant performance gaps in current AI agents."}}
{"id": "2506.23135", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23135", "abs": "https://arxiv.org/abs/2506.23135", "authors": ["Yu Shang", "Xin Zhang", "Yinzhou Tang", "Lei Jin", "Chen Gao", "Wei Wu", "Yong Li"], "title": "RoboScape: Physics-informed Embodied World Model", "comment": "17 pages", "summary": "World models have become indispensable tools for embodied intelligence,\nserving as powerful simulators capable of generating realistic robotic videos\nwhile addressing critical data scarcity challenges. However, current embodied\nworld models exhibit limited physical awareness, particularly in modeling 3D\ngeometry and motion dynamics, resulting in unrealistic video generation for\ncontact-rich robotic scenarios. In this paper, we present RoboScape, a unified\nphysics-informed world model that jointly learns RGB video generation and\nphysics knowledge within an integrated framework. We introduce two key\nphysics-informed joint training tasks: temporal depth prediction that enhances\n3D geometric consistency in video rendering, and keypoint dynamics learning\nthat implicitly encodes physical properties (e.g., object shape and material\ncharacteristics) while improving complex motion modeling. Extensive experiments\ndemonstrate that RoboScape generates videos with superior visual fidelity and\nphysical plausibility across diverse robotic scenarios. We further validate its\npractical utility through downstream applications including robotic policy\ntraining with generated data and policy evaluation. Our work provides new\ninsights for building efficient physics-informed world models to advance\nembodied intelligence research. The code is available at:\nhttps://github.com/tsinghua-fib-lab/RoboScape.", "AI": {"tldr": "RoboScape is a physics-informed world model that improves video generation for robotic scenarios by integrating 3D geometry and motion dynamics.", "motivation": "Current embodied world models lack physical awareness, leading to unrealistic video generation for contact-rich robotic tasks.", "method": "RoboScape jointly learns RGB video generation and physics knowledge, using temporal depth prediction and keypoint dynamics learning.", "result": "RoboScape generates videos with high visual fidelity and physical plausibility, validated in downstream robotic applications.", "conclusion": "RoboScape advances embodied intelligence research by efficiently integrating physics into world models."}}
{"id": "2506.23726", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23726", "abs": "https://arxiv.org/abs/2506.23726", "authors": ["Bartlomiej Sobieski", "Matthew Tivnan", "Yuang Wang", "Siyeop Yoon", "Pengfei Jin", "Dufan Wu", "Quanzheng Li", "Przemyslaw Biecek"], "title": "System-Embedded Diffusion Bridge Models", "comment": "Preprint", "summary": "Solving inverse problems -- recovering signals from incomplete or noisy\nmeasurements -- is fundamental in science and engineering. Score-based\ngenerative models (SGMs) have recently emerged as a powerful framework for this\ntask. Two main paradigms have formed: unsupervised approaches that adapt\npretrained generative models to inverse problems, and supervised bridge methods\nthat train stochastic processes conditioned on paired clean and corrupted data.\nWhile the former typically assume knowledge of the measurement model, the\nlatter have largely overlooked this structural information. We introduce System\nembedded Diffusion Bridge Models (SDBs), a new class of supervised bridge\nmethods that explicitly embed the known linear measurement system into the\ncoefficients of a matrix-valued SDE. This principled integration yields\nconsistent improvements across diverse linear inverse problems and demonstrates\nrobust generalization under system misspecification between training and\ndeployment, offering a promising solution to real-world applications.", "AI": {"tldr": "SDBs integrate known linear measurement systems into matrix-valued SDEs, improving performance in linear inverse problems and enhancing robustness under system misspecification.", "motivation": "To address the gap in supervised bridge methods that overlook structural information of measurement models, while leveraging the power of SGMs for inverse problems.", "method": "Introduces System embedded Diffusion Bridge Models (SDBs), embedding known linear measurement systems into matrix-valued SDE coefficients.", "result": "Consistent improvements in diverse linear inverse problems and robust generalization under system misspecification.", "conclusion": "SDBs offer a promising solution for real-world inverse problem applications by integrating structural information effectively."}}
{"id": "2506.23138", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23138", "abs": "https://arxiv.org/abs/2506.23138", "authors": ["Shiyu Wu", "Mingzhen Sun", "Weining Wang", "Yequan Wang", "Jing Liu"], "title": "VisualPrompter: Prompt Optimization with Visual Feedback for Text-to-Image Synthesis", "comment": "12 pages, 5 figures", "summary": "Since there exists a notable gap between user-provided and model-preferred\nprompts, generating high-quality and satisfactory images using diffusion models\noften requires prompt engineering to optimize user inputs. Current studies on\ntext-to-image prompt engineering can effectively enhance the style and\naesthetics of generated images. However, they often neglect the semantic\nalignment between generated images and user descriptions, resulting in visually\nappealing but content-wise unsatisfying outputs. In this work, we propose\nVisualPrompter, a novel training-free prompt engineering framework that refines\nuser inputs to model-preferred sentences. In particular, VisualPrompter\nutilizes an automatic self-reflection module to identify the missing concepts\nin generated images and a target-specific prompt optimization mechanism to\nrevise the prompts in a fine-grained manner. Extensive experiments demonstrate\nthe effectiveness of our VisualPrompter, which achieves new state-of-the-art\nperformance on multiple benchmarks for text-image alignment evaluation.\nAdditionally, our framework features a plug-and-play design, making it highly\nadaptable to various generative models.", "AI": {"tldr": "VisualPrompter is a training-free framework for refining user prompts to improve text-to-image alignment in diffusion models, outperforming benchmarks.", "motivation": "Address the gap between user-provided and model-preferred prompts, ensuring semantic alignment in generated images.", "method": "Uses self-reflection to identify missing concepts and fine-grained prompt optimization.", "result": "Achieves state-of-the-art performance in text-image alignment benchmarks.", "conclusion": "VisualPrompter is effective, adaptable, and enhances semantic alignment without training."}}
{"id": "2506.23731", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23731", "abs": "https://arxiv.org/abs/2506.23731", "authors": ["Michel Meintz", "Jan Dubi\u0144ski", "Franziska Boenisch", "Adam Dziedzic"], "title": "Radioactive Watermarks in Diffusion and Autoregressive Image Generative Models", "comment": null, "summary": "Image generative models have become increasingly popular, but training them\nrequires large datasets that are costly to collect and curate. To circumvent\nthese costs, some parties may exploit existing models by using the generated\nimages as training data for their own models. In general, watermarking is a\nvaluable tool for detecting unauthorized use of generated images. However, when\nthese images are used to train a new model, watermarking can only enable\ndetection if the watermark persists through training and remains identifiable\nin the outputs of the newly trained model - a property known as radioactivity.\nWe analyze the radioactivity of watermarks in images generated by diffusion\nmodels (DMs) and image autoregressive models (IARs). We find that existing\nwatermarking methods for DMs fail to retain radioactivity, as watermarks are\neither erased during encoding into the latent space or lost in the\nnoising-denoising process (during the training in the latent space). Meanwhile,\ndespite IARs having recently surpassed DMs in image generation quality and\nefficiency, no radioactive watermarking methods have been proposed for them. To\novercome this limitation, we propose the first watermarking method tailored for\nIARs and with radioactivity in mind - drawing inspiration from techniques in\nlarge language models (LLMs), which share IARs' autoregressive paradigm. Our\nextensive experimental evaluation highlights our method's effectiveness in\npreserving radioactivity within IARs, enabling robust provenance tracking, and\npreventing unauthorized use of their generated images.", "AI": {"tldr": "The paper analyzes watermark persistence (radioactivity) in diffusion and autoregressive models, proposes a new watermarking method for autoregressive models, and demonstrates its effectiveness.", "motivation": "Training image generative models requires costly datasets, and unauthorized use of generated images is a concern. Watermarking can detect misuse, but existing methods lack radioactivity (persistence through model training).", "method": "Analyzes watermark radioactivity in diffusion and autoregressive models, proposes a new watermarking method for autoregressive models inspired by techniques in large language models.", "result": "Existing watermarking methods for diffusion models fail to retain radioactivity, while the proposed method for autoregressive models effectively preserves it.", "conclusion": "The new watermarking method enables robust provenance tracking and prevents unauthorized use of images generated by autoregressive models."}}
{"id": "2506.23150", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23150", "abs": "https://arxiv.org/abs/2506.23150", "authors": ["Xinyue Liang", "Zhiyuan Ma", "Lingchen Sun", "Yanjun Guo", "Lei Zhang"], "title": "AlignCVC: Aligning Cross-View Consistency for Single-Image-to-3D Generation", "comment": null, "summary": "Single-image-to-3D models typically follow a sequential generation and\nreconstruction workflow. However, intermediate multi-view images synthesized by\npre-trained generation models often lack cross-view consistency (CVC),\nsignificantly degrading 3D reconstruction performance. While recent methods\nattempt to refine CVC by feeding reconstruction results back into the\nmulti-view generator, these approaches struggle with noisy and unstable\nreconstruction outputs that limit effective CVC improvement. We introduce\nAlignCVC, a novel framework that fundamentally re-frames single-image-to-3D\ngeneration through distribution alignment rather than relying on strict\nregression losses. Our key insight is to align both generated and reconstructed\nmulti-view distributions toward the ground-truth multi-view distribution,\nestablishing a principled foundation for improved CVC. Observing that generated\nimages exhibit weak CVC while reconstructed images display strong CVC due to\nexplicit rendering, we propose a soft-hard alignment strategy with distinct\nobjectives for generation and reconstruction models. This approach not only\nenhances generation quality but also dramatically accelerates inference to as\nfew as 4 steps. As a plug-and-play paradigm, our method, namely AlignCVC,\nseamlessly integrates various multi-view generation models with 3D\nreconstruction models. Extensive experiments demonstrate the effectiveness and\nefficiency of AlignCVC for single-image-to-3D generation.", "AI": {"tldr": "AlignCVC introduces a distribution alignment framework to improve cross-view consistency in single-image-to-3D generation, enhancing quality and speeding up inference.", "motivation": "Existing methods struggle with noisy and unstable reconstruction outputs, limiting cross-view consistency (CVC) improvement in single-image-to-3D tasks.", "method": "AlignCVC aligns generated and reconstructed multi-view distributions with ground-truth using a soft-hard alignment strategy, distinct for generation and reconstruction models.", "result": "The method enhances generation quality, accelerates inference to as few as 4 steps, and integrates seamlessly with various models.", "conclusion": "AlignCVC provides a principled and efficient solution for improving CVC in single-image-to-3D generation."}}
{"id": "2506.23757", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.23757", "abs": "https://arxiv.org/abs/2506.23757", "authors": ["Dan Yao", "Steve McLaughlin", "Yoann Altmann"], "title": "Training of Spiking Neural Networks with Expectation-Propagation", "comment": "10 pages", "summary": "In this paper, we propose a unifying message-passing framework for training\nspiking neural networks (SNNs) using Expectation-Propagation. Our gradient-free\nmethod is capable of learning the marginal distributions of network parameters\nand simultaneously marginalizes nuisance parameters, such as the outputs of\nhidden layers. This framework allows for the first time, training of discrete\nand continuous weights, for deterministic and stochastic spiking networks,\nusing batches of training samples. Although its convergence is not ensured, the\nalgorithm converges in practice faster than gradient-based methods, without\nrequiring a large number of passes through the training data. The\nclassification and regression results presented pave the way for new efficient\ntraining methods for deep Bayesian networks.", "AI": {"tldr": "A gradient-free, message-passing framework for training SNNs using Expectation-Propagation, enabling learning of marginal distributions and faster convergence than gradient-based methods.", "motivation": "To unify training for SNNs with discrete/continuous weights and deterministic/stochastic spiking, while marginalizing nuisance parameters.", "method": "Expectation-Propagation-based message-passing framework, gradient-free, works with batches of training samples.", "result": "Faster convergence in practice compared to gradient-based methods, successful classification and regression results.", "conclusion": "Paves the way for efficient training methods for deep Bayesian networks."}}
{"id": "2506.23151", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.23151", "abs": "https://arxiv.org/abs/2506.23151", "authors": ["Vladislav Bargatin", "Egor Chistov", "Alexander Yakovenko", "Dmitriy Vatolin"], "title": "MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation", "comment": "Accepted at ICCV 2025", "summary": "Recent advances in optical flow estimation have prioritized accuracy at the\ncost of growing GPU memory consumption, particularly for high-resolution\n(FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical\nflow method that identifies a favorable trade-off between multi-frame\nestimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU\nmemory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely\npositions our method to be trained at native 1080p without the need for\ncropping or downsampling. We systematically revisit design choices from\nRAFT-like architectures, integrating reduced correlation volumes and\nhigh-resolution training protocols alongside multi-frame estimation, to achieve\nstate-of-the-art performance across multiple benchmarks while substantially\nreducing memory overhead. Our method outperforms more resource-intensive\nalternatives in both accuracy and runtime efficiency, validating its robustness\nfor flow estimation at high resolutions. At the time of submission, our method\nranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289,\nleads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the\nbest Fl-all error on KITTI-2015 at 2.94%. The code is available at\nhttps://github.com/msu-video-group/memfof.", "AI": {"tldr": "MEMFOF is a memory-efficient multi-frame optical flow method that balances accuracy and GPU memory usage, achieving state-of-the-art performance with minimal memory overhead.", "motivation": "Address the high GPU memory consumption in optical flow estimation, especially for high-resolution inputs, without sacrificing accuracy.", "method": "Revisits RAFT-like architectures, integrates reduced correlation volumes, high-resolution training, and multi-frame estimation.", "result": "Outperforms alternatives in accuracy and efficiency, ranking first on benchmarks like Spring, Sintel, and KITTI-2015.", "conclusion": "MEMFOF is a robust solution for high-resolution flow estimation, offering significant memory savings and superior performance."}}
{"id": "2506.23776", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23776", "abs": "https://arxiv.org/abs/2506.23776", "authors": ["Jari Peeperkorn", "Johannes De Smedt", "Jochen De Weerdt"], "title": "Model-driven Stochastic Trace Clustering", "comment": null, "summary": "Process discovery algorithms automatically extract process models from event\nlogs, but high variability often results in complex and hard-to-understand\nmodels. To mitigate this issue, trace clustering techniques group process\nexecutions into clusters, each represented by a simpler and more understandable\nprocess model. Model-driven trace clustering improves on this by assigning\ntraces to clusters based on their conformity to cluster-specific process\nmodels. However, most existing clustering techniques rely on either no process\nmodel discovery, or non-stochastic models, neglecting the frequency or\nprobability of activities and transitions, thereby limiting their capability to\ncapture real-world execution dynamics. We propose a novel model-driven trace\nclustering method that optimizes stochastic process models within each cluster.\nOur approach uses entropic relevance, a stochastic conformance metric based on\ndirectly-follows probabilities, to guide trace assignment. This allows\nclustering decisions to consider both structural alignment with a cluster's\nprocess model and the likelihood that a trace originates from a given\nstochastic process model. The method is computationally efficient, scales\nlinearly with input size, and improves model interpretability by producing\nclusters with clearer control-flow patterns. Extensive experiments on public\nreal-life datasets show that our method outperforms existing alternatives in\nrepresenting process behavior and reveals how clustering performance rankings\ncan shift when stochasticity is considered.", "AI": {"tldr": "A novel model-driven trace clustering method optimizes stochastic process models using entropic relevance for better interpretability and performance.", "motivation": "High variability in process discovery leads to complex models; existing clustering techniques often ignore stochasticity, limiting their ability to capture real-world dynamics.", "method": "Proposes a method using entropic relevance, a stochastic conformance metric, to assign traces based on structural alignment and likelihood.", "result": "Computationally efficient, scales linearly, and outperforms alternatives in representing process behavior.", "conclusion": "The method improves interpretability and reveals performance shifts when stochasticity is considered."}}
{"id": "2506.23153", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23153", "abs": "https://arxiv.org/abs/2506.23153", "authors": ["Huiqiang Sun", "Xingyi Li", "Juewen Peng", "Liao Shen", "Zhiguo Cao", "Ke Xian", "Guosheng Lin"], "title": "Dynamic View Synthesis from Small Camera Motion Videos", "comment": "Accepted by TVCG", "summary": "Novel view synthesis for dynamic $3$D scenes poses a significant challenge.\nMany notable efforts use NeRF-based approaches to address this task and yield\nimpressive results. However, these methods rely heavily on sufficient motion\nparallax in the input images or videos. When the camera motion range becomes\nlimited or even stationary (i.e., small camera motion), existing methods\nencounter two primary challenges: incorrect representation of scene geometry\nand inaccurate estimation of camera parameters. These challenges make prior\nmethods struggle to produce satisfactory results or even become invalid. To\naddress the first challenge, we propose a novel Distribution-based Depth\nRegularization (DDR) that ensures the rendering weight distribution to align\nwith the true distribution. Specifically, unlike previous methods that use\ndepth loss to calculate the error of the expectation, we calculate the\nexpectation of the error by using Gumbel-softmax to differentiably sample\npoints from discrete rendering weight distribution. Additionally, we introduce\nconstraints that enforce the volume density of spatial points before the object\nboundary along the ray to be near zero, ensuring that our model learns the\ncorrect geometry of the scene. To demystify the DDR, we further propose a\nvisualization tool that enables observing the scene geometry representation at\nthe rendering weight level. For the second challenge, we incorporate camera\nparameter learning during training to enhance the robustness of our model to\ncamera parameters. We conduct extensive experiments to demonstrate the\neffectiveness of our approach in representing scenes with small camera motion\ninput, and our results compare favorably to state-of-the-art methods.", "AI": {"tldr": "The paper addresses challenges in novel view synthesis for dynamic 3D scenes with limited camera motion, proposing a Distribution-based Depth Regularization (DDR) and camera parameter learning to improve geometry representation and robustness.", "motivation": "Existing NeRF-based methods struggle with small camera motion, leading to incorrect geometry and inaccurate camera parameters.", "method": "Proposes DDR for accurate rendering weight distribution and introduces constraints for correct geometry. Also includes camera parameter learning during training.", "result": "The approach outperforms state-of-the-art methods in scenes with small camera motion.", "conclusion": "DDR and camera parameter learning effectively address challenges in dynamic scene synthesis with limited camera motion."}}
{"id": "2506.23782", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23782", "abs": "https://arxiv.org/abs/2506.23782", "authors": ["Xiaoyang Li", "Linwei Tao", "Haohui Lu", "Minjing Dong", "Junbin Gao", "Chang Xu"], "title": "Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling", "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated strong predictive performance\non relational data; however, their confidence estimates often misalign with\nactual predictive correctness, posing significant limitations for deployment in\nsafety-critical settings. While existing graph-aware calibration methods seek\nto mitigate this limitation, they primarily depend on coarse one-hop\nstatistics, such as neighbor-predicted confidence, or latent node embeddings,\nthereby neglecting the fine-grained structural heterogeneity inherent in graph\ntopology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a\npost-hoc calibration framework that assigns node-specific temperatures based on\ntunable heat-kernel graph wavelet features. Specifically, WATS harnesses the\nscalability and topology sensitivity of graph wavelets to refine confidence\nestimates, all without necessitating model retraining or access to neighboring\nlogits or predictions. Extensive evaluations across seven benchmark datasets\nwith varying graph structures and two GNN backbones demonstrate that WATS\nachieves the lowest Expected Calibration Error (ECE) among all compared\nmethods, outperforming both classical and graph-specific baselines by up to\n42.3\\% in ECE and reducing calibration variance by 17.24\\% on average compared\nwith graph-specific methods. Moreover, WATS remains computationally efficient,\nscaling well across graphs of diverse sizes and densities. Code will be\nreleased based on publication.", "AI": {"tldr": "WATS is a post-hoc calibration framework for GNNs that uses graph wavelet features to improve confidence estimates, outperforming existing methods in calibration error and variance.", "motivation": "GNNs often misalign confidence estimates with actual correctness, limiting their use in safety-critical applications. Existing methods rely on coarse statistics, ignoring fine-grained graph topology.", "method": "WATS assigns node-specific temperatures using heat-kernel graph wavelet features, refining confidence without retraining or neighbor data.", "result": "WATS achieves the lowest ECE, outperforming baselines by up to 42.3%, and reduces calibration variance by 17.24% on average. It scales efficiently across diverse graphs.", "conclusion": "WATS effectively addresses GNN calibration issues by leveraging graph wavelets, offering improved performance and scalability without additional training."}}
{"id": "2506.23156", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23156", "abs": "https://arxiv.org/abs/2506.23156", "authors": ["Jiale Chen"], "title": "Self-Supervised Contrastive Learning for Multi-Label Images", "comment": null, "summary": "Self-supervised learning (SSL) has demonstrated its effectiveness in learning\nrepresentations through comparison methods that align with human intuition.\nHowever, mainstream SSL methods heavily rely on high body datasets with single\nlabel, such as ImageNet, resulting in intolerable pre-training overhead.\nBesides, more general multi-label images are frequently overlooked in SSL,\ndespite their potential for richer semantic information and broader\napplicability in downstream scenarios. Therefore, we tailor the mainstream SSL\napproach to guarantee excellent representation learning capabilities using\nfewer multi-label images. Firstly, we propose a block-wise augmentation module\naimed at extracting additional potential positive view pairs from multi-label\nimages. Subsequently, an image-aware contrastive loss is devised to establish\nconnections between these views, thereby facilitating the extraction of\nsemantically consistent representations. Comprehensive linear fine-tuning and\ntransfer learning validate the competitiveness of our approach despite\nchallenging sample quality and quantity.", "AI": {"tldr": "The paper adapts self-supervised learning (SSL) for multi-label images, proposing a block-wise augmentation module and image-aware contrastive loss to improve representation learning with fewer samples.", "motivation": "Mainstream SSL methods rely on single-label datasets like ImageNet, ignoring multi-label images' richer semantics and causing high pre-training costs.", "method": "Introduces a block-wise augmentation module for multi-label images and an image-aware contrastive loss to link views for consistent representations.", "result": "Validated through linear fine-tuning and transfer learning, the approach shows competitiveness despite limited sample quality and quantity.", "conclusion": "The tailored SSL method effectively learns representations from fewer multi-label images, offering broader applicability."}}
{"id": "2506.23799", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23799", "abs": "https://arxiv.org/abs/2506.23799", "authors": ["Jiongli Zhu", "Parjanya Prajakta Prashant", "Alex Cloninger", "Babak Salimi"], "title": "KAIROS: Scalable Model-Agnostic Data Valuation", "comment": "19 pages, 9 figures", "summary": "Training data increasingly shapes not only model accuracy but also regulatory\ncompliance and market valuation of AI assets. Yet existing valuation methods\nremain inadequate: model-based techniques depend on a single fitted model and\ninherit its biases, while algorithm-based approaches such as Data Shapley\nrequire costly retrainings at web scale. Recent Wasserstein-based\nmodel-agnostic methods rely on approximations that misrank examples relative to\ntheir true leave-one-out (LOO) utility. We introduce KAIROS, a scalable,\nmodel-agnostic valuation framework that assigns each example a distributional\ninfluence score: its contribution to the Maximum Mean Discrepancy (MMD) between\nthe empirical training distribution and a clean reference set. Unlike\nWasserstein surrogates, our MMD-based influence admits a closed-form solution\nthat faithfully approximates the exact LOO ranking within $O(1/N^2)$ error,\nrequires no retraining, and naturally extends to conditional kernels for\nunified label- and feature-error detection. Moreover, KAIROS supports efficient\nonline updates: when a new batch of size m arrives, all scores can be updated\nin $O(mN)$ time, delivering up to 50x speedup without compromising ranking\nquality. Empirical evaluations on noise, mislabeling, and poisoning benchmarks\nshow that KAIROS consistently outperforms state-of-the-art model-, Shapley-,\nand Wasserstein-based baselines in both accuracy and runtime. We provide\nrigorous theoretical guarantees, including symmetry for reproducible rankings\nand density-separation for interpretable thresholds.", "AI": {"tldr": "KAIROS is a scalable, model-agnostic framework for data valuation, outperforming existing methods in accuracy and efficiency by using MMD-based influence scores without retraining.", "motivation": "Existing data valuation methods are biased, costly, or inaccurate, necessitating a scalable and reliable alternative.", "method": "KAIROS assigns distributional influence scores using MMD between training and reference sets, avoiding retraining and enabling online updates.", "result": "KAIROS outperforms baselines in accuracy and runtime, with rigorous theoretical guarantees and practical efficiency.", "conclusion": "KAIROS provides a scalable, accurate, and efficient solution for data valuation, addressing limitations of current methods."}}
{"id": "2506.23157", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23157", "abs": "https://arxiv.org/abs/2506.23157", "authors": ["Hanyu Zhou", "Haonan Wang", "Haoyue Liu", "Yuxing Duan", "Luxin Yan", "Gim Hee Lee"], "title": "STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene", "comment": null, "summary": "High-dynamic scene reconstruction aims to represent static background with\nrigid spatial features and dynamic objects with deformed continuous\nspatiotemporal features. Typically, existing methods adopt unified\nrepresentation model (e.g., Gaussian) to directly match the spatiotemporal\nfeatures of dynamic scene from frame camera. However, this unified paradigm\nfails in the potential discontinuous temporal features of objects due to frame\nimaging and the heterogeneous spatial features between background and objects.\nTo address this issue, we disentangle the spatiotemporal features into various\nlatent representations to alleviate the spatiotemporal mismatching between\nbackground and objects. In this work, we introduce event camera to compensate\nfor frame camera, and propose a spatiotemporal-disentangled Gaussian splatting\nframework for high-dynamic scene reconstruction. As for dynamic scene, we\nfigure out that background and objects have appearance discrepancy in\nframe-based spatial features and motion discrepancy in event-based temporal\nfeatures, which motivates us to distinguish the spatiotemporal features between\nbackground and objects via clustering. As for dynamic object, we discover that\nGaussian representations and event data share the consistent spatiotemporal\ncharacteristic, which could serve as a prior to guide the spatiotemporal\ndisentanglement of object Gaussians. Within Gaussian splatting framework, the\ncumulative scene-object disentanglement can improve the spatiotemporal\ndiscrimination between background and objects to render the time-continuous\ndynamic scene. Extensive experiments have been performed to verify the\nsuperiority of the proposed method.", "AI": {"tldr": "A spatiotemporal-disentangled Gaussian splatting framework is proposed for high-dynamic scene reconstruction, using event cameras to complement frame cameras and addressing spatiotemporal mismatches between background and objects.", "motivation": "Existing methods fail to handle discontinuous temporal features and heterogeneous spatial features in dynamic scenes due to unified representation models.", "method": "The approach disentangles spatiotemporal features into latent representations, uses event cameras alongside frame cameras, and employs clustering to distinguish features between background and objects.", "result": "The framework improves spatiotemporal discrimination, enabling time-continuous dynamic scene rendering, validated by extensive experiments.", "conclusion": "The proposed method effectively addresses spatiotemporal mismatches in high-dynamic scenes, outperforming existing approaches."}}
{"id": "2506.23800", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23800", "abs": "https://arxiv.org/abs/2506.23800", "authors": ["Chang Qi", "Matteo Forasassi", "Thomas Lukasiewicz", "Tommaso Salvatori"], "title": "Towards the Training of Deeper Predictive Coding Neural Networks", "comment": "18 Pages, 7 figures", "summary": "Predictive coding networks trained with equilibrium propagation are neural\nmodels that perform inference through an iterative energy minimization process.\nPrevious studies have demonstrated their effectiveness in shallow\narchitectures, but show significant performance degradation when depth exceeds\nfive to seven layers. In this work, we show that the reason behind this\ndegradation is due to exponentially imbalanced errors between layers during\nweight updates, and predictions from the previous layer not being effective in\nguiding updates in deeper layers. We address the first issue by introducing two\nnovel methods to optimize the latent variables that use precision-weighting to\nre-balance the distribution of energy among layers during the `relaxation\nphase', and the second issue by proposing a novel weight update mechanism that\nreduces error accumulation in deeper layers. Empirically, we test our methods\non a large number of image classification tasks, resulting in large\nimprovements in test accuracy across networks with more than seven layers, with\nperformances comparable to those of backprop on similar models. These findings\nsuggest that a better understanding of the relaxation phase is important to\ntrain models using equilibrium propagation at scale, and open new possibilities\nfor their application in complex tasks.", "AI": {"tldr": "The paper addresses performance degradation in deep predictive coding networks by re-balancing energy distribution and improving weight updates, achieving results comparable to backpropagation.", "motivation": "Performance drops in deep predictive coding networks (beyond 5-7 layers) due to imbalanced errors and ineffective guidance from previous layers.", "method": "Introduces precision-weighting for latent variables and a novel weight update mechanism to reduce error accumulation in deeper layers.", "result": "Improved test accuracy in deep networks (>7 layers), matching backpropagation performance on similar models.", "conclusion": "Better understanding of the relaxation phase is key for scaling equilibrium propagation, enabling its use in complex tasks."}}
{"id": "2506.23189", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23189", "abs": "https://arxiv.org/abs/2506.23189", "authors": ["Mustafa Hakan Kara", "Aysegul Dundar", "U\u011fur G\u00fcd\u00fckbay"], "title": "Trident: Detecting Face Forgeries with Adversarial Triplet Learning", "comment": "11 pages, 3 figures, and 7 tables", "summary": "As face forgeries generated by deep neural networks become increasingly\nsophisticated, detecting face manipulations in digital media has posed a\nsignificant challenge, underscoring the importance of maintaining digital media\nintegrity and combating visual disinformation. Current detection models,\npredominantly based on supervised training with domain-specific data, often\nfalter against forgeries generated by unencountered techniques. In response to\nthis challenge, we introduce \\textit{Trident}, a face forgery detection\nframework that employs triplet learning with a Siamese network architecture for\nenhanced adaptability across diverse forgery methods. \\textit{Trident} is\ntrained on curated triplets to isolate nuanced differences of forgeries,\ncapturing fine-grained features that distinguish pristine samples from\nmanipulated ones while controlling for other variables. To further enhance\ngeneralizability, we incorporate domain-adversarial training with a forgery\ndiscriminator. This adversarial component guides our embedding model towards\nforgery-agnostic representations, improving its robustness to unseen\nmanipulations. In addition, we prevent gradient flow from the classifier head\nto the embedding model, avoiding overfitting induced by artifacts peculiar to\ncertain forgeries. Comprehensive evaluations across multiple benchmarks and\nablation studies demonstrate the effectiveness of our framework. We will\nrelease our code in a GitHub repository.", "AI": {"tldr": "Trident is a face forgery detection framework using triplet learning and adversarial training to improve adaptability and robustness against unseen manipulations.", "motivation": "The increasing sophistication of deepfake technologies necessitates advanced detection methods to ensure digital media integrity and combat disinformation.", "method": "Trident employs triplet learning with a Siamese network and domain-adversarial training to capture fine-grained forgery features and enhance generalizability.", "result": "The framework demonstrates effectiveness across multiple benchmarks, showing improved robustness to unseen manipulations.", "conclusion": "Trident offers a promising solution for detecting diverse face forgeries, with code to be released for further research."}}
{"id": "2506.23802", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23802", "abs": "https://arxiv.org/abs/2506.23802", "authors": ["Konstantinos Bourazas", "Savvas Papaioannou", "Panayiotis Kolios"], "title": "Adaptive Out-of-Control Point Pattern Detection in Sequential Random Finite Set Observations", "comment": "23rd European Control Conference (ECC 2025), Thessaloniki, Greece,\n  24-27 June 2025", "summary": "In this work we introduce a novel adaptive anomaly detection framework\nspecifically designed for monitoring sequential random finite set (RFS)\nobservations. Our approach effectively distinguishes between In-Control data\n(normal) and Out-Of-Control data (anomalies) by detecting deviations from the\nexpected statistical behavior of the process. The primary contributions of this\nstudy include the development of an innovative RFS-based framework that not\nonly learns the normal behavior of the data-generating process online but also\ndynamically adapts to behavioral shifts to accurately identify abnormal point\npatterns. To achieve this, we introduce a new class of RFS-based posterior\ndistributions, named Power Discounting Posteriors (PD), which facilitate\nadaptation to systematic changes in data while enabling anomaly detection of\npoint pattern data through a novel predictive posterior density function. The\neffectiveness of the proposed approach is demonstrated by extensive qualitative\nand quantitative simulation experiments.", "AI": {"tldr": "A novel adaptive anomaly detection framework for sequential RFS observations, distinguishing normal and anomalous data by detecting deviations from expected behavior.", "motivation": "To monitor sequential RFS observations and accurately identify anomalies by learning normal behavior online and adapting to shifts.", "method": "Develops an RFS-based framework with Power Discounting Posteriors (PD) for dynamic adaptation and anomaly detection via a predictive posterior density function.", "result": "Demonstrated effectiveness through extensive qualitative and quantitative simulations.", "conclusion": "The proposed framework successfully adapts to behavioral shifts and detects anomalies in point pattern data."}}
{"id": "2506.23196", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23196", "abs": "https://arxiv.org/abs/2506.23196", "authors": ["Mona Ahmadian", "Amir Shirian", "Frank Guerin", "Andrew Gilbert"], "title": "DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding", "comment": null, "summary": "Real-world videos often contain overlapping events and complex temporal\ndependencies, making multimodal interaction modeling particularly challenging.\nWe introduce DEL, a framework for dense semantic action localization, aiming to\naccurately detect and classify multiple actions at fine-grained temporal\nresolutions in long untrimmed videos. DEL consists of two key modules: the\nalignment of audio and visual features that leverage masked self-attention to\nenhance intra-mode consistency and a multimodal interaction refinement module\nthat models cross-modal dependencies across multiple scales, enabling\nhigh-level semantics and fine-grained details. Our method achieves\nstate-of-the-art performance on multiple real-world Temporal Action\nLocalization (TAL) datasets, UnAV-100, THUMOS14, ActivityNet 1.3, and\nEPIC-Kitchens-100, surpassing previous approaches with notable average mAP\ngains of +3.3%, +2.6%, +1.2%, +1.7% (verb), and +1.4% (noun), respectively.", "AI": {"tldr": "DEL is a framework for dense semantic action localization in videos, using multimodal interaction modeling to achieve state-of-the-art performance on TAL datasets.", "motivation": "Real-world videos have overlapping events and complex temporal dependencies, making multimodal interaction modeling challenging.", "method": "DEL aligns audio and visual features with masked self-attention and refines multimodal interactions across scales.", "result": "Achieves notable mAP gains on UnAV-100 (+3.3%), THUMOS14 (+2.6%), ActivityNet 1.3 (+1.2%), and EPIC-Kitchens-100 (+1.7% verb, +1.4% noun).", "conclusion": "DEL outperforms previous methods, demonstrating effectiveness in dense action localization."}}
{"id": "2506.23219", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23219", "abs": "https://arxiv.org/abs/2506.23219", "authors": ["Jie Feng", "Shengyuan Wang", "Tianhui Liu", "Yanxin Xi", "Yong Li"], "title": "UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding", "comment": "Accepted by ICCV 2025", "summary": "Urban research involves a wide range of scenarios and tasks that require the\nunderstanding of multi-modal data. Current methods often focus on specific data\ntypes and lack a unified framework in urban field for processing them\ncomprehensively. The recent success of multi-modal large language models\n(MLLMs) presents a promising opportunity to overcome this limitation. In this\npaper, we introduce $\\textit{UrbanLLaVA}$, a multi-modal large language model\ndesigned to process these four types of data simultaneously and achieve strong\nperformance across diverse urban tasks compared with general MLLMs. In\n$\\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset\nencompassing both single-modal and cross-modal urban data, spanning from\nlocation view to global view of urban environment. Additionally, we propose a\nmulti-stage training framework that decouples spatial reasoning enhancement\nfrom domain knowledge learning, thereby improving the compatibility and\ndownstream performance of $\\textit{UrbanLLaVA}$ across diverse urban tasks.\nFinally, we also extend existing benchmark for urban research to assess the\nperformance of MLLMs across a wide range of urban tasks. Experimental results\nfrom three cities demonstrate that $\\textit{UrbanLLaVA}$ outperforms\nopen-source and proprietary MLLMs in both single-modal tasks and complex\ncross-modal tasks and shows robust generalization abilities across cities.\nSource codes and data are openly accessible to the research community via\nhttps://github.com/tsinghua-fib-lab/UrbanLLaVA.", "AI": {"tldr": "UrbanLLaVA is a multi-modal large language model designed for urban research, outperforming general MLLMs by processing diverse urban data types through a curated dataset and multi-stage training.", "motivation": "Current urban research methods lack a unified framework for multi-modal data, limiting comprehensive analysis. MLLMs offer a solution.", "method": "UrbanLLaVA uses a diverse urban instruction dataset and a multi-stage training framework to enhance spatial reasoning and domain knowledge.", "result": "UrbanLLaVA outperforms open-source and proprietary MLLMs in urban tasks, showing robust generalization across cities.", "conclusion": "UrbanLLaVA provides a scalable and effective solution for multi-modal urban research, with open-source availability for community use."}}
{"id": "2506.23803", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2506.23803", "abs": "https://arxiv.org/abs/2506.23803", "authors": ["Dmitry Kovalev"], "title": "SGD with Adaptive Preconditioning: Unified Analysis and Momentum Acceleration", "comment": null, "summary": "In this paper, we revisit stochastic gradient descent (SGD) with AdaGrad-type\npreconditioning. Our contributions are twofold. First, we develop a unified\nconvergence analysis of SGD with adaptive preconditioning under anisotropic or\nmatrix smoothness and noise assumptions. This allows us to recover\nstate-of-the-art convergence results for several popular adaptive gradient\nmethods, including AdaGrad-Norm, AdaGrad, and ASGO/One-sided Shampoo. In\naddition, we establish the fundamental connection between two recently proposed\nalgorithms, Scion and DASGO, and provide the first theoretical guarantees for\nthe latter. Second, we show that the convergence of methods like AdaGrad and\nDASGO can be provably accelerated beyond the best-known rates using Nesterov\nmomentum. Consequently, we obtain the first theoretical justification that\nAdaGrad-type algorithms can simultaneously benefit from both diagonal\npreconditioning and momentum, which may provide an ultimate explanation for the\npractical efficiency of Adam.", "AI": {"tldr": "The paper revisits SGD with AdaGrad-type preconditioning, unifying convergence analysis and connecting Scion and DASGO, while proving acceleration with Nesterov momentum.", "motivation": "To provide a unified theoretical framework for adaptive gradient methods and explore their acceleration with momentum.", "method": "Develops a unified convergence analysis under anisotropic smoothness and noise, linking Scion and DASGO, and applies Nesterov momentum.", "result": "Recovers state-of-the-art convergence results, connects Scion/DASGO, and shows acceleration beyond known rates for AdaGrad/DASGO.", "conclusion": "AdaGrad-type methods can benefit from both preconditioning and momentum, explaining Adam's practical efficiency."}}
{"id": "2506.23202", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23202", "abs": "https://arxiv.org/abs/2506.23202", "authors": ["Qilin Shu", "Qixian Zhang", "Qi Zhang", "Hongyun Zhang", "Duoqian Miao", "Cairong Zhao"], "title": "Transformer-Based Person Search with High-Frequency Augmentation and Multi-Wave Mixing", "comment": null, "summary": "The person search task aims to locate a target person within a set of scene\nimages. In recent years, transformer-based models in this field have made some\nprogress. However, they still face three primary challenges: 1) the\nself-attention mechanism tends to suppress high-frequency components in the\nfeatures, which severely impacts model performance; 2) the computational cost\nof transformers is relatively high. To address these issues, we propose a novel\nHigh-frequency Augmentation and Multi-Wave mixing (HAMW) method for person\nsearch. HAMW is designed to enhance the discriminative feature extraction\ncapabilities of transformers while reducing computational overhead and\nimproving efficiency. Specifically, we develop a three-stage framework that\nprogressively optimizes both detection and re-identification performance. Our\nmodel enhances the perception of high-frequency features by learning from\naugmented inputs containing additional high-frequency components. Furthermore,\nwe replace the self-attention layers in the transformer with a strategy based\non multi-level Haar wavelet fusion to capture multi-scale features. This not\nonly lowers the computational complexity but also alleviates the suppression of\nhigh-frequency features and enhances the ability to exploit multi-scale\ninformation. Extensive experiments demonstrate that HAMW achieves\nstate-of-the-art performance on both the CUHK-SYSU and PRW datasets.", "AI": {"tldr": "The paper proposes HAMW, a method to enhance transformer-based person search by addressing high-frequency feature suppression and high computational costs.", "motivation": "Transformer-based models for person search face challenges like high-frequency feature suppression and high computational costs, limiting performance.", "method": "HAMW introduces high-frequency augmentation and multi-wave mixing, replacing self-attention with multi-level Haar wavelet fusion for better feature extraction and efficiency.", "result": "HAMW achieves state-of-the-art performance on CUHK-SYSU and PRW datasets.", "conclusion": "HAMW effectively improves transformer-based person search by enhancing feature extraction and reducing computational overhead."}}
{"id": "2506.23824", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23824", "abs": "https://arxiv.org/abs/2506.23824", "authors": ["Durgesh Singh", "Ahcene Boubekki", "Robert Jenssen", "Michael C. Kampffmeyer"], "title": "Supercm: Revisiting Clustering for Semi-Supervised Learning", "comment": null, "summary": "The development of semi-supervised learning (SSL) has in recent years largely\nfocused on the development of new consistency regularization or entropy\nminimization approaches, often resulting in models with complex training\nstrategies to obtain the desired results. In this work, we instead propose a\nnovel approach that explicitly incorporates the underlying clustering\nassumption in SSL through extending a recently proposed differentiable\nclustering module. Leveraging annotated data to guide the cluster centroids\nresults in a simple end-to-end trainable deep SSL approach. We demonstrate that\nthe proposed model improves the performance over the supervised-only baseline\nand show that our framework can be used in conjunction with other SSL methods\nto further boost their performance.", "AI": {"tldr": "A novel semi-supervised learning approach using differentiable clustering to improve performance and simplicity.", "motivation": "Current SSL methods rely on complex training strategies; this work simplifies SSL by incorporating clustering assumptions.", "method": "Extends a differentiable clustering module, using annotated data to guide cluster centroids for an end-to-end trainable model.", "result": "Outperforms supervised-only baselines and enhances other SSL methods when combined.", "conclusion": "The proposed framework offers a simpler, effective SSL solution with potential for broader application."}}
{"id": "2506.23205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23205", "abs": "https://arxiv.org/abs/2506.23205", "authors": ["Dequan Kong", "Zhe Zhu", "Honghua Chen", "Mingqiang Wei"], "title": "BridgeShape: Latent Diffusion Schr\u00f6dinger Bridge for 3D Shape Completion", "comment": null, "summary": "Existing diffusion-based 3D shape completion methods typically use a\nconditional paradigm, injecting incomplete shape information into the denoising\nnetwork via deep feature interactions (e.g., concatenation, cross-attention) to\nguide sampling toward complete shapes, often represented by voxel-based\ndistance functions. However, these approaches fail to explicitly model the\noptimal global transport path, leading to suboptimal completions. Moreover,\nperforming diffusion directly in voxel space imposes resolution constraints,\nlimiting the generation of fine-grained geometric details. To address these\nchallenges, we propose BridgeShape, a novel framework for 3D shape completion\nvia latent diffusion Schr\\\"odinger bridge. The key innovations lie in two\naspects: (i) BridgeShape formulates shape completion as an optimal transport\nproblem, explicitly modeling the transition between incomplete and complete\nshapes to ensure a globally coherent transformation. (ii) We introduce a\nDepth-Enhanced Vector Quantized Variational Autoencoder (VQ-VAE) to encode 3D\nshapes into a compact latent space, leveraging self-projected multi-view depth\ninformation enriched with strong DINOv2 features to enhance geometric\nstructural perception. By operating in a compact yet structurally informative\nlatent space, BridgeShape effectively mitigates resolution constraints and\nenables more efficient and high-fidelity 3D shape completion. BridgeShape\nachieves state-of-the-art performance on large-scale 3D shape completion\nbenchmarks, demonstrating superior fidelity at higher resolutions and for\nunseen object classes.", "AI": {"tldr": "BridgeShape introduces a latent diffusion Schr\\\"odinger bridge framework for 3D shape completion, addressing suboptimal completions and resolution constraints by modeling optimal transport and using a Depth-Enhanced VQ-VAE.", "motivation": "Existing methods fail to model optimal global transport paths and face resolution limits in voxel space, leading to suboptimal completions and lack of fine details.", "method": "BridgeShape formulates shape completion as an optimal transport problem and uses a Depth-Enhanced VQ-VAE to encode shapes into a compact latent space, leveraging multi-view depth and DINOv2 features.", "result": "The framework achieves state-of-the-art performance, enabling high-fidelity completions at higher resolutions and for unseen classes.", "conclusion": "BridgeShape effectively addresses limitations of existing methods, offering superior fidelity and efficiency in 3D shape completion."}}
{"id": "2506.23843", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23843", "abs": "https://arxiv.org/abs/2506.23843", "authors": ["Joris Bekkers"], "title": "EFPI: Elastic Formation and Position Identification in Football (Soccer) using Template Matching and Linear Assignment", "comment": null, "summary": "Understanding team formations and player positioning is crucial for tactical\nanalysis in football (soccer). This paper presents a flexible method for\nformation recognition and player position assignment in football using\npredefined static formation templates and cost minimization from spatiotemporal\ntracking data, called EFPI. Our approach employs linear sum assignment to\noptimally match players to positions within a set of template formations by\nminimizing the total distance between actual player locations and template\npositions, subsequently selecting the formation with the lowest assignment\ncost. To improve accuracy, we scale actual player positions to match the\ndimensions of these formation templates in both width and length. While the\nmethod functions effectively on individual frames, it extends naturally to\nlarger game segments such as complete periods, possession sequences or specific\nintervals (e.g. 10 second intervals, 5 minute intervals etc.). Additionally, we\nincorporate an optional stability parameter that prevents unnecessary formation\nchanges when assignment costs differ only marginally between time segments.\nEFPI is available as open-source code through the unravelsports Python package.", "AI": {"tldr": "The paper introduces EFPI, a method for recognizing football team formations and assigning player positions using predefined templates and cost minimization from tracking data.", "motivation": "Understanding team formations and player positioning is key for tactical analysis in football.", "method": "EFPI uses linear sum assignment to match players to template formations by minimizing distance costs, scales player positions to template dimensions, and includes a stability parameter to avoid unnecessary changes.", "result": "The method effectively identifies formations for individual frames or larger game segments and is available as open-source code.", "conclusion": "EFPI provides a flexible and accurate approach for formation recognition in football, with potential applications in tactical analysis."}}
{"id": "2506.23207", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23207", "abs": "https://arxiv.org/abs/2506.23207", "authors": ["Zhen Tan", "Xieyuanli Chen", "Lei Feng", "Yangbing Ge", "Shuaifeng Zhi", "Jiaxiong Liu", "Dewen Hu"], "title": "TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints", "comment": null, "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled RGB-only SLAM\nsystems to achieve high-fidelity scene representation. However, the heavy\nreliance of existing systems on photometric rendering loss for camera tracking\nundermines their robustness, especially in unbounded outdoor environments with\nsevere viewpoint and illumination changes. To address these challenges, we\npropose TVG-SLAM, a robust RGB-only 3DGS SLAM system that leverages a novel\ntri-view geometry paradigm to ensure consistent tracking and high-quality\nmapping. We introduce a dense tri-view matching module that aggregates reliable\npairwise correspondences into consistent tri-view matches, forming robust\ngeometric constraints across frames. For tracking, we propose Hybrid Geometric\nConstraints, which leverage tri-view matches to construct complementary\ngeometric cues alongside photometric loss, ensuring accurate and stable pose\nestimation even under drastic viewpoint shifts and lighting variations. For\nmapping, we propose a new probabilistic initialization strategy that encodes\ngeometric uncertainty from tri-view correspondences into newly initialized\nGaussians. Additionally, we design a Dynamic Attenuation of Rendering Trust\nmechanism to mitigate tracking drift caused by mapping latency. Experiments on\nmultiple public outdoor datasets show that our TVG-SLAM outperforms prior\nRGB-only 3DGS-based SLAM systems. Notably, in the most challenging dataset, our\nmethod improves tracking robustness, reducing the average Absolute Trajectory\nError (ATE) by 69.0\\% while achieving state-of-the-art rendering quality. The\nimplementation of our method will be released as open-source.", "AI": {"tldr": "TVG-SLAM improves RGB-only 3DGS SLAM by using tri-view geometry for robust tracking and mapping, outperforming prior methods in challenging outdoor environments.", "motivation": "Existing 3DGS SLAM systems rely heavily on photometric loss, which lacks robustness in unbounded outdoor scenes with viewpoint and illumination changes.", "method": "TVG-SLAM introduces tri-view matching for geometric constraints, hybrid geometric-photometric tracking, probabilistic Gaussian initialization, and dynamic rendering trust attenuation.", "result": "TVG-SLAM reduces Absolute Trajectory Error by 69.0% in challenging datasets while maintaining high rendering quality.", "conclusion": "TVG-SLAM offers a robust and accurate RGB-only SLAM solution for outdoor environments, with open-source implementation planned."}}
{"id": "2506.23845", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.23845", "abs": "https://arxiv.org/abs/2506.23845", "authors": ["Kenny Peng", "Rajiv Movva", "Jon Kleinberg", "Emma Pierson", "Nikhil Garg"], "title": "Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts", "comment": null, "summary": "While sparse autoencoders (SAEs) have generated significant excitement, a\nseries of negative results have added to skepticism about their usefulness.\nHere, we establish a conceptual distinction that reconciles competing\nnarratives surrounding SAEs. We argue that while SAEs may be less effective for\nacting on known concepts, SAEs are powerful tools for discovering unknown\nconcepts. This distinction cleanly separates existing negative and positive\nresults, and suggests several classes of SAE applications. Specifically, we\noutline use cases for SAEs in (i) ML interpretability, explainability,\nfairness, auditing, and safety, and (ii) social and health sciences.", "AI": {"tldr": "SAEs are less effective for known concepts but powerful for discovering unknown ones, clarifying their mixed results and suggesting applications in ML interpretability and social/health sciences.", "motivation": "Reconcile competing narratives about the usefulness of sparse autoencoders (SAEs) by distinguishing their effectiveness for known vs. unknown concepts.", "method": "Conceptual distinction between SAEs' roles in acting on known concepts and discovering unknown ones.", "result": "SAEs are powerful for discovering unknown concepts, separating existing negative and positive results.", "conclusion": "SAEs have promising applications in ML interpretability, fairness, and social/health sciences."}}
{"id": "2506.23209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23209", "abs": "https://arxiv.org/abs/2506.23209", "authors": ["Chia-Wen Huang", "Haw Hwai", "Chien-Chang Lee", "Pei-Yuan Wu"], "title": "A Hierarchical Slice Attention Network for Appendicitis Classification in 3D CT Scans", "comment": "8 pages, 1 figure, 3 tables. Published in IEEE ISBI 2025. This\n  version corrects citation numbering errors", "summary": "Timely and accurate diagnosis of appendicitis is critical in clinical\nsettings to prevent serious complications. While CT imaging remains the\nstandard diagnostic tool, the growing number of cases can overwhelm\nradiologists, potentially causing delays. In this paper, we propose a deep\nlearning model that leverages 3D CT scans for appendicitis classification,\nincorporating Slice Attention mechanisms guided by external 2D datasets to\nenhance small lesion detection. Additionally, we introduce a hierarchical\nclassification framework using pre-trained 2D models to differentiate between\nsimple and complicated appendicitis. Our approach improves AUC by 3% for\nappendicitis and 5.9% for complicated appendicitis, offering a more efficient\nand reliable diagnostic solution compared to previous work.", "AI": {"tldr": "A deep learning model using 3D CT scans with Slice Attention and hierarchical classification improves appendicitis diagnosis accuracy.", "motivation": "Timely and accurate diagnosis of appendicitis is crucial to prevent complications, but CT imaging overload can delay diagnoses.", "method": "Proposes a deep learning model with 3D CT scans, Slice Attention, and hierarchical classification using pre-trained 2D models.", "result": "Improves AUC by 3% for appendicitis and 5.9% for complicated appendicitis.", "conclusion": "The model offers a more efficient and reliable diagnostic solution compared to prior methods."}}
{"id": "2506.23872", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23872", "abs": "https://arxiv.org/abs/2506.23872", "authors": ["Eduard Buss", "Till Aust", "Heiko Hamann"], "title": "When Plants Respond: Electrophysiology and Machine Learning for Green Monitoring Systems", "comment": "Submitted and Accepted at the 14th international conference on\n  biomimetic and biohybrid systems (Living Machines)", "summary": "Living plants, while contributing to ecological balance and climate\nregulation, also function as natural sensors capable of transmitting\ninformation about their internal physiological states and surrounding\nconditions. This rich source of data provides potential for applications in\nenvironmental monitoring and precision agriculture. With integration into\nbiohybrid systems, we establish novel channels of physiological signal flow\nbetween living plants and artificial devices. We equipped *Hedera helix* with a\nplant-wearable device called PhytoNode to continuously record the plant's\nelectrophysiological activity. We deployed plants in an uncontrolled outdoor\nenvironment to map electrophysiological patterns to environmental conditions.\nOver five months, we collected data that we analyzed using state-of-the-art and\nautomated machine learning (AutoML). Our classification models achieve high\nperformance, reaching macro F1 scores of up to 95 percent in binary tasks.\nAutoML approaches outperformed manual tuning, and selecting subsets of\nstatistical features further improved accuracy. Our biohybrid living system\nmonitors the electrophysiology of plants in harsh, real-world conditions. This\nwork advances scalable, self-sustaining, and plant-integrated living biohybrid\nsystems for sustainable environmental monitoring.", "AI": {"tldr": "The paper explores using plants as natural sensors for environmental monitoring, integrating them with biohybrid systems. A wearable device (PhytoNode) records plant electrophysiology, and AutoML achieves high classification accuracy.", "motivation": "To leverage plants as natural sensors for real-world environmental monitoring and precision agriculture, integrating them with artificial devices.", "method": "Equipped *Hedera helix* with PhytoNode to record electrophysiological activity outdoors. Used AutoML for data analysis, comparing it with manual tuning.", "result": "Achieved macro F1 scores up to 95% in binary tasks, with AutoML outperforming manual tuning. Feature selection further improved accuracy.", "conclusion": "Demonstrates a scalable, self-sustaining biohybrid system for plant-integrated environmental monitoring, advancing sustainable solutions."}}
{"id": "2506.23875", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23875", "abs": "https://arxiv.org/abs/2506.23875", "authors": ["Yuta Sato", "Kazuhiko Kawamoto", "Hiroshi Kera"], "title": "Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic", "comment": "14 pages, 10 figures", "summary": "The chain of thought is fundamental in Transformers, which is to perform\nstep-by-step reasoning. Besides what intermediate steps work, the order of\nthese steps critically affects the difficulty of the reasoning. This study\naddresses a novel task of unraveling chain of thought - reordering decoder\ninput tokens to a learning-friendly sequence for Transformers to learn\narithmetic tasks. The proposed pipeline first trains a Transformer on a mixture\nof target sequences arranged in different orders and then identifies benign\norders as those with fast loss drops in the early stage. As the search space\ngrows factorially with sequence length, we propose a two-stage hierarchical\napproach for inter- and intra-block reordering. Experiments on four\norder-sensitive arithmetic tasks show that our method identifies a\nlearning-friendly order out of a few billion candidates. Notably, on the\nmultiplication task, it recovered the reverse-digit order reported in prior\nstudies.", "AI": {"tldr": "The paper introduces a method to reorder decoder input tokens in Transformers to improve learning of arithmetic tasks by identifying learning-friendly sequences.", "motivation": "The order of intermediate steps in reasoning (chain of thought) affects the difficulty of learning in Transformers, but the search space for optimal orders is vast.", "method": "A pipeline trains a Transformer on mixed-order sequences, identifies benign orders via early loss drops, and uses a hierarchical approach for efficient reordering.", "result": "The method successfully identifies learning-friendly orders from billions of candidates, even recovering known optimal orders like reverse-digit for multiplication.", "conclusion": "Reordering decoder inputs can significantly improve Transformer learning for arithmetic tasks, with the proposed method efficiently navigating the large search space."}}
{"id": "2506.23227", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23227", "abs": "https://arxiv.org/abs/2506.23227", "authors": ["Lunhao Duan", "Shanshan Zhao", "Xingxing Weng", "Jing Zhang", "Gui-Song Xia"], "title": "High-quality Pseudo-labeling for Point Cloud Segmentation with Scene-level Annotation", "comment": "Accepted by TPAMI. Code: https://github.com/LHDuan/WSegPC", "summary": "This paper investigates indoor point cloud semantic segmentation under\nscene-level annotation, which is less explored compared to methods relying on\nsparse point-level labels. In the absence of precise point-level labels,\ncurrent methods first generate point-level pseudo-labels, which are then used\nto train segmentation models. However, generating accurate pseudo-labels for\neach point solely based on scene-level annotations poses a considerable\nchallenge, substantially affecting segmentation performance. Consequently, to\nenhance accuracy, this paper proposes a high-quality pseudo-label generation\nframework by exploring contemporary multi-modal information and region-point\nsemantic consistency. Specifically, with a cross-modal feature guidance module,\nour method utilizes 2D-3D correspondences to align point cloud features with\ncorresponding 2D image pixels, thereby assisting point cloud feature learning.\nTo further alleviate the challenge presented by the scene-level annotation, we\nintroduce a region-point semantic consistency module. It produces regional\nsemantics through a region-voting strategy derived from point-level semantics,\nwhich are subsequently employed to guide the point-level semantic predictions.\nLeveraging the aforementioned modules, our method can rectify inaccurate\npoint-level semantic predictions during training and obtain high-quality\npseudo-labels. Significant improvements over previous works on ScanNet v2 and\nS3DIS datasets under scene-level annotation can demonstrate the effectiveness.\nAdditionally, comprehensive ablation studies validate the contributions of our\napproach's individual components. The code is available at\nhttps://github.com/LHDuan/WSegPC .", "AI": {"tldr": "The paper proposes a framework for high-quality pseudo-label generation in indoor point cloud semantic segmentation using scene-level annotations, leveraging multi-modal information and region-point consistency to improve accuracy.", "motivation": "Current methods struggle with generating accurate pseudo-labels for point cloud segmentation under scene-level annotations, limiting performance.", "method": "The framework includes a cross-modal feature guidance module for 2D-3D alignment and a region-point semantic consistency module to refine predictions.", "result": "The method outperforms previous works on ScanNet v2 and S3DIS datasets, with ablation studies confirming the contributions of its components.", "conclusion": "The proposed approach effectively enhances pseudo-label quality and segmentation performance under scene-level annotations."}}
{"id": "2506.23714", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23714", "abs": "https://arxiv.org/abs/2506.23714", "authors": ["Md Moinul Islam", "Sofoklis Kakouros", "Janne Heikkil\u00e4", "Mourad Oussalah"], "title": "Towards an Automated Multimodal Approach for Video Summarization: Building a Bridge Between Text, Audio and Facial Cue-Based Summarization", "comment": "Accepted to HHAI WS 2025: Workshops at the Fourth International\n  Conference on Hybrid Human-Artificial Intelligence (HHAI)", "summary": "The increasing volume of video content in educational, professional, and\nsocial domains necessitates effective summarization techniques that go beyond\ntraditional unimodal approaches. This paper proposes a behaviour-aware\nmultimodal video summarization framework that integrates textual, audio, and\nvisual cues to generate timestamp-aligned summaries. By extracting prosodic\nfeatures, textual cues and visual indicators, the framework identifies\nsemantically and emotionally important moments. A key contribution is the\nidentification of bonus words, which are terms emphasized across multiple\nmodalities and used to improve the semantic relevance and expressive clarity of\nthe summaries. The approach is evaluated against pseudo-ground truth (pGT)\nsummaries generated using LLM-based extractive method. Experimental results\ndemonstrate significant improvements over traditional extractive method, such\nas the Edmundson method, in both text and video-based evaluation metrics.\nText-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore\nfrom 0.9152 to 0.9536, while in video-based evaluation, our proposed framework\nimproves F1-Score by almost 23%. The findings underscore the potential of\nmultimodal integration in producing comprehensive and behaviourally informed\nvideo summaries.", "AI": {"tldr": "A multimodal video summarization framework integrates text, audio, and visual cues to generate behavior-aware summaries, outperforming traditional methods in both text and video metrics.", "motivation": "The growing volume of video content demands advanced summarization techniques that leverage multiple modalities for more effective and expressive summaries.", "method": "The framework combines prosodic features, textual cues, and visual indicators to identify semantically and emotionally important moments, including bonus words emphasized across modalities.", "result": "The method significantly outperforms traditional approaches, with ROUGE-1 improving from 0.4769 to 0.7929 and F1-Score by 23% in video evaluation.", "conclusion": "Multimodal integration enhances the quality and behavioral relevance of video summaries, demonstrating its superiority over unimodal methods."}}
{"id": "2506.23923", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23923", "abs": "https://arxiv.org/abs/2506.23923", "authors": ["Miguel Camacho-S\u00e1nchez", "Fernando Garc\u00eda-Torres", "Jesper John Lisegaard", "Roc\u00edo del Amor", "Sankhya Mohanty", "Valery Naranjo"], "title": "Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion System", "comment": "11 pages, 4 figures, 45th Ris{\\o} International Symposium on\n  Materials Science", "summary": "Resin infusion (RI) and resin transfer moulding (RTM) are critical processes\nfor the manufacturing of high-performance fibre-reinforced polymer composites,\nparticularly for large-scale applications such as wind turbine blades.\nControlling the resin flow dynamics in these processes is critical to ensure\nthe uniform impregnation of the fibre reinforcements, thereby preventing\nresidual porosities and dry spots that impact the consequent structural\nintegrity of the final component. This paper presents a reinforcement learning\n(RL) based strategy, established using process simulations, for synchronising\nthe different resin flow fronts in an infusion scenario involving two resin\ninlets and a single outlet. Using Proximal Policy Optimisation (PPO), our\napproach addresses the challenge of managing the fluid dynamics in a partially\nobservable environment. The results demonstrate the effectiveness of the RL\napproach in achieving an accurate flow convergence, highlighting its potential\ntowards improving process control and product quality in composites\nmanufacturing.", "AI": {"tldr": "The paper proposes a reinforcement learning (RL) strategy using Proximal Policy Optimisation (PPO) to control resin flow dynamics in resin infusion (RI) and resin transfer moulding (RTM) processes, ensuring uniform impregnation and improved product quality.", "motivation": "To address the challenge of managing resin flow dynamics in composites manufacturing, particularly for large-scale applications like wind turbine blades, to prevent defects like porosities and dry spots.", "method": "A reinforcement learning (RL) approach, specifically Proximal Policy Optimisation (PPO), is used to synchronize resin flow fronts in a scenario with two inlets and one outlet, leveraging process simulations.", "result": "The RL strategy effectively achieves accurate flow convergence, demonstrating its potential for enhancing process control and product quality.", "conclusion": "The RL-based approach shows promise for improving resin infusion processes in composites manufacturing, ensuring better structural integrity of final components."}}
{"id": "2506.23236", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23236", "abs": "https://arxiv.org/abs/2506.23236", "authors": ["Marko Mihajlovic", "Siwei Zhang", "Gen Li", "Kaifeng Zhao", "Lea M\u00fcller", "Siyu Tang"], "title": "VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions", "comment": "[ICCV 2025] https://markomih.github.io/VolumetricSMPL", "summary": "Parametric human body models play a crucial role in computer graphics and\nvision, enabling applications ranging from human motion analysis to\nunderstanding human-environment interactions. Traditionally, these models use\nsurface meshes, which pose challenges in efficiently handling interactions with\nother geometric entities, such as objects and scenes, typically represented as\nmeshes or point clouds. To address this limitation, recent research has\nexplored volumetric neural implicit body models. However, existing works are\neither insufficiently robust for complex human articulations or impose high\ncomputational and memory costs, limiting their widespread use. To this end, we\nintroduce VolumetricSMPL, a neural volumetric body model that leverages Neural\nBlend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike\nprior approaches that rely on large MLPs, NBW dynamically blends a small set of\nlearned weight matrices using predicted shape- and pose-dependent coefficients,\nsignificantly improving computational efficiency while preserving\nexpressiveness. VolumetricSMPL outperforms prior volumetric occupancy model\nCOAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy,\nand a Signed Distance Function (SDF) for efficient and differentiable contact\nmodeling. We demonstrate VolumetricSMPL's strengths across four challenging\ntasks: (1) reconstructing human-object interactions from in-the-wild images,\n(2) recovering human meshes in 3D scenes from egocentric views, (3)\nscene-constrained motion synthesis, and (4) resolving self-intersections. Our\nresults highlight its broad applicability and significant performance and\nefficiency gains.", "AI": {"tldr": "VolumetricSMPL introduces a neural volumetric body model using Neural Blend Weights (NBW) for efficient MLP decoders, outperforming prior models in speed, memory, and accuracy.", "motivation": "Traditional surface mesh models struggle with interactions involving other geometric entities, and existing volumetric models are either inefficient or lack robustness for complex articulations.", "method": "VolumetricSMPL uses NBW to dynamically blend a small set of learned weight matrices, improving efficiency while maintaining expressiveness.", "result": "The model achieves 10x faster inference, 6x lower GPU memory usage, better accuracy, and includes an SDF for contact modeling. It excels in tasks like human-object interaction reconstruction and scene-constrained motion synthesis.", "conclusion": "VolumetricSMPL offers significant performance and efficiency gains, demonstrating broad applicability in computer graphics and vision tasks."}}
{"id": "2506.23958", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23958", "abs": "https://arxiv.org/abs/2506.23958", "authors": ["Ikechukwu Ogbonna", "Lesley Davidson", "Soumya Banerjee", "Abhishek Dasgupta", "Laurence Kenney", "Vikranth Harthikote Nagaraja"], "title": "Bridging the Gap with Retrieval-Augmented Generation: Making Prosthetic Device User Manuals Available in Marginalised Languages", "comment": "5 pages, 0 figures, 0 tables", "summary": "Millions of people in African countries face barriers to accessing healthcare\ndue to language and literacy gaps. This research tackles this challenge by\ntransforming complex medical documents -- in this case, prosthetic device user\nmanuals -- into accessible formats for underserved populations. This case study\nin cross-cultural translation is particularly pertinent/relevant for\ncommunities that receive donated prosthetic devices but may not receive the\naccompanying user documentation. Or, if available online, may only be available\nin formats (e.g., language and readability) that are inaccessible to local\npopulations (e.g., English-language, high resource settings/cultural context).\nThe approach is demonstrated using the widely spoken Pidgin dialect, but our\nopen-source framework has been designed to enable rapid and easy extension to\nother languages/dialects. This work presents an AI-powered framework designed\nto process and translate complex medical documents, e.g., user manuals for\nprosthetic devices, into marginalised languages. The system enables users --\nsuch as healthcare workers or patients -- to upload English-language medical\nequipment manuals, pose questions in their native language, and receive\naccurate, localised answers in real time. Technically, the system integrates a\nRetrieval-Augmented Generation (RAG) pipeline for processing and semantic\nunderstanding of the uploaded manuals. It then employs advanced Natural\nLanguage Processing (NLP) models for generative question-answering and\nmultilingual translation. Beyond simple translation, it ensures accessibility\nto device instructions, treatment protocols, and safety information, empowering\npatients and clinicians to make informed healthcare decisions.", "AI": {"tldr": "An AI-powered framework translates complex medical documents into marginalized languages, addressing healthcare access barriers in African communities.", "motivation": "Millions in Africa face healthcare access issues due to language and literacy gaps, especially with donated prosthetic devices lacking accessible documentation.", "method": "The system uses a Retrieval-Augmented Generation (RAG) pipeline and NLP models for real-time translation and question-answering in local languages.", "result": "The framework successfully translates English medical manuals into accessible formats, like Pidgin, enabling informed healthcare decisions.", "conclusion": "This open-source solution bridges language gaps in healthcare, scalable to other languages and dialects."}}
{"id": "2506.23247", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23247", "abs": "https://arxiv.org/abs/2506.23247", "authors": ["James Hinns", "David Martens"], "title": "Aggregating Local Saliency Maps for Semi-Global Explainable Image Classification", "comment": null, "summary": "Deep learning dominates image classification tasks, yet understanding how\nmodels arrive at predictions remains a challenge. Much research focuses on\nlocal explanations of individual predictions, such as saliency maps, which\nvisualise the influence of specific pixels on a model's prediction. However,\nreviewing many of these explanations to identify recurring patterns is\ninfeasible, while global methods often oversimplify and miss important local\nbehaviours. To address this, we propose Segment Attribution Tables (SATs), a\nmethod for summarising local saliency explanations into (semi-)global insights.\nSATs take image segments (such as \"eyes\" in Chihuahuas) and leverage saliency\nmaps to quantify their influence. These segments highlight concepts the model\nrelies on across instances and reveal spurious correlations, such as reliance\non backgrounds or watermarks, even when out-of-distribution test performance\nsees little change. SATs can explain any classifier for which a form of\nsaliency map can be produced, using segmentation maps that provide named\nsegments. SATs bridge the gap between oversimplified global summaries and\noverly detailed local explanations, offering a practical tool for analysing and\ndebugging image classifiers.", "AI": {"tldr": "Proposes Segment Attribution Tables (SATs) to summarize local saliency explanations into semi-global insights for analyzing image classifiers.", "motivation": "Addresses the gap between oversimplified global methods and overly detailed local explanations in understanding model predictions.", "method": "Uses saliency maps and image segments (e.g., 'eyes') to quantify segment influence, revealing model reliance and spurious correlations.", "result": "SATs provide actionable insights into model behavior, identifying recurring patterns and potential biases.", "conclusion": "SATs offer a practical tool for bridging the gap between local and global explanations, aiding in classifier analysis and debugging."}}
{"id": "2506.23978", "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.SI", "68T50, 68M10, 91B26", "I.2.11; I.2.7; H.4.5"], "pdf": "https://arxiv.org/pdf/2506.23978", "abs": "https://arxiv.org/abs/2506.23978", "authors": ["Samuele Marro", "Philip Torr"], "title": "LLM Agents Are the Antidote to Walled Gardens", "comment": null, "summary": "While the Internet's core infrastructure was designed to be open and\nuniversal, today's application layer is dominated by closed, proprietary\nplatforms. Open and interoperable APIs require significant investment, and\nmarket leaders have little incentive to enable data exchange that could erode\ntheir user lock-in. We argue that LLM-based agents fundamentally disrupt this\nstatus quo. Agents can automatically translate between data formats and\ninteract with interfaces designed for humans: this makes interoperability\ndramatically cheaper and effectively unavoidable. We name this shift universal\ninteroperability: the ability for any two digital services to exchange data\nseamlessly using AI-mediated adapters. Universal interoperability undermines\nmonopolistic behaviours and promotes data portability. However, it can also\nlead to new security risks and technical debt. Our position is that the ML\ncommunity should embrace this development while building the appropriate\nframeworks to mitigate the downsides. By acting now, we can harness AI to\nrestore user freedom and competitive markets without sacrificing security.", "AI": {"tldr": "LLM-based agents enable universal interoperability, disrupting closed platforms and promoting data portability, but require frameworks to address security risks.", "motivation": "The dominance of closed, proprietary platforms limits data exchange and interoperability, reinforcing monopolistic behaviors.", "method": "Proposes using LLM-based agents to automate data translation and interface interaction, making interoperability cost-effective and inevitable.", "result": "Universal interoperability undermines monopolies and enhances data portability but introduces security risks and technical debt.", "conclusion": "The ML community should adopt this shift while developing frameworks to mitigate risks, leveraging AI to restore user freedom and market competition."}}
{"id": "2506.23960", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.23960", "abs": "https://arxiv.org/abs/2506.23960", "authors": ["Mingfei Cheng", "Xiaofei Xie", "Renzhi Wang", "Yuan Zhou", "Ming Hu"], "title": "ADReFT: Adaptive Decision Repair for Safe Autonomous Driving via Reinforcement Fine-Tuning", "comment": null, "summary": "Autonomous Driving Systems (ADSs) continue to face safety-critical risks due\nto the inherent limitations in their design and performance capabilities.\nOnline repair plays a crucial role in mitigating such limitations, ensuring the\nruntime safety and reliability of ADSs. Existing online repair solutions\nenforce ADS compliance by transforming unacceptable trajectories into\nacceptable ones based on predefined specifications, such as rule-based\nconstraints or training datasets. However, these approaches often lack\ngeneralizability, adaptability and tend to be overly conservative, resulting in\nineffective repairs that not only fail to mitigate safety risks sufficiently\nbut also degrade the overall driving experience. To address this issue, we\npropose Adaptive Decision Repair (ADReFT), a novel and effective repair method\nthat identifies safety-critical states through offline learning from failed\ntests and generates appropriate mitigation actions to improve ADS safety.\nSpecifically, ADReFT incorporates a transformer-based model with two joint\nheads, State Monitor and Decision Adapter, designed to capture complex driving\nenvironment interactions to evaluate state safety severity and generate\nadaptive repair actions. Given the absence of oracles for state safety\nidentification, we first pretrain ADReFT using supervised learning with coarse\nannotations, i.e., labeling states preceding violations as positive samples and\nothers as negative samples. It establishes ADReFT's foundational capability to\nmitigate safety-critical violations, though it may result in somewhat\nconservative mitigation strategies. Therefore, we subsequently finetune ADReFT\nusing reinforcement learning to improve its initial capability and generate\nmore precise and contextually appropriate repair decisions. Our evaluation\nresults illustrate that ADReFT achieves better repair performance.", "AI": {"tldr": "ADReFT is a novel repair method for Autonomous Driving Systems (ADSs) that improves safety by identifying critical states and generating adaptive repair actions using a transformer-based model.", "motivation": "Existing online repair solutions for ADSs lack generalizability and adaptability, often being overly conservative and ineffective in mitigating safety risks.", "method": "ADReFT uses a transformer-based model with State Monitor and Decision Adapter heads, pretrained with supervised learning and finetuned with reinforcement learning to generate adaptive repair actions.", "result": "ADReFT achieves better repair performance compared to existing methods.", "conclusion": "ADReFT effectively enhances ADS safety by providing adaptive and precise repair actions, overcoming limitations of current approaches."}}
{"id": "2506.23252", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23252", "abs": "https://arxiv.org/abs/2506.23252", "authors": ["Kunwei Lv", "Ping Lan"], "title": "DGE-YOLO: Dual-Branch Gathering and Attention for Accurate UAV Object Detection", "comment": "8 pages, 5 figures", "summary": "The rapid proliferation of unmanned aerial vehicles (UAVs) has highlighted\nthe importance of robust and efficient object detection in diverse aerial\nscenarios. Detecting small objects under complex conditions, however, remains a\nsignificant challenge. Existing approaches often prioritize inference speed,\nleading to degraded performance when handling multi-modal inputs. To address\nthis, we present DGE-YOLO, an enhanced YOLO-based detection framework designed\nto effectively fuse multi-modal information. Specifically, we introduce a\ndual-branch architecture for modality-specific feature extraction, enabling the\nmodel to process both infrared and visible images. To further enrich semantic\nrepresentation, we propose an Efficient Multi-scale Attention (EMA) mechanism\nthat enhances feature learning across spatial scales. Additionally, we replace\nthe conventional neck with a Gather-and-Distribute module to mitigate\ninformation loss during feature aggregation. Extensive experiments on the Drone\nVehicle dataset demonstrate that DGE-YOLO achieves superior performance over\nstate-of-the-art methods, validating its effectiveness in multi-modal UAV\nobject detection tasks.", "AI": {"tldr": "DGE-YOLO is an enhanced YOLO-based framework for multi-modal UAV object detection, featuring dual-branch architecture, EMA mechanism, and Gather-and-Distribute module, outperforming state-of-the-art methods.", "motivation": "The challenge of detecting small objects in complex aerial scenarios and the limitations of existing approaches prioritizing speed over multi-modal input handling.", "method": "Introduces DGE-YOLO with dual-branch architecture for infrared and visible images, EMA mechanism for multi-scale feature learning, and Gather-and-Distribute module for feature aggregation.", "result": "Superior performance on the Drone Vehicle dataset compared to state-of-the-art methods.", "conclusion": "DGE-YOLO effectively addresses multi-modal UAV object detection challenges, proving its robustness and efficiency."}}
{"id": "2506.24019", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.24019", "abs": "https://arxiv.org/abs/2506.24019", "authors": ["Hongxin Zhang", "Zheyuan Zhang", "Zeyuan Wang", "Zunzhe Zhang", "Lixing Fang", "Qinhong Zhou", "Chuang Gan"], "title": "Ella: Embodied Social Agents with Lifelong Memory", "comment": null, "summary": "We introduce Ella, an embodied social agent capable of lifelong learning\nwithin a community in a 3D open world, where agents accumulate experiences and\nacquire knowledge through everyday visual observations and social interactions.\nAt the core of Ella's capabilities is a structured, long-term multimodal memory\nsystem that stores, updates, and retrieves information effectively. It consists\nof a name-centric semantic memory for organizing acquired knowledge and a\nspatiotemporal episodic memory for capturing multimodal experiences. By\nintegrating this lifelong memory system with foundation models, Ella retrieves\nrelevant information for decision-making, plans daily activities, builds social\nrelationships, and evolves autonomously while coexisting with other intelligent\nbeings in the open world. We conduct capability-oriented evaluations in a\ndynamic 3D open world where 15 agents engage in social activities for days and\nare assessed with a suite of unseen controlled evaluations. Experimental\nresults show that Ella can influence, lead, and cooperate with other agents\nwell to achieve goals, showcasing its ability to learn effectively through\nobservation and social interaction. Our findings highlight the transformative\npotential of combining structured memory systems with foundation models for\nadvancing embodied intelligence. More videos can be found at\nhttps://umass-embodied-agi.github.io/Ella/.", "AI": {"tldr": "Ella is an embodied social agent with lifelong learning in a 3D open world, using a multimodal memory system and foundation models for decision-making and social interactions.", "motivation": "To advance embodied intelligence by integrating structured memory systems with foundation models for lifelong learning in dynamic social environments.", "method": "Ella uses a name-centric semantic memory and spatiotemporal episodic memory, combined with foundation models, to store, update, and retrieve information for decision-making and social interactions.", "result": "Ella successfully influences, leads, and cooperates with other agents in a dynamic 3D world, demonstrating effective learning through observation and social interaction.", "conclusion": "The integration of structured memory systems with foundation models shows transformative potential for advancing embodied intelligence."}}
{"id": "2506.23971", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23971", "abs": "https://arxiv.org/abs/2506.23971", "authors": ["Brandon M. Wood", "Misko Dzamba", "Xiang Fu", "Meng Gao", "Muhammed Shuaibi", "Luis Barroso-Luque", "Kareem Abdelmaqsoud", "Vahe Gharakhanyan", "John R. Kitchin", "Daniel S. Levine", "Kyle Michel", "Anuroop Sriram", "Taco Cohen", "Abhishek Das", "Ammar Rizvi", "Sushree Jagriti Sahoo", "Zachary W. Ulissi", "C. Lawrence Zitnick"], "title": "UMA: A Family of Universal Models for Atoms", "comment": "29 pages, 5 figures", "summary": "The ability to quickly and accurately compute properties from atomic\nsimulations is critical for advancing a large number of applications in\nchemistry and materials science including drug discovery, energy storage, and\nsemiconductor manufacturing. To address this need, Meta FAIR presents a family\nof Universal Models for Atoms (UMA), designed to push the frontier of speed,\naccuracy, and generalization. UMA models are trained on half a billion unique\n3D atomic structures (the largest training runs to date) by compiling data\nacross multiple chemical domains, e.g. molecules, materials, and catalysts. We\ndevelop empirical scaling laws to help understand how to increase model\ncapacity alongside dataset size to achieve the best accuracy. The UMA small and\nmedium models utilize a novel architectural design we refer to as mixture of\nlinear experts that enables increasing model capacity without sacrificing\nspeed. For example, UMA-medium has 1.4B parameters but only ~50M active\nparameters per atomic structure. We evaluate UMA models on a diverse set of\napplications across multiple domains and find that, remarkably, a single model\nwithout any fine-tuning can perform similarly or better than specialized\nmodels. We are releasing the UMA code, weights, and associated data to\naccelerate computational workflows and enable the community to continue to\nbuild increasingly capable AI models.", "AI": {"tldr": "Meta FAIR introduces Universal Models for Atoms (UMA), a family of AI models trained on 500M atomic structures, achieving high speed, accuracy, and generalization across chemical domains without fine-tuning.", "motivation": "The need for fast and accurate atomic property computation in chemistry and materials science applications like drug discovery and energy storage drives the development of UMA.", "method": "UMA models are trained on a vast dataset of 3D atomic structures using empirical scaling laws and a novel 'mixture of linear experts' architecture to balance capacity and speed.", "result": "UMA models outperform specialized models in diverse applications without fine-tuning, with UMA-medium using only ~50M active parameters per structure despite having 1.4B total parameters.", "conclusion": "UMA's release of code, weights, and data aims to advance computational workflows and foster further AI model development in the field."}}
{"id": "2506.23254", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.23254", "abs": "https://arxiv.org/abs/2506.23254", "authors": ["Aradhana Mishra", "Bumshik Lee"], "title": "PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution", "comment": null, "summary": "Diffusion-model-based image super-resolution techniques often face a\ntrade-off between realistic image generation and computational efficiency. This\nissue is exacerbated when inference times by decreasing sampling steps,\nresulting in less realistic and hazy images. To overcome this challenge, we\nintroduce a novel diffusion model named PixelBoost that underscores the\nsignificance of embracing the stochastic nature of Brownian motion in advancing\nimage super-resolution, resulting in a high degree of realism, particularly\nfocusing on texture and edge definitions. By integrating controlled\nstochasticity into the training regimen, our proposed model avoids convergence\nto local optima, effectively capturing and reproducing the inherent uncertainty\nof image textures and patterns. Our proposed model demonstrates superior\nobjective results in terms of learned perceptual image patch similarity\n(LPIPS), lightness order error (LOE), peak signal-to-noise ratio(PSNR),\nstructural similarity index measure (SSIM), as well as visual quality. To\ndetermine the edge enhancement, we evaluated the gradient magnitude and pixel\nvalue, and our proposed model exhibited a better edge reconstruction\ncapability. Additionally, our model demonstrates adaptive learning capabilities\nby effectively adjusting to Brownian noise patterns and introduces a sigmoidal\nnoise sequencing method that simplifies training, resulting in faster inference\nspeeds.", "AI": {"tldr": "PixelBoost, a novel diffusion model, improves image super-resolution by leveraging Brownian motion's stochastic nature, enhancing realism and computational efficiency.", "motivation": "Addressing the trade-off between realistic image generation and computational efficiency in diffusion-model-based super-resolution.", "method": "Integrates controlled stochasticity into training, uses sigmoidal noise sequencing, and adapts to Brownian noise patterns.", "result": "Superior performance in LPIPS, LOE, PSNR, SSIM, and visual quality, with enhanced edge reconstruction.", "conclusion": "PixelBoost effectively balances realism and efficiency, advancing super-resolution techniques."}}
{"id": "2506.24086", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.24086", "abs": "https://arxiv.org/abs/2506.24086", "authors": ["Bingfan Zhu", "Biao Jiang", "Sunyi Wang", "Shixiang Tang", "Tao Chen", "Linjie Luo", "Youyi Zheng", "Xin Chen"], "title": "MotionGPT3: Human Motion as a Second Modality", "comment": "21 pages, 8 figures", "summary": "Though recent advances in multimodal models have demonstrated strong\ncapabilities and opportunities in unified understanding and generation, the\ndevelopment of unified motion-language models remains underexplored. To enable\nsuch models with high-fidelity human motion, two core challenges must be\naddressed. The first is the reconstruction gap between the continuous motion\nmodality and discrete representation in an autoregressive manner, and the\nsecond is the degradation of language intelligence during unified training.\nInspired by the mixture of experts, we propose MotionGPT3, a bimodal\nmotion-language model that treats human motion as a second modality, decoupling\nmotion modeling via separate model parameters and enabling both effective\ncross-modal interaction and efficient multimodal scaling training. To preserve\nlanguage intelligence, the text branch retains the original structure and\nparameters of the pretrained language model, while a new motion branch is\nintegrated via a shared attention mechanism, enabling bidirectional information\nflow between two modalities. We first employ a motion Variational Autoencoder\n(VAE) to encode raw human motion into latent representations. Based on this\ncontinuous latent space, the motion branch predicts motion latents directly\nfrom intermediate hidden states using a diffusion head, bypassing discrete\ntokenization. Extensive experiments show that our approach achieves competitive\nperformance on both motion understanding and generation tasks while preserving\nstrong language capabilities, establishing a unified bimodal motion diffusion\nframework within an autoregressive manner.", "AI": {"tldr": "MotionGPT3 is a bimodal motion-language model addressing challenges in unified motion-language tasks by decoupling motion modeling and preserving language intelligence.", "motivation": "To bridge the gap between continuous motion and discrete language representation while maintaining language capabilities in multimodal models.", "method": "Uses a mixture of experts approach with separate motion and text branches, leveraging a motion VAE and diffusion head for motion prediction.", "result": "Achieves competitive performance in motion understanding and generation without degrading language intelligence.", "conclusion": "Establishes a unified bimodal framework for motion-language tasks, enabling effective cross-modal interaction."}}
{"id": "2506.23977", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23977", "abs": "https://arxiv.org/abs/2506.23977", "authors": ["Zain ul Abdeen", "Vassilis Kekatos", "Ming Jin"], "title": "A Scalable Approach for Safe and Robust Learning via Lipschitz-Constrained Networks", "comment": null, "summary": "Certified robustness is a critical property for deploying neural networks\n(NN) in safety-critical applications. A principle approach to achieving such\nguarantees is to constrain the global Lipschitz constant of the network.\nHowever, accurate methods for Lipschitz-constrained training often suffer from\nnon-convex formulations and poor scalability due to reliance on global\nsemidefinite programs (SDPs). In this letter, we propose a convex training\nframework that enforces global Lipschitz constraints via semidefinite\nrelaxation. By reparameterizing the NN using loop transformation, we derive a\nconvex admissibility condition that enables tractable and certifiable training.\nWhile the resulting formulation guarantees robustness, its scalability is\nlimited by the size of global SDP. To overcome this, we develop a randomized\nsubspace linear matrix inequalities (RS-LMI) approach that decomposes the\nglobal constraints into sketched layerwise constraints projected onto\nlow-dimensional subspaces, yielding a smooth and memory-efficient training\nobjective. Empirical results on MNIST, CIFAR-10, and ImageNet demonstrate that\nthe proposed framework achieves competitive accuracy with significantly\nimproved Lipschitz bounds and runtime performance.", "AI": {"tldr": "A convex training framework for neural networks ensures certified robustness via semidefinite relaxation and loop transformation, improving scalability with a randomized subspace approach.", "motivation": "Certified robustness is essential for safety-critical applications, but existing methods suffer from non-convexity and poor scalability due to global semidefinite programs.", "method": "Proposes a convex training framework using semidefinite relaxation and loop transformation, with a randomized subspace approach (RS-LMI) to enhance scalability.", "result": "Achieves competitive accuracy on MNIST, CIFAR-10, and ImageNet with improved Lipschitz bounds and runtime performance.", "conclusion": "The framework provides a scalable and certifiable solution for training robust neural networks."}}
{"id": "2506.23257", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23257", "abs": "https://arxiv.org/abs/2506.23257", "authors": ["Chongke Bi", "Xin Gao", "Baofeng Fu", "Yuheng Zhao", "Siming Chen", "Ying Zhao", "Yunhai Wang"], "title": "PCLVis: Visual Analytics of Process Communication Latency in Large-Scale Simulation", "comment": null, "summary": "Large-scale simulations on supercomputers have become important tools for\nusers. However, their scalability remains a problem due to the huge\ncommunication cost among parallel processes. Most of the existing communication\nlatency analysis methods rely on the physical link layer information, which is\nonly available to administrators. In this paper, a framework called PCLVis is\nproposed to help general users analyze process communication latency (PCL)\nevents. Instead of the physical link layer information, the PCLVis uses the MPI\nprocess communication data for the analysis. First, a spatial PCL event\nlocating method is developed. All processes with high correlation are\nclassified into a single cluster by constructing a process-correlation tree.\nSecond, the propagation path of PCL events is analyzed by constructing a\ncommunication-dependency-based directed acyclic graph (DAG), which can help\nusers interactively explore a PCL event from the temporal evolution of a\nlocated PCL event cluster. In this graph, a sliding window algorithm is\ndesigned to generate the PCL events abstraction. Meanwhile, a new glyph called\nthe communication state glyph (CS-Glyph) is designed for each process to show\nits communication states, including its in/out messages and load balance. Each\nleaf node can be further unfolded to view additional information. Third, a PCL\nevent attribution strategy is formulated to help users optimize their\nsimulations. The effectiveness of the PCLVis framework is demonstrated by\nanalyzing the PCL events of several simulations running on the TH-1A\nsupercomputer. By using the proposed framework, users can greatly improve the\nefficiency of their simulations.", "AI": {"tldr": "PCLVis is a framework for analyzing process communication latency (PCL) in large-scale simulations using MPI data, improving simulation efficiency.", "motivation": "Address scalability issues in supercomputing simulations caused by high communication costs, without needing physical link layer data.", "method": "Uses MPI process communication data, spatial PCL event locating, correlation clustering, DAG-based propagation analysis, and CS-Glyphs for visualization.", "result": "Demonstrated effectiveness on TH-1A supercomputer, enabling users to optimize simulations.", "conclusion": "PCLVis helps users analyze and improve simulation efficiency by focusing on communication latency."}}
{"id": "2506.23263", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23263", "abs": "https://arxiv.org/abs/2506.23263", "authors": ["Lei-lei Li", "Jianwu Fang", "Junbin Xiao", "Shanmin Pang", "Hongkai Yu", "Chen Lv", "Jianru Xue", "Tat-Seng Chua"], "title": "Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis", "comment": "Accepted by ICCV2025", "summary": "Egocentricly comprehending the causes and effects of car accidents is crucial\nfor the safety of self-driving cars, and synthesizing causal-entity reflected\naccident videos can facilitate the capability test to respond to unaffordable\naccidents in reality. However, incorporating causal relations as seen in\nreal-world videos into synthetic videos remains challenging. This work argues\nthat precisely identifying the accident participants and capturing their\nrelated behaviors are of critical importance. In this regard, we propose a\nnovel diffusion model, Causal-VidSyn, for synthesizing egocentric traffic\naccident videos. To enable causal entity grounding in video diffusion,\nCausal-VidSyn leverages the cause descriptions and driver fixations to identify\nthe accident participants and behaviors, facilitated by accident reason\nanswering and gaze-conditioned selection modules. To support Causal-VidSyn, we\nfurther construct Drive-Gaze, the largest driver gaze dataset (with 1.54M\nframes of fixations) in driving accident scenarios. Extensive experiments show\nthat Causal-VidSyn surpasses state-of-the-art video diffusion models in terms\nof frame quality and causal sensitivity in various tasks, including accident\nvideo editing, normal-to-accident video diffusion, and text-to-video\ngeneration.", "AI": {"tldr": "Causal-VidSyn is a novel diffusion model for synthesizing egocentric traffic accident videos, leveraging cause descriptions and driver fixations to improve causal entity grounding. It outperforms state-of-the-art models in frame quality and causal sensitivity.", "motivation": "Understanding and simulating car accidents is vital for self-driving car safety, but synthesizing realistic accident videos with causal relations is challenging.", "method": "Proposes Causal-VidSyn, a diffusion model using cause descriptions and driver fixations, aided by accident reason answering and gaze-conditioned selection modules.", "result": "Causal-VidSyn excels in frame quality and causal sensitivity, demonstrated in tasks like accident video editing and text-to-video generation.", "conclusion": "The model and dataset (Drive-Gaze) advance the synthesis of realistic, causally-aware accident videos, enhancing self-driving car testing."}}
{"id": "2506.23996", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2506.23996", "abs": "https://arxiv.org/abs/2506.23996", "authors": ["Juan Maro\u00f1as"], "title": "The Jacobian and Hessian of the Kullback-Leibler Divergence between Multivariate Gaussian Distributions (Technical Report)", "comment": null, "summary": "This document shows how to obtain the Jacobian and Hessian matrices of the\nKullback-Leibler divergence between two multivariate Gaussian distributions,\nusing the first and second-order differentials. The presented derivations are\nbased on the theory presented by \\cite{magnus99}. I've also got great\ninspiration from some of the derivations in \\cite{minka}.\n  Since I pretend to be at most didactic, the document is split into a summary\nof results and detailed derivations on each of the elements involved, with\nspecific references to the tricks used in the derivations, and to many of the\nunderlying concepts.", "AI": {"tldr": "Derivation of Jacobian and Hessian matrices for Kullback-Leibler divergence between multivariate Gaussians using differentials.", "motivation": "To provide a clear, didactic explanation of the derivations for Jacobian and Hessian matrices in this context.", "method": "Uses first and second-order differentials, inspired by existing theory and derivations.", "result": "Detailed derivations and summary of results for the matrices.", "conclusion": "The document successfully breaks down complex derivations into understandable steps with references to underlying concepts."}}
{"id": "2506.23270", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23270", "abs": "https://arxiv.org/abs/2506.23270", "authors": ["Yi Li", "Hualiang Wang", "Xinpeng Ding", "Haonan Wang", "Xiaomeng Li"], "title": "Token Activation Map to Visually Explain Multimodal LLMs", "comment": "ICCV2025 Accepted", "summary": "Multimodal large language models (MLLMs) are broadly empowering various\nfields. Despite their advancements, the explainability of MLLMs remains less\nexplored, hindering deeper understanding, model credibility, and effective\nvisualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that\nproduce a single output, MLLMs generate sequences of tokens progressively,\nwhere each generated token depends on the previous context. Therefore, earlier\ncontext tokens can introduce redundant activations that interfere with the\nexplanation of later tokens beyond their original information. Existing studies\noften overlook this issue, but our observations reveal that these redundant\ncorrelations can significantly hurt the reliability of explanations. To address\nthis, we propose an estimated causal inference method to mitigate the\ninterference of context to achieve high-quality MLLM explanation, with a novel\nrank Gaussian filter to further reduce activation noises. We term this method\nToken Activation Map (TAM) to highlight the consideration of interactions\nbetween tokens. TAM also indicates that it excels at explaining multiple tokens\nof MLLM, which is different from the Class Activation Map (CAM) for a single\nprediction. Our TAM method significantly outperforms existing SoTA methods,\nshowcasing high-quality visualization results that can be utilized for various\nscenarios, such as object localization, failure case analysis, video\nvisualization, MLLMs visual comparison, and model understanding (e.g., color,\nshape, action, location, visual reasoning, multi-turn conversation, etc). The\ncode is available atgithub.com/xmed-lab/TAM.", "AI": {"tldr": "The paper introduces Token Activation Map (TAM), a method to improve the explainability of Multimodal Large Language Models (MLLMs) by addressing redundant activations and context interference.", "motivation": "The explainability of MLLMs is understudied, and existing methods overlook redundant activations that harm reliability.", "method": "Proposes an estimated causal inference method with a rank Gaussian filter to mitigate context interference, termed Token Activation Map (TAM).", "result": "TAM outperforms state-of-the-art methods, providing high-quality visualizations for various applications.", "conclusion": "TAM enhances MLLM explainability and is versatile for multiple use cases."}}
{"id": "2506.24000", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24000", "abs": "https://arxiv.org/abs/2506.24000", "authors": ["Lijun Sheng", "Jian Liang", "Ran He", "Zilei Wang", "Tieniu Tan"], "title": "The Illusion of Progress? A Critical Look at Test-Time Adaptation for Vision-Language Models", "comment": "Github link: https://github.com/TomSheng21/tta-vlm", "summary": "Test-time adaptation (TTA) methods have gained significant attention for\nenhancing the performance of vision-language models (VLMs) such as CLIP during\ninference, without requiring additional labeled data. However, current TTA\nresearches generally suffer from major limitations such as duplication of\nbaseline results, limited evaluation metrics, inconsistent experimental\nsettings, and insufficient analysis. These problems hinder fair comparisons\nbetween TTA methods and obscure their practical strengths and weaknesses. To\naddress these challenges, we introduce TTA-VLM, a comprehensive benchmark for\nevaluating TTA methods on VLMs. Our benchmark implements 8 episodic TTA and 7\nonline TTA methods within a unified and reproducible framework, and evaluates\nthem across 15 widely used datasets. Unlike prior studies focused solely on\nCLIP, we extend the evaluation to SigLIP--a model trained with a Sigmoid\nloss--and include training-time tuning methods such as CoOp, MaPLe, and TeCoA\nto assess generality. Beyond classification accuracy, TTA-VLM incorporates\nvarious evaluation metrics, including robustness, calibration,\nout-of-distribution detection, and stability, enabling a more holistic\nassessment of TTA methods. Through extensive experiments, we find that 1)\nexisting TTA methods produce limited gains compared to the previous pioneering\nwork; 2) current TTA methods exhibit poor collaboration with training-time\nfine-tuning methods; 3) accuracy gains frequently come at the cost of reduced\nmodel trustworthiness. We release TTA-VLM to provide fair comparison and\ncomprehensive evaluation of TTA methods for VLMs, and we hope it encourages the\ncommunity to develop more reliable and generalizable TTA strategies.", "AI": {"tldr": "TTA-VLM is a benchmark for evaluating test-time adaptation (TTA) methods on vision-language models (VLMs), addressing limitations like inconsistent evaluations and lack of comprehensive metrics. It includes 15 datasets, extends beyond CLIP to SigLIP, and assesses robustness, calibration, and more. Findings show limited gains from TTA methods, poor collaboration with fine-tuning, and trade-offs in model trustworthiness.", "motivation": "Current TTA research lacks fair comparisons due to duplicated results, limited metrics, and inconsistent settings. TTA-VLM aims to standardize evaluation and provide a holistic assessment of TTA methods for VLMs.", "method": "TTA-VLM implements 8 episodic and 7 online TTA methods in a unified framework, evaluates them on 15 datasets, and includes training-time tuning methods. Metrics cover accuracy, robustness, calibration, out-of-distribution detection, and stability.", "result": "Key findings: 1) TTA methods offer limited improvements over prior work; 2) they poorly collaborate with fine-tuning methods; 3) accuracy gains often reduce model trustworthiness.", "conclusion": "TTA-VLM provides a fair, comprehensive benchmark for TTA methods, encouraging development of more reliable and generalizable strategies."}}
{"id": "2506.23271", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23271", "abs": "https://arxiv.org/abs/2506.23271", "authors": ["Jinxing Zhou", "Zhihui Li", "Yongqiang Yu", "Yanghao Zhou", "Ruohao Guo", "Guangyao Li", "Yuxin Mao", "Mingfei Han", "Xiaojun Chang", "Meng Wang"], "title": "Mettle: Meta-Token Learning for Memory-Efficient Audio-Visual Adaptation", "comment": "Technical Report", "summary": "We present \\textbf{Met}a-\\textbf{T}oken \\textbf{Le}arning (Mettle), a simple\nand memory-efficient method for adapting large-scale pretrained transformer\nmodels to downstream audio-visual tasks. Instead of sequentially modifying the\noutput feature distribution of the transformer backbone, Mettle utilizes a\nlightweight \\textit{Layer-Centric Distillation (LCD)} module to distill in\nparallel the intact audio or visual features embedded by each transformer layer\ninto compact meta-tokens. This distillation process considers both pretrained\nknowledge preservation and task-specific adaptation. The obtained meta-tokens\ncan be directly applied to classification tasks, such as audio-visual event\nlocalization and audio-visual video parsing. To further support fine-grained\nsegmentation tasks, such as audio-visual segmentation, we introduce a\n\\textit{Meta-Token Injection (MTI)} module, which utilizes the audio and visual\nmeta-tokens distilled from the top transformer layer to guide feature\nadaptation in earlier layers. Extensive experiments on multiple audiovisual\nbenchmarks demonstrate that our method significantly reduces memory usage and\ntraining time while maintaining parameter efficiency and competitive accuracy.", "AI": {"tldr": "Mettle is a memory-efficient method for adapting pretrained transformers to audio-visual tasks using Layer-Centric Distillation (LCD) and Meta-Token Injection (MTI).", "motivation": "To efficiently adapt large-scale pretrained transformers for downstream audio-visual tasks while preserving pretrained knowledge and enabling task-specific adaptation.", "method": "Uses LCD to distill audio/visual features into meta-tokens in parallel and MTI to guide feature adaptation in earlier layers for fine-grained tasks.", "result": "Reduces memory usage and training time while maintaining parameter efficiency and competitive accuracy.", "conclusion": "Mettle offers a lightweight, efficient solution for adapting transformers to diverse audio-visual tasks."}}
{"id": "2506.24005", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.24005", "abs": "https://arxiv.org/abs/2506.24005", "authors": ["He Wang", "Xingyu Xu", "Yuejie Chi"], "title": "Provably Efficient and Agile Randomized Q-Learning", "comment": null, "summary": "While Bayesian-based exploration often demonstrates superior empirical\nperformance compared to bonus-based methods in model-based reinforcement\nlearning (RL), its theoretical understanding remains limited for model-free\nsettings. Existing provable algorithms either suffer from computational\nintractability or rely on stage-wise policy updates which reduce responsiveness\nand slow down the learning process. In this paper, we propose a novel variant\nof Q-learning algorithm, refereed to as RandomizedQ, which integrates\nsampling-based exploration with agile, step-wise, policy updates, for episodic\ntabular RL. We establish an $\\widetilde{O}(\\sqrt{H^5SAT})$ regret bound, where\n$S$ is the number of states, $A$ is the number of actions, $H$ is the episode\nlength, and $T$ is the total number of episodes. In addition, we present a\nlogarithmic regret bound under a mild positive sub-optimality condition on the\noptimal Q-function. Empirically, RandomizedQ exhibits outstanding performance\ncompared to existing Q-learning variants with both bonus-based and\nBayesian-based exploration on standard benchmarks.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.23275", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23275", "abs": "https://arxiv.org/abs/2506.23275", "authors": ["Chengyou Jia", "Xin Shen", "Zhuohang Dang", "Zhuohang Dang", "Changliang Xia", "Weijia Wu", "Xinyu Zhang", "Hangwei Qian", "Ivor W. Tsang", "Minnan Luo"], "title": "Why Settle for One? Text-to-ImageSet Generation and Evaluation", "comment": null, "summary": "Despite remarkable progress in Text-to-Image models, many real-world\napplications require generating coherent image sets with diverse consistency\nrequirements. Existing consistent methods often focus on a specific domain with\nspecific aspects of consistency, which significantly constrains their\ngeneralizability to broader applications. In this paper, we propose a more\nchallenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate\nsets of images that meet various consistency requirements based on user\ninstructions. To systematically study this problem, we first introduce\n$\\textbf{T2IS-Bench}$ with 596 diverse instructions across 26 subcategories,\nproviding comprehensive coverage for T2IS generation. Building on this, we\npropose $\\textbf{T2IS-Eval}$, an evaluation framework that transforms user\ninstructions into multifaceted assessment criteria and employs effective\nevaluators to adaptively assess consistency fulfillment between criteria and\ngenerated sets. Subsequently, we propose $\\textbf{AutoT2IS}$, a training-free\nframework that maximally leverages pretrained Diffusion Transformers'\nin-context capabilities to harmonize visual elements to satisfy both\nimage-level prompt alignment and set-level visual consistency. Extensive\nexperiments on T2IS-Bench reveal that diverse consistency challenges all\nexisting methods, while our AutoT2IS significantly outperforms current\ngeneralized and even specialized approaches. Our method also demonstrates the\nability to enable numerous underexplored real-world applications, confirming\nits substantial practical value. Visit our project in\nhttps://chengyou-jia.github.io/T2IS-Home.", "AI": {"tldr": "The paper introduces Text-to-ImageSet (T2IS) generation, a novel problem for creating coherent image sets with diverse consistency requirements. It proposes T2IS-Bench, T2IS-Eval, and AutoT2IS to address the challenge, demonstrating superior performance over existing methods.", "motivation": "Existing methods for consistent image generation are domain-specific, limiting generalizability. The paper aims to address this by tackling the broader T2IS problem.", "method": "The authors introduce T2IS-Bench for diverse instructions, T2IS-Eval for assessment, and AutoT2IS, a training-free framework leveraging Diffusion Transformers for consistency.", "result": "AutoT2IS outperforms existing methods on T2IS-Bench, handling diverse consistency challenges and enabling real-world applications.", "conclusion": "The proposed framework advances T2IS generation, offering practical value and outperforming specialized approaches."}}
{"id": "2506.24018", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.24018", "abs": "https://arxiv.org/abs/2506.24018", "authors": ["Veronica Lachi", "Francesco Ferrini", "Antonio Longa", "Bruno Lepri", "Andrea Passerini", "Manfred Jaeger"], "title": "Bridging Theory and Practice in Link Representation with Graph Neural Networks", "comment": null, "summary": "Graph Neural Networks (GNNs) are widely used to compute representations of\nnode pairs for downstream tasks such as link prediction. Yet, theoretical\nunderstanding of their expressive power has focused almost entirely on\ngraph-level representations. In this work, we shift the focus to links and\nprovide the first comprehensive study of GNN expressiveness in link\nrepresentation. We introduce a unifying framework, the $k_\\phi$-$k_\\rho$-$m$\nframework, that subsumes existing message-passing link models and enables\nformal expressiveness comparisons. Using this framework, we derive a hierarchy\nof state-of-the-art methods and offer theoretical tools to analyze future\narchitectures. To complement our analysis, we propose a synthetic evaluation\nprotocol comprising the first benchmark specifically designed to assess\nlink-level expressiveness. Finally, we ask: does expressiveness matter in\npractice? We use a graph symmetry metric that quantifies the difficulty of\ndistinguishing links and show that while expressive models may underperform on\nstandard benchmarks, they significantly outperform simpler ones as symmetry\nincreases, highlighting the need for dataset-aware model selection.", "AI": {"tldr": "The paper studies GNN expressiveness in link representation, introduces a unifying framework for comparison, and shows expressive models outperform simpler ones in high-symmetry scenarios.", "motivation": "To bridge the gap in theoretical understanding of GNNs for link-level tasks, focusing on expressiveness and practical impact.", "method": "Introduces the $k_\\phi$-$k_\\rho$-$m$ framework to compare link models, derives a hierarchy of methods, and proposes a synthetic benchmark for evaluation.", "result": "Expressive models underperform on standard benchmarks but excel in high-symmetry scenarios, emphasizing dataset-aware selection.", "conclusion": "The study provides tools for analyzing link-level GNN expressiveness and highlights the importance of model selection based on graph symmetry."}}
{"id": "2506.23282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23282", "abs": "https://arxiv.org/abs/2506.23282", "authors": ["Hanwen Zhang", "Congqi Cao", "Qinyi Lv", "Lingtong Min", "Yanning Zhang"], "title": "Autoregressive Denoising Score Matching is a Good Video Anomaly Detector", "comment": null, "summary": "Video anomaly detection (VAD) is an important computer vision problem. Thanks\nto the mode coverage capabilities of generative models, the likelihood-based\nparadigm is catching growing interest, as it can model normal distribution and\ndetect out-of-distribution anomalies. However, these likelihood-based methods\nare blind to the anomalies located in local modes near the learned\ndistribution. To handle these ``unseen\" anomalies, we dive into three gaps\nuniquely existing in VAD regarding scene, motion and appearance. Specifically,\nwe first build a noise-conditioned score transformer for denoising score\nmatching. Then, we introduce a scene-dependent and motion-aware score function\nby embedding the scene condition of input sequences into our model and\nassigning motion weights based on the difference between key frames of input\nsequences. Next, to solve the problem of blindness in principle, we integrate\nunaffected visual information via a novel autoregressive denoising score\nmatching mechanism for inference. Through autoregressively injecting\nintensifying Gaussian noise into the denoised data and estimating the\ncorresponding score function, we compare the denoised data with the original\ndata to get a difference and aggregate it with the score function for an\nenhanced appearance perception and accumulate the abnormal context. With all\nthree gaps considered, we can compute a more comprehensive anomaly indicator.\nExperiments on three popular VAD benchmarks demonstrate the state-of-the-art\nperformance of our method.", "AI": {"tldr": "The paper proposes a novel method for video anomaly detection (VAD) using a noise-conditioned score transformer and motion-aware scoring to address gaps in scene, motion, and appearance. It achieves state-of-the-art results.", "motivation": "Likelihood-based VAD methods fail to detect anomalies near learned distributions. The paper aims to address this by tackling gaps in scene, motion, and appearance.", "method": "Uses a noise-conditioned score transformer for denoising score matching, introduces scene-dependent and motion-aware scoring, and employs autoregressive denoising for enhanced anomaly detection.", "result": "Demonstrates state-of-the-art performance on three VAD benchmarks.", "conclusion": "The proposed method effectively addresses gaps in VAD, improving anomaly detection by considering scene, motion, and appearance."}}
{"id": "2506.24042", "categories": ["cs.LG", "cs.NA", "math.NA", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2506.24042", "abs": "https://arxiv.org/abs/2506.24042", "authors": ["Gen Li", "Yuchen Zhou", "Yuting Wei", "Yuxin Chen"], "title": "Faster Diffusion Models via Higher-Order Approximation", "comment": null, "summary": "In this paper, we explore provable acceleration of diffusion models without\nany additional retraining. Focusing on the task of approximating a target data\ndistribution in $\\mathbb{R}^d$ to within $\\varepsilon$ total-variation\ndistance, we propose a principled, training-free sampling algorithm that\nrequires only the order of\n  $$ d^{1+2/K} \\varepsilon^{-1/K} $$\n  score function evaluations (up to log factor) in the presence of accurate\nscores, where $K$ is an arbitrarily large fixed integer. This result applies to\na broad class of target data distributions, without the need for assumptions\nsuch as smoothness or log-concavity. Our theory is robust vis-a-vis inexact\nscore estimation, degrading gracefully as the score estimation error increases\n-- without demanding higher-order smoothness on the score estimates as assumed\nin previous work. The proposed algorithm draws insight from high-order ODE\nsolvers, leveraging high-order Lagrange interpolation and successive refinement\nto approximate the integral derived from the probability flow ODE.", "AI": {"tldr": "A training-free sampling algorithm for diffusion models accelerates approximation of target data distributions without retraining, using high-order ODE solvers and Lagrange interpolation.", "motivation": "To achieve provable acceleration in diffusion models without retraining, addressing limitations like smoothness or log-concavity assumptions.", "method": "Proposes a principled, training-free algorithm using high-order ODE solvers, Lagrange interpolation, and successive refinement for score function evaluations.", "result": "The algorithm requires only order of $d^{1+2/K} \\varepsilon^{-1/K}$ score evaluations, robust to inexact score estimation and applicable to broad data distributions.", "conclusion": "The method provides efficient, robust acceleration for diffusion models, eliminating the need for restrictive assumptions or retraining."}}
{"id": "2506.23283", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23283", "abs": "https://arxiv.org/abs/2506.23283", "authors": ["Yuhuan Yang", "Chaofan Ma", "Zhenjie Mao", "Jiangchao Yao", "Ya Zhang", "Yanfeng Wang"], "title": "MoMa: Modulating Mamba for Adapting Image Foundation Models to Video Recognition", "comment": "ICML 2025 paper", "summary": "Video understanding is a complex challenge that requires effective modeling\nof spatial-temporal dynamics. With the success of image foundation models\n(IFMs) in image understanding, recent approaches have explored\nparameter-efficient fine-tuning (PEFT) to adapt IFMs for video. However, most\nof these methods tend to process spatial and temporal information separately,\nwhich may fail to capture the full intricacy of video dynamics. In this paper,\nwe propose MoMa, an efficient adapter framework that achieves full\nspatial-temporal modeling by integrating Mamba's selective state space modeling\ninto IFMs. We propose a novel SeqMod operation to inject spatial-temporal\ninformation into pre-trained IFMs, without disrupting their original features.\nBy incorporating SeqMod into a Divide-and-Modulate architecture, MoMa enhances\nvideo understanding while maintaining computational efficiency. Extensive\nexperiments on multiple video benchmarks demonstrate the effectiveness of MoMa,\nachieving superior performance with reduced computational cost.", "AI": {"tldr": "MoMa is an efficient adapter framework for video understanding, integrating Mamba's selective state space modeling into image foundation models (IFMs) for full spatial-temporal modeling.", "motivation": "Existing methods for adapting IFMs to video often separate spatial and temporal information, failing to capture video dynamics fully.", "method": "Proposes MoMa with SeqMod operation to inject spatial-temporal information into IFMs without disrupting original features, using a Divide-and-Modulate architecture.", "result": "MoMa achieves superior performance on video benchmarks with reduced computational cost.", "conclusion": "MoMa effectively enhances video understanding by integrating spatial-temporal modeling efficiently."}}
{"id": "2506.24093", "categories": ["cs.LG", "cs.AI", "I.2.1; I.2.0; F.2.3"], "pdf": "https://arxiv.org/pdf/2506.24093", "abs": "https://arxiv.org/abs/2506.24093", "authors": ["Paul Wachter", "Lukas Niehaus", "Julius Sch\u00f6ning"], "title": "Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies", "comment": "21pages, 14 figures, 2 tables", "summary": "Synthetic data has emerged as a cost-effective alternative to real data for\ntraining artificial neural networks (ANN). However, the disparity between\nsynthetic and real data results in a domain gap. That gap leads to poor\nperformance and generalization of the trained ANN when applied to real-world\nscenarios. Several strategies have been developed to bridge this gap, which\ncombine synthetic and real data, known as mixed training using hybrid datasets.\nWhile these strategies have been shown to mitigate the domain gap, a systematic\nevaluation of their generalizability and robustness across various tasks and\narchitectures remains underexplored. To address this challenge, our study\ncomprehensively analyzes two widely used mixing strategies on three prevalent\narchitectures and three distinct hybrid datasets. From these datasets, we\nsample subsets with varying proportions of synthetic to real data to\ninvestigate the impact of synthetic and real components. The findings of this\npaper provide valuable insights into optimizing the use of synthetic data in\nthe training process of any ANN, contributing to enhancing robustness and\nefficacy.", "AI": {"tldr": "The paper evaluates mixed training strategies using synthetic and real data to bridge the domain gap in ANN training, analyzing their generalizability and robustness across tasks and architectures.", "motivation": "The disparity between synthetic and real data causes poor ANN performance in real-world scenarios, prompting the need to evaluate mixed training strategies systematically.", "method": "The study analyzes two mixing strategies on three architectures and three hybrid datasets, varying synthetic-to-real data proportions.", "result": "The findings offer insights into optimizing synthetic data use in ANN training to improve robustness and efficacy.", "conclusion": "The study contributes to better understanding and enhancing the effectiveness of synthetic data in ANN training."}}
{"id": "2506.23285", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23285", "abs": "https://arxiv.org/abs/2506.23285", "authors": ["Daqian Shi", "Xiaolei Diao", "Xu Chen", "C\u00e9dric M. John"], "title": "Competitive Distillation: A Simple Learning Strategy for Improving Visual Classification", "comment": "Accepted by ICCV 2025", "summary": "Deep Neural Networks (DNNs) have significantly advanced the field of computer\nvision. To improve DNN training process, knowledge distillation methods\ndemonstrate their effectiveness in accelerating network training by introducing\na fixed learning direction from the teacher network to student networks. In\nthis context, several distillation-based optimization strategies are proposed,\ne.g., deep mutual learning and self-distillation, as an attempt to achieve\ngeneric training performance enhancement through the cooperative training of\nmultiple networks. However, such strategies achieve limited improvements due to\nthe poor understanding of the impact of learning directions among networks\nacross different iterations. In this paper, we propose a novel competitive\ndistillation strategy that allows each network in a group to potentially act as\na teacher based on its performance, enhancing the overall learning performance.\nCompetitive distillation organizes a group of networks to perform a shared task\nand engage in competition, where competitive optimization is proposed to\nimprove the parameter updating process. We further introduce stochastic\nperturbation in competitive distillation, aiming to motivate networks to induce\nmutations to achieve better visual representations and global optimum. The\nexperimental results show that competitive distillation achieves promising\nperformance in diverse tasks and datasets.", "AI": {"tldr": "The paper proposes a competitive distillation strategy for DNN training, where networks dynamically act as teachers based on performance, improving learning efficiency and performance.", "motivation": "Current distillation methods like mutual learning and self-distillation have limited improvements due to unclear learning direction impacts across iterations.", "method": "Introduces competitive distillation, where networks compete and act as teachers based on performance, with stochastic perturbation to enhance representations.", "result": "Competitive distillation shows promising performance across diverse tasks and datasets.", "conclusion": "The strategy effectively enhances DNN training by leveraging competition and dynamic teacher roles."}}
{"id": "2506.24120", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.24120", "abs": "https://arxiv.org/abs/2506.24120", "authors": ["Yuqing Wang", "Shangding Gu"], "title": "Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime", "comment": null, "summary": "Data selection plays a crucial role in data-driven decision-making, including\nin large language models (LLMs), and is typically task-dependent. Properties\nsuch as data quality and diversity have been extensively studied and are known\nto enhance model performance. However, it remains unclear whether there exist\nother quantitative and general principles of data selection that can\nconsistently improve performance, especially for complex tasks with limited\nprior knowledge. In this paper, we demonstrate that selecting more uniformly\ndistributed data can improve training efficiency while enhancing performance.\nSpecifically, we establish that more uniform (less biased) distribution leads\nto a larger minimum pairwise distance between data points, denoted by\n$h_{\\min}$, and prove that a smaller $h_{\\min}$ can slow down the training\ndynamics of gradient descent (GD). Moreover, we theoretically show that the\napproximation error of neural networks decreases as $h_{\\min}$ increases. Our\nanalysis introduces a convergence framework for GD beyond the Neural Tangent\nKernel (NTK) regime, applicable to a broad class of architectures, including\ntransformers, without requiring Lipschitz smoothness. This framework further\nprovides theoretical justification for the use of residual connections and\nfunction compositions in deep neural architectures. In the end, we conduct\ncomprehensive experiments for supervised fine-tuning across various settings,\nincluding different optimization strategies, model sizes, and training\ndatasets. The results consistently demonstrate that selecting data by\nmaximizing pairwise distance significantly accelerates training and achieves\ncomparable or better performance in LLMs across diverse datasets. Code and\nDatasets are available at the link:\nhttps://github.com/SafeRL-Lab/data-uniformity.", "AI": {"tldr": "Selecting uniformly distributed data improves training efficiency and performance in LLMs by maximizing pairwise distance between data points.", "motivation": "To identify general principles of data selection that enhance performance, especially for complex tasks with limited prior knowledge.", "method": "Theoretical analysis shows uniform data distribution increases pairwise distance, improving GD dynamics and approximation error. Experiments validate this across various settings.", "result": "Uniform data selection accelerates training and achieves comparable or better performance in LLMs.", "conclusion": "Maximizing pairwise distance in data selection is a general principle that enhances training efficiency and model performance."}}
{"id": "2506.23292", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23292", "abs": "https://arxiv.org/abs/2506.23292", "authors": ["Changtao Miao", "Yi Zhang", "Weize Gao", "Man Luo", "Weiwei Feng", "Zhiya Tan", "Jianshu Li", "Ajian Liu", "Yunfeng Diao", "Qi Chu", "Tao Gong", "Zhe Li", "Weibin Yao", "Joey Tianyi Zhou"], "title": "DDL: A Dataset for Interpretable Deepfake Detection and Localization in Real-World Scenarios", "comment": "This paper is a preliminary version, with an extended and\n  comprehensive version currently under development", "summary": "Recent advances in AIGC have exacerbated the misuse of malicious deepfake\ncontent, making the development of reliable deepfake detection methods an\nessential means to address this challenge. Although existing deepfake detection\nmodels demonstrate outstanding performance in detection metrics, most methods\nonly provide simple binary classification results, lacking interpretability. In\ncritical domains such as law, interpretability is crucial for enhancing the\ncredibility and authority of decisions. Recent studies attempt to improve the\ninterpretability of classification results by providing spatial manipulation\nmasks or temporal forgery segments. However, the practical effectiveness of\nthese methods remains suboptimal due to limitations of the forgery data. Most\ncurrent deepfake datasets predominantly offer binary labels, only a few\ndatasets with localization annotations. However, they suffer from restricted\nforgery scenarios, limited diversity in deepfake types, and insufficient data\nscale, making them inadequate for complex real-world scenarios. To address this\npredicament, we construct a novel large-scale deepfake detection and\nlocalization ($\\textbf{DDL}$) dataset containing over $\\textbf{1.8M}$ forged\nsamples and encompassing up to $\\textbf{75}$ distinct deepfake methods. The DDL\ndesign incorporates four key innovations: (1) $\\textbf{Diverse Forgery\nScenarios}$, (2) $\\textbf{Comprehensive Deepfake Methods}$, (3) $\\textbf{Varied\nManipulation Modes}$, and (4) $\\textbf{Fine-grained Forgery Annotations}$.\nThrough these improvements, our DDL not only provides a more challenging\nbenchmark for complex real-world forgeries, but also offers crucial support for\nbuilding next-generation deepfake detection, localization, and interpretability\nmethods. The DDL dataset project page is on\nhttps://deepfake-workshop-ijcai2025.github.io/main/index.html.", "AI": {"tldr": "A new large-scale deepfake detection and localization dataset (DDL) is introduced to address the lack of interpretability and diversity in existing methods, featuring 1.8M samples and 75 deepfake methods.", "motivation": "The misuse of deepfake content necessitates reliable detection methods with interpretability, especially in critical domains like law, where current datasets are limited in diversity and scale.", "method": "The DDL dataset is constructed with diverse forgery scenarios, comprehensive deepfake methods, varied manipulation modes, and fine-grained annotations.", "result": "The DDL dataset provides a challenging benchmark for real-world forgeries and supports next-generation detection and interpretability methods.", "conclusion": "The DDL dataset addresses limitations of existing datasets, enhancing deepfake detection and interpretability for complex real-world scenarios."}}
{"id": "2506.24124", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24124", "abs": "https://arxiv.org/abs/2506.24124", "authors": ["Dong Sixun", "Fan Wei", "Teresa Wu", "Fu Yanjie"], "title": "Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives", "comment": "Code: https://github.com/Ironieser/TimesCLIP", "summary": "Time series forecasting traditionally relies on unimodal numerical inputs,\nwhich often struggle to capture high-level semantic patterns due to their dense\nand unstructured nature. While recent approaches have explored representing\ntime series as text using large language models (LLMs), these methods remain\nlimited by the discrete nature of token sequences and lack the perceptual\nintuition humans typically apply, such as interpreting visual patterns. In this\npaper, we propose a multimodal contrastive learning framework that transforms\nraw time series into structured visual and textual perspectives. Rather than\nusing natural language or real-world images, we construct both modalities\ndirectly from numerical sequences. We then align these views in a shared\nsemantic space via contrastive learning, enabling the model to capture richer\nand more complementary representations. Furthermore, we introduce a variate\nselection module that leverages the aligned representations to identify the\nmost informative variables for multivariate forecasting. Extensive experiments\non fifteen short-term and six long-term forecasting benchmarks demonstrate that\nour approach consistently outperforms strong unimodal and cross-modal\nbaselines, highlighting the effectiveness of multimodal alignment in enhancing\ntime series forecasting. Code is available at:\nhttps://github.com/Ironieser/TimesCLIP.", "AI": {"tldr": "A multimodal contrastive learning framework enhances time series forecasting by aligning visual and textual representations derived from numerical sequences, outperforming unimodal and cross-modal baselines.", "motivation": "Traditional unimodal numerical inputs for time series forecasting fail to capture high-level semantic patterns, while text-based methods lack perceptual intuition. This paper addresses these limitations by proposing a multimodal approach.", "method": "The framework transforms raw time series into structured visual and textual perspectives, aligns them via contrastive learning, and uses a variate selection module for informative variable identification.", "result": "The approach outperforms unimodal and cross-modal baselines on fifteen short-term and six long-term forecasting benchmarks.", "conclusion": "Multimodal alignment significantly enhances time series forecasting, demonstrating the effectiveness of the proposed framework."}}
{"id": "2506.23295", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23295", "abs": "https://arxiv.org/abs/2506.23295", "authors": ["Xiang Xu"], "title": "DiffFit: Disentangled Garment Warping and Texture Refinement for Virtual Try-On", "comment": null, "summary": "Virtual try-on (VTON) aims to synthesize realistic images of a person wearing\na target garment, with broad applications in e-commerce and digital fashion.\nWhile recent advances in latent diffusion models have substantially improved\nvisual quality, existing approaches still struggle with preserving fine-grained\ngarment details, achieving precise garment-body alignment, maintaining\ninference efficiency, and generalizing to diverse poses and clothing styles. To\naddress these challenges, we propose DiffFit, a novel two-stage latent\ndiffusion framework for high-fidelity virtual try-on. DiffFit adopts a\nprogressive generation strategy: the first stage performs geometry-aware\ngarment warping, aligning the garment with the target body through fine-grained\ndeformation and pose adaptation. The second stage refines texture fidelity via\na cross-modal conditional diffusion model that integrates the warped garment,\nthe original garment appearance, and the target person image for high-quality\nrendering. By decoupling geometric alignment and appearance refinement, DiffFit\neffectively reduces task complexity and enhances both generation stability and\nvisual realism. It excels in preserving garment-specific attributes such as\ntextures, wrinkles, and lighting, while ensuring accurate alignment with the\nhuman body. Extensive experiments on large-scale VTON benchmarks demonstrate\nthat DiffFit achieves superior performance over existing state-of-the-art\nmethods in both quantitative metrics and perceptual evaluations.", "AI": {"tldr": "DiffFit is a two-stage latent diffusion framework for high-fidelity virtual try-on, addressing garment detail preservation, alignment, and efficiency.", "motivation": "Existing VTON methods struggle with preserving garment details, alignment, efficiency, and generalization to diverse poses/styles.", "method": "DiffFit uses a two-stage approach: geometry-aware garment warping followed by texture refinement via cross-modal conditional diffusion.", "result": "DiffFit outperforms state-of-the-art methods in preserving garment attributes and alignment, validated by benchmarks.", "conclusion": "DiffFit effectively decouples geometric alignment and appearance refinement, enhancing realism and stability in virtual try-on."}}
{"id": "2506.23308", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23308", "abs": "https://arxiv.org/abs/2506.23308", "authors": ["Yiming Huang", "Long Bai", "Beilei Cui", "Yanheng Li", "Tong Chen", "Jie Wang", "Jinlin Wu", "Zhen Lei", "Hongbin Liu", "Hongliang Ren"], "title": "Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting", "comment": "MICCAI 2025. Project Page:\n  https://lastbasket.github.io/MICCAI-2025-Endo-4DGX/", "summary": "Accurate reconstruction of soft tissue is crucial for advancing automation in\nimage-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS)\ntechniques and their variants, 4DGS, achieve high-quality renderings of dynamic\nsurgical scenes in real-time. However, 3D-GS-based methods still struggle in\nscenarios with varying illumination, such as low light and over-exposure.\nTraining 3D-GS in such extreme light conditions leads to severe optimization\nproblems and devastating rendering quality. To address these challenges, we\npresent Endo-4DGX, a novel reconstruction method with illumination-adaptive\nGaussian Splatting designed specifically for endoscopic scenes with uneven\nlighting. By incorporating illumination embeddings, our method effectively\nmodels view-dependent brightness variations. We introduce a region-aware\nenhancement module to model the sub-area lightness at the Gaussian level and a\nspatial-aware adjustment module to learn the view-consistent brightness\nadjustment. With the illumination adaptive design, Endo-4DGX achieves superior\nrendering performance under both low-light and over-exposure conditions while\nmaintaining geometric accuracy. Additionally, we employ an exposure control\nloss to restore the appearance from adverse exposure to the normal level for\nillumination-adaptive optimization. Experimental results demonstrate that\nEndo-4DGX significantly outperforms combinations of state-of-the-art\nreconstruction and restoration methods in challenging lighting environments,\nunderscoring its potential to advance robot-assisted surgical applications. Our\ncode is available at https://github.com/lastbasket/Endo-4DGX.", "AI": {"tldr": "Endo-4DGX improves 3D Gaussian Splatting for endoscopic scenes by addressing illumination challenges, achieving better rendering under low-light and over-exposure conditions.", "motivation": "Accurate soft tissue reconstruction in robotic surgery is hindered by varying illumination, which degrades 3D-GS performance.", "method": "Endo-4DGX uses illumination embeddings, a region-aware enhancement module, and a spatial-aware adjustment module to adapt to lighting variations.", "result": "The method outperforms state-of-the-art techniques in challenging lighting, maintaining geometric accuracy.", "conclusion": "Endo-4DGX advances robotic surgery by enabling reliable reconstruction under uneven lighting."}}
{"id": "2506.23323", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23323", "abs": "https://arxiv.org/abs/2506.23323", "authors": ["Quang-Huy Che", "Vinh-Tiep Nguyen"], "title": "FastSeg: Efficient Training-Free Open-Vocabulary Segmentation via Hierarchical Attention Refinement Method", "comment": null, "summary": "Open-vocabulary semantic segmentation (OVSS) aims to segment objects from\narbitrary text categories without requiring densely annotated datasets.\nAlthough contrastive learning based models enable zero-shot segmentation, they\noften lose fine spatial precision at pixel level, due to global representation\nbias. In contrast, diffusion-based models naturally encode fine-grained spatial\nfeatures via attention mechanisms that capture both global context and local\ndetails. However, they often face challenges in balancing the number of\niterations with the quality of the segmentation. In this work, we propose\nFastSeg, a novel and efficient training-free framework with only (1+1)-step of\nreverse process of a pretrained diffusion model (e.g., Stable Diffusion).\nMoreover, instead of running multiple times for different classes, FastSeg\nperforms segmentation for all classes at once. To further enhance the\nsegmentation quality, FastSeg introduces three key components: (i) a\ndual-prompt mechanism for discriminative, class-aware attention extraction,\n(ii) a Hierarchical Attention Refinement Method (HARD) that enhances fused\ncross-attention using scale-aligned selfattention maps, and (iii) a Test-Time\nFlipping (TTF) scheme designed to improve spatial consistency. Extensive\nexperiments show that FastSeg achieves state-of-the-art training-free\nperformance, obtaining 43.8% average mIoU across PASCAL VOC, PASCAL Context,\nand COCO Object benchmarks while maintaining superior inference efficiency. Our\nresults demonstrate that FastSeg provides a strong foundation for\nextendability, bridging the gap between segmentation quality and inference\nefficiency.", "AI": {"tldr": "FastSeg is a training-free framework for open-vocabulary semantic segmentation, using a pretrained diffusion model with minimal steps, achieving high efficiency and state-of-the-art performance.", "motivation": "Existing methods lose spatial precision (contrastive learning) or struggle with iteration-quality balance (diffusion models). FastSeg aims to bridge this gap.", "method": "Uses (1+1)-step reverse process of a pretrained diffusion model, dual-prompt mechanism, HARD for attention refinement, and TTF for spatial consistency.", "result": "Achieves 43.8% mIoU on PASCAL VOC, PASCAL Context, and COCO Object benchmarks with superior efficiency.", "conclusion": "FastSeg balances segmentation quality and efficiency, offering a strong foundation for future extensions."}}
{"id": "2506.23329", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23329", "abs": "https://arxiv.org/abs/2506.23329", "authors": ["Parker Liu", "Chenxin Li", "Zhengxin Li", "Yipeng Wu", "Wuyang Li", "Zhiqin Yang", "Zhenyuan Zhang", "Yunlong Lin", "Sirui Han", "Brandon Y. Feng"], "title": "IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering", "comment": "Project Page: https://ir3d-bench.github.io/", "summary": "Vision-language models (VLMs) excel at descriptive tasks, but whether they\ntruly understand scenes from visual observations remains uncertain. We\nintroduce IR3D-Bench, a benchmark challenging VLMs to demonstrate understanding\nthrough active creation rather than passive recognition. Grounded in the\nanalysis-by-synthesis paradigm, IR3D-Bench tasks Vision-Language Agents (VLAs)\nwith actively using programming and rendering tools to recreate the underlying\n3D structure of an input image, achieving agentic inverse rendering through\ntool use. This \"understanding-by-creating\" approach probes the tool-using\ngenerative capacity of VLAs, moving beyond the descriptive or conversational\ncapacity measured by traditional scene understanding benchmarks. We provide a\ncomprehensive suite of metrics to evaluate geometric accuracy, spatial\nrelations, appearance attributes, and overall plausibility. Initial experiments\non agentic inverse rendering powered by various state-of-the-art VLMs highlight\ncurrent limitations, particularly in visual precision rather than basic tool\nusage. IR3D-Bench, including data and evaluation protocols, is released to\nfacilitate systematic study and development of tool-using VLAs towards genuine\nscene understanding by creating.", "AI": {"tldr": "IR3D-Bench is a benchmark challenging VLMs to demonstrate scene understanding by actively recreating 3D structures from images using tools, moving beyond passive recognition.", "motivation": "To assess if VLMs truly understand scenes by testing their ability to actively create rather than passively describe.", "method": "VLAs use programming and rendering tools to recreate 3D structures from images, following an analysis-by-synthesis paradigm.", "result": "Initial experiments reveal limitations in visual precision, not tool usage, among state-of-the-art VLMs.", "conclusion": "IR3D-Bench enables systematic study of tool-using VLAs for genuine scene understanding through creation."}}
{"id": "2506.23347", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23347", "abs": "https://arxiv.org/abs/2506.23347", "authors": ["Yi Liu", "Shengqian Li", "Zuzeng Lin", "Feng Wang", "Si Liu"], "title": "CycleVAR: Repurposing Autoregressive Model for Unsupervised One-Step Image Translation", "comment": null, "summary": "The current conditional autoregressive image generation methods have shown\npromising results, yet their potential remains largely unexplored in the\npractical unsupervised image translation domain, which operates without\nexplicit cross-domain correspondences. A critical limitation stems from the\ndiscrete quantization inherent in traditional Vector Quantization-based\nframeworks, which disrupts gradient flow between the Variational Autoencoder\ndecoder and causal Transformer, impeding end-to-end optimization during\nadversarial training in image space. To tackle this issue, we propose using\nSoftmax Relaxed Quantization, a novel approach that reformulates codebook\nselection as a continuous probability mixing process via Softmax, thereby\npreserving gradient propagation. Building upon this differentiable foundation,\nwe introduce CycleVAR, which reformulates image-to-image translation as\nimage-conditional visual autoregressive generation by injecting multi-scale\nsource image tokens as contextual prompts, analogous to prefix-based\nconditioning in language models. CycleVAR exploits two modes to generate the\ntarget image tokens, including (1) serial multi-step generation, enabling\niterative refinement across scales, and (2) parallel one-step generation\nsynthesizing all resolution outputs in a single forward pass. Experimental\nfindings indicate that the parallel one-step generation mode attains superior\ntranslation quality with quicker inference speed than the serial multi-step\nmode in unsupervised scenarios. Furthermore, both quantitative and qualitative\nresults indicate that CycleVAR surpasses previous state-of-the-art unsupervised\nimage translation models, \\textit{e}.\\textit{g}., CycleGAN-Turbo.", "AI": {"tldr": "CycleVAR introduces Softmax Relaxed Quantization and a dual-mode autoregressive approach for unsupervised image translation, outperforming existing methods like CycleGAN-Turbo.", "motivation": "Addressing the gradient flow disruption in traditional Vector Quantization-based frameworks for unsupervised image translation.", "method": "Proposes Softmax Relaxed Quantization for continuous gradient flow and CycleVAR, which uses multi-scale source image tokens for autoregressive generation in serial or parallel modes.", "result": "Parallel one-step generation achieves better translation quality and faster inference than serial mode, surpassing state-of-the-art models.", "conclusion": "CycleVAR effectively improves unsupervised image translation by preserving gradients and leveraging autoregressive generation."}}
{"id": "2506.23352", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23352", "abs": "https://arxiv.org/abs/2506.23352", "authors": ["Shunsuke Yasuki", "Taiki Miyanishi", "Nakamasa Inoue", "Shuhei Kurita", "Koya Sakamoto", "Daichi Azuma", "Masato Taki", "Yutaka Matsuo"], "title": "GeoProg3D: Compositional Visual Reasoning for City-Scale 3D Language Fields", "comment": "Accepted by ICCV 2025", "summary": "The advancement of 3D language fields has enabled intuitive interactions with\n3D scenes via natural language. However, existing approaches are typically\nlimited to small-scale environments, lacking the scalability and compositional\nreasoning capabilities necessary for large, complex urban settings. To overcome\nthese limitations, we propose GeoProg3D, a visual programming framework that\nenables natural language-driven interactions with city-scale high-fidelity 3D\nscenes. GeoProg3D consists of two key components: (i) a Geography-aware\nCity-scale 3D Language Field (GCLF) that leverages a memory-efficient\nhierarchical 3D model to handle large-scale data, integrated with geographic\ninformation for efficiently filtering vast urban spaces using directional cues,\ndistance measurements, elevation data, and landmark references; and (ii)\nGeographical Vision APIs (GV-APIs), specialized geographic vision tools such as\narea segmentation and object detection. Our framework employs large language\nmodels (LLMs) as reasoning engines to dynamically combine GV-APIs and operate\nGCLF, effectively supporting diverse geographic vision tasks. To assess\nperformance in city-scale reasoning, we introduce GeoEval3D, a comprehensive\nbenchmark dataset containing 952 query-answer pairs across five challenging\ntasks: grounding, spatial reasoning, comparison, counting, and measurement.\nExperiments demonstrate that GeoProg3D significantly outperforms existing 3D\nlanguage fields and vision-language models across multiple tasks. To our\nknowledge, GeoProg3D is the first framework enabling compositional geographic\nreasoning in high-fidelity city-scale 3D environments via natural language. The\ncode is available at https://snskysk.github.io/GeoProg3D/.", "AI": {"tldr": "GeoProg3D is a visual programming framework for natural language-driven interactions with city-scale 3D scenes, combining geography-aware 3D language fields and specialized vision APIs, outperforming existing methods.", "motivation": "Existing 3D language approaches lack scalability and compositional reasoning for large urban settings.", "method": "GeoProg3D uses a Geography-aware City-scale 3D Language Field (GCLF) and Geographical Vision APIs (GV-APIs), powered by LLMs for dynamic task execution.", "result": "GeoProg3D outperforms existing methods on the GeoEval3D benchmark across five tasks.", "conclusion": "GeoProg3D is the first framework enabling compositional geographic reasoning in city-scale 3D environments via natural language."}}
{"id": "2506.23353", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.23353", "abs": "https://arxiv.org/abs/2506.23353", "authors": ["Siyuan Chai", "Xiaodong Guo", "Tong Liu"], "title": "Layer Decomposition and Morphological Reconstruction for Task-Oriented Infrared Image Enhancement", "comment": null, "summary": "Infrared image helps improve the perception capabilities of autonomous\ndriving in complex weather conditions such as fog, rain, and low light.\nHowever, infrared image often suffers from low contrast, especially in\nnon-heat-emitting targets like bicycles, which significantly affects the\nperformance of downstream high-level vision tasks. Furthermore, achieving\ncontrast enhancement without amplifying noise and losing important information\nremains a challenge. To address these challenges, we propose a task-oriented\ninfrared image enhancement method. Our approach consists of two key components:\nlayer decomposition and saliency information extraction. First, we design an\nlayer decomposition method for infrared images, which enhances scene details\nwhile preserving dark region features, providing more features for subsequent\nsaliency information extraction. Then, we propose a morphological\nreconstruction-based saliency extraction method that effectively extracts and\nenhances target information without amplifying noise. Our method improves the\nimage quality for object detection and semantic segmentation tasks. Extensive\nexperiments demonstrate that our approach outperforms state-of-the-art methods.", "AI": {"tldr": "A task-oriented infrared image enhancement method improves contrast for autonomous driving by decomposing layers and extracting saliency information, outperforming existing methods.", "motivation": "Infrared images enhance perception in complex weather but suffer from low contrast, especially for non-heat-emitting targets, impacting downstream vision tasks.", "method": "Proposes layer decomposition to preserve dark region features and morphological reconstruction-based saliency extraction to enhance targets without noise amplification.", "result": "Improves image quality for object detection and semantic segmentation, outperforming state-of-the-art methods.", "conclusion": "The method effectively addresses contrast enhancement challenges in infrared images, benefiting high-level vision tasks."}}
{"id": "2506.23361", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23361", "abs": "https://arxiv.org/abs/2506.23361", "authors": ["Yuanhao Cai", "He Zhang", "Xi Chen", "Jinbo Xing", "Yiwei Hu", "Yuqian Zhou", "Kai Zhang", "Zhifei Zhang", "Soo Ye Kim", "Tianyu Wang", "Yulun Zhang", "Xiaokang Yang", "Zhe Lin", "Alan Yuille"], "title": "OmniVCus: Feedforward Subject-driven Video Customization with Multimodal Control Conditions", "comment": "A data construction pipeline and a diffusion Transformer framework\n  for controllable subject-driven video customization", "summary": "Existing feedforward subject-driven video customization methods mainly study\nsingle-subject scenarios due to the difficulty of constructing multi-subject\ntraining data pairs. Another challenging problem that how to use the signals\nsuch as depth, mask, camera, and text prompts to control and edit the subject\nin the customized video is still less explored. In this paper, we first propose\na data construction pipeline, VideoCus-Factory, to produce training data pairs\nfor multi-subject customization from raw videos without labels and control\nsignals such as depth-to-video and mask-to-video pairs. Based on our\nconstructed data, we develop an Image-Video Transfer Mixed (IVTM) training with\nimage editing data to enable instructive editing for the subject in the\ncustomized video. Then we propose a diffusion Transformer framework, OmniVCus,\nwith two embedding mechanisms, Lottery Embedding (LE) and Temporally Aligned\nEmbedding (TAE). LE enables inference with more subjects by using the training\nsubjects to activate more frame embeddings. TAE encourages the generation\nprocess to extract guidance from temporally aligned control signals by\nassigning the same frame embeddings to the control and noise tokens.\nExperiments demonstrate that our method significantly surpasses\nstate-of-the-art methods in both quantitative and qualitative evaluations.\nVideo demos are at our project page:\nhttps://caiyuanhao1998.github.io/project/OmniVCus/. Our code will be released\nat https://github.com/caiyuanhao1998/Open-OmniVCus", "AI": {"tldr": "The paper introduces VideoCus-Factory for multi-subject video customization data construction and OmniVCus, a diffusion Transformer framework with Lottery Embedding and Temporally Aligned Embedding for improved video editing.", "motivation": "Addressing the lack of multi-subject training data and unexplored control signals (depth, mask, camera, text prompts) in video customization.", "method": "Proposes VideoCus-Factory for data construction and OmniVCus framework with Lottery Embedding (LE) and Temporally Aligned Embedding (TAE) for multi-subject inference and control signal utilization.", "result": "Outperforms state-of-the-art methods in quantitative and qualitative evaluations.", "conclusion": "The approach enables effective multi-subject video customization and control signal integration, with promising results."}}
{"id": "2506.23382", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23382", "abs": "https://arxiv.org/abs/2506.23382", "authors": ["Vikram Rangarajan", "Shishira Maiya", "Max Ehrlich", "Abhinav Shrivastava"], "title": "SIEDD: Shared-Implicit Encoder with Discrete Decoders", "comment": "Project page at https://vikramrangarajan.github.io/SIEDD . Project\n  code at https://github.com/VikramRangarajan/SIEDD", "summary": "Implicit Neural Representations (INRs) offer exceptional fidelity for video\ncompression by learning per-video optimized functions, but their adoption is\ncrippled by impractically slow encoding times. Existing attempts to accelerate\nINR encoding often sacrifice reconstruction quality or crucial coordinate-level\ncontrol essential for adaptive streaming and transcoding. We introduce SIEDD\n(Shared-Implicit Encoder with Discrete Decoders), a novel architecture that\nfundamentally accelerates INR encoding without these compromises. SIEDD first\nrapidly trains a shared, coordinate-based encoder on sparse anchor frames to\nefficiently capture global, low-frequency video features. This encoder is then\nfrozen, enabling massively parallel training of lightweight, discrete decoders\nfor individual frame groups, further expedited by aggressive coordinate-space\nsampling. This synergistic design delivers a remarkable 20-30X encoding\nspeed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while\nmaintaining competitive reconstruction quality and compression ratios.\nCritically, SIEDD retains full coordinate-based control, enabling continuous\nresolution decoding and eliminating costly transcoding. Our approach\nsignificantly advances the practicality of high-fidelity neural video\ncompression, demonstrating a scalable and efficient path towards real-world\ndeployment. Our codebase is available at\nhttps://github.com/VikramRangarajan/SIEDD .", "AI": {"tldr": "SIEDD accelerates INR video encoding by 20-30X without sacrificing quality or control, using a shared encoder and discrete decoders.", "motivation": "Slow encoding times of INRs hinder adoption despite their high fidelity for video compression.", "method": "SIEDD uses a shared encoder for global features and lightweight decoders for frame groups, with aggressive sampling for speed.", "result": "Achieves 20-30X faster encoding than state-of-the-art INR codecs while maintaining quality and compression ratios.", "conclusion": "SIEDD advances neural video compression practicality, enabling real-world deployment with scalable efficiency."}}
{"id": "2506.23414", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23414", "abs": "https://arxiv.org/abs/2506.23414", "authors": ["Ming-Zher Poh", "Jonathan Wang", "Jonathan Hsu", "Lawrence Cai", "Eric Teasley", "James A. Taylor", "Jameson K. Rogers", "Anupam Pathak", "Shwetak Patel"], "title": "A High-Throughput Platform to Bench Test Smartphone-Based Heart Rate Measurements Derived From Video", "comment": null, "summary": "Smartphone-based heart rate (HR) monitoring apps using finger-over-camera\nphotoplethysmography (PPG) face significant challenges in performance\nevaluation and device compatibility due to device variability and\nfragmentation. Manual testing is impractical, and standardized methods are\nlacking. This paper presents a novel, high-throughput bench-testing platform to\naddress this critical need. We designed a system comprising a test rig capable\nof holding 12 smartphones for parallel testing, a method for generating\nsynthetic PPG test videos with controllable HR and signal quality, and a host\nmachine for coordinating video playback and data logging. The system achieved a\nmean absolute percentage error (MAPE) of 0.11% +/- 0.001% between input and\nmeasured HR, and a correlation coefficient of 0.92 +/- 0.008 between input and\nmeasured PPG signals using a clinically-validated smartphone-based HR app.\nBench-testing results of 20 different smartphone models correctly classified\nall the devices as meeting the ANSI/CTA accuracy standards for HR monitors\n(MAPE <10%) when compared to a prospective clinical study with 80 participants,\ndemonstrating high positive predictive value. This platform offers a scalable\nsolution for pre-deployment testing of smartphone HR apps to improve app\nperformance, ensure device compatibility, and advance the field of mobile\nhealth.", "AI": {"tldr": "A high-throughput bench-testing platform for smartphone-based heart rate (HR) apps addresses device variability and lack of standardized testing. It uses synthetic PPG videos and parallel testing, achieving high accuracy and device compatibility.", "motivation": "Challenges in evaluating smartphone HR apps due to device variability and lack of standardized testing methods.", "method": "A system with a test rig for 12 smartphones, synthetic PPG video generation, and a host machine for coordination and data logging.", "result": "Achieved 0.11% MAPE in HR accuracy and 0.92 correlation for PPG signals, with all tested devices meeting ANSI/CTA standards.", "conclusion": "The platform provides scalable pre-deployment testing to enhance app performance and device compatibility in mobile health."}}
{"id": "2506.23418", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23418", "abs": "https://arxiv.org/abs/2506.23418", "authors": ["Parham Rezaei", "Arash Marioriyad", "Mahdieh Soleymani Baghshah", "Mohammad Hossein Rohban"], "title": "Why Settle for Mid: A Probabilistic Viewpoint to Spatial Relationship Alignment in Text-to-image Models", "comment": "12 main pages, 18 figures, and 16 tables", "summary": "Despite the ability of text-to-image models to generate high-quality,\nrealistic, and diverse images, they face challenges in compositional\ngeneration, often struggling to accurately represent details specified in the\ninput prompt. A prevalent issue in compositional generation is the misalignment\nof spatial relationships, as models often fail to faithfully generate images\nthat reflect the spatial configurations specified between objects in the input\nprompts. To address this challenge, we propose a novel probabilistic framework\nfor modeling the relative spatial positioning of objects in a scene, leveraging\nthe concept of Probability of Superiority (PoS). Building on this insight, we\nmake two key contributions. First, we introduce a novel evaluation metric,\nPoS-based Evaluation (PSE), designed to assess the alignment of 2D and 3D\nspatial relationships between text and image, with improved adherence to human\njudgment. Second, we propose PoS-based Generation (PSG), an inference-time\nmethod that improves the alignment of 2D and 3D spatial relationships in T2I\nmodels without requiring fine-tuning. PSG employs a Part-of-Speech PoS-based\nreward function that can be utilized in two distinct ways: (1) as a\ngradient-based guidance mechanism applied to the cross-attention maps during\nthe denoising steps, or (2) as a search-based strategy that evaluates a set of\ninitial noise vectors to select the best one. Extensive experiments demonstrate\nthat the PSE metric exhibits stronger alignment with human judgment compared to\ntraditional center-based metrics, providing a more nuanced and reliable measure\nof complex spatial relationship accuracy in text-image alignment. Furthermore,\nPSG significantly enhances the ability of text-to-image models to generate\nimages with specified spatial configurations, outperforming state-of-the-art\nmethods across multiple evaluation metrics and benchmarks.", "AI": {"tldr": "The paper addresses spatial misalignment in text-to-image models by introducing a probabilistic framework (PoS) and two contributions: PSE for evaluation and PSG for generation, improving spatial accuracy.", "motivation": "Text-to-image models struggle with accurately representing spatial relationships in input prompts, leading to misaligned compositions.", "method": "Proposes PoS-based Evaluation (PSE) for assessing spatial alignment and PoS-based Generation (PSG) for improving generation without fine-tuning, using gradient-based or search-based strategies.", "result": "PSE aligns better with human judgment than traditional metrics, and PSG outperforms state-of-the-art methods in generating spatially accurate images.", "conclusion": "The proposed framework and methods significantly enhance spatial relationship accuracy in text-to-image generation, validated by experiments."}}
{"id": "2506.23426", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23426", "abs": "https://arxiv.org/abs/2506.23426", "authors": ["Menna Taha", "Aya Ahmed", "Mohammed Karmoose", "Yasser Gadallah"], "title": "Detecting What Matters: A Novel Approach for Out-of-Distribution 3D Object Detection in Autonomous Vehicles", "comment": null, "summary": "Autonomous vehicles (AVs) use object detection models to recognize their\nsurroundings and make driving decisions accordingly. Conventional object\ndetection approaches classify objects into known classes, which limits the AV's\nability to detect and appropriately respond to Out-of-Distribution (OOD)\nobjects. This problem is a significant safety concern since the AV may fail to\ndetect objects or misclassify them, which can potentially lead to hazardous\nsituations such as accidents. Consequently, we propose a novel object detection\napproach that shifts the emphasis from conventional class-based classification\nto object harmfulness determination. Instead of object detection by their\nspecific class, our method identifies them as either 'harmful' or 'harmless'\nbased on whether they pose a danger to the AV. This is done based on the object\nposition relative to the AV and its trajectory. With this metric, our model can\neffectively detect previously unseen objects to enable the AV to make safer\nreal-time decisions. Our results demonstrate that the proposed model\neffectively detects OOD objects, evaluates their harmfulness, and classifies\nthem accordingly, thus enhancing the AV decision-making effectiveness in\ndynamic environments.", "AI": {"tldr": "The paper proposes a novel object detection method for AVs that focuses on classifying objects as 'harmful' or 'harmless' instead of traditional class-based detection, improving safety for OOD objects.", "motivation": "Current AV object detection methods struggle with OOD objects, posing safety risks due to misclassification or failure to detect.", "method": "The approach evaluates object harmfulness based on position and trajectory relative to the AV, bypassing class-based classification.", "result": "The model effectively detects OOD objects, assesses their danger, and enhances AV decision-making in dynamic environments.", "conclusion": "The proposed method improves AV safety by focusing on harmfulness rather than class, addressing OOD challenges."}}
{"id": "2506.23434", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23434", "abs": "https://arxiv.org/abs/2506.23434", "authors": ["Tianran Liu", "Shengwen Zhao", "Nicholas Rhinehart"], "title": "Towards foundational LiDAR world models with efficient latent flow matching", "comment": "25 pages, 13 figures", "summary": "LiDAR-based world models offer more structured and geometry-aware\nrepresentations than their image-based counterparts. However, existing LiDAR\nworld models are narrowly trained; each model excels only in the domain for\nwhich it was built. Can we develop LiDAR world models that exhibit strong\ntransferability across multiple domains? We conduct the first systematic domain\ntransfer study across three demanding scenarios: (i) outdoor to indoor\ngeneralization, (ii) sparse-beam \\& dense-beam adaptation, and (iii)\nnon-semantic to semantic transfer. Given different amounts of fine-tuning data,\nour experiments show that a single pre-trained model can achieve up to 11%\nabsolute improvement (83\\% relative) over training from scratch and outperforms\ntraining from scratch in 30/36 of our comparisons. This transferability of\ndynamic learning significantly reduces the reliance on manually annotated data\nfor semantic occupancy forecasting: our method exceed the previous semantic\noccupancy forecasting models with only 5% of the labeled training data required\nby prior models. We also observed inefficiencies of current LiDAR world models,\nmainly through their under-compression of LiDAR data and inefficient training\nobjectives. To address this, we propose a latent conditional flow matching\n(CFM)-based frameworks that achieves state-of-the-art reconstruction accuracy\nusing only half the training data and a compression ratio 6 times higher than\nthat of prior methods. Our model achieves SOTA performance on\nfuture-trajectory-conditioned semantic occupancy forecasting while being 23x\nmore computationally efficient (a 28x FPS speedup); and achieves SOTA\nperformance on semantic occupancy forecasting while being 2x more\ncomputationally efficient (a 1.1x FPS speedup).", "AI": {"tldr": "A study on improving LiDAR world models' transferability across domains, achieving significant performance gains with less data and higher efficiency.", "motivation": "Existing LiDAR world models lack transferability across domains, requiring domain-specific training. This work aims to develop models that generalize better.", "method": "Conducted domain transfer studies across three scenarios, proposed a latent conditional flow matching (CFM)-based framework for efficient training and compression.", "result": "Achieved up to 11% absolute improvement over training from scratch, reduced labeled data reliance by 95%, and improved computational efficiency (23x faster).", "conclusion": "The proposed CFM-based framework enhances transferability, efficiency, and performance of LiDAR world models, setting new benchmarks."}}
{"id": "2506.23440", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23440", "abs": "https://arxiv.org/abs/2506.23440", "authors": ["Mahesh Bhosale", "Abdul Wasi", "Yuanhao Zhai", "Yunjie Tian", "Samuel Border", "Nan Xi", "Pinaki Sarder", "Junsong Yuan", "David Doermann", "Xuan Gong"], "title": "PathDiff: Histopathology Image Synthesis with Unpaired Text and Mask Conditions", "comment": "Accepted to ICCV 2025", "summary": "Diffusion-based generative models have shown promise in synthesizing\nhistopathology images to address data scarcity caused by privacy constraints.\nDiagnostic text reports provide high-level semantic descriptions, and masks\noffer fine-grained spatial structures essential for representing distinct\nmorphological regions. However, public datasets lack paired text and mask data\nfor the same histopathological images, limiting their joint use in image\ngeneration. This constraint restricts the ability to fully exploit the benefits\nof combining both modalities for enhanced control over semantics and spatial\ndetails. To overcome this, we propose PathDiff, a diffusion framework that\neffectively learns from unpaired mask-text data by integrating both modalities\ninto a unified conditioning space. PathDiff allows precise control over\nstructural and contextual features, generating high-quality, semantically\naccurate images. PathDiff also improves image fidelity, text-image alignment,\nand faithfulness, enhancing data augmentation for downstream tasks like nuclei\nsegmentation and classification. Extensive experiments demonstrate its\nsuperiority over existing methods.", "AI": {"tldr": "PathDiff is a diffusion framework for generating histopathology images by integrating unpaired mask-text data, improving control over semantics and spatial details.", "motivation": "Addressing data scarcity in histopathology due to privacy constraints and the lack of paired text-mask datasets for joint use in image generation.", "method": "Proposes PathDiff, a diffusion framework that learns from unpaired mask-text data by unifying both modalities in a conditioning space.", "result": "Generates high-quality, semantically accurate images with improved fidelity, text-image alignment, and faithfulness for downstream tasks.", "conclusion": "PathDiff outperforms existing methods, offering enhanced control and quality in histopathology image synthesis."}}
{"id": "2506.23460", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23460", "abs": "https://arxiv.org/abs/2506.23460", "authors": ["Dewen Zeng", "Xinrong Hu", "Yu-Jen Chen", "Yawen Wu", "Xiaowei Xu", "Yiyu Shi"], "title": "Contrastive Learning with Diffusion Features for Weakly Supervised Medical Image Segmentation", "comment": null, "summary": "Weakly supervised semantic segmentation (WSSS) methods using class labels\noften rely on class activation maps (CAMs) to localize objects. However,\ntraditional CAM-based methods struggle with partial activations and imprecise\nobject boundaries due to optimization discrepancies between classification and\nsegmentation. Recently, the conditional diffusion model (CDM) has been used as\nan alternative for generating segmentation masks in WSSS, leveraging its strong\nimage generation capabilities tailored to specific class distributions. By\nmodifying or perturbing the condition during diffusion sampling, the related\nobjects can be highlighted in the generated images. Yet, the saliency maps\ngenerated by CDMs are prone to noise from background alterations during reverse\ndiffusion. To alleviate the problem, we introduce Contrastive Learning with\nDiffusion Features (CLDF), a novel method that uses contrastive learning to\ntrain a pixel decoder to map the diffusion features from a frozen CDM to a\nlow-dimensional embedding space for segmentation. Specifically, we integrate\ngradient maps generated from CDM external classifier with CAMs to identify\nforeground and background pixels with fewer false positives/negatives for\ncontrastive learning, enabling robust pixel embedding learning. Experimental\nresults on four segmentation tasks from two public medical datasets demonstrate\nthat our method significantly outperforms existing baselines.", "AI": {"tldr": "A novel method, CLDF, uses contrastive learning with diffusion features to improve weakly supervised semantic segmentation by addressing noise in CDM-generated saliency maps.", "motivation": "Traditional CAM-based methods struggle with partial activations and imprecise boundaries, while CDM-generated saliency maps are noisy.", "method": "CLDF integrates gradient maps from CDM with CAMs for contrastive learning, training a pixel decoder to map diffusion features for segmentation.", "result": "CLDF outperforms baselines on four segmentation tasks across two medical datasets.", "conclusion": "CLDF effectively reduces noise and improves segmentation accuracy in weakly supervised settings."}}
{"id": "2506.23461", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23461", "abs": "https://arxiv.org/abs/2506.23461", "authors": ["Yun Xing", "Qing Guo", "Xiaoguang Li", "Yihao Huang", "Xiaofeng Cao", "Di Lin", "Ivor Tsang", "Lei Ma"], "title": "Time-variant Image Inpainting via Interactive Distribution Transition Estimation", "comment": null, "summary": "In this work, we focus on a novel and practical task, i.e., Time-vAriant\niMage inPainting (TAMP). The aim of TAMP is to restore a damaged target image\nby leveraging the complementary information from a reference image, where both\nimages captured the same scene but with a significant time gap in between,\ni.e., time-variant images. Different from conventional reference-guided image\ninpainting, the reference image under TAMP setup presents significant content\ndistinction to the target image and potentially also suffers from damages. Such\nan application frequently happens in our daily lives to restore a damaged image\nby referring to another reference image, where there is no guarantee of the\nreference image's source and quality. In particular, our study finds that even\nstate-of-the-art (SOTA) reference-guided image inpainting methods fail to\nachieve plausible results due to the chaotic image complementation. To address\nsuch an ill-posed problem, we propose a novel Interactive Distribution\nTransition Estimation (InDiTE) module which interactively complements the\ntime-variant images with adaptive semantics thus facilitate the restoration of\ndamaged regions. To further boost the performance, we propose our TAMP\nsolution, namely Interactive Distribution Transition Estimation-driven\nDiffusion (InDiTE-Diff), which integrates InDiTE with SOTA diffusion model and\nconducts latent cross-reference during sampling. Moreover, considering the lack\nof benchmarks for TAMP task, we newly assembled a dataset, i.e., TAMP-Street,\nbased on existing image and mask datasets. We conduct experiments on the\nTAMP-Street datasets under two different time-variant image inpainting\nsettings, which show our method consistently outperform SOTA reference-guided\nimage inpainting methods for solving TAMP.", "AI": {"tldr": "The paper introduces Time-vAriant iMage inPainting (TAMP), a task to restore damaged images using time-variant reference images. It proposes the InDiTE-Diff method, combining interactive distribution transition estimation with diffusion models, and introduces a new dataset, TAMP-Street. Results show superiority over SOTA methods.", "motivation": "To address the challenge of restoring damaged images using time-variant reference images, which often have significant content differences and may also be damaged. Existing methods fail in this scenario.", "method": "Proposes InDiTE-Diff, integrating the InDiTE module (for adaptive semantics) with a diffusion model, and introduces latent cross-reference during sampling.", "result": "InDiTE-Diff outperforms SOTA reference-guided inpainting methods on the TAMP-Street dataset under two settings.", "conclusion": "The proposed method effectively addresses the TAMP task, demonstrating superior performance and providing a new benchmark for future research."}}
{"id": "2506.23465", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23465", "abs": "https://arxiv.org/abs/2506.23465", "authors": ["Nazanin Mahjourian", "Vinh Nguyen"], "title": "Sanitizing Manufacturing Dataset Labels Using Vision-Language Models", "comment": null, "summary": "The success of machine learning models in industrial applications is heavily\ndependent on the quality of the datasets used to train the models. However,\nlarge-scale datasets, specially those constructed from crowd-sourcing and\nweb-scraping, often suffer from label noise, inconsistencies, and errors. This\nproblem is particularly pronounced in manufacturing domains, where obtaining\nhigh-quality labels is costly and time-consuming. This paper introduces\nVision-Language Sanitization and Refinement (VLSR), which is a\nvision-language-based framework for label sanitization and refinement in\nmulti-label manufacturing image datasets. This method embeds both images and\ntheir associated textual labels into a shared semantic space leveraging the\nCLIP vision-language model. Then two key tasks are addressed in this process by\ncomputing the cosine similarity between embeddings. First, label sanitization\nis performed to identify irrelevant, misspelled, or semantically weak labels,\nand surface the most semantically aligned label for each image by comparing\nimage-label pairs using cosine similarity between image and label embeddings.\nSecond, the method applies density-based clustering on text embeddings,\nfollowed by iterative cluster merging, to group semantically similar labels\ninto unified label groups. The Factorynet dataset, which includes noisy labels\nfrom both human annotations and web-scraped sources, is employed to evaluate\nthe effectiveness of the proposed framework. Experimental results demonstrate\nthat the VLSR framework successfully identifies problematic labels and improves\nlabel consistency. This method enables a significant reduction in label\nvocabulary through clustering, which ultimately enhances the dataset's quality\nfor training robust machine learning models in industrial applications with\nminimal human intervention.", "AI": {"tldr": "VLSR is a vision-language framework for cleaning and refining noisy labels in manufacturing datasets using CLIP embeddings and cosine similarity.", "motivation": "Large-scale datasets, especially in manufacturing, often have noisy labels due to crowd-sourcing or web-scraping, making high-quality labels costly and rare.", "method": "Uses CLIP to embed images and labels into a shared space, computes cosine similarity for sanitization, and applies density-based clustering to merge similar labels.", "result": "VLSR effectively identifies and fixes problematic labels, reduces label vocabulary, and improves dataset quality with minimal human effort.", "conclusion": "VLSR enhances label consistency and dataset quality, benefiting industrial machine learning applications."}}
{"id": "2506.23467", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23467", "abs": "https://arxiv.org/abs/2506.23467", "authors": ["Chenlang Yi", "Zizhan Xiong", "Qi Qi", "Xiyuan Wei", "Girish Bathla", "Ching-Long Lin", "Bobak Jack Mortazavi", "Tianbao Yang"], "title": "AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training for Chest X-rays", "comment": "This preprint has been accepted by MICCAI 2025", "summary": "Contrastive Language-Image Pre-training (CLIP) models have demonstrated\nsuperior performance across various visual tasks including medical image\nclassification. However, fairness concerns, including demographic biases, have\nreceived limited attention for CLIP models. This oversight leads to critical\nissues, particularly those related to race and gender, resulting in disparities\nin diagnostic outcomes and reduced reliability for underrepresented groups. To\naddress these challenges, we introduce AdFair-CLIP, a novel framework employing\nadversarial feature intervention to suppress sensitive attributes, thereby\nmitigating spurious correlations and improving prediction fairness. We conduct\ncomprehensive experiments on chest X-ray (CXR) datasets, and show that\nAdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while\nmaintaining robust generalization in zero-shot and few-shot scenarios. These\nresults establish new benchmarks for fairness-aware learning in CLIP-based\nmedical diagnostic models, particularly for CXR analysis.", "AI": {"tldr": "AdFair-CLIP improves fairness and accuracy in CLIP-based medical image classification by suppressing demographic biases.", "motivation": "Address fairness concerns in CLIP models, particularly demographic biases affecting race and gender, which lead to disparities in diagnostic outcomes.", "method": "Introduces AdFair-CLIP, a framework using adversarial feature intervention to mitigate spurious correlations.", "result": "Significantly enhances fairness and diagnostic accuracy in chest X-ray datasets, with robust generalization in zero-shot and few-shot scenarios.", "conclusion": "Sets new benchmarks for fairness-aware learning in CLIP-based medical diagnostic models, especially for CXR analysis."}}
{"id": "2506.23468", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23468", "abs": "https://arxiv.org/abs/2506.23468", "authors": ["Xuan Yao", "Junyu Gao", "Changsheng Xu"], "title": "NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments", "comment": "Accepted by ICCV 2025", "summary": "Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires\nagents to execute sequential navigation actions in complex environments guided\nby natural language instructions. Current approaches often struggle with\ngeneralizing to novel environments and adapting to ongoing changes during\nnavigation. Inspired by human cognition, we present NavMorph, a self-evolving\nworld model framework that enhances environmental understanding and\ndecision-making in VLN-CE tasks. NavMorph employs compact latent\nrepresentations to model environmental dynamics, equipping agents with\nforesight for adaptive planning and policy refinement. By integrating a novel\nContextual Evolution Memory, NavMorph leverages scene-contextual information to\nsupport effective navigation while maintaining online adaptability. Extensive\nexperiments demonstrate that our method achieves notable performance\nimprovements on popular VLN-CE benchmarks. Code is available at\n\\href{https://github.com/Feliciaxyao/NavMorph}{this https URL}.", "AI": {"tldr": "NavMorph is a self-evolving world model framework for VLN-CE tasks, improving generalization and adaptability through compact latent representations and Contextual Evolution Memory.", "motivation": "Current VLN-CE methods struggle with novel environments and dynamic changes, prompting the need for a more adaptive and generalizable approach.", "method": "NavMorph uses compact latent representations to model environmental dynamics and integrates Contextual Evolution Memory for scene-contextual navigation.", "result": "NavMorph achieves significant performance improvements on VLN-CE benchmarks.", "conclusion": "NavMorph enhances VLN-CE performance by combining adaptive planning and contextual memory, demonstrating its effectiveness in dynamic environments."}}
{"id": "2506.23470", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23470", "abs": "https://arxiv.org/abs/2506.23470", "authors": ["Ngoc-Do Tran", "Minh-Tuan Huynh", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "title": "Interactive Interface For Semantic Segmentation Dataset Synthesis", "comment": null, "summary": "The rapid advancement of AI and computer vision has significantly increased\nthe demand for high-quality annotated datasets, particularly for semantic\nsegmentation. However, creating such datasets is resource-intensive, requiring\nsubstantial time, labor, and financial investment, and often raises privacy\nconcerns due to the use of real-world data. To mitigate these challenges, we\npresent SynthLab, consisting of a modular platform for visual data synthesis\nand a user-friendly interface. The modular architecture of SynthLab enables\neasy maintenance, scalability with centralized updates, and seamless\nintegration of new features. Each module handles distinct aspects of computer\nvision tasks, enhancing flexibility and adaptability. Meanwhile, its\ninteractive, user-friendly interface allows users to quickly customize their\ndata pipelines through drag-and-drop actions. Extensive user studies involving\na diverse range of users across different ages, professions, and expertise\nlevels, have demonstrated flexible usage, and high accessibility of SynthLab,\nenabling users without deep technical expertise to harness AI for real-world\napplications.", "AI": {"tldr": "SynthLab is a modular platform for visual data synthesis with a user-friendly interface, addressing the challenges of high-quality dataset creation for semantic segmentation.", "motivation": "The high cost and privacy concerns of creating annotated datasets for AI and computer vision tasks motivate the need for an efficient, scalable solution.", "method": "SynthLab uses a modular architecture for data synthesis and an interactive interface for easy customization, enabling flexible and adaptable usage.", "result": "User studies show SynthLab is accessible and usable by diverse users, including those without technical expertise.", "conclusion": "SynthLab provides a scalable, user-friendly solution for generating high-quality datasets, reducing resource and privacy barriers."}}
{"id": "2506.23478", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23478", "abs": "https://arxiv.org/abs/2506.23478", "authors": ["Pedro Alonso", "Tianrui Li", "Chongshou Li"], "title": "GeoCD: A Differential Local Approximation for Geodesic Chamfer Distance", "comment": null, "summary": "Chamfer Distance (CD) is a widely adopted metric in 3D point cloud learning\ndue to its simplicity and efficiency. However, it suffers from a fundamental\nlimitation: it relies solely on Euclidean distances, which often fail to\ncapture the intrinsic geometry of 3D shapes. To address this limitation, we\npropose GeoCD, a topology-aware and fully differentiable approximation of\ngeodesic distance designed to serve as a metric for 3D point cloud learning.\nOur experiments show that GeoCD consistently improves reconstruction quality\nover standard CD across various architectures and datasets. We demonstrate this\nby fine-tuning several models, initially trained with standard CD, using GeoCD.\nRemarkably, fine-tuning for a single epoch with GeoCD yields significant gains\nacross multiple evaluation metrics.", "AI": {"tldr": "GeoCD, a topology-aware geodesic distance metric, outperforms Chamfer Distance (CD) in 3D point cloud learning by better capturing intrinsic geometry.", "motivation": "Chamfer Distance (CD) is limited by its reliance on Euclidean distances, which fail to capture the intrinsic geometry of 3D shapes.", "method": "Proposed GeoCD, a topology-aware and differentiable geodesic distance approximation, and fine-tuned models trained with CD using GeoCD.", "result": "GeoCD consistently improves reconstruction quality over CD, with significant gains observed after just one epoch of fine-tuning.", "conclusion": "GeoCD is a superior metric for 3D point cloud learning, enhancing performance across architectures and datasets."}}
{"id": "2506.23479", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23479", "abs": "https://arxiv.org/abs/2506.23479", "authors": ["Zhaojie Zeng", "Yuesong Wang", "Chao Yang", "Tao Guan", "Lili Ju"], "title": "Instant GaussianImage: A Generalizable and Self-Adaptive Image Representation via 2D Gaussian Splatting", "comment": null, "summary": "Implicit Neural Representation (INR) has demonstrated remarkable advances in\nthe field of image representation but demands substantial GPU resources.\nGaussianImage recently pioneered the use of Gaussian Splatting to mitigate this\ncost, however, the slow training process limits its practicality, and the fixed\nnumber of Gaussians per image limits its adaptability to varying information\nentropy. To address these issues, we propose in this paper a generalizable and\nself-adaptive image representation framework based on 2D Gaussian Splatting.\nOur method employs a network to quickly generate a coarse Gaussian\nrepresentation, followed by minimal fine-tuning steps, achieving comparable\nrendering quality of GaussianImage while significantly reducing training time.\nMoreover, our approach dynamically adjusts the number of Gaussian points based\non image complexity to further enhance flexibility and efficiency in practice.\nExperiments on DIV2K and Kodak datasets show that our method matches or exceeds\nGaussianImage's rendering performance with far fewer iterations and shorter\ntraining times. Specifically, our method reduces the training time by up to one\norder of magnitude while achieving superior rendering performance with the same\nnumber of Gaussians.", "AI": {"tldr": "Proposes a faster, adaptive 2D Gaussian Splatting method for image representation, reducing training time significantly while maintaining quality.", "motivation": "Address the high GPU resource demands and slow training of existing methods like GaussianImage, and improve adaptability to varying image complexity.", "method": "Uses a network to generate a coarse Gaussian representation quickly, followed by minimal fine-tuning, and dynamically adjusts Gaussian points based on image complexity.", "result": "Achieves comparable or better rendering quality than GaussianImage with up to 10x faster training and adaptive Gaussian point allocation.", "conclusion": "The method offers a practical, efficient solution for high-quality image representation with reduced computational costs."}}
{"id": "2506.23481", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.23481", "abs": "https://arxiv.org/abs/2506.23481", "authors": ["Xian Zhang", "Xiang Cheng"], "title": "Evaluation of Geolocation Capabilities of Multimodal Large Language Models and Analysis of Associated Privacy Risks", "comment": null, "summary": "Objectives: The rapid advancement of Multimodal Large Language Models (MLLMs)\nhas significantly enhanced their reasoning capabilities, enabling a wide range\nof intelligent applications. However, these advancements also raise critical\nconcerns regarding privacy and ethics. MLLMs are now capable of inferring the\ngeographic location of images -- such as those shared on social media or\ncaptured from street views -- based solely on visual content, thereby posing\nserious risks of privacy invasion, including doxxing, surveillance, and other\nsecurity threats.\n  Methods: This study provides a comprehensive analysis of existing geolocation\ntechniques based on MLLMs. It systematically reviews relevant litera-ture and\nevaluates the performance of state-of-the-art visual reasoning models on\ngeolocation tasks, particularly in identifying the origins of street view\nimagery.\n  Results: Empirical evaluation reveals that the most advanced visual large\nmodels can successfully localize the origin of street-level imagery with up to\n$49\\%$ accuracy within a 1-kilometer radius. This performance underscores the\nmodels' powerful capacity to extract and utilize fine-grained geographic cues\nfrom visual data.\n  Conclusions: Building on these findings, the study identifies key visual\nelements that contribute to suc-cessful geolocation, such as text,\narchitectural styles, and environmental features. Furthermore, it discusses the\npotential privacy implications associated with MLLM-enabled geolocation and\ndiscuss several technical and policy-based coun-termeasures to mitigate\nassociated risks. Our code and dataset are available at\nhttps://github.com/zxyl1003/MLLM-Geolocation-Evaluation.", "AI": {"tldr": "MLLMs can geolocate images with 49% accuracy within 1km, raising privacy concerns. The study reviews techniques, evaluates performance, and suggests countermeasures.", "motivation": "To analyze the privacy risks posed by MLLMs' ability to infer geographic locations from images, focusing on street views.", "method": "Systematic review of geolocation techniques and evaluation of state-of-the-art visual reasoning models on street view imagery.", "result": "Advanced models achieve 49% accuracy within 1km, highlighting their ability to extract geographic cues.", "conclusion": "Identifies key visual elements for geolocation and discusses privacy implications and countermeasures."}}
{"id": "2506.23482", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23482", "abs": "https://arxiv.org/abs/2506.23482", "authors": ["Jun Huang", "Ting Liu", "Yihang Wu", "Xiaochao Qu", "Luoqi Liu", "Xiaolin Hu"], "title": "MTADiffusion: Mask Text Alignment Diffusion Model for Object Inpainting", "comment": "CVPR 2025", "summary": "Advancements in generative models have enabled image inpainting models to\ngenerate content within specific regions of an image based on provided prompts\nand masks. However, existing inpainting methods often suffer from problems such\nas semantic misalignment, structural distortion, and style inconsistency. In\nthis work, we present MTADiffusion, a Mask-Text Alignment diffusion model\ndesigned for object inpainting. To enhance the semantic capabilities of the\ninpainting model, we introduce MTAPipeline, an automatic solution for\nannotating masks with detailed descriptions. Based on the MTAPipeline, we\nconstruct a new MTADataset comprising 5 million images and 25 million mask-text\npairs. Furthermore, we propose a multi-task training strategy that integrates\nboth inpainting and edge prediction tasks to improve structural stability. To\npromote style consistency, we present a novel inpainting style-consistency loss\nusing a pre-trained VGG network and the Gram matrix. Comprehensive evaluations\non BrushBench and EditBench demonstrate that MTADiffusion achieves\nstate-of-the-art performance compared to other methods.", "AI": {"tldr": "MTADiffusion is a Mask-Text Alignment diffusion model for object inpainting, addressing issues like semantic misalignment, structural distortion, and style inconsistency. It introduces MTAPipeline for mask annotation, a large MTADataset, multi-task training, and a style-consistency loss, achieving state-of-the-art results.", "motivation": "Existing inpainting methods often suffer from semantic misalignment, structural distortion, and style inconsistency, limiting their effectiveness.", "method": "Proposes MTADiffusion with MTAPipeline for mask annotation, a multi-task training strategy (inpainting and edge prediction), and a style-consistency loss using VGG and Gram matrix.", "result": "Achieves state-of-the-art performance on BrushBench and EditBench benchmarks.", "conclusion": "MTADiffusion effectively addresses key challenges in inpainting, offering improved semantic alignment, structural stability, and style consistency."}}
{"id": "2506.23529", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23529", "abs": "https://arxiv.org/abs/2506.23529", "authors": ["Jisu Han", "Jihee Park", "Dongyoon Han", "Wonjun Hwang"], "title": "When Test-Time Adaptation Meets Self-Supervised Models", "comment": "15 pages, 7 figures", "summary": "Training on test-time data enables deep learning models to adapt to dynamic\nenvironmental changes, enhancing their practical applicability. Online\nadaptation from source to target domains is promising but it remains highly\nreliant on the performance of source pretrained model. In this paper, we\ninvestigate whether test-time adaptation (TTA) methods can continuously improve\nmodels trained via self-supervised learning (SSL) without relying on source\npretraining. We introduce a self-supervised TTA protocol after observing that\nexisting TTA approaches struggle when directly applied to self-supervised\nmodels with low accuracy on the source domain. Furthermore, we propose a\ncollaborative learning framework that integrates SSL and TTA models, leveraging\ncontrastive learning and knowledge distillation for stepwise representation\nrefinement. We validate our method on diverse self-supervised models, including\nDINO, MoCo, and iBOT, across TTA benchmarks. Extensive experiments validate the\neffectiveness of our approach in SSL, showing that it achieves competitive\nperformance even without source pretraining.", "AI": {"tldr": "The paper explores test-time adaptation (TTA) for self-supervised learning (SSL) models without relying on source pretraining, proposing a collaborative framework to enhance performance.", "motivation": "To address the limitations of existing TTA methods when applied to SSL models with low source accuracy, enabling continuous improvement without source pretraining.", "method": "Introduces a self-supervised TTA protocol and a collaborative learning framework combining SSL and TTA, using contrastive learning and knowledge distillation for refinement.", "result": "Validated on SSL models (DINO, MoCo, iBOT) across TTA benchmarks, achieving competitive performance without source pretraining.", "conclusion": "The proposed framework effectively enhances SSL models via TTA, demonstrating practical applicability without dependency on source pretraining."}}
{"id": "2506.23491", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23491", "abs": "https://arxiv.org/abs/2506.23491", "authors": ["ZongHan Hsieh", "Tzer-Jen Wei"], "title": "Qwen-GUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding", "comment": null, "summary": "This paper introduces Qwen-GUI-3B, a lightweight Vision-Language Model (VLM)\nspecifically designed for Graphical User Interface grounding tasks, achieving\nperformance competitive with significantly larger models. Unlike large-scale\nVLMs (>7B parameters) that are computationally intensive and impractical for\nconsumer-grade hardware, Qwen-GUI-3B delivers strong grounding accuracy while\nbeing fully trainable on a single GPU (RTX 4090). The model incorporates\nseveral key innovations: (i) combine cross-platform, multi-resolution dataset\nof 24K examples from diverse sources including mobile, desktop, and web GUI\nscreenshots to effectively address data scarcity in high-resolution desktop\nenvironments; (ii) a two-stage fine-tuning strategy, where initial\ncross-platform training establishes robust GUI understanding, followed by\nspecialized fine-tuning on high-resolution data to significantly enhance model\nadaptability; and (iii) data curation and redundancy reduction strategies,\ndemonstrating that randomly sampling a smaller subset with reduced redundancy\nachieves performance comparable to larger datasets, emphasizing data diversity\nover sheer volume. Empirical evaluation on standard GUI grounding\nbenchmarks-including ScreenSpot, ScreenSpot-v2, and the challenging\nScreenSpot-Pro, highlights Qwen-GUI-3B's exceptional accuracy, achieving 84.9%\non ScreenSpot and 86.4% on ScreenSpot-v2, surpassing prior models under 4B\nparameters. Ablation studies validate the critical role of balanced sampling\nand two-stage fine-tuning in enhancing robustness, particularly in\nhigh-resolution desktop scenarios. The Qwen-GUI-3B is available at:\nhttps://github.com/Han1018/Qwen-GUI-3B", "AI": {"tldr": "Qwen-GUI-3B is a lightweight Vision-Language Model for GUI grounding, achieving competitive performance with larger models while being trainable on a single GPU. Key innovations include cross-platform dataset, two-stage fine-tuning, and data curation strategies.", "motivation": "Address the impracticality of large-scale VLMs for consumer-grade hardware by developing a lightweight model that maintains strong grounding accuracy.", "method": "Combines a diverse dataset, two-stage fine-tuning (cross-platform training followed by specialized fine-tuning), and data redundancy reduction.", "result": "Achieves 84.9% on ScreenSpot and 86.4% on ScreenSpot-v2, surpassing prior models under 4B parameters.", "conclusion": "Qwen-GUI-3B demonstrates that lightweight models can achieve high accuracy with innovative training and data strategies, making GUI grounding more accessible."}}
{"id": "2506.23532", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23532", "abs": "https://arxiv.org/abs/2506.23532", "authors": ["Jefferson Hernandez", "Ruozhen He", "Guha Balakrishnan", "Alexander C. Berg", "Vicente Ordonez"], "title": "GViT: Representing Images as Gaussians for Visual Recognition", "comment": null, "summary": "We introduce GVIT, a classification framework that abandons conventional\npixel or patch grid input representations in favor of a compact set of\nlearnable 2D Gaussians. Each image is encoded as a few hundred Gaussians whose\npositions, scales, orientations, colors, and opacities are optimized jointly\nwith a ViT classifier trained on top of these representations. We reuse the\nclassifier gradients as constructive guidance, steering the Gaussians toward\nclass-salient regions while a differentiable renderer optimizes an image\nreconstruction loss. We demonstrate that by 2D Gaussian input representations\ncoupled with our GVIT guidance, using a relatively standard ViT architecture,\nclosely matches the performance of a traditional patch-based ViT, reaching a\n76.9% top-1 accuracy on Imagenet-1k using a ViT-B architecture.", "AI": {"tldr": "GVIT replaces pixel/patch grids with learnable 2D Gaussians for image classification, achieving competitive performance with ViT-B on Imagenet-1k.", "motivation": "To explore alternative input representations beyond traditional pixel or patch grids for vision tasks.", "method": "Encodes images as optimized 2D Gaussians, jointly trained with a ViT classifier using gradients for guidance and a differentiable renderer for reconstruction.", "result": "Achieves 76.9% top-1 accuracy on Imagenet-1k with ViT-B, comparable to patch-based ViT.", "conclusion": "GVIT demonstrates the viability of Gaussian-based representations for efficient and effective image classification."}}
{"id": "2506.23502", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23502", "abs": "https://arxiv.org/abs/2506.23502", "authors": ["Mengxiao Tian", "Xinxiao Wu", "Shuo Yang"], "title": "LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text Matching", "comment": "accepted by ICCV 2025", "summary": "Driven by large-scale contrastive vision-language pre-trained models such as\nCLIP, recent advancements in the image-text matching task have achieved\nremarkable success in representation learning. Due to image-level\nvisual-language alignment, CLIP falls short in understanding fine-grained\ndetails such as object attributes and spatial relationships between objects.\nRecent efforts have attempted to compel CLIP to acquire structured visual\nrepresentations by introducing prompt learning to achieve object-level\nalignment. While achieving promising results, they still lack the capability to\nperceive actions, which are crucial for describing the states or relationships\nbetween objects. Therefore, we propose to endow CLIP with fine-grained\naction-level understanding by introducing an LLM-enhanced action-aware\nmulti-modal prompt-tuning method, incorporating the action-related external\nknowledge generated by large language models (LLMs). Specifically, we design an\naction triplet prompt and an action state prompt to exploit compositional\nsemantic knowledge and state-related causal knowledge implicitly stored in\nLLMs. Subsequently, we propose an adaptive interaction module to aggregate\nattentive visual features conditioned on action-aware prompted knowledge for\nestablishing discriminative and action-aware visual representations, which\nfurther improves the performance. Comprehensive experimental results on two\nbenchmark datasets demonstrate the effectiveness of our method.", "AI": {"tldr": "The paper introduces an LLM-enhanced action-aware multi-modal prompt-tuning method to improve CLIP's fine-grained action-level understanding, addressing its limitations in object attributes and spatial relationships.", "motivation": "CLIP lacks fine-grained understanding of actions and object states, which are crucial for detailed image-text matching.", "method": "The method uses LLM-generated action-related knowledge, designing action triplet and state prompts, and an adaptive interaction module for visual feature aggregation.", "result": "Experiments on benchmark datasets show the method's effectiveness in improving action-aware visual representations.", "conclusion": "The proposed method successfully enhances CLIP's action-level understanding, demonstrating significant performance improvements."}}
{"id": "2506.23538", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23538", "abs": "https://arxiv.org/abs/2506.23538", "authors": ["Yuhao Huang", "Yueyue Xu", "Haoran Dou", "Jiaxiao Deng", "Xin Yang", "Hongyu Zheng", "Dong Ni"], "title": "Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound", "comment": "Accepted by MICCAI 2025;10 pages, 3 figures", "summary": "Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage,\npreterm birth, and an increased risk of pregnancy complications. Compared to\ntraditional 2D ultrasound (US), 3D US can reconstruct the coronal plane,\nproviding a clear visualization of the uterine morphology for assessing CUAs\naccurately. In this paper, we propose an intelligent system for simultaneous\nautomated plane localization and CUA diagnosis. Our highlights are: 1) we\ndevelop a denoising diffusion model with local (plane) and global (volume/text)\nguidance, using an adaptive weighting strategy to optimize attention allocation\nto different conditions; 2) we introduce a reinforcement learning-based\nframework with unsupervised rewards to extract the key slice summary from\nredundant sequences, fully integrating information across multiple planes to\nreduce learning difficulty; 3) we provide text-driven uncertainty modeling for\ncoarse prediction, and leverage it to adjust the classification probability for\noverall performance improvement. Extensive experiments on a large 3D uterine US\ndataset show the efficacy of our method, in terms of plane localization and CUA\ndiagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.", "AI": {"tldr": "Proposes an intelligent system for automated plane localization and CUA diagnosis using 3D ultrasound, combining denoising diffusion, reinforcement learning, and text-driven uncertainty modeling.", "motivation": "CUAs cause infertility and pregnancy complications; 3D US improves diagnosis accuracy over 2D US by visualizing uterine morphology.", "method": "Uses a denoising diffusion model with local/global guidance, reinforcement learning for key slice extraction, and text-driven uncertainty modeling for classification adjustment.", "result": "Effective plane localization and CUA diagnosis demonstrated on a large 3D uterine US dataset.", "conclusion": "The system enhances CUA diagnosis accuracy and efficiency, with code publicly available."}}
{"id": "2506.23505", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23505", "abs": "https://arxiv.org/abs/2506.23505", "authors": ["Tinh Nguyen"], "title": "Improve Underwater Object Detection through YOLOv12 Architecture and Physics-informed Augmentation", "comment": null, "summary": "Underwater object detection is crucial for autonomous navigation,\nenvironmental monitoring, and marine exploration, but it is severely hampered\nby light attenuation, turbidity, and occlusion. Current methods balance\naccuracy and computational efficiency, but they have trouble deploying in\nreal-time under low visibility conditions. Through the integration of\nphysics-informed augmentation techniques with the YOLOv12 architecture, this\nstudy advances underwater detection. With Residual ELAN blocks to preserve\nstructural features in turbid waters and Area Attention to maintain large\nreceptive fields for occluded objects while reducing computational complexity.\nUnderwater optical properties are addressed by domain-specific augmentations\nsuch as turbulence adaptive blurring, biologically grounded occlusion\nsimulation, and spectral HSV transformations for color distortion. Extensive\ntests on four difficult datasets show state-of-the-art performance, with\nBrackish data registering 98.30% mAP at 142 FPS. YOLOv12 improves occlusion\nrobustness by 18.9%, small-object recall by 22.4%, and detection precision by\nup to 7.94% compared to previous models. The crucial role of augmentation\nstrategy is validated by ablation studies. This work offers a precise and\neffective solution for conservation and underwater robotics applications.", "AI": {"tldr": "The paper introduces YOLOv12, integrating physics-informed augmentation techniques for underwater object detection, achieving high accuracy and real-time performance in challenging conditions.", "motivation": "Underwater object detection is hindered by poor visibility, occlusion, and turbidity, requiring solutions that balance accuracy and computational efficiency.", "method": "The study combines YOLOv12 with Residual ELAN blocks and Area Attention, along with domain-specific augmentations like turbulence blurring and spectral HSV transformations.", "result": "YOLOv12 achieves 98.30% mAP at 142 FPS, improving occlusion robustness by 18.9%, small-object recall by 22.4%, and detection precision by up to 7.94%.", "conclusion": "The work provides a precise, efficient solution for underwater robotics and conservation, validated by ablation studies."}}
{"id": "2506.23513", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23513", "abs": "https://arxiv.org/abs/2506.23513", "authors": ["Zixun Fang", "Kai Zhu", "Zhiheng Liu", "Yu Liu", "Wei Zhai", "Yang Cao", "Zheng-Jun Zha"], "title": "ViewPoint: Panoramic Video Generation with Pretrained Diffusion Models", "comment": "https://becauseimbatman0.github.io/ViewPoint", "summary": "Panoramic video generation aims to synthesize 360-degree immersive videos,\nholding significant importance in the fields of VR, world models, and spatial\nintelligence. Existing works fail to synthesize high-quality panoramic videos\ndue to the inherent modality gap between panoramic data and perspective data,\nwhich constitutes the majority of the training data for modern diffusion\nmodels. In this paper, we propose a novel framework utilizing pretrained\nperspective video models for generating panoramic videos. Specifically, we\ndesign a novel panorama representation named ViewPoint map, which possesses\nglobal spatial continuity and fine-grained visual details simultaneously. With\nour proposed Pano-Perspective attention mechanism, the model benefits from\npretrained perspective priors and captures the panoramic spatial correlations\nof the ViewPoint map effectively. Extensive experiments demonstrate that our\nmethod can synthesize highly dynamic and spatially consistent panoramic videos,\nachieving state-of-the-art performance and surpassing previous methods.", "AI": {"tldr": "A novel framework for generating high-quality 360-degree videos by leveraging pretrained perspective video models and a new panorama representation called ViewPoint map.", "motivation": "Existing methods struggle with synthesizing panoramic videos due to the modality gap between panoramic and perspective data, which limits the quality of outputs.", "method": "Introduces a ViewPoint map for panorama representation and a Pano-Perspective attention mechanism to utilize pretrained perspective priors and capture panoramic spatial correlations.", "result": "The method produces highly dynamic and spatially consistent panoramic videos, outperforming previous approaches.", "conclusion": "The proposed framework effectively bridges the modality gap, achieving state-of-the-art performance in panoramic video generation."}}
{"id": "2506.23566", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23566", "abs": "https://arxiv.org/abs/2506.23566", "authors": ["Luigi Sigillo", "Renato Giamba", "Danilo Comminiello"], "title": "Metadata, Wavelet, and Time Aware Diffusion Models for Satellite Image Super Resolution", "comment": "ICLR 2025 Workshop on Machine Learning for Remote Sensing (ML4RS)", "summary": "The acquisition of high-resolution satellite imagery is often constrained by\nthe spatial and temporal limitations of satellite sensors, as well as the high\ncosts associated with frequent observations. These challenges hinder\napplications such as environmental monitoring, disaster response, and\nagricultural management, which require fine-grained and high-resolution data.\nIn this paper, we propose MWT-Diff, an innovative framework for satellite image\nsuper-resolution (SR) that combines latent diffusion models with wavelet\ntransforms to address these challenges. At the core of the framework is a novel\nmetadata-, wavelet-, and time-aware encoder (MWT-Encoder), which generates\nembeddings that capture metadata attributes, multi-scale frequency information,\nand temporal relationships. The embedded feature representations steer the\nhierarchical diffusion dynamics, through which the model progressively\nreconstructs high-resolution satellite imagery from low-resolution inputs. This\nprocess preserves critical spatial characteristics including textural patterns,\nboundary discontinuities, and high-frequency spectral components essential for\ndetailed remote sensing analysis. The comparative analysis of MWT-Diff across\nmultiple datasets demonstrated favorable performance compared to recent\napproaches, as measured by standard perceptual quality metrics including FID\nand LPIPS.", "AI": {"tldr": "MWT-Diff is a framework for satellite image super-resolution using latent diffusion models and wavelet transforms, outperforming recent methods in perceptual quality.", "motivation": "High-resolution satellite imagery is limited by sensor constraints and high costs, hindering applications like environmental monitoring and disaster response.", "method": "Combines latent diffusion models with wavelet transforms, using a novel MWT-Encoder to capture metadata, multi-scale frequency, and temporal relationships for hierarchical reconstruction.", "result": "MWT-Diff outperforms recent approaches in perceptual quality metrics (FID, LPIPS) across multiple datasets.", "conclusion": "The framework effectively reconstructs high-resolution satellite imagery while preserving critical spatial details, addressing key limitations in remote sensing."}}
{"id": "2506.23518", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23518", "abs": "https://arxiv.org/abs/2506.23518", "authors": ["Jiwoo Park", "Tae Eun Choi", "Youngjun Jun", "Seong Jae Hwang"], "title": "WAVE: Warp-Based View Guidance for Consistent Novel View Synthesis Using a Single Image", "comment": null, "summary": "Generating high-quality novel views of a scene from a single image requires\nmaintaining structural coherence across different views, referred to as view\nconsistency. While diffusion models have driven advancements in novel view\nsynthesis, they still struggle to preserve spatial continuity across views.\nDiffusion models have been combined with 3D models to address the issue, but\nsuch approaches lack efficiency due to their complex multi-step pipelines. This\npaper proposes a novel view-consistent image generation method which utilizes\ndiffusion models without additional modules. Our key idea is to enhance\ndiffusion models with a training-free method that enables adaptive attention\nmanipulation and noise reinitialization by leveraging view-guided warping to\nensure view consistency. Through our comprehensive metric framework suitable\nfor novel-view datasets, we show that our method improves view consistency\nacross various diffusion models, demonstrating its broader applicability.", "AI": {"tldr": "A method to improve view consistency in novel view synthesis using diffusion models without extra modules, enhancing attention and noise handling.", "motivation": "Addressing the challenge of maintaining structural coherence (view consistency) in novel view synthesis from a single image, especially with diffusion models.", "method": "Uses a training-free approach with adaptive attention manipulation and noise reinitialization, leveraging view-guided warping for consistency.", "result": "Improves view consistency across diffusion models, validated by a comprehensive metric framework.", "conclusion": "The method is broadly applicable and effective for enhancing view consistency in novel view synthesis."}}
{"id": "2506.23581", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23581", "abs": "https://arxiv.org/abs/2506.23581", "authors": ["Xiao Li", "Yiming Zhu", "Yifan Huang", "Wei Zhang", "Yingzhe He", "Jie Shi", "Xiaolin Hu"], "title": "PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection", "comment": "Accepted by ICCV 2025", "summary": "Object detection plays a crucial role in many security-sensitive\napplications. However, several recent studies have shown that object detectors\ncan be easily fooled by physically realizable attacks, \\eg, adversarial patches\nand recent adversarial textures, which pose realistic and urgent threats.\nAdversarial Training (AT) has been recognized as the most effective defense\nagainst adversarial attacks. While AT has been extensively studied in the\n$l_\\infty$ attack settings on classification models, AT against physically\nrealizable attacks on object detectors has received limited exploration. Early\nattempts are only performed to defend against adversarial patches, leaving AT\nagainst a wider range of physically realizable attacks under-explored. In this\nwork, we consider defending against various physically realizable attacks with\na unified AT method. We propose PBCAT, a novel Patch-Based Composite\nAdversarial Training strategy. PBCAT optimizes the model by incorporating the\ncombination of small-area gradient-guided adversarial patches and imperceptible\nglobal adversarial perturbations covering the entire image. With these designs,\nPBCAT has the potential to defend against not only adversarial patches but also\nunseen physically realizable attacks such as adversarial textures. Extensive\nexperiments in multiple settings demonstrated that PBCAT significantly improved\nrobustness against various physically realizable attacks over state-of-the-art\ndefense methods. Notably, it improved the detection accuracy by 29.7\\% over\nprevious defense methods under one recent adversarial texture attack.", "AI": {"tldr": "The paper introduces PBCAT, a unified adversarial training method to defend against various physically realizable attacks on object detectors, improving robustness significantly.", "motivation": "Object detectors are vulnerable to physically realizable attacks like adversarial patches and textures, but existing adversarial training methods are limited in scope.", "method": "Proposes PBCAT, combining small-area gradient-guided adversarial patches and imperceptible global perturbations for training.", "result": "PBCAT improves detection accuracy by 29.7% under adversarial texture attacks and outperforms state-of-the-art defenses.", "conclusion": "PBCAT is a robust and versatile defense against diverse physically realizable attacks on object detectors."}}
{"id": "2506.23519", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23519", "abs": "https://arxiv.org/abs/2506.23519", "authors": ["Qi Qin", "Runmin Cong", "Gen Zhan", "Yiting Liao", "Sam Kwong"], "title": "From Sight to Insight: Unleashing Eye-Tracking in Weakly Supervised Video Salient Object Detection", "comment": "15 Pages, 9 Figures", "summary": "The eye-tracking video saliency prediction (VSP) task and video salient\nobject detection (VSOD) task both focus on the most attractive objects in video\nand show the result in the form of predictive heatmaps and pixel-level saliency\nmasks, respectively. In practical applications, eye tracker annotations are\nmore readily obtainable and align closely with the authentic visual patterns of\nhuman eyes. Therefore, this paper aims to introduce fixation information to\nassist the detection of video salient objects under weak supervision. On the\none hand, we ponder how to better explore and utilize the information provided\nby fixation, and then propose a Position and Semantic Embedding (PSE) module to\nprovide location and semantic guidance during the feature learning process. On\nthe other hand, we achieve spatiotemporal feature modeling under weak\nsupervision from the aspects of feature selection and feature contrast. A\nSemantics and Locality Query (SLQ) Competitor with semantic and locality\nconstraints is designed to effectively select the most matching and accurate\nobject query for spatiotemporal modeling. In addition, an Intra-Inter Mixed\nContrastive (IIMC) model improves the spatiotemporal modeling capabilities\nunder weak supervision by forming an intra-video and inter-video contrastive\nlearning paradigm. Experimental results on five popular VSOD benchmarks\nindicate that our model outperforms other competitors on various evaluation\nmetrics.", "AI": {"tldr": "The paper introduces fixation information to improve video salient object detection (VSOD) under weak supervision, using a Position and Semantic Embedding (PSE) module and a Semantics and Locality Query (SLQ) Competitor, along with an Intra-Inter Mixed Contrastive (IIMC) model for better spatiotemporal feature modeling.", "motivation": "Eye-tracking annotations are more accessible and align with human visual patterns, making them useful for weakly supervised VSOD.", "method": "Proposes PSE for location and semantic guidance, SLQ for feature selection, and IIMC for contrastive learning.", "result": "Outperforms competitors on five VSOD benchmarks across multiple metrics.", "conclusion": "The approach effectively leverages fixation data and weak supervision to enhance VSOD performance."}}
{"id": "2506.23627", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23627", "abs": "https://arxiv.org/abs/2506.23627", "authors": ["Roham Maiti", "Debasmita Bhoumik"], "title": "Brain Tumor Detection through Thermal Imaging and MobileNET", "comment": null, "summary": "Brain plays a crucial role in regulating body functions and cognitive\nprocesses, with brain tumors posing significant risks to human health. Precise\nand prompt detection is a key factor in proper treatment and better patient\noutcomes. Traditional methods for detecting brain tumors, that include\nbiopsies, MRI, and CT scans often face challenges due to their high costs and\nthe need for specialized medical expertise. Recent developments in machine\nlearning (ML) and deep learning (DL) has exhibited strong capabilities in\nautomating the identification and categorization of brain tumors from medical\nimages, especially MRI scans. However, these classical ML models have\nlimitations, such as high computational demands, the need for large datasets,\nand long training times, which hinder their accessibility and efficiency. Our\nresearch uses MobileNET model for efficient detection of these tumors. The\nnovelty of this project lies in building an accurate tumor detection model\nwhich use less computing re-sources and runs in less time followed by efficient\ndecision making through the use of image processing technique for accurate\nresults. The suggested method attained an average accuracy of 98.5%.", "AI": {"tldr": "The paper proposes using MobileNET for efficient brain tumor detection from MRI scans, addressing limitations of traditional methods and classical ML models, achieving 98.5% accuracy.", "motivation": "Brain tumors pose significant health risks, and traditional detection methods like biopsies and MRI/CT scans are costly and require expertise. Classical ML models for detection are computationally demanding and inefficient.", "method": "The research employs the MobileNET model for tumor detection, focusing on reduced computational resources and faster processing while maintaining accuracy through image processing techniques.", "result": "The proposed method achieved an average accuracy of 98.5% in detecting brain tumors from MRI scans.", "conclusion": "The MobileNET-based approach offers an efficient, accurate, and resource-friendly solution for brain tumor detection, improving accessibility and speed compared to traditional and classical ML methods."}}
{"id": "2506.23523", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23523", "abs": "https://arxiv.org/abs/2506.23523", "authors": ["Tuong Do", "Binh X. Nguyen", "Quang D. Tran", "Erman Tjiputra", "Te-Chuan Chiu", "Anh Nguyen"], "title": "Lightweight Temporal Transformer Decomposition for Federated Autonomous Driving", "comment": "Accepted in IROS 2025", "summary": "Traditional vision-based autonomous driving systems often face difficulties\nin navigating complex environments when relying solely on single-image inputs.\nTo overcome this limitation, incorporating temporal data such as past image\nframes or steering sequences, has proven effective in enhancing robustness and\nadaptability in challenging scenarios. While previous high-performance methods\nexist, they often rely on resource-intensive fusion networks, making them\nimpractical for training and unsuitable for federated learning. To address\nthese challenges, we propose lightweight temporal transformer decomposition, a\nmethod that processes sequential image frames and temporal steering data by\nbreaking down large attention maps into smaller matrices. This approach reduces\nmodel complexity, enabling efficient weight updates for convergence and\nreal-time predictions while leveraging temporal information to enhance\nautonomous driving performance. Intensive experiments on three datasets\ndemonstrate that our method outperforms recent approaches by a clear margin\nwhile achieving real-time performance. Additionally, real robot experiments\nfurther confirm the effectiveness of our method.", "AI": {"tldr": "Proposes a lightweight temporal transformer decomposition method for autonomous driving, improving efficiency and performance by breaking down large attention maps.", "motivation": "Overcome limitations of single-image inputs and resource-intensive fusion networks in autonomous driving systems.", "method": "Lightweight temporal transformer decomposition processes sequential image frames and steering data by decomposing large attention maps into smaller matrices.", "result": "Outperforms recent methods on three datasets and achieves real-time performance, confirmed by real robot experiments.", "conclusion": "The method effectively enhances autonomous driving performance while being efficient and practical for real-time applications."}}
{"id": "2506.23663", "categories": ["cs.CV", "cs.LG", "I.4"], "pdf": "https://arxiv.org/pdf/2506.23663", "abs": "https://arxiv.org/abs/2506.23663", "authors": ["Mario Koddenbrock", "Rudolf Hoffmann", "David Brodmann", "Erik Rodner"], "title": "On the Domain Robustness of Contrastive Vision-Language Models", "comment": "Deepbench is available at https://github.com/ml-lab-htw/deepbench", "summary": "In real-world vision-language applications, practitioners increasingly rely\non large, pretrained foundation models rather than custom-built solutions,\ndespite limited transparency regarding their training data and processes. While\nthese models achieve impressive performance on general benchmarks, their\neffectiveness can decline notably under specialized domain shifts, such as\nunique imaging conditions or environmental variations. In this work, we\nintroduce Deepbench, a framework designed to assess domain-specific robustness\nof vision-language models (VLMs). Deepbench leverages a large language model\n(LLM) to generate realistic, context-aware image corruptions tailored to\nspecific deployment domains without requiring labeled data. We evaluate a range\nof contrastive vision-language architectures and architectural variants across\nsix real-world domains and observe substantial variability in robustness,\nhighlighting the need for targeted, domain-aware evaluation. Deepbench is\nreleased as open-source software to support further research into domain-aware\nrobustness assessment.", "AI": {"tldr": "Deepbench is a framework to evaluate domain-specific robustness of vision-language models (VLMs) using LLM-generated corruptions, revealing variability in performance across domains.", "motivation": "Practitioners use pretrained foundation models despite limited transparency, but their performance drops under domain shifts. Deepbench addresses this gap.", "method": "Deepbench uses an LLM to generate realistic, domain-specific image corruptions for evaluating VLMs without labeled data.", "result": "Evaluation shows significant robustness variability across six domains, emphasizing the need for domain-aware assessment.", "conclusion": "Deepbench, released as open-source, supports domain-aware robustness research for VLMs."}}
{"id": "2506.23783", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23783", "abs": "https://arxiv.org/abs/2506.23783", "authors": ["Shiao Wang", "Ju Huang", "Qingchuan Ma", "Jinfeng Gao", "Chunyi Xu", "Xiao Wang", "Lan Chen", "Bo Jiang"], "title": "Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking", "comment": "Journal extension of Mamba-FETrack which was published on Pattern\n  Recognition and Computer Vision (PRCV) 2024", "summary": "Combining traditional RGB cameras with bio-inspired event cameras for robust\nobject tracking has garnered increasing attention in recent years. However,\nmost existing multimodal tracking algorithms depend heavily on high-complexity\nVision Transformer architectures for feature extraction and fusion across\nmodalities. This not only leads to substantial computational overhead but also\nlimits the effectiveness of cross-modal interactions. In this paper, we propose\nan efficient RGB-Event object tracking framework based on the linear-complexity\nVision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a\nlightweight Prompt Generator that utilizes embedded features from each\nmodality, together with a shared prompt pool, to dynamically generate\nmodality-specific learnable prompt vectors. These prompts, along with the\nmodality-specific embedded features, are then fed into a Vision Mamba-based\nFEMamba backbone, which facilitates prompt-guided feature extraction,\ncross-modal interaction, and fusion in a unified manner. Finally, the fused\nrepresentations are passed to the tracking head for accurate target\nlocalization. Extensive experimental evaluations on multiple RGB-Event tracking\nbenchmarks, including short-term COESOT dataset and long-term datasets, i.e.,\nFE108 and FELT V2, demonstrate the superior performance and efficiency of the\nproposed tracking framework. The source code and pre-trained models will be\nreleased on https://github.com/Event-AHU/Mamba_FETrack", "AI": {"tldr": "Proposes Mamba-FETrack V2, an efficient RGB-Event object tracking framework using Vision Mamba for low-complexity, high-performance cross-modal fusion.", "motivation": "Addresses the computational overhead and limited cross-modal interaction in existing RGB-Event tracking methods reliant on high-complexity Vision Transformers.", "method": "Uses a lightweight Prompt Generator and Vision Mamba-based FEMamba backbone for dynamic prompt-guided feature extraction, interaction, and fusion.", "result": "Demonstrates superior performance on benchmarks like COESOT, FE108, and FELT V2.", "conclusion": "Mamba-FETrack V2 offers an efficient, high-performance solution for RGB-Event object tracking."}}
{"id": "2506.23542", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23542", "abs": "https://arxiv.org/abs/2506.23542", "authors": ["Weida Wang", "Changyong He", "Jin Zeng", "Di Qiu"], "title": "Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric Attention", "comment": "This paper has been accepted for publication at the International\n  Conference on Computer Vision (ICCV) 2025", "summary": "Depth images captured by Time-of-Flight (ToF) sensors are prone to noise,\nrequiring denoising for reliable downstream applications. Previous works either\nfocus on single-frame processing, or perform multi-frame processing without\nconsidering depth variations at corresponding pixels across frames, leading to\nundesirable temporal inconsistency and spatial ambiguity. In this paper, we\npropose a novel ToF depth denoising network leveraging motion-invariant graph\nfusion to simultaneously enhance temporal stability and spatial sharpness.\nSpecifically, despite depth shifts across frames, graph structures exhibit\ntemporal self-similarity, enabling cross-frame geometric attention for graph\nfusion. Then, by incorporating an image smoothness prior on the fused graph and\ndata fidelity term derived from ToF noise distribution, we formulate a maximum\na posterior problem for ToF denoising. Finally, the solution is unrolled into\niterative filters whose weights are adaptively learned from the graph-informed\ngeometric attention, producing a high-performance yet interpretable network.\nExperimental results demonstrate that the proposed scheme achieves\nstate-of-the-art performance in terms of accuracy and consistency on synthetic\nDVToF dataset and exhibits robust generalization on the real Kinectv2 dataset.\nSource code will be released at\n\\href{https://github.com/davidweidawang/GIGA-ToF}{https://github.com/davidweidawang/GIGA-ToF}.", "AI": {"tldr": "A novel ToF depth denoising network using motion-invariant graph fusion improves temporal stability and spatial sharpness by leveraging cross-frame geometric attention and a maximum a posteriori formulation.", "motivation": "Depth images from ToF sensors are noisy, and existing methods either ignore multi-frame depth variations or lack temporal consistency, leading to poor results.", "method": "Proposes a network using motion-invariant graph fusion, geometric attention, and a maximum a posteriori problem for denoising, unrolled into iterative filters.", "result": "Achieves state-of-the-art performance on synthetic DVToF and real Kinectv2 datasets, with robust generalization.", "conclusion": "The method effectively denoises ToF depth images while maintaining temporal and spatial quality, with interpretable and adaptive learning."}}
{"id": "2506.23881", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23881", "abs": "https://arxiv.org/abs/2506.23881", "authors": ["Reihaneh Zohrabi", "Hosein Hasani", "Mahdieh Soleymani Baghshah", "Anna Rohrbach", "Marcus Rohrbach", "Mohammad Hossein Rohban"], "title": "Spurious-Aware Prototype Refinement for Reliable Out-of-Distribution Detection", "comment": null, "summary": "Out-of-distribution (OOD) detection is crucial for ensuring the reliability\nand safety of machine learning models in real-world applications, where they\nfrequently face data distributions unseen during training. Despite progress,\nexisting methods are often vulnerable to spurious correlations that mislead\nmodels and compromise robustness. To address this, we propose SPROD, a novel\nprototype-based OOD detection approach that explicitly addresses the challenge\nposed by unknown spurious correlations. Our post-hoc method refines class\nprototypes to mitigate bias from spurious features without additional data or\nhyperparameter tuning, and is broadly applicable across diverse backbones and\nOOD detection settings. We conduct a comprehensive spurious correlation OOD\ndetection benchmarking, comparing our method against existing approaches and\ndemonstrating its superior performance across challenging OOD datasets, such as\nCelebA, Waterbirds, UrbanCars, Spurious Imagenet, and the newly introduced\nAnimals MetaCoCo. On average, SPROD improves AUROC by 4.7% and FPR@95 by 9.3%\nover the second best.", "AI": {"tldr": "SPROD is a prototype-based OOD detection method that mitigates spurious correlations, improving robustness without extra data or tuning.", "motivation": "Existing OOD detection methods are vulnerable to spurious correlations, compromising reliability in real-world applications.", "method": "SPROD refines class prototypes post-hoc to reduce bias from spurious features, applicable across diverse models and settings.", "result": "SPROD outperforms existing methods, improving AUROC by 4.7% and FPR@95 by 9.3% on challenging datasets.", "conclusion": "SPROD effectively addresses spurious correlation challenges in OOD detection, enhancing model reliability."}}
{"id": "2506.23543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23543", "abs": "https://arxiv.org/abs/2506.23543", "authors": ["Hui Li", "Baoyou Chen", "Liwei Zhang", "Jiaye Li", "Jingdong Wang", "Siyu Zhu"], "title": "Pyramidal Patchification Flow for Visual Generation", "comment": "10 pages, 9figures", "summary": "Diffusion transformers (DiTs) adopt Patchify, mapping patch representations\nto token representations through linear projections, to adjust the number of\ntokens input to DiT blocks and thus the computation cost. Instead of a single\npatch size for all the timesteps, we introduce a Pyramidal Patchification Flow\n(PPFlow) approach: Large patch sizes are used for high noise timesteps and\nsmall patch sizes for low noise timesteps; Linear projections are learned for\neach patch size; and Unpatchify is accordingly modified. Unlike Pyramidal Flow,\nour approach operates over full latent representations other than pyramid\nrepresentations, and adopts the normal denoising process without requiring the\nrenoising trick. We demonstrate the effectiveness of our approach through two\ntraining manners. Training from scratch achieves a $1.6\\times$ ($2.0\\times$)\ninference speed over SiT-B/2 for 2-level (3-level) pyramid patchification with\nslightly lower training FLOPs and similar image generation performance.\nTraining from pretrained normal DiTs achieves even better performance with\nsmall training time. The code and checkpoint are at\nhttps://github.com/fudan-generative-vision/PPFlow.", "AI": {"tldr": "PPFlow introduces a Pyramidal Patchification Flow for Diffusion Transformers (DiTs), varying patch sizes by noise timestep to optimize computation cost and performance.", "motivation": "To improve efficiency and performance of DiTs by adapting patch sizes dynamically based on noise levels, avoiding the need for renoising tricks.", "method": "Uses PPFlow with large patches for high noise and small patches for low noise, learning linear projections for each size and modifying Unpatchify. Operates on full latent representations.", "result": "Achieves 1.6\u00d7-2.0\u00d7 faster inference than SiT-B/2 with similar performance. Pretrained DiTs show even better results with minimal training time.", "conclusion": "PPFlow effectively balances computation cost and performance, offering a scalable solution for DiTs."}}
{"id": "2506.23547", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23547", "abs": "https://arxiv.org/abs/2506.23547", "authors": ["Jiwon Kim", "Soohyun Hwang", "Dong-O Kim", "Changsu Han", "Min Kyu Park", "Chang-Su Kim"], "title": "Oneta: Multi-Style Image Enhancement Using Eigentransformation Functions", "comment": null, "summary": "The first algorithm, called Oneta, for a novel task of multi-style image\nenhancement is proposed in this work. Oneta uses two point operators\nsequentially: intensity enhancement with a transformation function (TF) and\ncolor correction with a color correction matrix (CCM). This two-step\nenhancement model, though simple, achieves a high performance upper bound.\nAlso, we introduce eigentransformation function (eigenTF) to represent TF\ncompactly. The Oneta network comprises Y-Net and C-Net to predict eigenTF and\nCCM parameters, respectively. To support $K$ styles, Oneta employs $K$\nlearnable tokens. During training, each style token is learned using image\npairs from the corresponding dataset. In testing, Oneta selects one of the $K$\nstyle tokens to enhance an image accordingly. Extensive experiments show that\nthe single Oneta network can effectively undertake six enhancement tasks --\nretouching, image signal processing, low-light image enhancement, dehazing,\nunderwater image enhancement, and white balancing -- across 30 datasets.", "AI": {"tldr": "Oneta is a novel multi-style image enhancement algorithm using two sequential operators (TF and CCM) and achieves high performance. It employs Y-Net and C-Net for parameter prediction and supports K styles via learnable tokens.", "motivation": "To address the need for a versatile and high-performing multi-style image enhancement solution capable of handling diverse tasks like retouching, dehazing, and white balancing.", "method": "Oneta uses intensity enhancement (TF) and color correction (CCM) sequentially. It introduces eigenTF for compact TF representation and employs Y-Net and C-Net for parameter prediction. K learnable tokens enable multi-style support.", "result": "Oneta effectively performs six enhancement tasks across 30 datasets, demonstrating versatility and high performance.", "conclusion": "Oneta is a simple yet powerful solution for multi-style image enhancement, achieving strong results across diverse tasks."}}
{"id": "2506.23552", "categories": ["cs.CV", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23552", "abs": "https://arxiv.org/abs/2506.23552", "authors": ["Mingi Kwon", "Joonghyuk Shin", "Jaeseok Jung", "Jaesik Park", "Youngjung Uh"], "title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching", "comment": "project page: https://joonghyuk.com/jamflow-web Under review.\n  Preprint published on arXiv", "summary": "The intrinsic link between facial motion and speech is often overlooked in\ngenerative modeling, where talking head synthesis and text-to-speech (TTS) are\ntypically addressed as separate tasks. This paper introduces JAM-Flow, a\nunified framework to simultaneously synthesize and condition on both facial\nmotion and speech. Our approach leverages flow matching and a novel Multi-Modal\nDiffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT\nand Audio-DiT modules. These are coupled via selective joint attention layers\nand incorporate key architectural choices, such as temporally aligned\npositional embeddings and localized joint attention masking, to enable\neffective cross-modal interaction while preserving modality-specific strengths.\nTrained with an inpainting-style objective, JAM-Flow supports a wide array of\nconditioning inputs-including text, reference audio, and reference\nmotion-facilitating tasks such as synchronized talking head generation from\ntext, audio-driven animation, and much more, within a single, coherent model.\nJAM-Flow significantly advances multi-modal generative modeling by providing a\npractical solution for holistic audio-visual synthesis. project page:\nhttps://joonghyuk.com/jamflow-web", "AI": {"tldr": "JAM-Flow is a unified framework for synthesizing facial motion and speech together, using flow matching and a Multi-Modal Diffusion Transformer (MM-DiT) architecture.", "motivation": "Current generative models treat talking head synthesis and text-to-speech as separate tasks, missing their intrinsic link. JAM-Flow aims to bridge this gap.", "method": "Uses flow matching and MM-DiT with Motion-DiT and Audio-DiT modules, coupled via selective joint attention layers and temporally aligned embeddings.", "result": "Enables tasks like synchronized talking head generation from text and audio-driven animation in a single model.", "conclusion": "JAM-Flow advances multi-modal generative modeling by unifying audio-visual synthesis."}}
{"id": "2506.23555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23555", "abs": "https://arxiv.org/abs/2506.23555", "authors": ["Fan Xie", "Pan Cao"], "title": "LH2Face: Loss function for Hard High-quality Face", "comment": null, "summary": "In current practical face authentication systems, most face recognition (FR)\nalgorithms are based on cosine similarity with softmax classification. Despite\nits reliable classification performance, this method struggles with hard\nsamples. A popular strategy to improve FR performance is incorporating angular\nor cosine margins. However, it does not take face quality or recognition\nhardness into account, simply increasing the margin value and thus causing an\noverly uniform training strategy. To address this problem, a novel loss\nfunction is proposed, named Loss function for Hard High-quality Face (LH2Face).\nFirstly, a similarity measure based on the von Mises-Fisher (vMF) distribution\nis stated, specifically focusing on the logarithm of the Probability Density\nFunction (PDF), which represents the distance between a probability\ndistribution and a vector. Then, an adaptive margin-based multi-classification\nmethod using softmax, called the Uncertainty-Aware Margin Function, is\nimplemented in the article. Furthermore, proxy-based loss functions are used to\napply extra constraints between the proxy and sample to optimize their\nrepresentation space distribution. Finally, a renderer is constructed that\noptimizes FR through face reconstruction and vice versa. Our LH2Face is\nsuperior to similiar schemes on hard high-quality face datasets, achieving\n49.39% accuracy on the IJB-B dataset, which surpasses the second-place method\nby 2.37%.", "AI": {"tldr": "The paper introduces LH2Face, a novel loss function for face recognition that addresses hard samples by incorporating adaptive margins and face quality awareness, outperforming existing methods.", "motivation": "Current face recognition systems using cosine similarity and softmax struggle with hard samples and lack consideration for face quality or recognition hardness, leading to overly uniform training strategies.", "method": "The proposed LH2Face uses a vMF-based similarity measure, an adaptive margin function, proxy-based constraints, and a face reconstruction renderer to optimize performance.", "result": "LH2Face achieves 49.39% accuracy on IJB-B, surpassing the second-best method by 2.37%.", "conclusion": "LH2Face effectively improves face recognition performance on hard high-quality datasets by addressing key limitations of existing methods."}}
{"id": "2506.23565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23565", "abs": "https://arxiv.org/abs/2506.23565", "authors": ["Mingqian Ji", "Jian Yang", "Shanshan Zhang"], "title": "OcRFDet: Object-Centric Radiance Fields for Multi-View 3D Object Detection in Autonomous Driving", "comment": "Accepted by ICCV2025", "summary": "Current multi-view 3D object detection methods typically transfer 2D features\ninto 3D space using depth estimation or 3D position encoder, but in a fully\ndata-driven and implicit manner, which limits the detection performance.\nInspired by the success of radiance fields on 3D reconstruction, we assume they\ncan be used to enhance the detector's ability of 3D geometry estimation.\nHowever, we observe a decline in detection performance, when we directly use\nthem for 3D rendering as an auxiliary task. From our analysis, we find the\nperformance drop is caused by the strong responses on the background when\nrendering the whole scene. To address this problem, we propose object-centric\nradiance fields, focusing on modeling foreground objects while discarding\nbackground noises. Specifically, we employ Object-centric Radiance Fields\n(OcRF) to enhance 3D voxel features via an auxiliary task of rendering\nforeground objects. We further use opacity - the side-product of rendering- to\nenhance the 2D foreground BEV features via Height-aware Opacity-based Attention\n(HOA), where attention maps at different height levels are generated separately\nvia multiple networks in parallel. Extensive experiments on the nuScenes\nvalidation and test datasets demonstrate that our OcRFDet achieves superior\nperformance, outperforming previous state-of-the-art methods with 57.2$\\%$ mAP\nand 64.8$\\%$ NDS on the nuScenes test benchmark. Code will be available at\nhttps://github.com/Mingqj/OcRFDet.", "AI": {"tldr": "The paper proposes Object-centric Radiance Fields (OcRF) to improve 3D object detection by focusing on foreground objects and avoiding background noise, achieving state-of-the-art performance.", "motivation": "Current methods for multi-view 3D object detection rely on implicit data-driven approaches, limiting performance. Radiance fields, successful in 3D reconstruction, are explored but degrade detection when used directly due to background interference.", "method": "The authors introduce OcRF to model foreground objects, discarding background noise. They enhance 3D voxel features via rendering foreground objects and use opacity for Height-aware Opacity-based Attention (HOA) to improve 2D BEV features.", "result": "OcRFDet achieves 57.2% mAP and 64.8% NDS on nuScenes test benchmark, outperforming prior methods.", "conclusion": "Focusing on foreground objects with OcRF and HOA significantly improves 3D object detection performance."}}
{"id": "2506.23575", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23575", "abs": "https://arxiv.org/abs/2506.23575", "authors": ["Nuo Chen", "Chao Xiao", "Yimian Dai", "Shiman He", "Miao Li", "Wei An"], "title": "Event-based Tiny Object Detection: A Benchmark Dataset and Baseline", "comment": null, "summary": "Small object detection (SOD) in anti-UAV task is a challenging problem due to\nthe small size of UAVs and complex backgrounds. Traditional frame-based cameras\nstruggle to detect small objects in complex environments due to their low frame\nrates, limited dynamic range, and data redundancy. Event cameras, with\nmicrosecond temporal resolution and high dynamic range, provide a more\neffective solution for SOD. However, existing event-based object detection\ndatasets are limited in scale, feature large targets size, and lack diverse\nbackgrounds, making them unsuitable for SOD benchmarks. In this paper, we\nintroduce a Event-based Small object detection (EVSOD) dataset (namely EV-UAV),\nthe first large-scale, highly diverse benchmark for anti-UAV tasks. It includes\n147 sequences with over 2.3 million event-level annotations, featuring\nextremely small targets (averaging 6.8 $\\times$ 5.4 pixels) and diverse\nscenarios such as urban clutter and extreme lighting conditions. Furthermore,\nbased on the observation that small moving targets form continuous curves in\nspatiotemporal event point clouds, we propose Event based Sparse Segmentation\nNetwork (EV-SpSegNet), a novel baseline for event segmentation in point cloud\nspace, along with a Spatiotemporal Correlation (STC) loss that leverages motion\ncontinuity to guide the network in retaining target events. Extensive\nexperiments on the EV-UAV dataset demonstrate the superiority of our method and\nprovide a benchmark for future research in EVSOD. The dataset and code are at\nhttps://github.com/ChenYichen9527/Ev-UAV.", "AI": {"tldr": "The paper introduces EV-UAV, a large-scale event-based dataset for small object detection (SOD) in anti-UAV tasks, and proposes EV-SpSegNet, a novel baseline method with a Spatiotemporal Correlation loss for improved detection.", "motivation": "Traditional cameras struggle with SOD due to low frame rates and data redundancy, while existing event-based datasets lack scale and diversity for SOD benchmarks.", "method": "The authors introduce the EV-UAV dataset and propose EV-SpSegNet, a network for event segmentation in point cloud space, using a Spatiotemporal Correlation loss to leverage motion continuity.", "result": "EV-SpSegNet outperforms existing methods on the EV-UAV dataset, which includes 147 sequences with 2.3 million annotations and small targets (6.8\u00d75.4 pixels).", "conclusion": "The EV-UAV dataset and EV-SpSegNet provide a robust benchmark and method for future research in event-based small object detection."}}
{"id": "2506.23577", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23577", "abs": "https://arxiv.org/abs/2506.23577", "authors": ["Yanning Hou", "Yanran Ruan", "Junfa Li", "Shanshan Wang", "Jianfeng Qiu", "Ke Xu"], "title": "StackCLIP: Clustering-Driven Stacked Prompt in Zero-Shot Industrial Anomaly Detection", "comment": null, "summary": "Enhancing the alignment between text and image features in the CLIP model is\na critical challenge in zero-shot industrial anomaly detection tasks. Recent\nstudies predominantly utilize specific category prompts during pretraining,\nwhich can cause overfitting to the training categories and limit model\ngeneralization. To address this, we propose a method that transforms category\nnames through multicategory name stacking to create stacked prompts, forming\nthe basis of our StackCLIP model. Our approach introduces two key components.\nThe Clustering-Driven Stacked Prompts (CSP) module constructs generic prompts\nby stacking semantically analogous categories, while utilizing multi-object\ntextual feature fusion to amplify discriminative anomalies among similar\nobjects. The Ensemble Feature Alignment (EFA) module trains knowledge-specific\nlinear layers tailored for each stack cluster and adaptively integrates them\nbased on the attributes of test categories. These modules work together to\ndeliver superior training speed, stability, and convergence, significantly\nboosting anomaly segmentation performance. Additionally, our stacked prompt\nframework offers robust generalization across classification tasks. To further\nimprove performance, we introduce the Regulating Prompt Learning (RPL) module,\nwhich leverages the generalization power of stacked prompts to refine prompt\nlearning, elevating results in anomaly detection classification tasks.\nExtensive testing on seven industrial anomaly detection datasets demonstrates\nthat our method achieves state-of-the-art performance in both zero-shot anomaly\ndetection and segmentation tasks.", "AI": {"tldr": "The paper proposes StackCLIP, a method using stacked prompts to enhance CLIP's text-image alignment for zero-shot industrial anomaly detection, improving generalization and performance.", "motivation": "Existing methods overfit to training categories, limiting generalization. StackCLIP addresses this by transforming category names into stacked prompts for better alignment.", "method": "StackCLIP introduces Clustering-Driven Stacked Prompts (CSP) and Ensemble Feature Alignment (EFA) modules, along with Regulating Prompt Learning (RPL), to improve anomaly detection.", "result": "The method achieves state-of-the-art performance in zero-shot anomaly detection and segmentation across seven datasets.", "conclusion": "StackCLIP enhances generalization and performance in industrial anomaly detection, offering superior training speed and stability."}}
{"id": "2506.23580", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23580", "abs": "https://arxiv.org/abs/2506.23580", "authors": ["Yawen Zou", "Guang Li", "Duo Su", "Zi Wang", "Jun Yu", "Chao Zhang"], "title": "Dataset Distillation via Vision-Language Category Prototype", "comment": "accepted by ICCV2025", "summary": "Dataset distillation (DD) condenses large datasets into compact yet\ninformative substitutes, preserving performance comparable to the original\ndataset while reducing storage, transmission costs, and computational\nconsumption. However, previous DD methods mainly focus on distilling\ninformation from images, often overlooking the semantic information inherent in\nthe data. The disregard for context hinders the model's generalization ability,\nparticularly in tasks involving complex datasets, which may result in illogical\noutputs or the omission of critical objects. In this study, we integrate\nvision-language methods into DD by introducing text prototypes to distill\nlanguage information and collaboratively synthesize data with image prototypes,\nthereby enhancing dataset distillation performance. Notably, the text\nprototypes utilized in this study are derived from descriptive text information\ngenerated by an open-source large language model. This framework demonstrates\nbroad applicability across datasets without pre-existing text descriptions,\nexpanding the potential of dataset distillation beyond traditional image-based\napproaches. Compared to other methods, the proposed approach generates\nlogically coherent images containing target objects, achieving state-of-the-art\nvalidation performance and demonstrating robust generalization. Source code and\ngenerated data are available in\nhttps://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/", "AI": {"tldr": "The paper introduces a vision-language method for dataset distillation, enhancing performance by incorporating text prototypes derived from a large language model, improving generalization and logical coherence in outputs.", "motivation": "Existing dataset distillation methods focus on images and overlook semantic context, limiting generalization and causing illogical outputs.", "method": "Integrates vision-language methods, using text prototypes (from a large language model) alongside image prototypes to collaboratively synthesize data.", "result": "Achieves state-of-the-art validation performance, generating logically coherent images with target objects and robust generalization.", "conclusion": "The proposed framework expands dataset distillation beyond image-based approaches, demonstrating broad applicability and improved performance."}}
{"id": "2506.23590", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23590", "abs": "https://arxiv.org/abs/2506.23590", "authors": ["Qiming Li", "Zekai Ye", "Xiaocheng Feng", "Weihong Zhong", "Libo Qin", "Ruihan Chen", "Baohang Li", "Kui Jiang", "Yaowei Wang", "Ting Liu", "Bing Qin"], "title": "CAI: Caption-Sensitive Attention Intervention for Mitigating Object Hallucination in Large Vision-Language Models", "comment": null, "summary": "Although Large Vision-Language Models (LVLMs) have demonstrated powerful\ncapabilities in interpreting visual information, they frequently produce\ncontent that deviates from visual information, leading to object hallucination.\nTo tackle this, recent works mostly depend on expensive manual annotations and\ntraining cost, or significantly increase inference time. In this work, we\nobserve that LVLMs' attention to visual information is significantly stronger\nwhen answering caption queries compared to non-caption queries. Inspired by\nthis phenomenon, we propose Caption-sensitive Attention Intervention (CAI), a\ntraining-free, plug-and-play hallucination mitigation method that leverages the\nattention activation pattern in response to caption queries to enhance LVLMs'\nvisual perception capability. Extensive experimental results across four\nbenchmarks covering both discriminative and generative tasks, demonstrate that\nCAI achieves state-of-the-art (SOTA) hallucination mitigating performance only\nwith minimal additional inference cost.", "AI": {"tldr": "A training-free method, Caption-sensitive Attention Intervention (CAI), mitigates object hallucination in LVLMs by leveraging attention patterns from caption queries, achieving SOTA performance with minimal inference cost.", "motivation": "LVLMs often produce inaccurate visual interpretations (object hallucination). Existing solutions are costly or slow. The paper addresses this by exploiting LVLMs' stronger attention to caption queries.", "method": "Proposes CAI, a plug-and-play method that uses attention activation patterns from caption queries to enhance visual perception without training or significant inference overhead.", "result": "CAI achieves state-of-the-art hallucination mitigation across four benchmarks for discriminative and generative tasks, with minimal added inference cost.", "conclusion": "CAI effectively reduces object hallucination in LVLMs by leveraging attention patterns, offering a cost-efficient and scalable solution."}}
{"id": "2506.23605", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23605", "abs": "https://arxiv.org/abs/2506.23605", "authors": ["Suyash Maniyar", "Vishvesh Trivedi", "Ajoy Mondal", "Anand Mishra", "C. V. Jawahar"], "title": "AI-Generated Lecture Slides for Improving Slide Element Detection and Retrieval", "comment": "40 pages including supplementary, accepted at ICDAR 2025", "summary": "Lecture slide element detection and retrieval are key problems in slide\nunderstanding. Training effective models for these tasks often depends on\nextensive manual annotation. However, annotating large volumes of lecture\nslides for supervised training is labor intensive and requires domain\nexpertise. To address this, we propose a large language model (LLM)-guided\nsynthetic lecture slide generation pipeline, SynLecSlideGen, which produces\nhigh-quality, coherent and realistic slides. We also create an evaluation\nbenchmark, namely RealSlide by manually annotating 1,050 real lecture slides.\nTo assess the utility of our synthetic slides, we perform few-shot transfer\nlearning on real data using models pre-trained on them. Experimental results\nshow that few-shot transfer learning with pretraining on synthetic slides\nsignificantly improves performance compared to training only on real data. This\ndemonstrates that synthetic data can effectively compensate for limited labeled\nlecture slides. The code and resources of our work are publicly available on\nour project website: https://synslidegen.github.io/.", "AI": {"tldr": "A pipeline (SynLecSlideGen) generates synthetic lecture slides using LLMs to reduce manual annotation. Few-shot transfer learning with synthetic data improves performance on real slides.", "motivation": "Manual annotation of lecture slides is labor-intensive and requires expertise. Synthetic data can address this limitation.", "method": "Proposed SynLecSlideGen, an LLM-guided pipeline for synthetic slide generation, and evaluated using RealSlide (1,050 annotated slides). Few-shot transfer learning tested utility.", "result": "Pretraining on synthetic slides significantly boosts performance compared to training only on real data.", "conclusion": "Synthetic data effectively compensates for limited labeled lecture slides, reducing annotation effort."}}
{"id": "2506.23606", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23606", "abs": "https://arxiv.org/abs/2506.23606", "authors": ["Zhengkang Xiang", "Zizhao Li", "Amir Khodabandeh", "Kourosh Khoshelham"], "title": "SG-LDM: Semantic-Guided LiDAR Generation via Latent-Aligned Diffusion", "comment": null, "summary": "Lidar point cloud synthesis based on generative models offers a promising\nsolution to augment deep learning pipelines, particularly when real-world data\nis scarce or lacks diversity. By enabling flexible object manipulation, this\nsynthesis approach can significantly enrich training datasets and enhance\ndiscriminative models. However, existing methods focus on unconditional lidar\npoint cloud generation, overlooking their potential for real-world\napplications. In this paper, we propose SG-LDM, a Semantic-Guided Lidar\nDiffusion Model that employs latent alignment to enable robust\nsemantic-to-lidar synthesis. By directly operating in the native lidar space\nand leveraging explicit semantic conditioning, SG-LDM achieves state-of-the-art\nperformance in generating high-fidelity lidar point clouds guided by semantic\nlabels. Moreover, we propose the first diffusion-based lidar translation\nframework based on SG-LDM, which enables cross-domain translation as a domain\nadaptation strategy to enhance downstream perception performance. Systematic\nexperiments demonstrate that SG-LDM significantly outperforms existing lidar\ndiffusion models and the proposed lidar translation framework further improves\ndata augmentation performance in the downstream lidar segmentation task.", "AI": {"tldr": "SG-LDM is a semantic-guided lidar diffusion model for high-fidelity point cloud synthesis, outperforming existing methods and enhancing downstream tasks like segmentation.", "motivation": "Real-world lidar data is often scarce or lacks diversity, limiting deep learning pipelines. Existing methods overlook semantic guidance for practical applications.", "method": "SG-LDM uses latent alignment and explicit semantic conditioning to generate lidar point clouds. It also introduces a diffusion-based lidar translation framework for cross-domain adaptation.", "result": "SG-LDM achieves state-of-the-art performance in lidar synthesis and improves downstream segmentation via the proposed translation framework.", "conclusion": "SG-LDM advances lidar synthesis with semantic guidance and domain adaptation, proving effective for enhancing perception tasks."}}
{"id": "2506.23607", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23607", "abs": "https://arxiv.org/abs/2506.23607", "authors": ["Shiqi Zhang", "Sha Zhang", "Jiajun Deng", "Yedong Shen", "Mingxiao MA", "Yanyong Zhang"], "title": "PGOV3D: Open-Vocabulary 3D Semantic Segmentation with Partial-to-Global Curriculum", "comment": null, "summary": "Existing open-vocabulary 3D semantic segmentation methods typically supervise\n3D segmentation models by merging text-aligned features (e.g., CLIP) extracted\nfrom multi-view images onto 3D points. However, such approaches treat\nmulti-view images merely as intermediaries for transferring open-vocabulary\ninformation, overlooking their rich semantic content and cross-view\ncorrespondences, which limits model effectiveness. To address this, we propose\nPGOV3D, a novel framework that introduces a Partial-to-Global curriculum for\nimproving open-vocabulary 3D semantic segmentation. The key innovation lies in\na two-stage training strategy. In the first stage, we pre-train the model on\npartial scenes that provide dense semantic information but relatively simple\ngeometry. These partial point clouds are derived from multi-view RGB-D inputs\nvia pixel-wise depth projection. To enable open-vocabulary learning, we\nleverage a multi-modal large language model (MLLM) and a 2D segmentation\nfoundation model to generate open-vocabulary labels for each viewpoint,\noffering rich and aligned supervision. An auxiliary inter-frame consistency\nmodule is introduced to enforce feature consistency across varying viewpoints\nand enhance spatial understanding. In the second stage, we fine-tune the model\non complete scene-level point clouds, which are sparser and structurally more\ncomplex. We aggregate the partial vocabularies associated with each scene and\ngenerate pseudo labels using the pre-trained model, effectively bridging the\nsemantic gap between dense partial observations and large-scale 3D\nenvironments. Extensive experiments on ScanNet, ScanNet200, and S3DIS\nbenchmarks demonstrate that PGOV3D achieves competitive performance in\nopen-vocabulary 3D semantic segmentation.", "AI": {"tldr": "PGOV3D introduces a two-stage Partial-to-Global curriculum for open-vocabulary 3D semantic segmentation, leveraging multi-view images and MLLMs for improved performance.", "motivation": "Existing methods overlook rich semantic content and cross-view correspondences in multi-view images, limiting effectiveness.", "method": "Two-stage training: pre-training on dense partial scenes with MLLM-generated labels, then fine-tuning on complete scenes with pseudo labels.", "result": "Competitive performance on ScanNet, ScanNet200, and S3DIS benchmarks.", "conclusion": "PGOV3D effectively bridges the semantic gap between partial and complete 3D scenes, enhancing open-vocabulary segmentation."}}
{"id": "2506.23611", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23611", "abs": "https://arxiv.org/abs/2506.23611", "authors": ["Ziao Liu", "Zhenjia Li", "Yifeng Shi", "Xiangang Li"], "title": "AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention", "comment": null, "summary": "3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance\nFields (NeRF), excelling in complex scene reconstruction and efficient\nrendering. However, it relies on high-quality point clouds from\nStructure-from-Motion (SfM), limiting its applicability. SfM also fails in\ntexture-deficient or constrained-view scenarios, causing severe degradation in\n3DGS reconstruction. To address this limitation, we propose AttentionGS, a\nnovel framework that eliminates the dependency on high-quality initial point\nclouds by leveraging structural attention for direct 3D reconstruction from\nrandomly initialization. In the early training stage, we introduce geometric\nattention to rapidly recover the global scene structure. As training\nprogresses, we incorporate texture attention to refine fine-grained details and\nenhance rendering quality. Furthermore, we employ opacity-weighted gradients to\nguide Gaussian densification, leading to improved surface reconstruction.\nExtensive experiments on multiple benchmark datasets demonstrate that\nAttentionGS significantly outperforms state-of-the-art methods, particularly in\nscenarios where point cloud initialization is unreliable. Our approach paves\nthe way for more robust and flexible 3D Gaussian Splatting in real-world\napplications.", "AI": {"tldr": "AttentionGS eliminates the need for high-quality initial point clouds in 3D Gaussian Splatting (3DGS) by using structural attention for direct 3D reconstruction from random initialization, improving robustness in texture-deficient or constrained-view scenarios.", "motivation": "3DGS relies on high-quality point clouds from SfM, which fails in texture-deficient or constrained-view scenarios, limiting its applicability.", "method": "AttentionGS uses geometric attention early in training to recover global structure and texture attention later to refine details. It also employs opacity-weighted gradients for better Gaussian densification.", "result": "AttentionGS outperforms state-of-the-art methods, especially when point cloud initialization is unreliable.", "conclusion": "AttentionGS enables more robust and flexible 3DGS, expanding its real-world applicability."}}
{"id": "2506.23618", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23618", "abs": "https://arxiv.org/abs/2506.23618", "authors": ["Zhongdao Wang", "Guodongfang Zhao", "Jingjing Ren", "Bailan Feng", "Shifeng Zhang", "Wenbo Li"], "title": "TurboVSR: Fantastic Video Upscalers and Where to Find Them", "comment": "ICCV, 2025", "summary": "Diffusion-based generative models have demonstrated exceptional promise in\nthe video super-resolution (VSR) task, achieving a substantial advancement in\ndetail generation relative to prior methods. However, these approaches face\nsignificant computational efficiency challenges. For instance, current\ntechniques may require tens of minutes to super-resolve a mere 2-second, 1080p\nvideo. In this paper, we present TurboVSR, an ultra-efficient diffusion-based\nvideo super-resolution model. Our core design comprises three key aspects: (1)\nWe employ an autoencoder with a high compression ratio of 32$\\times$32$\\times$8\nto reduce the number of tokens. (2) Highly compressed latents pose substantial\nchallenges for training. We introduce factorized conditioning to mitigate the\nlearning complexity: we first learn to super-resolve the initial frame;\nsubsequently, we condition the super-resolution of the remaining frames on the\nhigh-resolution initial frame and the low-resolution subsequent frames. (3) We\nconvert the pre-trained diffusion model to a shortcut model to enable fewer\nsampling steps, further accelerating inference. As a result, TurboVSR performs\non par with state-of-the-art VSR methods, while being 100+ times faster, taking\nonly 7 seconds to process a 2-second long 1080p video. TurboVSR also supports\nimage resolution by considering image as a one-frame video. Our efficient\ndesign makes SR beyond 1080p possible, results on 4K (3648$\\times$2048) image\nSR show surprising fine details.", "AI": {"tldr": "TurboVSR is an ultra-efficient diffusion-based video super-resolution model that achieves state-of-the-art performance while being 100+ times faster than existing methods.", "motivation": "Current diffusion-based VSR methods are computationally inefficient, taking too long to process videos. TurboVSR aims to address this by improving efficiency without sacrificing quality.", "method": "TurboVSR uses a high-compression autoencoder, factorized conditioning for training, and converts a pre-trained diffusion model into a shortcut model for faster inference.", "result": "TurboVSR matches state-of-the-art VSR quality while processing a 2-second 1080p video in just 7 seconds. It also supports 4K image super-resolution with fine details.", "conclusion": "TurboVSR successfully balances efficiency and performance, making high-quality video and image super-resolution practical for real-world applications."}}
{"id": "2506.23623", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23623", "abs": "https://arxiv.org/abs/2506.23623", "authors": ["Shaofei Huang", "Rui Ling", "Tianrui Hui", "Hongyu Li", "Xu Zhou", "Shifeng Zhang", "Si Liu", "Richang Hong", "Meng Wang"], "title": "Revisiting Audio-Visual Segmentation with Vision-Centric Transformer", "comment": "Accepted by CVPR 2025; Code: https://github.com/spyflying/VCT_AVS;\n  Models: https://huggingface.co/nowherespyfly/VCT_AVS", "summary": "Audio-Visual Segmentation (AVS) aims to segment sound-producing objects in\nvideo frames based on the associated audio signal. Prevailing AVS methods\ntypically adopt an audio-centric Transformer architecture, where object queries\nare derived from audio features. However, audio-centric Transformers suffer\nfrom two limitations: perception ambiguity caused by the mixed nature of audio,\nand weakened dense prediction ability due to visual detail loss. To address\nthese limitations, we propose a new Vision-Centric Transformer (VCT) framework\nthat leverages vision-derived queries to iteratively fetch corresponding audio\nand visual information, enabling queries to better distinguish between\ndifferent sounding objects from mixed audio and accurately delineate their\ncontours. Additionally, we also introduce a Prototype Prompted Query Generation\n(PPQG) module within our VCT framework to generate vision-derived queries that\nare both semantically aware and visually rich through audio prototype prompting\nand pixel context grouping, facilitating audio-visual information aggregation.\nExtensive experiments demonstrate that our VCT framework achieves new\nstate-of-the-art performances on three subsets of the AVSBench dataset. The\ncode is available at https://github.com/spyflying/VCT_AVS.", "AI": {"tldr": "The paper introduces a Vision-Centric Transformer (VCT) framework for Audio-Visual Segmentation (AVS), addressing limitations of audio-centric methods by using vision-derived queries and a PPQG module for improved performance.", "motivation": "Current AVS methods using audio-centric Transformers face issues like perception ambiguity and weakened dense prediction due to visual detail loss.", "method": "Proposes a VCT framework with vision-derived queries and a PPQG module to enhance audio-visual information aggregation.", "result": "Achieves state-of-the-art performance on three subsets of the AVSBench dataset.", "conclusion": "The VCT framework effectively improves AVS by leveraging vision-derived queries and audio-visual aggregation."}}
{"id": "2506.23630", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23630", "abs": "https://arxiv.org/abs/2506.23630", "authors": ["Lorenzo Olearo", "Giorgio Longari", "Alessandro Raganato", "Rafael Pe\u00f1aloza", "Simone Melzi"], "title": "Blending Concepts with Text-to-Image Diffusion Models", "comment": "Currently under review", "summary": "Diffusion models have dramatically advanced text-to-image generation in\nrecent years, translating abstract concepts into high-fidelity images with\nremarkable ease. In this work, we examine whether they can also blend distinct\nconcepts, ranging from concrete objects to intangible ideas, into coherent new\nvisual entities under a zero-shot framework. Specifically, concept blending\nmerges the key attributes of multiple concepts (expressed as textual prompts)\ninto a single, novel image that captures the essence of each concept. We\ninvestigate four blending methods, each exploiting different aspects of the\ndiffusion pipeline (e.g., prompt scheduling, embedding interpolation, or\nlayer-wise conditioning). Through systematic experimentation across diverse\nconcept categories, such as merging concrete concepts, synthesizing compound\nwords, transferring artistic styles, and blending architectural landmarks, we\nshow that modern diffusion models indeed exhibit creative blending capabilities\nwithout further training or fine-tuning. Our extensive user study, involving\n100 participants, reveals that no single approach dominates in all scenarios:\neach blending technique excels under certain conditions, with factors like\nprompt ordering, conceptual distance, and random seed affecting the outcome.\nThese findings highlight the remarkable compositional potential of diffusion\nmodels while exposing their sensitivity to seemingly minor input variations.", "AI": {"tldr": "Diffusion models can blend diverse concepts into coherent images without training, using various methods, but no single method works best in all cases.", "motivation": "To explore if diffusion models can blend distinct concepts into novel images under a zero-shot framework.", "method": "Four blending methods exploiting different aspects of the diffusion pipeline (e.g., prompt scheduling, embedding interpolation, layer-wise conditioning).", "result": "Diffusion models show creative blending capabilities, but outcomes vary based on method, prompt order, conceptual distance, and randomness.", "conclusion": "Diffusion models have strong compositional potential but are sensitive to input variations, with no single blending method universally superior."}}
{"id": "2506.23639", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23639", "abs": "https://arxiv.org/abs/2506.23639", "authors": ["Wanpeng Zhang", "Yicheng Feng", "Hao Luo", "Yijiang Li", "Zihao Yue", "Sipeng Zheng", "Zongqing Lu"], "title": "Unified Multimodal Understanding via Byte-Pair Visual Encoding", "comment": null, "summary": "Multimodal large language models (MLLMs) have made significant progress in\nvision-language understanding, yet effectively aligning different modalities\nremains a fundamental challenge. We present a framework that unifies multimodal\nunderstanding by applying byte-pair encoding to visual tokens. Unlike\nconventional approaches that rely on modality-specific encoders, our method\ndirectly incorporates structural information into visual tokens, mirroring\nsuccessful tokenization strategies in text-only language models. We introduce a\npriority-guided encoding scheme that considers both frequency and spatial\nconsistency, coupled with a multi-stage training procedure based on\ncurriculum-driven data composition. These enhancements enable the transformer\nmodel to better capture cross-modal relationships and reason with visual\ninformation. Comprehensive experiments demonstrate improved performance across\ndiverse vision-language tasks. By bridging the gap between visual and textual\nrepresentations, our approach contributes to the advancement of more capable\nand efficient multimodal foundation models.", "AI": {"tldr": "A framework for aligning modalities in MLLMs using byte-pair encoding for visual tokens, improving vision-language understanding.", "motivation": "Addressing the challenge of aligning different modalities in multimodal large language models (MLLMs) for better vision-language understanding.", "method": "Uses byte-pair encoding for visual tokens, priority-guided encoding (frequency and spatial consistency), and multi-stage training with curriculum-driven data.", "result": "Improved performance in diverse vision-language tasks by better capturing cross-modal relationships.", "conclusion": "The approach bridges visual and textual representations, advancing more capable and efficient multimodal foundation models."}}
{"id": "2506.23641", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23641", "abs": "https://arxiv.org/abs/2506.23641", "authors": ["Peng Huang", "Junhu Fu", "Bowen Guo", "Zeju Li", "Yuanyuan Wang", "Yi Guo"], "title": "VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation", "comment": null, "summary": "As the appearance of medical images is influenced by multiple underlying\nfactors, generative models require rich attribute information beyond labels to\nproduce realistic and diverse images. For instance, generating an image of skin\nlesion with specific patterns demands descriptions that go beyond diagnosis,\nsuch as shape, size, texture, and color. However, such detailed descriptions\nare not always accessible. To address this, we explore a framework, termed\nVisual Attribute Prompts (VAP)-Diffusion, to leverage external knowledge from\npre-trained Multi-modal Large Language Models (MLLMs) to improve the quality\nand diversity of medical image generation. First, to derive descriptions from\nMLLMs without hallucination, we design a series of prompts following\nChain-of-Thoughts for common medical imaging tasks, including dermatologic,\ncolorectal, and chest X-ray images. Generated descriptions are utilized during\ntraining and stored across different categories. During testing, descriptions\nare randomly retrieved from the corresponding category for inference. Moreover,\nto make the generator robust to unseen combination of descriptions at the test\ntime, we propose a Prototype Condition Mechanism that restricts test embeddings\nto be similar to those from training. Experiments on three common types of\nmedical imaging across four datasets verify the effectiveness of VAP-Diffusion.", "AI": {"tldr": "VAP-Diffusion leverages MLLMs to generate detailed descriptions for medical images, improving realism and diversity in generation.", "motivation": "Detailed attribute descriptions (e.g., shape, size) for medical images are often unavailable, limiting generative models.", "method": "Uses MLLMs with Chain-of-Thought prompts to derive descriptions, stores them by category, and employs a Prototype Condition Mechanism for robustness.", "result": "Effective across three medical imaging types in four datasets.", "conclusion": "VAP-Diffusion enhances medical image generation quality and diversity by integrating external knowledge."}}
{"id": "2506.23648", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23648", "abs": "https://arxiv.org/abs/2506.23648", "authors": ["Zhe Liu", "Yuhao Huang", "Lian Liu", "Chengrui Zhang", "Haotian Lin", "Tong Han", "Zhiyuan Zhu", "Yanlin Chen", "Yuerui Chen", "Dong Ni", "Zhongshan Gou", "Xin Yang"], "title": "MReg: A Novel Regression Model with MoE-based Video Feature Mining for Mitral Regurgitation Diagnosis", "comment": "10 pages, 5 figures, accepted by MICCAI 2025", "summary": "Color Doppler echocardiography is a crucial tool for diagnosing mitral\nregurgitation (MR). Recent studies have explored intelligent methods for MR\ndiagnosis to minimize user dependence and improve accuracy. However, these\napproaches often fail to align with clinical workflow and may lead to\nsuboptimal accuracy and interpretability. In this study, we introduce an\nautomated MR diagnosis model (MReg) developed on the 4-chamber cardiac color\nDoppler echocardiography video (A4C-CDV). It follows comprehensive feature\nmining strategies to detect MR and assess its severity, considering clinical\nrealities. Our contribution is threefold. First, we formulate the MR diagnosis\nas a regression task to capture the continuity and ordinal relationships\nbetween categories. Second, we design a feature selection and amplification\nmechanism to imitate the sonographer's diagnostic logic for accurate MR\ngrading. Third, inspired by the Mixture-of-Experts concept, we introduce a\nfeature summary module to extract the category-level features, enhancing the\nrepresentational capacity for more accurate grading. We trained and evaluated\nour proposed MReg on a large in-house A4C-CDV dataset comprising 1868 cases\nwith three graded regurgitation labels. Compared to other weakly supervised\nvideo anomaly detection and supervised classification methods, MReg\ndemonstrated superior performance in MR diagnosis. Our code is available at:\nhttps://github.com/cskdstz/MReg.", "AI": {"tldr": "An automated MR diagnosis model (MReg) using 4-chamber cardiac color Doppler echocardiography videos improves accuracy and interpretability by mimicking clinical workflows and leveraging feature mining strategies.", "motivation": "Current intelligent methods for MR diagnosis often misalign with clinical workflows, leading to suboptimal accuracy and interpretability.", "method": "MReg formulates MR diagnosis as a regression task, uses feature selection/amplification to mimic sonographer logic, and introduces a feature summary module for category-level feature extraction.", "result": "MReg outperforms other methods on a dataset of 1868 cases, demonstrating superior performance in MR diagnosis.", "conclusion": "MReg offers a clinically aligned, accurate, and interpretable solution for automated MR diagnosis."}}
{"id": "2506.23657", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23657", "abs": "https://arxiv.org/abs/2506.23657", "authors": ["Connor Daly", "Elettra Marconi", "Marco Riva", "Jinendra Ekanayake", "Daniel S. Elson", "Ferdinando Rodriguez y Baena"], "title": "Towards Markerless Intraoperative Tracking of Deformable Spine Tissue", "comment": "Preprint of paper, submitted", "summary": "Consumer-grade RGB-D imaging for intraoperative orthopedic tissue tracking is\na promising method with high translational potential. Unlike bone-mounted\ntracking devices, markerless tracking can reduce operating time and complexity.\nHowever, its use has been limited to cadaveric studies. This paper introduces\nthe first real-world clinical RGB-D dataset for spine surgery and develops\nSpineAlign, a system for capturing deformation between preoperative and\nintraoperative spine states. We also present an intraoperative segmentation\nnetwork trained on this data and introduce CorrespondNet, a multi-task\nframework for predicting key regions for registration in both intraoperative\nand preoperative scenes.", "AI": {"tldr": "The paper introduces SpineAlign, a system for tracking spine deformations in surgery using RGB-D imaging, along with a dataset and segmentation network.", "motivation": "To reduce operating time and complexity by replacing bone-mounted tracking devices with markerless RGB-D imaging.", "method": "Developed SpineAlign for deformation tracking, an intraoperative segmentation network, and CorrespondNet for key region prediction.", "result": "First real-world clinical RGB-D dataset for spine surgery and a functional system for deformation tracking.", "conclusion": "The work demonstrates the feasibility of markerless RGB-D tracking in spine surgery, with potential for broader clinical use."}}
{"id": "2506.23724", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23724", "abs": "https://arxiv.org/abs/2506.23724", "authors": ["Chang'an Yi", "Xiaohui Deng", "Guohao Chen", "Yan Zhou", "Qinghua Lu", "Shuaicheng Niu"], "title": "When Small Guides Large: Cross-Model Co-Learning for Test-Time Adaptation", "comment": "15 pages, 5 figures", "summary": "Test-time Adaptation (TTA) adapts a given model to testing domain data with\npotential domain shifts through online unsupervised learning, yielding\nimpressive performance. However, to date, existing TTA methods primarily focus\non single-model adaptation. In this work, we investigate an intriguing\nquestion: how does cross-model knowledge influence the TTA process? Our\nfindings reveal that, in TTA's unsupervised online setting, each model can\nprovide complementary, confident knowledge to the others, even when there are\nsubstantial differences in model size. For instance, a smaller model like\nMobileViT (10.6M parameters) can effectively guide a larger model like ViT-Base\n(86.6M parameters). In light of this, we propose COCA, a Cross-Model\nCo-Learning framework for TTA, which mainly consists of two main strategies. 1)\nCo-adaptation adaptively integrates complementary knowledge from other models\nthroughout the TTA process, reducing individual model biases. 2)\nSelf-adaptation enhances each model's unique strengths via unsupervised\nlearning, enabling diverse adaptation to the target domain. Extensive\nexperiments show that COCA, which can also serve as a plug-and-play module,\nsignificantly boosts existing SOTAs, on models with various sizes--including\nResNets, ViTs, and Mobile-ViTs--via cross-model co-learned TTA. For example,\nwith Mobile-ViT's guidance, COCA raises ViT-Base's average adaptation accuracy\non ImageNet-C from 51.7% to 64.5%. The code is publicly available at\nhttps://github.com/ycarobot/COCA.", "AI": {"tldr": "COCA introduces a Cross-Model Co-Learning framework for Test-time Adaptation (TTA), leveraging complementary knowledge across models to enhance performance, even with varying model sizes.", "motivation": "Existing TTA methods focus on single-model adaptation, but cross-model knowledge can provide complementary insights, improving adaptation accuracy.", "method": "COCA uses two strategies: co-adaptation (integrating knowledge from other models) and self-adaptation (enhancing individual model strengths via unsupervised learning).", "result": "COCA significantly improves adaptation accuracy, e.g., boosting ViT-Base's performance on ImageNet-C from 51.7% to 64.5% with Mobile-ViT's guidance.", "conclusion": "COCA demonstrates the value of cross-model co-learning in TTA, offering a plug-and-play solution to enhance existing methods."}}
{"id": "2506.23674", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23674", "abs": "https://arxiv.org/abs/2506.23674", "authors": ["Dongyue Wu", "Zilin Guo", "Jialong Zuo", "Nong Sang", "Changxin Gao"], "title": "Partial Forward Blocking: A Novel Data Pruning Paradigm for Lossless Training Acceleration", "comment": "Accepted by ICCV2025", "summary": "The ever-growing size of training datasets enhances the generalization\ncapability of modern machine learning models but also incurs exorbitant\ncomputational costs. Existing data pruning approaches aim to accelerate\ntraining by removing those less important samples. However, they often rely on\ngradients or proxy models, leading to prohibitive additional costs of gradient\nback-propagation and proxy model training. In this paper, we propose Partial\nForward Blocking (PFB), a novel framework for lossless training acceleration.\nThe efficiency of PFB stems from its unique adaptive pruning pipeline: sample\nimportance is assessed based on features extracted from the shallow layers of\nthe target model. Less important samples are then pruned, allowing only the\nretained ones to proceed with the subsequent forward pass and loss\nback-propagation. This mechanism significantly reduces the computational\noverhead of deep-layer forward passes and back-propagation for pruned samples,\nwhile also eliminating the need for auxiliary backward computations and proxy\nmodel training. Moreover, PFB introduces probability density as an indicator of\nsample importance. Combined with an adaptive distribution estimation module,\nour method dynamically prioritizes relatively rare samples, aligning with the\nconstantly evolving training state. Extensive experiments demonstrate the\nsignificant superiority of PFB in performance and speed. On ImageNet, PFB\nachieves a 0.5% accuracy improvement and 33% training time reduction with 40%\ndata pruned.", "AI": {"tldr": "PFB is a novel framework for lossless training acceleration by adaptively pruning less important samples based on shallow-layer features, reducing computational costs without needing proxy models or extra back-propagation.", "motivation": "To address the high computational costs of large training datasets and inefficiencies in existing data pruning methods that rely on gradients or proxy models.", "method": "Uses Partial Forward Blocking (PFB) to assess sample importance via shallow-layer features, prune less important samples, and dynamically prioritize rare samples using probability density.", "result": "Achieves 0.5% accuracy improvement and 33% training time reduction on ImageNet with 40% data pruned.", "conclusion": "PFB offers a computationally efficient and effective solution for training acceleration without compromising model performance."}}
{"id": "2506.23675", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23675", "abs": "https://arxiv.org/abs/2506.23675", "authors": ["Patrick Glandorf", "Bodo Rosenhahn"], "title": "Pruning by Block Benefit: Exploring the Properties of Vision Transformer Blocks during Domain Adaptation", "comment": "ICCV'25 Workshops", "summary": "Vision Transformer have set new benchmarks in several tasks, but these models\ncome with the lack of high computational costs which makes them impractical for\nresource limited hardware. Network pruning reduces the computational complexity\nby removing less important operations while maintaining performance. However,\npruning a model on an unseen data domain, leads to a misevaluation of weight\nsignificance, resulting in suboptimal resource assignment. In this work, we\nfind that task-sensitive layers initially fail to improve the feature\nrepresentation on downstream tasks, leading to performance loss for early\npruning decisions. To address this problem, we introduce Pruning by Block\nBenefit (P3B), a pruning method that utilizes the relative contribution on\nblock level to globally assign parameter resources. P3B identifies low-impact\ncomponents to reduce parameter allocation while preserving critical ones.\nClassical pruning mask optimization struggles to reactivate zero-mask-elements.\nIn contrast, P3B sets a layerwise keep ratio based on global performance\nmetrics, ensuring the reactivation of late-converging blocks. We show in\nextensive experiments that P3B is a state of the art pruning method with most\nnoticeable gains in transfer learning tasks. Notably, P3B is able to conserve\nhigh performance, even in high sparsity regimes of 70% parameter reduction\nwhile only losing 0.64% in accuracy.", "AI": {"tldr": "P3B introduces a pruning method for Vision Transformers that globally assigns parameter resources by evaluating block-level contributions, maintaining performance even at high sparsity.", "motivation": "Vision Transformers are computationally expensive, and existing pruning methods misevaluate weight significance on unseen domains, leading to suboptimal performance.", "method": "P3B uses block-level contributions to assign parameters, reactivates late-converging blocks, and sets layerwise keep ratios based on global metrics.", "result": "P3B achieves state-of-the-art pruning, preserving accuracy even at 70% sparsity with only a 0.64% drop.", "conclusion": "P3B is an effective pruning method for Vision Transformers, especially in transfer learning, balancing performance and computational efficiency."}}
{"id": "2506.23676", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23676", "abs": "https://arxiv.org/abs/2506.23676", "authors": ["Gaozheng Pei", "Ke Ma", "Dongpeng Zhang", "Chengzhi Sun", "Qianqian Xu", "Qingming Huang"], "title": "A Unified Framework for Stealthy Adversarial Generation via Latent Optimization and Transferability Enhancement", "comment": null, "summary": "Due to their powerful image generation capabilities, diffusion-based\nadversarial example generation methods through image editing are rapidly\ngaining popularity. However, due to reliance on the discriminative capability\nof the diffusion model, these diffusion-based methods often struggle to\ngeneralize beyond conventional image classification tasks, such as in Deepfake\ndetection. Moreover, traditional strategies for enhancing adversarial example\ntransferability are challenging to adapt to these methods. To address these\nchallenges, we propose a unified framework that seamlessly incorporates\ntraditional transferability enhancement strategies into diffusion model-based\nadversarial example generation via image editing, enabling their application\nacross a wider range of downstream tasks. Our method won first place in the\n\"1st Adversarial Attacks on Deepfake Detectors: A Challenge in the Era of\nAI-Generated Media\" competition at ACM MM25, which validates the effectiveness\nof our approach.", "AI": {"tldr": "A unified framework integrates traditional transferability strategies into diffusion-based adversarial example generation, enhancing performance in tasks like Deepfake detection.", "motivation": "Diffusion-based adversarial methods struggle with generalization beyond image classification and adapting transferability strategies.", "method": "Proposes a unified framework combining traditional transferability enhancement with diffusion model-based adversarial example generation.", "result": "Achieved first place in a competition, validating the framework's effectiveness.", "conclusion": "The framework successfully broadens the applicability of diffusion-based adversarial methods."}}
{"id": "2506.23690", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23690", "abs": "https://arxiv.org/abs/2506.23690", "authors": ["Shuai Tan", "Biao Gong", "Yujie Wei", "Shiwei Zhang", "Zhuoxin Liu", "Dandan Zheng", "Jingdong Chen", "Yan Wang", "Hao Ouyang", "Kecheng Zheng", "Yujun Shen"], "title": "SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation", "comment": "Project page: https://lucaria-academy.github.io/SynMotion/", "summary": "Diffusion-based video motion customization facilitates the acquisition of\nhuman motion representations from a few video samples, while achieving\narbitrary subjects transfer through precise textual conditioning. Existing\napproaches often rely on semantic-level alignment, expecting the model to learn\nnew motion concepts and combine them with other entities (e.g., ''cats'' or\n''dogs'') to produce visually appealing results. However, video data involve\ncomplex spatio-temporal patterns, and focusing solely on semantics cause the\nmodel to overlook the visual complexity of motion. Conversely, tuning only the\nvisual representation leads to semantic confusion in representing the intended\naction. To address these limitations, we propose SynMotion, a new\nmotion-customized video generation model that jointly leverages semantic\nguidance and visual adaptation. At the semantic level, we introduce the\ndual-embedding semantic comprehension mechanism which disentangles subject and\nmotion representations, allowing the model to learn customized motion features\nwhile preserving its generative capabilities for diverse subjects. At the\nvisual level, we integrate parameter-efficient motion adapters into a\npre-trained video generation model to enhance motion fidelity and temporal\ncoherence. Furthermore, we introduce a new embedding-specific training strategy\nwhich \\textbf{alternately optimizes} subject and motion embeddings, supported\nby the manually constructed Subject Prior Video (SPV) training dataset. This\nstrategy promotes motion specificity while preserving generalization across\ndiverse subjects. Lastly, we introduce MotionBench, a newly curated benchmark\nwith diverse motion patterns. Experimental results across both T2V and I2V\nsettings demonstrate that \\method outperforms existing baselines. Project page:\nhttps://lucaria-academy.github.io/SynMotion/", "AI": {"tldr": "SynMotion is a motion-customized video generation model that combines semantic guidance and visual adaptation to address limitations in existing approaches. It introduces dual-embedding semantic comprehension and parameter-efficient motion adapters, outperforming baselines.", "motivation": "Existing methods focus on semantic-level alignment or visual representation alone, leading to overlooked motion complexity or semantic confusion. SynMotion aims to balance both aspects for better motion customization.", "method": "SynMotion uses dual-embedding semantic comprehension to disentangle subject and motion representations, integrates motion adapters for visual fidelity, and employs an alternate optimization training strategy with the SPV dataset.", "result": "SynMotion outperforms existing baselines in both text-to-video (T2V) and image-to-video (I2V) settings, demonstrating improved motion specificity and generalization.", "conclusion": "SynMotion effectively balances semantic and visual adaptation for motion-customized video generation, validated by experimental results and the new MotionBench benchmark."}}
{"id": "2506.23705", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23705", "abs": "https://arxiv.org/abs/2506.23705", "authors": ["Smriti Joshi", "Richard Osuala", "Lidia Garrucho", "Kaisar Kushibar", "Dimitri Kessler", "Oliver Diaz", "Karim Lekadir"], "title": "Single Image Test-Time Adaptation via Multi-View Co-Training", "comment": "MICCAI 2025", "summary": "Test-time adaptation enables a trained model to adjust to a new domain during\ninference, making it particularly valuable in clinical settings where such\non-the-fly adaptation is required. However, existing techniques depend on large\ntarget domain datasets, which are often impractical and unavailable in medical\nscenarios that demand per-patient, real-time inference. Moreover, current\nmethods commonly focus on two-dimensional images, failing to leverage the\nvolumetric richness of medical imaging data. Bridging this gap, we propose a\nPatch-Based Multi-View Co-Training method for Single Image Test-Time\nadaptation. Our method enforces feature and prediction consistency through\nuncertainty-guided self-training, enabling effective volumetric segmentation in\nthe target domain with only a single test-time image. Validated on three\npublicly available breast magnetic resonance imaging datasets for tumor\nsegmentation, our method achieves performance close to the upper bound\nsupervised benchmark while also outperforming all existing state-of-the-art\nmethods, on average by a Dice Similarity Coefficient of 3.75%. We publicly\nshare our accessible codebase, readily integrable with the popular nnUNet\nframework, at https://github.com/smriti-joshi/muvi.git.", "AI": {"tldr": "Proposes a patch-based multi-view co-training method for single-image test-time adaptation in medical imaging, outperforming existing methods by 3.75% Dice score.", "motivation": "Addresses the impracticality of large target datasets in clinical settings and the underutilization of volumetric data in current methods.", "method": "Uses uncertainty-guided self-training to enforce feature and prediction consistency for volumetric segmentation with a single test-time image.", "result": "Achieves performance close to supervised benchmarks and outperforms state-of-the-art methods by 3.75% Dice score.", "conclusion": "The method is effective for real-time, per-patient adaptation in medical imaging and is integrated with nnUNet for accessibility."}}
{"id": "2506.23711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23711", "abs": "https://arxiv.org/abs/2506.23711", "authors": ["Haoyang Chen", "Dongfang Sun", "Caoyuan Ma", "Shiqin Wang", "Kewei Zhang", "Zheng Wang", "Zhixiang Wang"], "title": "Subjective Camera: Bridging Human Cognition and Visual Reconstruction through Sequence-Aware Sketch-Guided Diffusion", "comment": null, "summary": "We propose Subjective Camera, a human-as-imaging-device paradigm that\nreconstructs real-world scenes from mental impressions through synergistic use\nof verbal descriptions and progressive rough sketches. This approach overcomes\ndual limitations of language ambiguity and sketch abstraction by treating the\nuser's drawing sequence as priors, effectively translating subjective\nperceptual expectations into photorealistic images.\n  Existing approaches face three fundamental barriers: (1) user-specific\nsubjective input biases, (2) huge modality gap between planar sketch and 3D\npriors in diffusion, and (3) sketch quality-sensitive performance degradation.\nCurrent solutions either demand resource-intensive model adaptation or impose\nimpractical requirements on sketch precision.\n  Our framework addresses these challenges through concept-sequential\ngeneration. (1) We establish robust appearance priors through text-reward\noptimization, and then implement sequence-aware disentangled generation that\nprocesses concepts in sketching order; these steps accommodate user-specific\nsubjective expectation in a train-free way. (2) We employ latent optimization\nthat effectively bridges the modality gap between planar sketches and 3D priors\nin diffusion. (3) Our hierarchical reward-guided framework enables the use of\nrough sketches without demanding artistic expertise. Comprehensive evaluation\nacross diverse datasets demonstrates that our approach achieves\nstate-of-the-art performance in maintaining both semantic and spatial\ncoherence.", "AI": {"tldr": "Subjective Camera reconstructs scenes from mental impressions using verbal descriptions and sketches, overcoming language and sketch limitations by treating drawings as priors for photorealistic images.", "motivation": "Existing methods struggle with subjective input biases, modality gaps between sketches and 3D priors, and sketch quality issues, often requiring impractical precision or resource-heavy adaptations.", "method": "The framework uses concept-sequential generation, text-reward optimization, sequence-aware disentangled generation, and latent optimization to bridge modality gaps and handle rough sketches.", "result": "The approach achieves state-of-the-art performance in semantic and spatial coherence across diverse datasets.", "conclusion": "Subjective Camera effectively translates subjective perceptions into photorealistic images without demanding high sketch precision or extensive training."}}
{"id": "2506.23903", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23903", "abs": "https://arxiv.org/abs/2506.23903", "authors": ["Hamza Rasaee", "Taha Koleilat", "Hassan Rivaz"], "title": "GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models", "comment": "11 pages, 3 figures, 6 figures", "summary": "Accurate and generalizable object segmentation in ultrasound imaging remains\na significant challenge due to anatomical variability, diverse imaging\nprotocols, and limited annotated data. In this study, we propose a\nprompt-driven vision-language model (VLM) that integrates Grounding DINO with\nSAM2 to enable object segmentation across multiple ultrasound organs. A total\nof 18 public ultrasound datasets, encompassing the breast, thyroid, liver,\nprostate, kidney, and paraspinal muscle, were utilized. These datasets were\ndivided into 15 for fine-tuning and validation of Grounding DINO using Low Rank\nAdaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for\ntesting to evaluate performance in unseen distributions. Comprehensive\nexperiments demonstrate that our approach outperforms state-of-the-art\nsegmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse,\nand SAMUS on most seen datasets while maintaining strong performance on unseen\ndatasets without additional fine-tuning. These results underscore the promise\nof VLMs in scalable and robust ultrasound image analysis, reducing dependence\non large, organ-specific annotated datasets. We will publish our code on\ncode.sonography.ai after acceptance.", "AI": {"tldr": "A prompt-driven vision-language model (VLM) integrating Grounding DINO with SAM2 improves ultrasound object segmentation across multiple organs, outperforming state-of-the-art methods without needing large annotated datasets.", "motivation": "Addressing challenges in ultrasound segmentation due to anatomical variability, diverse protocols, and limited annotated data.", "method": "Uses a VLM with Grounding DINO and SAM2, fine-tuned on 15 ultrasound datasets via LoRA, and tested on 3 unseen datasets.", "result": "Outperforms UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse, and SAMUS on seen datasets and maintains strong performance on unseen ones.", "conclusion": "VLMs show promise for scalable, robust ultrasound analysis, reducing reliance on organ-specific annotated data."}}
{"id": "2506.23729", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23729", "abs": "https://arxiv.org/abs/2506.23729", "authors": ["Guiyu Zhang", "Chen Shi", "Zijian Jiang", "Xunzhi Xiang", "Jingjing Qian", "Shaoshuai Shi", "Li Jiang"], "title": "Proteus-ID: ID-Consistent and Motion-Coherent Video Customization", "comment": "Preprint. Work in progress", "summary": "Video identity customization seeks to synthesize realistic, temporally\ncoherent videos of a specific subject, given a single reference image and a\ntext prompt. This task presents two core challenges: (1) maintaining identity\nconsistency while aligning with the described appearance and actions, and (2)\ngenerating natural, fluid motion without unrealistic stiffness. To address\nthese challenges, we introduce Proteus-ID, a novel diffusion-based framework\nfor identity-consistent and motion-coherent video customization. First, we\npropose a Multimodal Identity Fusion (MIF) module that unifies visual and\ntextual cues into a joint identity representation using a Q-Former, providing\ncoherent guidance to the diffusion model and eliminating modality imbalance.\nSecond, we present a Time-Aware Identity Injection (TAII) mechanism that\ndynamically modulates identity conditioning across denoising steps, improving\nfine-detail reconstruction. Third, we propose Adaptive Motion Learning (AML), a\nself-supervised strategy that reweights the training loss based on\noptical-flow-derived motion heatmaps, enhancing motion realism without\nrequiring additional inputs. To support this task, we construct Proteus-Bench,\na high-quality dataset comprising 200K curated clips for training and 150\nindividuals from diverse professions and ethnicities for evaluation. Extensive\nexperiments demonstrate that Proteus-ID outperforms prior methods in identity\npreservation, text alignment, and motion quality, establishing a new benchmark\nfor video identity customization. Codes and data are publicly available at\nhttps://grenoble-zhang.github.io/Proteus-ID/.", "AI": {"tldr": "Proteus-ID is a diffusion-based framework for video identity customization, addressing identity consistency and motion realism with novel modules like MIF, TAII, and AML, outperforming prior methods.", "motivation": "The task of synthesizing identity-consistent and motion-coherent videos from a single image and text prompt is challenging due to maintaining identity alignment and generating natural motion.", "method": "Proteus-ID uses Multimodal Identity Fusion (MIF) for joint identity representation, Time-Aware Identity Injection (TAII) for dynamic conditioning, and Adaptive Motion Learning (AML) for motion realism.", "result": "Proteus-ID outperforms existing methods in identity preservation, text alignment, and motion quality, validated on the Proteus-Bench dataset.", "conclusion": "Proteus-ID sets a new benchmark for video identity customization, with publicly available code and data."}}
{"id": "2506.23751", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23751", "abs": "https://arxiv.org/abs/2506.23751", "authors": ["Annika M\u00fctze", "Sadia Ilyas", "Christian D\u00f6rpelkus", "Matthias Rottmann"], "title": "Can We Challenge Open-Vocabulary Object Detectors with Generated Content in Street Scenes?", "comment": null, "summary": "Open-vocabulary object detectors such as Grounding DINO are trained on vast\nand diverse data, achieving remarkable performance on challenging datasets. Due\nto that, it is unclear where to find their limitations, which is of major\nconcern when using in safety-critical applications. Real-world data does not\nprovide sufficient control, required for a rigorous evaluation of model\ngeneralization. In contrast, synthetically generated data allows to\nsystematically explore the boundaries of model competence/generalization. In\nthis work, we address two research questions: 1) Can we challenge\nopen-vocabulary object detectors with generated image content? 2) Can we find\nsystematic failure modes of those models? To address these questions, we design\ntwo automated pipelines using stable diffusion to inpaint unusual objects with\nhigh diversity in semantics, by sampling multiple substantives from WordNet and\nChatGPT. On the synthetically generated data, we evaluate and compare multiple\nopen-vocabulary object detectors as well as a classical object detector. The\nsynthetic data is derived from two real-world datasets, namely LostAndFound, a\nchallenging out-of-distribution (OOD) detection benchmark, and the NuImages\ndataset. Our results indicate that inpainting can challenge open-vocabulary\nobject detectors in terms of overlooking objects. Additionally, we find a\nstrong dependence of open-vocabulary models on object location, rather than on\nobject semantics. This provides a systematic approach to challenge\nopen-vocabulary models and gives valuable insights on how data could be\nacquired to effectively improve these models.", "AI": {"tldr": "The paper explores the limitations of open-vocabulary object detectors using synthetic data, revealing their challenges in overlooking objects and dependence on object location rather than semantics.", "motivation": "To rigorously evaluate the generalization of open-vocabulary object detectors, as real-world data lacks control, and to identify systematic failure modes.", "method": "Two automated pipelines using Stable Diffusion to inpaint unusual objects with diverse semantics, evaluated on synthetic data derived from LostAndFound and NuImages datasets.", "result": "Open-vocabulary detectors struggle with overlooking objects and show strong dependence on object location, not semantics.", "conclusion": "Synthetic data provides a systematic way to challenge and improve open-vocabulary models, offering insights for better data acquisition."}}
{"id": "2506.23785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23785", "abs": "https://arxiv.org/abs/2506.23785", "authors": ["Yongjian Wu", "Yang Zhou", "Jiya Saiyin", "Bingzheng Wei", "Yan Xu"], "title": "Visual Textualization for Image Prompted Object Detection", "comment": "Accepted by ICCV 2025", "summary": "We propose VisTex-OVLM, a novel image prompted object detection method that\nintroduces visual textualization -- a process that projects a few visual\nexemplars into the text feature space to enhance Object-level Vision-Language\nModels' (OVLMs) capability in detecting rare categories that are difficult to\ndescribe textually and nearly absent from their pre-training data, while\npreserving their pre-trained object-text alignment. Specifically, VisTex-OVLM\nleverages multi-scale textualizing blocks and a multi-stage fusion strategy to\nintegrate visual information from visual exemplars, generating textualized\nvisual tokens that effectively guide OVLMs alongside text prompts. Unlike\nprevious methods, our method maintains the original architecture of OVLM,\nmaintaining its generalization capabilities while enhancing performance in\nfew-shot settings. VisTex-OVLM demonstrates superior performance across\nopen-set datasets which have minimal overlap with OVLM's pre-training data and\nachieves state-of-the-art results on few-shot benchmarks PASCAL VOC and MSCOCO.\nThe code will be released at https://github.com/WitGotFlg/VisTex-OVLM.", "AI": {"tldr": "VisTex-OVLM enhances object detection in rare categories by projecting visual exemplars into text feature space, maintaining OVLM's architecture and achieving state-of-the-art results.", "motivation": "To improve Object-level Vision-Language Models' (OVLMs) ability to detect rare categories that are hard to describe textually and lack pre-training data.", "method": "Uses multi-scale textualizing blocks and multi-stage fusion to integrate visual exemplars into textualized visual tokens, guiding OVLMs alongside text prompts.", "result": "Superior performance on open-set datasets and state-of-the-art results on PASCAL VOC and MSCOCO few-shot benchmarks.", "conclusion": "VisTex-OVLM effectively enhances OVLMs for rare category detection without altering their architecture, achieving top performance."}}
{"id": "2506.23801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23801", "abs": "https://arxiv.org/abs/2506.23801", "authors": ["Ce Wang", "Wanjie Sun"], "title": "Controllable Reference-Based Real-World Remote Sensing Image Super-Resolution with Generative Diffusion Priors", "comment": null, "summary": "Super-resolution (SR) techniques can enhance the spatial resolution of remote\nsensing images by utilizing low-resolution (LR) images to reconstruct\nhigh-resolution (HR) images, enabling more efficient large-scale earth\nobservation applications. While single-image super-resolution (SISR) methods\nhave shown progress, reference-based super-resolution (RefSR) offers superior\nperformance by incorporating historical HR images alongside current LR\nobservations. However, existing RefSR methods struggle with real-world\ncomplexities, such as cross-sensor resolution gap and significant land cover\nchanges, often leading to under-generation or over-reliance on reference image.\nTo address these challenges, we propose CRefDiff, a novel controllable\nreference-based diffusion model for real-world remote sensing image SR. To\naddress the under-generation problem, CRefDiff is built upon the pretrained\nStable Diffusion model, leveraging its powerful generative prior to produce\naccurate structures and textures. To mitigate over-reliance on the reference,\nwe introduce a dual-branch fusion mechanism that adaptively integrates both\nlocal and global information from the reference image. Moreover, this novel\ndual-branch design enables reference strength control during inference,\nenhancing interactivity and flexibility of the model. Finally, a strategy named\nBetter Start is proposed to significantly reduce the number of denoising steps,\nthereby accelerating the inference process. To support further research, we\nintroduce Real-RefRSSRD, a new real-world RefSR dataset for remote sensing\nimages, consisting of HR NAIP and LR Sentinel-2 image pairs with diverse land\ncover changes and significant temporal gaps. Extensive experiments on\nReal-RefRSSRD show that CRefDiff achieves state-of-the-art performance across\nvarious metrics and improves downstream tasks such as scene classification and\nsemantic segmentation.", "AI": {"tldr": "CRefDiff, a controllable reference-based diffusion model, enhances remote sensing image super-resolution by addressing under-generation and over-reliance on reference images, achieving state-of-the-art results.", "motivation": "Existing RefSR methods struggle with real-world complexities like cross-sensor resolution gaps and land cover changes, leading to under-generation or over-reliance on reference images.", "method": "CRefDiff leverages Stable Diffusion's generative prior, introduces a dual-branch fusion mechanism for adaptive reference integration, and uses a 'Better Start' strategy to reduce denoising steps.", "result": "CRefDiff outperforms existing methods on the Real-RefRSSRD dataset, improving downstream tasks like scene classification and semantic segmentation.", "conclusion": "CRefDiff offers superior performance, flexibility, and efficiency for real-world remote sensing SR, supported by a new dataset for future research."}}
{"id": "2506.24044", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.24044", "abs": "https://arxiv.org/abs/2506.24044", "authors": ["Sicong Jiang", "Zilin Huang", "Kangan Qian", "Ziang Luo", "Tianze Zhu", "Yang Zhong", "Yihong Tang", "Menglin Kong", "Yunlong Wang", "Siwen Jiao", "Hao Ye", "Zihao Sheng", "Xin Zhao", "Tuopu Wen", "Zheng Fu", "Sikai Chen", "Kun Jiang", "Diange Yang", "Seongjin Choi", "Lijun Sun"], "title": "A Survey on Vision-Language-Action Models for Autonomous Driving", "comment": null, "summary": "The rapid progress of multimodal large language models (MLLM) has paved the\nway for Vision-Language-Action (VLA) paradigms, which integrate visual\nperception, natural language understanding, and control within a single policy.\nResearchers in autonomous driving are actively adapting these methods to the\nvehicle domain. Such models promise autonomous vehicles that can interpret\nhigh-level instructions, reason about complex traffic scenes, and make their\nown decisions. However, the literature remains fragmented and is rapidly\nexpanding. This survey offers the first comprehensive overview of VLA for\nAutonomous Driving (VLA4AD). We (i) formalize the architectural building blocks\nshared across recent work, (ii) trace the evolution from early explainer to\nreasoning-centric VLA models, and (iii) compare over 20 representative models\naccording to VLA's progress in the autonomous driving domain. We also\nconsolidate existing datasets and benchmarks, highlighting protocols that\njointly measure driving safety, accuracy, and explanation quality. Finally, we\ndetail open challenges - robustness, real-time efficiency, and formal\nverification - and outline future directions of VLA4AD. This survey provides a\nconcise yet complete reference for advancing interpretable socially aligned\nautonomous vehicles. Github repo is available at\n\\href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.", "AI": {"tldr": "This survey provides a comprehensive overview of Vision-Language-Action (VLA) models for Autonomous Driving (VLA4AD), covering architecture, evolution, model comparisons, datasets, benchmarks, and open challenges.", "motivation": "To consolidate fragmented and rapidly expanding research on VLA models in autonomous driving, offering a structured reference for advancing interpretable and socially aligned autonomous vehicles.", "method": "The survey formalizes architectural building blocks, traces the evolution of VLA models, compares over 20 representative models, and consolidates datasets and benchmarks.", "result": "A detailed comparison of VLA models, identification of key protocols for measuring driving safety and explanation quality, and highlighting of open challenges like robustness and real-time efficiency.", "conclusion": "The survey serves as a concise yet complete reference for future research in VLA4AD, addressing challenges and outlining future directions."}}
{"id": "2506.23808", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23808", "abs": "https://arxiv.org/abs/2506.23808", "authors": ["Carl Olsson", "Amanda Nilsson"], "title": "Towards Initialization-free Calibrated Bundle Adjustment", "comment": null, "summary": "A recent series of works has shown that initialization-free BA can be\nachieved using pseudo Object Space Error (pOSE) as a surrogate objective. The\ninitial reconstruction-step optimizes an objective where all terms are\nprojectively invariant and it cannot incorporate knowledge of the camera\ncalibration. As a result, the solution is only determined up to a projective\ntransformation of the scene and the process requires more data for successful\nreconstruction.\n  In contrast, we present a method that is able to use the known camera\ncalibration thereby producing near metric solutions, that is, reconstructions\nthat are accurate up to a similarity transformation. To achieve this we\nintroduce pairwise relative rotation estimates that carry information about\ncamera calibration. These are only invariant to similarity transformations,\nthus encouraging solutions that preserve metric features of the real scene. Our\nmethod can be seen as integrating rotation averaging into the pOSE framework\nstriving towards initialization-free calibrated SfM.\n  Our experimental evaluation shows that we are able to reliably optimize our\nobjective, achieving convergence to the global minimum with high probability\nfrom random starting solutions, resulting in accurate near metric\nreconstructions.", "AI": {"tldr": "The paper introduces a method for initialization-free bundle adjustment (BA) that leverages camera calibration to produce near-metric reconstructions, improving accuracy over projective-invariant approaches.", "motivation": "Existing initialization-free BA methods using pseudo Object Space Error (pOSE) lack camera calibration knowledge, leading to projective ambiguity and requiring more data for successful reconstruction.", "method": "The proposed method integrates pairwise relative rotation estimates, which carry calibration information and are similarity-invariant, into the pOSE framework to achieve near-metric reconstructions.", "result": "Experiments show reliable optimization of the objective, with high-probability convergence to the global minimum from random starts, yielding accurate near-metric reconstructions.", "conclusion": "The method successfully combines rotation averaging with pOSE, enabling initialization-free calibrated Structure from Motion (SfM) with improved metric accuracy."}}
{"id": "2506.23810", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23810", "abs": "https://arxiv.org/abs/2506.23810", "authors": ["Mahshid Shiri", "Cigdem Beyan", "Vittorio Murino"], "title": "MadCLIP: Few-shot Medical Anomaly Detection with CLIP", "comment": "Accepted to MICCAI 2025 (this version is not peer-reviewed; it is the\n  submitted version). MICCAI proceedings DOI will appear here", "summary": "An innovative few-shot anomaly detection approach is presented, leveraging\nthe pre-trained CLIP model for medical data, and adapting it for both\nimage-level anomaly classification (AC) and pixel-level anomaly segmentation\n(AS). A dual-branch design is proposed to separately capture normal and\nabnormal features through learnable adapters in the CLIP vision encoder. To\nimprove semantic alignment, learnable text prompts are employed to link visual\nfeatures. Furthermore, SigLIP loss is applied to effectively handle the\nmany-to-one relationship between images and unpaired text prompts, showcasing\nits adaptation in the medical field for the first time. Our approach is\nvalidated on multiple modalities, demonstrating superior performance over\nexisting methods for AC and AS, in both same-dataset and cross-dataset\nevaluations. Unlike prior work, it does not rely on synthetic data or memory\nbanks, and an ablation study confirms the contribution of each component. The\ncode is available at https://github.com/mahshid1998/MadCLIP.", "AI": {"tldr": "A novel few-shot anomaly detection method using CLIP for medical data, achieving superior performance in classification and segmentation without synthetic data or memory banks.", "motivation": "To address the challenge of few-shot anomaly detection in medical data by leveraging pre-trained models and improving semantic alignment.", "method": "Uses a dual-branch design with learnable adapters in CLIP, learnable text prompts, and SigLIP loss for handling unpaired text prompts.", "result": "Outperforms existing methods in same-dataset and cross-dataset evaluations for anomaly classification and segmentation.", "conclusion": "The approach is effective, adaptable, and validated across multiple modalities, with each component contributing to its success."}}
{"id": "2506.24085", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.24085", "abs": "https://arxiv.org/abs/2506.24085", "authors": ["Wonwoong Cho", "Yanxia Zhang", "Yan-Ying Chen", "David I. Inouye"], "title": "Imagine for Me: Creative Conceptual Blending of Real Images and Text via Blended Attention", "comment": "Project website is available at https://imagineforme.github.io/", "summary": "Blending visual and textual concepts into a new visual concept is a unique\nand powerful trait of human beings that can fuel creativity. However, in\npractice, cross-modal conceptual blending for humans is prone to cognitive\nbiases, like design fixation, which leads to local minima in the design space.\nIn this paper, we propose a T2I diffusion adapter \"IT-Blender\" that can\nautomate the blending process to enhance human creativity. Prior works related\nto cross-modal conceptual blending are limited in encoding a real image without\nloss of details or in disentangling the image and text inputs. To address these\ngaps, IT-Blender leverages pretrained diffusion models (SD and FLUX) to blend\nthe latent representations of a clean reference image with those of the noisy\ngenerated image. Combined with our novel blended attention, IT-Blender encodes\nthe real reference image without loss of details and blends the visual concept\nwith the object specified by the text in a disentangled way. Our experiment\nresults show that IT-Blender outperforms the baselines by a large margin in\nblending visual and textual concepts, shedding light on the new application of\nimage generative models to augment human creativity.", "AI": {"tldr": "IT-Blender, a T2I diffusion adapter, automates blending visual and textual concepts to enhance creativity, outperforming baselines by leveraging pretrained models and blended attention.", "motivation": "Human cross-modal conceptual blending is prone to cognitive biases like design fixation, limiting creativity. IT-Blender aims to automate and improve this process.", "method": "IT-Blender uses pretrained diffusion models (SD and FLUX) to blend latent representations of a clean reference image with a noisy generated image, aided by novel blended attention for detail preservation and disentanglement.", "result": "IT-Blender outperforms baselines in blending visual and textual concepts, demonstrating its effectiveness in augmenting human creativity.", "conclusion": "IT-Blender showcases the potential of image generative models to enhance creativity by automating cross-modal conceptual blending."}}
{"id": "2506.23822", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23822", "abs": "https://arxiv.org/abs/2506.23822", "authors": ["Shiming Chen", "Bowen Duan", "Salman Khan", "Fahad Shahbaz Khan"], "title": "Interpretable Zero-Shot Learning with Locally-Aligned Vision-Language Model", "comment": "Accepted to ICCV'25", "summary": "Large-scale vision-language models (VLMs), such as CLIP, have achieved\nremarkable success in zero-shot learning (ZSL) by leveraging large-scale\nvisual-text pair datasets. However, these methods often lack interpretability,\nas they compute the similarity between an entire query image and the embedded\ncategory words, making it difficult to explain their predictions. One approach\nto address this issue is to develop interpretable models by integrating\nlanguage, where classifiers are built using discrete attributes, similar to\nhuman perception. This introduces a new challenge: how to effectively align\nlocal visual features with corresponding attributes based on pre-trained VLMs.\nTo tackle this, we propose LaZSL, a locally-aligned vision-language model for\ninterpretable ZSL. LaZSL employs local visual-semantic alignment via optimal\ntransport to perform interaction between visual regions and their associated\nattributes, facilitating effective alignment and providing interpretable\nsimilarity without the need for additional training. Extensive experiments\ndemonstrate that our method offers several advantages, including enhanced\ninterpretability, improved accuracy, and strong domain generalization. Codes\navailable at: https://github.com/shiming-chen/LaZSL.", "AI": {"tldr": "LaZSL is a locally-aligned vision-language model for interpretable zero-shot learning, improving interpretability and accuracy without extra training.", "motivation": "Existing VLMs like CLIP lack interpretability in predictions due to global similarity computation. LaZSL aims to align local visual features with attributes for better explainability.", "method": "LaZSL uses optimal transport for local visual-semantic alignment, linking visual regions to attributes without additional training.", "result": "The method enhances interpretability, improves accuracy, and shows strong domain generalization in experiments.", "conclusion": "LaZSL provides an effective, interpretable solution for zero-shot learning by aligning local features with attributes."}}
{"id": "2506.23825", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23825", "abs": "https://arxiv.org/abs/2506.23825", "authors": ["Haoji Zhang", "Yiqin Wang", "Yansong Tang", "Yong Liu", "Jiashi Feng", "Xiaojie Jin"], "title": "Flash-VStream: Efficient Real-Time Understanding for Long Video Streams", "comment": "Accepted by ICCV 2025", "summary": "Benefiting from the advances in large language models and cross-modal\nalignment, existing multimodal large language models have achieved prominent\nperformance in image and short video understanding. However, the understanding\nof long videos is still challenging, as their long-context nature results in\nsignificant computational and memory overhead. Most existing work treats long\nvideos in the same way as short videos, which is inefficient for real-world\napplications and hard to generalize to even longer videos. To address these\nissues, we propose Flash-VStream, an efficient video language model capable of\nprocessing extremely long videos and responding to user queries in real time.\nParticularly, we design a Flash Memory module, containing a low-capacity\ncontext memory to aggregate long-context temporal information and model the\ndistribution of information density, and a high-capacity augmentation memory to\nretrieve detailed spatial information based on this distribution. Compared to\nexisting models, Flash-VStream achieves significant reductions in inference\nlatency. Extensive experiments on long video benchmarks and comprehensive video\nbenchmarks, i.e., EgoSchema, MLVU, LVBench, MVBench and Video-MME, demonstrate\nthe state-of-the-art performance and outstanding efficiency of our method. Code\nis available at https://github.com/IVGSZ/Flash-VStream.", "AI": {"tldr": "Flash-VStream is an efficient video language model for processing long videos, reducing latency with a novel Flash Memory module.", "motivation": "Existing models struggle with long videos due to computational overhead and inefficiency.", "method": "Uses a Flash Memory module with low-capacity context memory and high-capacity augmentation memory for efficient processing.", "result": "Achieves state-of-the-art performance on benchmarks like EgoSchema and MLVU, with reduced inference latency.", "conclusion": "Flash-VStream offers efficient, real-time processing for long videos, outperforming existing methods."}}
{"id": "2506.23827", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23827", "abs": "https://arxiv.org/abs/2506.23827", "authors": ["Mingcheng Qu", "Yuncong Wu", "Donglin Di", "Yue Gao", "Tonghua Su", "Yang Song", "Lei Fan"], "title": "Spatially Gene Expression Prediction using Dual-Scale Contrastive Learning", "comment": "Our paper has been accepted by MICCAI 2025", "summary": "Spatial transcriptomics (ST) provides crucial insights into tissue\nmicro-environments, but is limited to its high cost and complexity. As an\nalternative, predicting gene expression from pathology whole slide images (WSI)\nis gaining increasing attention. However, existing methods typically rely on\nsingle patches or a single pathology modality, neglecting the complex spatial\nand molecular interactions between target and neighboring information (e.g.,\ngene co-expression). This leads to a failure in establishing connections among\nadjacent regions and capturing intricate cross-modal relationships. To address\nthese issues, we propose NH2ST, a framework that integrates spatial context and\nboth pathology and gene modalities for gene expression prediction. Our model\ncomprises a query branch and a neighbor branch to process paired target patch\nand gene data and their neighboring regions, where cross-attention and\ncontrastive learning are employed to capture intrinsic associations and ensure\nalignments between pathology and gene expression. Extensive experiments on six\ndatasets demonstrate that our model consistently outperforms existing methods,\nachieving over 20% in PCC metrics. Codes are available at\nhttps://github.com/MCPathology/NH2ST", "AI": {"tldr": "NH2ST is a framework for predicting gene expression from pathology images by integrating spatial context and multi-modal data, outperforming existing methods by over 20% in PCC metrics.", "motivation": "Existing methods for predicting gene expression from pathology images lack spatial and molecular interaction modeling, leading to poor performance in capturing cross-modal relationships.", "method": "NH2ST uses a query branch and neighbor branch with cross-attention and contrastive learning to process target patches and neighboring regions, integrating pathology and gene modalities.", "result": "The model outperforms existing methods on six datasets, achieving over 20% improvement in PCC metrics.", "conclusion": "NH2ST effectively addresses the limitations of current methods by leveraging spatial context and multi-modal data for accurate gene expression prediction."}}
{"id": "2506.23832", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23832", "abs": "https://arxiv.org/abs/2506.23832", "authors": ["Ronit D. Gross", "Tal Halevi", "Ella Koresh", "Yarden Tzach", "Ido Kanter"], "title": "Low-latency vision transformers via large-scale multi-head attention", "comment": "23 pages, 4 figures, 7 tables", "summary": "The emergence of spontaneous symmetry breaking among a few heads of\nmulti-head attention (MHA) across transformer blocks in classification tasks\nwas recently demonstrated through the quantification of single-nodal\nperformance (SNP). This finding indicates that each head focuses its attention\non a subset of labels through cooperation among its SNPs. This underlying\nlearning mechanism is generalized to large-scale MHA (LS-MHA) using a single\nmatrix value representing single-head performance (SHP), analogous to\nsingle-filter performance in convolutional neural networks (CNNs). The results\nindicate that each SHP matrix comprises multiple unit clusters such that each\nlabel being explicitly recognized by a few heads with negligible noise. This\nleads to an increased signal-to-noise ratio (SNR) along the transformer blocks,\nthereby improving classification accuracy. These features give rise to several\ndistinct vision transformer (ViT) architectures that achieve the same accuracy\nbut differ in their LS-MHA structures. As a result, their soft committee yields\nsuperior accuracy, an outcome not typically observed in CNNs which rely on\nhundreds of filters. In addition, a significant reduction in latency is\nachieved without affecting the accuracy by replacing the initial transformer\nblocks with convolutional layers. This substitution accelerates early-stage\nlearning, which is then improved by subsequent transformer layers. The\nextension of this learning mechanism to natural language processing tasks,\nbased on quantitative differences between CNNs and ViT architectures, has the\npotential to yield new insights in deep learning. The findings are demonstrated\nusing compact convolutional transformer architectures trained on the CIFAR-100\ndataset.", "AI": {"tldr": "The paper explores spontaneous symmetry breaking in multi-head attention (MHA) in transformers, generalizing it to large-scale MHA (LS-MHA) for improved classification accuracy and reduced latency.", "motivation": "To understand and leverage the learning mechanism of MHA in transformers, particularly how heads focus on subsets of labels, and to improve classification tasks.", "method": "Quantifies single-head performance (SHP) analogous to CNNs, analyzes SHP matrices, and replaces initial transformer blocks with convolutional layers for efficiency.", "result": "Increased signal-to-noise ratio (SNR) and superior accuracy in vision transformers (ViTs), with reduced latency without compromising accuracy.", "conclusion": "The findings suggest potential extensions to NLP tasks and highlight the advantages of hybrid architectures combining CNNs and transformers."}}
{"id": "2506.24125", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.24125", "abs": "https://arxiv.org/abs/2506.24125", "authors": ["Jiacheng Cui", "Xinyue Bi", "Yaxin Luo", "Xiaohan Zhao", "Jiacheng Liu", "Zhiqiang Shen"], "title": "FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation", "comment": "Code at: https://github.com/Jiacheng8/FADRM", "summary": "Residual connection has been extensively studied and widely applied at the\nmodel architecture level. However, its potential in the more challenging\ndata-centric approaches remains unexplored. In this work, we introduce the\nconcept of Data Residual Matching for the first time, leveraging data-level\nskip connections to facilitate data generation and mitigate data information\nvanishing. This approach maintains a balance between newly acquired knowledge\nthrough pixel space optimization and existing core local information\nidentification within raw data modalities, specifically for the dataset\ndistillation task. Furthermore, by incorporating optimization-level\nrefinements, our method significantly improves computational efficiency,\nachieving superior performance while reducing training time and peak GPU memory\nusage by 50%. Consequently, the proposed method Fast and Accurate Data Residual\nMatching for Dataset Distillation (FADRM) establishes a new state-of-the-art,\ndemonstrating substantial improvements over existing methods across multiple\ndataset benchmarks in both efficiency and effectiveness. For instance, with\nResNet-18 as the student model and a 0.8% compression ratio on ImageNet-1K, the\nmethod achieves 47.7% test accuracy in single-model dataset distillation and\n50.0% in multi-model dataset distillation, surpassing RDED by +5.7% and\noutperforming state-of-the-art multi-model approaches, EDC and CV-DD, by +1.4%\nand +4.0%. Code is available at: https://github.com/Jiacheng8/FADRM.", "AI": {"tldr": "The paper introduces Data Residual Matching (FADRM) for dataset distillation, improving efficiency and accuracy by leveraging data-level skip connections and optimization refinements.", "motivation": "To explore the untapped potential of residual connections in data-centric approaches and address data information vanishing in dataset distillation.", "method": "Proposes Data Residual Matching, using data-level skip connections to balance new knowledge and core local information, with optimization-level refinements for efficiency.", "result": "Achieves 47.7% test accuracy on ImageNet-1K (single-model) and 50.0% (multi-model), reducing training time and GPU memory usage by 50%.", "conclusion": "FADRM sets a new state-of-the-art, outperforming existing methods in both efficiency and effectiveness for dataset distillation."}}
{"id": "2506.23833", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23833", "abs": "https://arxiv.org/abs/2506.23833", "authors": ["Oscar Ovanger", "Ragnar Hauge", "Jacob Skauvold", "Michael J. Pyrcz", "Jo Eidsvik"], "title": "PointSSIM: A novel low dimensional resolution invariant image-to-image comparison metric", "comment": "13 pages, 20 figures", "summary": "This paper presents PointSSIM, a novel low-dimensional image-to-image\ncomparison metric that is resolution invariant. Drawing inspiration from the\nstructural similarity index measure and mathematical morphology, PointSSIM\nenables robust comparison across binary images of varying resolutions by\ntransforming them into marked point pattern representations. The key features\nof the image, referred to as anchor points, are extracted from binary images by\nidentifying locally adaptive maxima from the minimal distance transform. Image\ncomparisons are then performed using a summary vector, capturing intensity,\nconnectivity, complexity, and structural attributes. Results show that this\napproach provides an efficient and reliable method for image comparison,\nparticularly suited to applications requiring structural analysis across\ndifferent resolutions.", "AI": {"tldr": "PointSSIM is a resolution-invariant image comparison metric for binary images, using marked point patterns and anchor points for robust analysis.", "motivation": "The need for a reliable method to compare binary images of varying resolutions, especially for structural analysis.", "method": "Transforms binary images into marked point patterns, extracts anchor points via distance transform, and compares using a summary vector of attributes.", "result": "Efficient and reliable image comparison, particularly effective for structural analysis across resolutions.", "conclusion": "PointSSIM is a promising tool for applications requiring resolution-invariant structural image comparison."}}
{"id": "2506.23835", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23835", "abs": "https://arxiv.org/abs/2506.23835", "authors": ["Ziwei Chen", "Ziling Liu", "Zitong Huang", "Mingqi Gao", "Feng Zheng"], "title": "Refine Any Object in Any Scene", "comment": "9 pages with 6 figures", "summary": "Viewpoint missing of objects is common in scene reconstruction, as camera\npaths typically prioritize capturing the overall scene structure rather than\nindividual objects. This makes it highly challenging to achieve high-fidelity\nobject-level modeling while maintaining accurate scene-level representation.\nAddressing this issue is critical for advancing downstream tasks requiring\ndetailed object understanding and appearance modeling. In this paper, we\nintroduce Refine Any object In any ScenE (RAISE), a novel 3D enhancement\nframework that leverages 3D generative priors to recover fine-grained object\ngeometry and appearance under missing views. Starting from substituting\ndegraded objects with proxies, via a 3D generative model with strong 3D\nunderstanding, RAISE progressively refines geometry and texture by aligning\neach proxy to its degraded counterpart in 7-DOF pose, followed by correcting\nspatial and appearance inconsistencies via registration-constrained\nenhancement. This two-stage refinement ensures the high-fidelity geometry and\nappearance of the original object in unseen views while maintaining consistency\nin spatial positioning, observed geometry, and appearance. Extensive\nexperiments on challenging benchmarks show that RAISE significantly outperforms\nstate-of-the-art methods in both novel view synthesis and geometry completion\ntasks. RAISE is made publicly available at https://github.com/PolySummit/RAISE.", "AI": {"tldr": "RAISE introduces a 3D enhancement framework to recover fine-grained object geometry and appearance in scenes with missing views, outperforming state-of-the-art methods.", "motivation": "Addressing the challenge of high-fidelity object-level modeling in scenes with missing viewpoints, critical for detailed object understanding.", "method": "A two-stage refinement: substitutes degraded objects with proxies using a 3D generative model, then aligns and corrects inconsistencies.", "result": "RAISE significantly outperforms existing methods in novel view synthesis and geometry completion.", "conclusion": "RAISE effectively enhances object geometry and appearance in scenes with missing views, advancing detailed object modeling."}}
{"id": "2506.23852", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23852", "abs": "https://arxiv.org/abs/2506.23852", "authors": ["Jianing Jin", "Jiangyong Ying", "Huiyu Duan", "Liu Yang", "Sijing Wu", "Yunhao Li", "Yushuo Zheng", "Xiongkuo Min", "Guangtao Zhai"], "title": "RGC-VQA: An Exploration Database for Robotic-Generated Video Quality Assessment", "comment": null, "summary": "As camera-equipped robotic platforms become increasingly integrated into\ndaily life, robotic-generated videos have begun to appear on streaming media\nplatforms, enabling us to envision a future where humans and robots coexist. We\ninnovatively propose the concept of Robotic-Generated Content (RGC) to term\nthese videos generated from egocentric perspective of robots. The perceptual\nquality of RGC videos is critical in human-robot interaction scenarios, and RGC\nvideos exhibit unique distortions and visual requirements that differ markedly\nfrom those of professionally-generated content (PGC) videos and user-generated\ncontent (UGC) videos. However, dedicated research on quality assessment of RGC\nvideos is still lacking. To address this gap and to support broader robotic\napplications, we establish the first Robotic-Generated Content Database (RGCD),\nwhich contains a total of 2,100 videos drawn from three robot categories and\nsourced from diverse platforms. A subjective VQA experiment is conducted\nsubsequently to assess human visual perception of robotic-generated videos.\nFinally, we conduct a benchmark experiment to evaluate the performance of 11\nstate-of-the-art VQA models on our database. Experimental results reveal\nsignificant limitations in existing VQA models when applied to complex,\nrobotic-generated content, highlighting a critical need for RGC-specific VQA\nmodels. Our RGCD is publicly available at:\nhttps://github.com/IntMeGroup/RGC-VQA.", "AI": {"tldr": "The paper introduces Robotic-Generated Content (RGC) and its unique quality assessment challenges, establishing the first RGC database (RGCD) and evaluating existing VQA models.", "motivation": "The rise of robotic-generated videos necessitates dedicated research on their perceptual quality, as existing VQA models are inadequate for RGC.", "method": "The authors create the RGCD with 2,100 videos, conduct subjective VQA experiments, and benchmark 11 state-of-the-art VQA models.", "result": "Existing VQA models perform poorly on RGC, indicating a need for specialized models.", "conclusion": "The study highlights the gap in RGC quality assessment and provides a foundational database for future research."}}
{"id": "2506.23854", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2506.23854", "abs": "https://arxiv.org/abs/2506.23854", "authors": ["Yida Wang", "Xueyang Zhang", "Kun Zhan", "Peng Jia", "Xianpeng Lang"], "title": "HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity", "comment": "Published in International Conference on Computer Vision (ICCV) 2025", "summary": "Neural surface reconstruction faces persistent challenges in reconciling\ngeometric fidelity with photometric consistency under complex scene conditions.\nWe present HiNeuS, a unified framework that holistically addresses three core\nlimitations in existing approaches: multi-view radiance inconsistency, missing\nkeypoints in textureless regions, and structural degradation from over-enforced\nEikonal constraints during joint optimization. To resolve these issues through\na unified pipeline, we introduce: 1) Differential visibility verification\nthrough SDF-guided ray tracing, resolving reflection ambiguities via continuous\nocclusion modeling; 2) Planar-conformal regularization via ray-aligned geometry\npatches that enforce local surface coherence while preserving sharp edges\nthrough adaptive appearance weighting; and 3) Physically-grounded Eikonal\nrelaxation that dynamically modulates geometric constraints based on local\nradiance gradients, enabling detail preservation without sacrificing global\nregularity. Unlike prior methods that handle these aspects through sequential\noptimizations or isolated modules, our approach achieves cohesive integration\nwhere appearance-geometry constraints evolve synergistically throughout\ntraining. Comprehensive evaluations across synthetic and real-world datasets\ndemonstrate state-of-the-art performance, including a 21.4% reduction in\nChamfer distance over reflection-aware baselines and 2.32 dB PSNR improvement\nagainst neural rendering counterparts. Qualitative analyses reveal superior\ncapability in recovering specular instruments, urban layouts with\ncentimeter-scale infrastructure, and low-textured surfaces without local patch\ncollapse. The method's generalizability is further validated through successful\napplication to inverse rendering tasks, including material decomposition and\nview-consistent relighting.", "AI": {"tldr": "HiNeuS is a unified framework for neural surface reconstruction that addresses multi-view radiance inconsistency, missing keypoints in textureless regions, and structural degradation from Eikonal constraints. It introduces differential visibility verification, planar-conformal regularization, and physically-grounded Eikonal relaxation for cohesive integration of appearance-geometry constraints.", "motivation": "The paper aims to resolve persistent challenges in neural surface reconstruction, such as geometric fidelity and photometric consistency under complex conditions, by addressing core limitations in existing methods.", "method": "HiNeuS uses differential visibility verification, planar-conformal regularization, and physically-grounded Eikonal relaxation to integrate appearance-geometry constraints synergistically.", "result": "The framework achieves state-of-the-art performance, reducing Chamfer distance by 21.4% and improving PSNR by 2.32 dB, with superior qualitative results in recovering specular surfaces and low-textured regions.", "conclusion": "HiNeuS successfully integrates appearance and geometry constraints, demonstrating generalizability in tasks like material decomposition and view-consistent relighting."}}
{"id": "2506.23856", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23856", "abs": "https://arxiv.org/abs/2506.23856", "authors": ["Ji Zhang", "Shihan Wu", "Lianli Gao", "Jingkuan Song", "Nicu Sebe", "Heng Tao Shen"], "title": "A Closer Look at Conditional Prompt Tuning for Vision-Language Models", "comment": "18 pages", "summary": "Despite the great promise of Prompt Tuning (PT) in adapting large\nVision-Language Pretrained Models (VLPMs) to downstream tasks, they often\nstruggle to overcome the Base-New Tradeoff (BNT) dilemma: as VLPMs are better\ntuned to a base task, their ability to generalize to new tasks diminishes.\nRecent work on conditional PT addresses this problem by replacing static\nprompts with dynamic Visual Image Information (VII)-conditioned prompts,\nimproving the model's generalization to new tasks to some extent. In this work,\nwe first identify a critical issue with existing conditional PT methods: using\nVII as the \"condition\" of prompts yields suboptimal performance, and even\nrandom noise-conditioned prompts can outperform the VII-conditioned\ncounterparts. On further analysis, we find that learning dynamic prompts\nconditioned on Textual Class Information (TCI) is the key to solving the BNT\nproblem. Motivated by this, we then propose Class-adaptive Prompt Tuning\n(CaPT), which enables fast adaptation of tuned models to new classes by\nlearning TCI-conditioned prompts from base classes. Remarkably, CaPT can be\nused as a plugin to mitigate the BNT problem for existing unconditional PT\nschemes. Extensive experiments on 11 datasets show that CaPT consistently\nimproves the performance of five strong unconditional PT baselines with\nnegligible additional computational cost. Additionally, by integrating CaPT\nwith our recently proposed DePT framework, we devise a new conditional PT\napproach, termed DeCaPT, which outperforms the H ACC of the state-of-the-art\nconditional PT scheme by 3.49%, averaged over the 11 datasets. Code:\nhttps://github.com/Koorye/CaPT.", "AI": {"tldr": "CaPT improves generalization in Prompt Tuning by using Textual Class Information (TCI)-conditioned prompts, outperforming existing methods with minimal computational cost.", "motivation": "Address the Base-New Tradeoff (BNT) dilemma in Prompt Tuning (PT) by identifying suboptimal performance of Visual Image Information (VII)-conditioned prompts and proposing TCI-conditioned prompts.", "method": "Propose Class-adaptive Prompt Tuning (CaPT), which learns TCI-conditioned prompts from base classes for fast adaptation to new tasks. Also integrates with DePT to form DeCaPT.", "result": "CaPT improves performance of five PT baselines on 11 datasets. DeCaPT outperforms state-of-the-art by 3.49% in H ACC.", "conclusion": "TCI-conditioned prompts are key to solving BNT, and CaPT/DeCaPT offer efficient, high-performance solutions for PT."}}
{"id": "2506.23858", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23858", "abs": "https://arxiv.org/abs/2506.23858", "authors": ["Jianzong Wu", "Liang Hou", "Haotian Yang", "Xin Tao", "Ye Tian", "Pengfei Wan", "Di Zhang", "Yunhai Tong"], "title": "VMoBA: Mixture-of-Block Attention for Video Diffusion Models", "comment": "Code is at https://github.com/KwaiVGI/VMoBA", "summary": "The quadratic complexity of full attention mechanisms poses a significant\nbottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration,\nhigh-resolution videos. While various sparse attention methods have been\nproposed, many are designed as training-free inference accelerators or do not\noptimally capture the unique spatio-temporal characteristics inherent in video\ndata when trained natively. This paper introduces Video Mixture of Block\nAttention (VMoBA), a novel sparse attention mechanism specifically adapted for\nVDMs. Motivated by an in-depth analysis of attention patterns within\npre-trained video transformers, which revealed strong spatio-temporal locality,\nvarying query importance, and head-specific concentration levels, VMoBA\nenhances the original MoBA framework with three key modifications: (1) a\nlayer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to\ndiverse spatio-temporal attention patterns and improve efficiency; (2) global\nblock selection to prioritize the most salient query-key block interactions\nacross an entire attention head; and (3) threshold-based block selection to\ndynamically determine the number of attended blocks based on their cumulative\nsimilarity. Extensive experiments demonstrate that VMoBA significantly\naccelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and\n1.48x latency speedup, while attaining comparable or even superior generation\nquality to full attention. Furthermore, VMoBA exhibits competitive performance\nin training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for\nhigh-res video generation.", "AI": {"tldr": "VMoBA is a sparse attention mechanism for Video Diffusion Models (VDMs) that improves efficiency and quality by adapting to spatio-temporal patterns, achieving significant speedups in training and inference.", "motivation": "The quadratic complexity of full attention in VDMs limits long-duration, high-resolution video generation. Existing sparse methods are suboptimal for video data.", "method": "VMoBA introduces layer-wise block partitioning, global block selection, and threshold-based block selection to optimize spatio-temporal attention.", "result": "VMoBA achieves 2.92x FLOPs and 1.48x latency speedup in training, with comparable or better generation quality. Inference sees 2.40x FLOPs and 1.35x speedup.", "conclusion": "VMoBA effectively addresses attention bottlenecks in VDMs, offering efficiency and quality improvements for video generation."}}
{"id": "2506.23863", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23863", "abs": "https://arxiv.org/abs/2506.23863", "authors": ["Jiahao Ma", "Lei Wang", "Miaomiao liu", "David Ahmedt-Aristizabal", "Chuong Nguyen"], "title": "Puzzles: Unbounded Video-Depth Augmentation for Scalable End-to-End 3D Reconstruction", "comment": "Feed-forward 3D reconstruction, Data Augmentation", "summary": "Multi-view 3D reconstruction remains a core challenge in computer vision.\nRecent methods, such as DUST3R and its successors, directly regress pointmaps\nfrom image pairs without relying on known scene geometry or camera parameters.\nHowever, the performance of these models is constrained by the diversity and\nscale of available training data. In this work, we introduce Puzzles, a data\naugmentation strategy that synthesizes an unbounded volume of high-quality\nposed video-depth data from a single image or video clip. By simulating diverse\ncamera trajectories and realistic scene geometry through targeted image\ntransformations, Puzzles significantly enhances data variety. Extensive\nexperiments show that integrating Puzzles into existing video-based 3D\nreconstruction pipelines consistently boosts performance without modifying the\nunderlying network architecture. Notably, models trained on only ten percent of\nthe original data augmented with Puzzles still achieve accuracy comparable to\nthose trained on the full dataset. Code is available at\nhttps://jiahao-ma.github.io/puzzles/.", "AI": {"tldr": "Puzzles is a data augmentation method for multi-view 3D reconstruction, enhancing training data diversity and performance without altering network architecture.", "motivation": "Overcoming limitations in training data diversity and scale for 3D reconstruction methods like DUST3R.", "method": "Synthesizes high-quality posed video-depth data from single images or clips using simulated camera trajectories and scene geometry.", "result": "Boosts performance; models trained on 10% of original data with Puzzles match full-dataset accuracy.", "conclusion": "Puzzles effectively augments data, improving 3D reconstruction without architectural changes."}}
{"id": "2506.23897", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23897", "abs": "https://arxiv.org/abs/2506.23897", "authors": ["Longliang Liu", "Miaojie Feng", "Junda Cheng", "Jijun Xiang", "Xuan Zhu", "Xin Yang"], "title": "PriOr-Flow: Enhancing Primitive Panoramic Optical Flow with Orthogonal View", "comment": "11 pages", "summary": "Panoramic optical flow enables a comprehensive understanding of temporal\ndynamics across wide fields of view. However, severe distortions caused by\nsphere-to-plane projections, such as the equirectangular projection (ERP),\nsignificantly degrade the performance of conventional perspective-based optical\nflow methods, especially in polar regions. To address this challenge, we\npropose PriOr-Flow, a novel dual-branch framework that leverages the\nlow-distortion nature of the orthogonal view to enhance optical flow estimation\nin these regions. Specifically, we introduce the Dual-Cost Collaborative Lookup\n(DCCL) operator, which jointly retrieves correlation information from both the\nprimitive and orthogonal cost volumes, effectively mitigating distortion noise\nduring cost volume construction. Furthermore, our Ortho-Driven Distortion\nCompensation (ODDC) module iteratively refines motion features from both\nbranches, further suppressing polar distortions. Extensive experiments\ndemonstrate that PriOr-Flow is compatible with various perspective-based\niterative optical flow methods and consistently achieves state-of-the-art\nperformance on publicly available panoramic optical flow datasets, setting a\nnew benchmark for wide-field motion estimation. The code is publicly available\nat: https://github.com/longliangLiu/PriOr-Flow.", "AI": {"tldr": "PriOr-Flow is a dual-branch framework enhancing panoramic optical flow by leveraging orthogonal views to reduce distortion, achieving state-of-the-art performance.", "motivation": "Conventional optical flow methods perform poorly in polar regions due to distortions from sphere-to-plane projections.", "method": "Uses a Dual-Cost Collaborative Lookup (DCCL) operator and Ortho-Driven Distortion Compensation (ODDC) module to refine motion features and mitigate distortion.", "result": "Achieves state-of-the-art performance on panoramic optical flow datasets.", "conclusion": "PriOr-Flow effectively addresses distortion challenges in panoramic optical flow, setting a new benchmark."}}
{"id": "2506.23916", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23916", "abs": "https://arxiv.org/abs/2506.23916", "authors": ["Radhika Juglan", "Marta Ligero", "Zunamys I. Carrero", "Asier Rabasco", "Tim Lenz", "Leo Misera", "Gregory Patrick Veldhuizen", "Paul Kuntke", "Hagen H. Kitzler", "Sven Nebelung", "Daniel Truhn", "Jakob Nikolas Kather"], "title": "Three-dimensional end-to-end deep learning for brain MRI analysis", "comment": null, "summary": "Deep learning (DL) methods are increasingly outperforming classical\napproaches in brain imaging, yet their generalizability across diverse imaging\ncohorts remains inadequately assessed. As age and sex are key neurobiological\nmarkers in clinical neuroscience, influencing brain structure and disease risk,\nthis study evaluates three of the existing three-dimensional architectures,\nnamely Simple Fully Connected Network (SFCN), DenseNet, and Shifted Window\n(Swin) Transformers, for age and sex prediction using T1-weighted MRI from four\nindependent cohorts: UK Biobank (UKB, n=47,390), Dallas Lifespan Brain Study\n(DLBS, n=132), Parkinson's Progression Markers Initiative (PPMI, n=108 healthy\ncontrols), and Information eXtraction from Images (IXI, n=319). We found that\nSFCN consistently outperformed more complex architectures with AUC of 1.00\n[1.00-1.00] in UKB (internal test set) and 0.85-0.91 in external test sets for\nsex classification. For the age prediction task, SFCN demonstrated a mean\nabsolute error (MAE) of 2.66 (r=0.89) in UKB and 4.98-5.81 (r=0.55-0.70) across\nexternal datasets. Pairwise DeLong and Wilcoxon signed-rank tests with\nBonferroni corrections confirmed SFCN's superiority over Swin Transformer\nacross most cohorts (p<0.017, for three comparisons). Explainability analysis\nfurther demonstrates the regional consistency of model attention across cohorts\nand specific to each task. Our findings reveal that simpler convolutional\nnetworks outperform the denser and more complex attention-based DL\narchitectures in brain image analysis by demonstrating better generalizability\nacross different datasets.", "AI": {"tldr": "Simpler convolutional networks (SFCN) outperform complex architectures (DenseNet, Swin Transformers) in age and sex prediction from brain MRI, showing better generalizability across diverse cohorts.", "motivation": "Assess generalizability of deep learning methods in brain imaging, focusing on age and sex prediction, given their neurobiological significance.", "method": "Evaluated three 3D architectures (SFCN, DenseNet, Swin Transformers) using T1-weighted MRI from four cohorts (UKB, DLBS, PPMI, IXI) for age and sex prediction.", "result": "SFCN outperformed others with AUC 1.00 (UKB) and 0.85-0.91 (external) for sex, MAE 2.66 (UKB) and 4.98-5.81 (external) for age. Superiority confirmed via statistical tests.", "conclusion": "Simpler networks generalize better than complex architectures in brain image analysis, highlighting their robustness across diverse datasets."}}
{"id": "2506.23918", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23918", "abs": "https://arxiv.org/abs/2506.23918", "authors": ["Zhaochen Su", "Peng Xia", "Hangyu Guo", "Zhenhua Liu", "Yan Ma", "Xiaoye Qu", "Jiaqi Liu", "Yanshu Li", "Kaide Zeng", "Zhengyuan Yang", "Linjie Li", "Yu Cheng", "Heng Ji", "Junxian He", "Yi R.", "Fung"], "title": "Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers", "comment": "We maintain a real-time GitHub repository tracking progress at:\n  https://github.com/zhaochen0110/Awesome_Think_With_Images", "summary": "Recent progress in multimodal reasoning has been significantly advanced by\ntextual Chain-of-Thought (CoT), a paradigm where models conduct reasoning\nwithin language. This text-centric approach, however, treats vision as a\nstatic, initial context, creating a fundamental \"semantic gap\" between rich\nperceptual data and discrete symbolic thought. Human cognition often transcends\nlanguage, utilizing vision as a dynamic mental sketchpad. A similar evolution\nis now unfolding in AI, marking a fundamental paradigm shift from models that\nmerely think about images to those that can truly think with images. This\nemerging paradigm is characterized by models leveraging visual information as\nintermediate steps in their thought process, transforming vision from a passive\ninput into a dynamic, manipulable cognitive workspace. In this survey, we chart\nthis evolution of intelligence along a trajectory of increasing cognitive\nautonomy, which unfolds across three key stages: from external tool\nexploration, through programmatic manipulation, to intrinsic imagination. To\nstructure this rapidly evolving field, our survey makes four key contributions.\n(1) We establish the foundational principles of the think with image paradigm\nand its three-stage framework. (2) We provide a comprehensive review of the\ncore methods that characterize each stage of this roadmap. (3) We analyze the\ncritical landscape of evaluation benchmarks and transformative applications.\n(4) We identify significant challenges and outline promising future directions.\nBy providing this structured overview, we aim to offer a clear roadmap for\nfuture research towards more powerful and human-aligned multimodal AI.", "AI": {"tldr": "The paper surveys the shift from text-centric multimodal reasoning to models that dynamically think with images, outlining a three-stage evolution and key contributions.", "motivation": "The text-centric approach in multimodal reasoning creates a semantic gap between perceptual data and symbolic thought. The paper aims to explore models that use vision dynamically, akin to human cognition.", "method": "The survey establishes principles of the 'think with image' paradigm, reviews core methods for each stage, analyzes benchmarks, and identifies challenges.", "result": "The paper provides a structured overview of the evolution in multimodal reasoning, highlighting key stages and contributions.", "conclusion": "The survey offers a roadmap for future research to develop more powerful and human-aligned multimodal AI."}}
{"id": "2506.23963", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23963", "abs": "https://arxiv.org/abs/2506.23963", "authors": ["Vannkinh Nom", "Souhail Bakkali", "Muhammad Muzzamil Luqman", "Mickael Coustaty", "Jean-Marc Ogier"], "title": "Evaluating the Impact of Khmer Font Types on Text Recognition", "comment": null, "summary": "Text recognition is significantly influenced by font types, especially for\ncomplex scripts like Khmer. The variety of Khmer fonts, each with its unique\ncharacter structure, presents challenges for optical character recognition\n(OCR) systems. In this study, we evaluate the impact of 19 randomly selected\nKhmer font types on text recognition accuracy using Pytesseract. The fonts\ninclude Angkor, Battambang, Bayon, Bokor, Chenla, Dangrek, Freehand, Kh Kompong\nChhnang, Kh SN Kampongsom, Khmer, Khmer CN Stueng Songke, Khmer Savuth Pen,\nMetal, Moul, Odor MeanChey, Preah Vihear, Siemreap, Sithi Manuss, and iSeth\nFirst. Our comparison of OCR performance across these fonts reveals that Khmer,\nOdor MeanChey, Siemreap, Sithi Manuss, and Battambang achieve high accuracy,\nwhile iSeth First, Bayon, and Dangrek perform poorly. This study underscores\nthe critical importance of font selection in optimizing Khmer text recognition\nand provides valuable insights for developing more robust OCR systems.", "AI": {"tldr": "The study evaluates the impact of 19 Khmer fonts on OCR accuracy, identifying high-performing (e.g., Khmer, Odor MeanChey) and low-performing fonts (e.g., iSeth First, Bayon).", "motivation": "Text recognition is challenging for complex scripts like Khmer due to font diversity, necessitating an evaluation of font impact on OCR accuracy.", "method": "Used Pytesseract to assess OCR accuracy across 19 randomly selected Khmer fonts.", "result": "High accuracy for fonts like Khmer and Odor MeanChey; low accuracy for iSeth First and Bayon.", "conclusion": "Font selection is crucial for optimizing Khmer OCR, offering insights for robust system development."}}
{"id": "2506.23972", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23972", "abs": "https://arxiv.org/abs/2506.23972", "authors": ["Boyue Xu", "Ruichao Hou", "Tongwei Ren", "Gangshan Wu"], "title": "Visual and Memory Dual Adapter for Multi-Modal Object Tracking", "comment": null, "summary": "Prompt-learning-based multi-modal trackers have achieved promising progress\nby employing lightweight visual adapters to incorporate auxiliary modality\nfeatures into frozen foundation models. However, existing approaches often\nstruggle to learn reliable prompts due to limited exploitation of critical cues\nacross frequency and temporal domains. In this paper, we propose a novel visual\nand memory dual adapter (VMDA) to construct more robust and discriminative\nrepresentations for multi-modal tracking. Specifically, we develop a simple but\neffective visual adapter that adaptively transfers discriminative cues from\nauxiliary modality to dominant modality by jointly modeling the frequency,\nspatial, and channel-wise features. Additionally, we design the memory adapter\ninspired by the human memory mechanism, which stores global temporal cues and\nperforms dynamic update and retrieval operations to ensure the consistent\npropagation of reliable temporal information across video sequences. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\non the various multi-modal tracking tasks, including RGB-Thermal, RGB-Depth,\nand RGB-Event tracking. Code and models are available at\nhttps://github.com/xuboyue1999/mmtrack.git.", "AI": {"tldr": "The paper proposes a dual adapter (VMDA) for multi-modal tracking, combining visual and memory adapters to enhance robustness by leveraging frequency, spatial, and temporal cues.", "motivation": "Existing prompt-learning-based trackers underutilize critical cues across frequency and temporal domains, limiting reliability.", "method": "Introduces VMDA with a visual adapter for adaptive feature transfer and a memory adapter for temporal cue propagation.", "result": "Achieves state-of-the-art performance in RGB-Thermal, RGB-Depth, and RGB-Event tracking tasks.", "conclusion": "VMDA effectively improves multi-modal tracking by integrating visual and temporal cues, validated by extensive experiments."}}
{"id": "2506.23975", "categories": ["cs.CV", "68T07", "I.2; I.4"], "pdf": "https://arxiv.org/pdf/2506.23975", "abs": "https://arxiv.org/abs/2506.23975", "authors": ["Yuliia Kaidashova", "Bettina Finzel", "Ute Schmid"], "title": "Toward Simple and Robust Contrastive Explanations for Image Classification by Leveraging Instance Similarity and Concept Relevance", "comment": "17 pages, 6 figures, KI2025 - 48th German Conference on Artificial\n  Intelligence", "summary": "Understanding why a classification model prefers one class over another for\nan input instance is the challenge of contrastive explanation. This work\nimplements concept-based contrastive explanations for image classification by\nleveraging the similarity of instance embeddings and relevance of\nhuman-understandable concepts used by a fine-tuned deep learning model. Our\napproach extracts concepts with their relevance score, computes contrasts for\nsimilar instances, and evaluates the resulting contrastive explanations based\non explanation complexity. Robustness is tested for different image\naugmentations. Two research questions are addressed: (1) whether explanation\ncomplexity varies across different relevance ranges, and (2) whether\nexplanation complexity remains consistent under image augmentations such as\nrotation and noise. The results confirm that for our experiments higher concept\nrelevance leads to shorter, less complex explanations, while lower relevance\nresults in longer, more diffuse explanations. Additionally, explanations show\nvarying degrees of robustness. The discussion of these findings offers insights\ninto the potential of building more interpretable and robust AI systems.", "AI": {"tldr": "The paper introduces a concept-based method for contrastive explanations in image classification, analyzing explanation complexity and robustness under image augmentations.", "motivation": "To understand why a classification model prefers one class over another by providing human-understandable contrastive explanations.", "method": "Leverages instance embeddings and human-understandable concepts, extracts concepts with relevance scores, computes contrasts for similar instances, and evaluates explanation complexity and robustness.", "result": "Higher concept relevance leads to simpler explanations, while lower relevance results in more complex ones. Explanations show varying robustness under augmentations.", "conclusion": "The findings highlight the potential for more interpretable and robust AI systems through concept-based contrastive explanations."}}
{"id": "2506.23982", "categories": ["cs.CV", "cs.RO", "I.4.9"], "pdf": "https://arxiv.org/pdf/2506.23982", "abs": "https://arxiv.org/abs/2506.23982", "authors": ["Ruiyang Hao", "Bowen Jing", "Haibao Yu", "Zaiqing Nie"], "title": "StyleDrive: Towards Driving-Style Aware Benchmarking of End-To-End Autonomous Driving", "comment": "14 pages, 4 figures", "summary": "While personalization has been explored in traditional autonomous driving\nsystems, it remains largely overlooked in end-to-end autonomous driving\n(E2EAD), despite its growing prominence. This gap is critical, as user-aligned\nbehavior is essential for trust, comfort, and widespread adoption of autonomous\nvehicles. A core challenge is the lack of large-scale real-world datasets\nannotated with diverse and fine-grained driving preferences, hindering the\ndevelopment and evaluation of personalized E2EAD models. In this work, we\npresent the first large-scale real-world dataset enriched with annotations\ncapturing diverse driving preferences, establishing a foundation for\npersonalization in E2EAD. We extract static environmental features from\nreal-world road topology and infer dynamic contextual cues using a fine-tuned\nvisual language model (VLM), enabling consistent and fine-grained scenario\nconstruction. Based on these scenarios, we derive objective preference\nannotations through behavioral distribution analysis and rule-based heuristics.\nTo address the inherent subjectivity of driving style, we further employ the\nVLM to generate subjective annotations by jointly modeling scene semantics and\ndriver behavior. Final high-quality labels are obtained through a\nhuman-in-the-loop verification process that fuses both perspectives. Building\non this dataset, we propose the first benchmark for evaluating personalized\nE2EAD models. We assess several state-of-the-art models with and without\npreference conditioning, demonstrating that incorporating personalized\npreferences results in behavior more aligned with human driving. Our work lays\nthe foundation for personalized E2EAD by providing a standardized platform to\nsystematically integrate human preferences into data-driven E2EAD systems,\ncatalyzing future research in human-centric autonomy.", "AI": {"tldr": "The paper addresses the lack of personalization in end-to-end autonomous driving (E2EAD) by introducing a large-scale dataset with diverse driving preferences, enabling the development and evaluation of personalized models.", "motivation": "Personalization is crucial for trust and adoption of autonomous vehicles, but existing E2EAD systems lack datasets capturing diverse driving preferences.", "method": "The authors create a dataset with static and dynamic features, derive objective and subjective preference annotations using a visual language model (VLM), and validate labels through human-in-the-loop verification.", "result": "Incorporating personalized preferences improves alignment with human driving behavior, as demonstrated by benchmarking state-of-the-art models.", "conclusion": "This work establishes a foundation for personalized E2EAD, providing a standardized platform for integrating human preferences and advancing human-centric autonomy."}}
{"id": "2506.24039", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.24039", "abs": "https://arxiv.org/abs/2506.24039", "authors": ["Shubhabrata Mukherjee", "Jack Lang", "Obeen Kwon", "Iryna Zenyuk", "Valerie Brogden", "Adam Weber", "Daniela Ushizima"], "title": "Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data", "comment": "This manuscript is a draft on arxiv. A final version has been\n  submitted to the 59th ICPP 2025, DRAI workshop", "summary": "Zero-shot and prompt-based technologies capitalized on using frequently\noccurring images to transform visual reasoning tasks, which explains why such\ntechnologies struggle with valuable yet scarce scientific image sets. In this\nwork, we propose Zenesis, a comprehensive no-code interactive platform designed\nto minimize barriers posed by data readiness for scientific images. We develop\nlightweight multi-modal adaptation techniques that enable zero-shot operation\non raw scientific data, along with human-in-the-loop refinement and\nheuristic-based temporal enhancement options. We demonstrate the performance of\nour approach through comprehensive comparison and validation on challenging\nFocused Ion Beam Scanning Electron Microscopy (FIB-SEM) data of catalyst-loaded\nmembranes. Zenesis significantly outperforms baseline methods, achieving an\naverage accuracy of 0.947, an Intersection over Union (IOU) of 0.858, and a\nDice score of 0.923 for amorphous catalyst samples and accuracy of 0.987, an\nIOU of 0.857, and a Dice score of 0.923 for crystalline samples. These results\nmark a substantial improvement over traditional methods like Otsu thresholding\nand even advanced models like Segment Anything Model (SAM) when used in\nisolation. Our results demonstrate that Zenesis is a powerful tool for\nscientific applications, particularly in fields where high-quality annotated\ndatasets are unavailable, accelerating accurate analysis of experimental\nimaging.", "AI": {"tldr": "Zenesis is a no-code platform for scientific image analysis, outperforming traditional and advanced methods with high accuracy and efficiency.", "motivation": "Zero-shot and prompt-based methods struggle with scarce scientific images, necessitating a solution like Zenesis to overcome data readiness barriers.", "method": "Zenesis uses lightweight multi-modal adaptation, human-in-the-loop refinement, and heuristic-based temporal enhancement for zero-shot operation on raw scientific data.", "result": "Zenesis achieved high accuracy (0.947-0.987), IOU (0.857-0.858), and Dice scores (0.923) on FIB-SEM data, surpassing baselines like Otsu and SAM.", "conclusion": "Zenesis is a powerful tool for scientific imaging, especially where annotated datasets are scarce, enabling faster and more accurate analysis."}}
{"id": "2506.24063", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24063", "abs": "https://arxiv.org/abs/2506.24063", "authors": ["Deng Li", "Aming Wu", "Yang Li", "Yaowei Wang", "Yahong Han"], "title": "Continual Adaptation: Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios", "comment": null, "summary": "In practice, environments constantly change over time and space, posing\nsignificant challenges for object detectors trained based on a closed-set\nassumption, i.e., training and test data share the same distribution. To this\nend, continual test-time adaptation has attracted much attention, aiming to\nimprove detectors' generalization by fine-tuning a few specific parameters,\ne.g., BatchNorm layers. However, based on a small number of test images,\nfine-tuning certain parameters may affect the representation ability of other\nfixed parameters, leading to performance degradation. Instead, we explore a new\nmechanism, i.e., converting the fine-tuning process to a specific-parameter\ngeneration. Particularly, we first design a dual-path LoRA-based domain-aware\nadapter that disentangles features into domain-invariant and domain-specific\ncomponents, enabling efficient adaptation. Additionally, a conditional\ndiffusion-based parameter generation mechanism is presented to synthesize the\nadapter's parameters based on the current environment, preventing the\noptimization from getting stuck in local optima. Finally, we propose a\nclass-centered optimal transport alignment method to mitigate catastrophic\nforgetting. Extensive experiments conducted on various continuous domain\nadaptive object detection tasks demonstrate the effectiveness. Meanwhile,\nvisualization results show that the representation extracted by the generated\nparameters can capture more object-related information and strengthen the\ngeneralization ability.", "AI": {"tldr": "The paper proposes a novel method for continual test-time adaptation in object detection, using a dual-path LoRA-based adapter and conditional diffusion for parameter generation to improve generalization and avoid performance degradation.", "motivation": "Environments change over time and space, challenging object detectors trained on closed-set assumptions. Existing fine-tuning methods can degrade performance due to limited test images.", "method": "The approach includes a dual-path LoRA-based domain-aware adapter, conditional diffusion-based parameter generation, and class-centered optimal transport alignment to mitigate forgetting.", "result": "Experiments show the method's effectiveness in continuous domain adaptation, with improved generalization and object-related information capture.", "conclusion": "The proposed mechanism enhances adaptation and generalization in dynamic environments while preventing catastrophic forgetting."}}
{"id": "2506.24092", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.24092", "abs": "https://arxiv.org/abs/2506.24092", "authors": ["Moein Heidari", "Yasamin Medghalchi", "Mahdi Khoursha", "Reza Rezaeian", "Ilker Hacihaliloglu"], "title": "WaRA: Wavelet Low Rank Adaptation", "comment": "Submitted to BMVC 2025", "summary": "Parameter-efficient fine-tuning (PEFT) has gained widespread adoption across\nvarious applications. Among PEFT techniques, Low-Rank Adaptation (LoRA) and its\nextensions have emerged as particularly effective, allowing efficient model\nadaptation while significantly reducing computational overhead. However,\nexisting approaches typically rely on global low-rank factorizations, which\noverlook local or multi-scale structure, failing to capture complex patterns in\nthe weight updates. To address this, we propose WaRA, a novel PEFT method that\nleverages wavelet transforms to decompose the weight update matrix into a\nmulti-resolution representation. By performing low-rank factorization in the\nwavelet domain and reconstructing updates through an inverse transform, WaRA\nobtains compressed adaptation parameters that harness multi-resolution\nanalysis, enabling it to capture both coarse and fine-grained features while\nproviding greater flexibility and sparser representations than standard LoRA.\nThrough comprehensive experiments and analysis, we demonstrate that WaRA\nperforms superior on diverse vision tasks, including image generation,\nclassification, and semantic segmentation, significantly enhancing generated\nimage quality while reducing computational complexity. Although WaRA was\nprimarily designed for vision tasks, we further showcase its effectiveness in\nlanguage tasks, highlighting its broader applicability and generalizability.\nThe code is publicly available at\n\\href{GitHub}{https://github.com/moeinheidari7829/WaRA}.", "AI": {"tldr": "WaRA introduces wavelet transforms for PEFT, improving multi-resolution analysis and outperforming LoRA in vision and language tasks.", "motivation": "Existing PEFT methods like LoRA use global low-rank factorizations, missing local or multi-scale structures in weight updates.", "method": "WaRA uses wavelet transforms to decompose weight updates into multi-resolution representations, enabling low-rank factorization in the wavelet domain.", "result": "WaRA excels in vision tasks (image generation, classification, segmentation) and shows promise in language tasks, reducing computational complexity.", "conclusion": "WaRA offers a flexible, sparse, and efficient PEFT method, generalizing well across tasks."}}
{"id": "2506.24096", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24096", "abs": "https://arxiv.org/abs/2506.24096", "authors": ["Antoine Gu\u00e9don", "Diego Gomez", "Nissim Maruani", "Bingchen Gong", "George Drettakis", "Maks Ovsjanikov"], "title": "MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction", "comment": "10 pages. A presentation video of our approach is available at\n  https://youtu.be/_SGNhhNz0fE", "summary": "While recent advances in Gaussian Splatting have enabled fast reconstruction\nof high-quality 3D scenes from images, extracting accurate surface meshes\nremains a challenge. Current approaches extract the surface through costly\npost-processing steps, resulting in the loss of fine geometric details or\nrequiring significant time and leading to very dense meshes with millions of\nvertices. More fundamentally, the a posteriori conversion from a volumetric to\na surface representation limits the ability of the final mesh to preserve all\ngeometric structures captured during training. We present MILo, a novel\nGaussian Splatting framework that bridges the gap between volumetric and\nsurface representations by differentiably extracting a mesh from the 3D\nGaussians. We design a fully differentiable procedure that constructs the\nmesh-including both vertex locations and connectivity-at every iteration\ndirectly from the parameters of the Gaussians, which are the only quantities\noptimized during training. Our method introduces three key technical\ncontributions: a bidirectional consistency framework ensuring both\nrepresentations-Gaussians and the extracted mesh-capture the same underlying\ngeometry during training; an adaptive mesh extraction process performed at each\ntraining iteration, which uses Gaussians as differentiable pivots for Delaunay\ntriangulation; a novel method for computing signed distance values from the 3D\nGaussians that enables precise surface extraction while avoiding geometric\nerosion. Our approach can reconstruct complete scenes, including backgrounds,\nwith state-of-the-art quality while requiring an order of magnitude fewer mesh\nvertices than previous methods. Due to their light weight and empty interior,\nour meshes are well suited for downstream applications such as physics\nsimulations or animation.", "AI": {"tldr": "MILo introduces a differentiable Gaussian Splatting framework to extract accurate surface meshes directly from 3D Gaussians, preserving geometric details and reducing vertex count.", "motivation": "Current methods for extracting surface meshes from Gaussian Splatting lose fine details or produce overly dense meshes, limiting downstream applications.", "method": "MILo uses a differentiable procedure to construct meshes from Gaussians, featuring bidirectional consistency, adaptive mesh extraction, and signed distance computation.", "result": "The method achieves state-of-the-art quality with fewer vertices, producing lightweight meshes suitable for simulations and animation.", "conclusion": "MILo bridges volumetric and surface representations, enabling high-quality mesh extraction without post-processing."}}
{"id": "2506.24102", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24102", "abs": "https://arxiv.org/abs/2506.24102", "authors": ["Xiangtai Li", "Tao Zhang", "Yanwei Li", "Haobo Yuan", "Shihao Chen", "Yikang Zhou", "Jiahao Meng", "Yueyi Sun", "Shilin Xu", "Lu Qi", "Tianheng Cheng", "Yi Lin", "Zilong Huang", "Wenhao Huang", "Jiashi Feng", "Guang Shi"], "title": "DenseWorld-1M: Towards Detailed Dense Grounded Caption in the Real World", "comment": "Datasets and Models: https://github.com/lxtGH/DenseWorld-1M", "summary": "Multimodal Large Language Models (MLLMs) demonstrate a complex understanding\nof scenes, benefiting from large-scale and high-quality datasets. Most existing\ncaption datasets lack the ground locations and relations for visual entities.\nSeveral grounded caption datasets face the problems of missing detailed\ndescriptions, relations, and massive object descriptions on high-resolution\nimages. To fill this gap for the community, we present DenseWorld-1M, the first\nmassive, detailed, dense grounded caption dataset in the real world. We design\na three-stage labeling pipeline, containing open-world perception, detailed\nobject caption generation, and dense caption merging. The first stage obtains\nentity-level masks and labels. The second stage generates the object-level,\ndetailed captions with the guidance of masks and labels from the first stage.\nThe final stage merges object captions and masks into spatial and relational\ndense captions. To accelerate the labeling process and improve caption quality,\nwe present two VLM models: the Detailed Region Caption model and the Spatial\nCaption Merging model. Extensive experiments on various settings, including\nvision-language understanding, visual grounding, and region caption generation,\ndemonstrate the effectiveness of our DenseWorld-1M dataset and labeling models.", "AI": {"tldr": "DenseWorld-1M is a new dataset addressing gaps in existing caption datasets by providing detailed, dense grounded captions for high-resolution images, using a three-stage labeling pipeline and two VLM models.", "motivation": "Existing caption datasets lack detailed descriptions, relations, and object-level grounding, limiting the potential of MLLMs.", "method": "A three-stage labeling pipeline (open-world perception, detailed object caption generation, dense caption merging) and two VLM models (Detailed Region Caption, Spatial Caption Merging) are introduced.", "result": "DenseWorld-1M proves effective in vision-language understanding, visual grounding, and region caption generation tasks.", "conclusion": "The dataset and models fill a critical gap, enhancing multimodal understanding and grounding capabilities."}}
{"id": "2506.24113", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24113", "abs": "https://arxiv.org/abs/2506.24113", "authors": ["Kaiwen Zhang", "Zhenyu Tang", "Xiaotao Hu", "Xingang Pan", "Xiaoyang Guo", "Yuan Liu", "Jingwei Huang", "Li Yuan", "Qian Zhang", "Xiao-Xiao Long", "Xun Cao", "Wei Yin"], "title": "Epona: Autoregressive Diffusion World Model for Autonomous Driving", "comment": "ICCV2025, Project Page: https://kevin-thu.github.io/Epona/", "summary": "Diffusion models have demonstrated exceptional visual quality in video\ngeneration, making them promising for autonomous driving world modeling.\nHowever, existing video diffusion-based world models struggle with\nflexible-length, long-horizon predictions and integrating trajectory planning.\nThis is because conventional video diffusion models rely on global joint\ndistribution modeling of fixed-length frame sequences rather than sequentially\nconstructing localized distributions at each timestep. In this work, we propose\nEpona, an autoregressive diffusion world model that enables localized\nspatiotemporal distribution modeling through two key innovations: 1) Decoupled\nspatiotemporal factorization that separates temporal dynamics modeling from\nfine-grained future world generation, and 2) Modular trajectory and video\nprediction that seamlessly integrate motion planning with visual modeling in an\nend-to-end framework. Our architecture enables high-resolution, long-duration\ngeneration while introducing a novel chain-of-forward training strategy to\naddress error accumulation in autoregressive loops. Experimental results\ndemonstrate state-of-the-art performance with 7.4\\% FVD improvement and minutes\nlonger prediction duration compared to prior works. The learned world model\nfurther serves as a real-time motion planner, outperforming strong end-to-end\nplanners on NAVSIM benchmarks. Code will be publicly available at\n\\href{https://github.com/Kevin-thu/Epona/}{https://github.com/Kevin-thu/Epona/}.", "AI": {"tldr": "Epona introduces an autoregressive diffusion world model for autonomous driving, improving long-horizon predictions and integrating trajectory planning via decoupled spatiotemporal factorization and modular prediction.", "motivation": "Existing video diffusion-based world models lack flexibility for long-horizon predictions and struggle to integrate trajectory planning due to fixed-length frame modeling.", "method": "Epona uses decoupled spatiotemporal factorization and modular trajectory-video prediction, along with a chain-of-forward training strategy to reduce autoregressive errors.", "result": "Achieves 7.4% FVD improvement and longer prediction duration, outperforming prior works. Also serves as a real-time motion planner, excelling on NAVSIM benchmarks.", "conclusion": "Epona advances autonomous driving world modeling by enabling localized, long-horizon predictions and seamless planning integration."}}
{"id": "2506.24121", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24121", "abs": "https://arxiv.org/abs/2506.24121", "authors": ["Sisi Dai", "Xinxin Su", "Boyan Wan", "Ruizhen Hu", "Kai Xu"], "title": "TextMesh4D: High-Quality Text-to-4D Mesh Generation", "comment": null, "summary": "Recent advancements in diffusion generative models significantly advanced\nimage, video, and 3D content creation from user-provided text prompts. However,\nthe challenging problem of dynamic 3D content generation (text-to-4D) with\ndiffusion guidance remains largely unexplored. In this paper, we introduce\nTextMesh4D, a novel framework for high-quality text-to-4D generation. Our\napproach leverages per-face Jacobians as a differentiable mesh representation\nand decomposes 4D generation into two stages: static object creation and\ndynamic motion synthesis. We further propose a flexibility-rigidity\nregularization term to stabilize Jacobian optimization under video diffusion\npriors, ensuring robust geometric performance. Experiments demonstrate that\nTextMesh4D achieves state-of-the-art results in terms of temporal consistency,\nstructural fidelity, and visual realism. Moreover, TextMesh4D operates with a\nlow GPU memory overhead-requiring only a single 24GB GPU-offering a\ncost-effective yet high-quality solution for text-driven 4D mesh generation.\nThe code will be released to facilitate future research in text-to-4D\ngeneration.", "AI": {"tldr": "TextMesh4D is a novel framework for high-quality text-to-4D generation, leveraging per-face Jacobians and a two-stage process for static object creation and dynamic motion synthesis.", "motivation": "The paper addresses the unexplored challenge of dynamic 3D content generation (text-to-4D) with diffusion guidance, aiming to advance beyond current text-to-image and text-to-3D models.", "method": "The approach uses per-face Jacobians as a differentiable mesh representation, decomposing 4D generation into static object creation and dynamic motion synthesis, with a flexibility-rigidity regularization term for stability.", "result": "TextMesh4D achieves state-of-the-art results in temporal consistency, structural fidelity, and visual realism, while operating efficiently on a single 24GB GPU.", "conclusion": "TextMesh4D offers a cost-effective, high-quality solution for text-driven 4D mesh generation, with plans to release code for future research."}}
{"id": "2506.24123", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24123", "abs": "https://arxiv.org/abs/2506.24123", "authors": ["Yue Ma", "Qingyan Bai", "Hao Ouyang", "Ka Leong Cheng", "Qiuyu Wang", "Hongyu Liu", "Zichen Liu", "Haofan Wang", "Jingye Chen", "Yujun Shen", "Qifeng Chen"], "title": "Calligrapher: Freestyle Text Image Customization", "comment": "Project page: https://calligrapher2025.github.io/Calligrapher Code:\n  https://github.com/Calligrapher2025/Calligrapher", "summary": "We introduce Calligrapher, a novel diffusion-based framework that\ninnovatively integrates advanced text customization with artistic typography\nfor digital calligraphy and design applications. Addressing the challenges of\nprecise style control and data dependency in typographic customization, our\nframework incorporates three key technical contributions. First, we develop a\nself-distillation mechanism that leverages the pre-trained text-to-image\ngenerative model itself alongside the large language model to automatically\nconstruct a style-centric typography benchmark. Second, we introduce a\nlocalized style injection framework via a trainable style encoder, which\ncomprises both Qformer and linear layers, to extract robust style features from\nreference images. An in-context generation mechanism is also employed to\ndirectly embed reference images into the denoising process, further enhancing\nthe refined alignment of target styles. Extensive quantitative and qualitative\nevaluations across diverse fonts and design contexts confirm Calligrapher's\naccurate reproduction of intricate stylistic details and precise glyph\npositioning. By automating high-quality, visually consistent typography,\nCalligrapher surpasses traditional models, empowering creative practitioners in\ndigital art, branding, and contextual typographic design.", "AI": {"tldr": "Calligrapher is a diffusion-based framework for digital calligraphy, combining text customization and artistic typography with self-distillation, localized style injection, and in-context generation for precise style control.", "motivation": "To address challenges in precise style control and data dependency for typographic customization in digital calligraphy and design.", "method": "Uses self-distillation to create a style-centric benchmark, a trainable style encoder (Qformer + linear layers) for localized style injection, and in-context generation for embedding reference images into denoising.", "result": "Accurately reproduces stylistic details and glyph positioning, outperforming traditional models in diverse fonts and design contexts.", "conclusion": "Calligrapher automates high-quality typography, enhancing digital art, branding, and typographic design."}}
{"id": "2506.24127", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24127", "abs": "https://arxiv.org/abs/2506.24127", "authors": ["Matthew Gwilliam", "Roy Zhang", "Namitha Padmanabhan", "Hongyang Du", "Abhinav Shrivastava"], "title": "How to Design and Train Your Implicit Neural Representation for Video Compression", "comment": "21 pages, 41 figures, 5 tables", "summary": "Implicit neural representation (INR) methods for video compression have\nrecently achieved visual quality and compression ratios that are competitive\nwith traditional pipelines. However, due to the need for per-sample network\ntraining, the encoding speeds of these methods are too slow for practical\nadoption. We develop a library to allow us to disentangle and review the\ncomponents of methods from the NeRV family, reframing their performance in\nterms of not only size-quality trade-offs, but also impacts on training time.\nWe uncover principles for effective video INR design and propose a\nstate-of-the-art configuration of these components, Rabbit NeRV (RNeRV). When\nall methods are given equal training time (equivalent to 300 NeRV epochs) for 7\ndifferent UVG videos at 1080p, RNeRV achieves +1.27% PSNR on average compared\nto the best-performing alternative for each video in our NeRV library. We then\ntackle the encoding speed issue head-on by investigating the viability of\nhyper-networks, which predict INR weights from video inputs, to disentangle\ntraining from encoding to allow for real-time encoding. We propose masking the\nweights of the predicted INR during training to allow for variable, higher\nquality compression, resulting in 1.7% improvements to both PSNR and MS-SSIM at\n0.037 bpp on the UCF-101 dataset, and we increase hyper-network parameters by\n0.4% for 2.5%/2.7% improvements to PSNR/MS-SSIM with equal bpp and similar\nspeeds. Our project website is available at https://mgwillia.github.io/vinrb/\nand our code is available at https://github.com/mgwillia/vinrb.", "AI": {"tldr": "The paper introduces Rabbit NeRV (RNeRV), a state-of-the-art implicit neural representation (INR) method for video compression, improving PSNR and addressing slow encoding speeds using hyper-networks.", "motivation": "Current INR methods for video compression suffer from slow encoding speeds due to per-sample network training, limiting practical adoption.", "method": "The authors analyze NeRV family methods, propose RNeRV for better performance, and investigate hyper-networks to enable real-time encoding by predicting INR weights.", "result": "RNeRV achieves +1.27% PSNR improvement over alternatives. Hyper-networks with masked weights improve PSNR and MS-SSIM by 1.7% at 0.037 bpp.", "conclusion": "The work advances video INR design, offering faster encoding and higher quality compression, with open-source code and project details available."}}

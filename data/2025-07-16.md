<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 36]
- [cs.LG](#cs.LG) [Total: 83]
- [cs.CV](#cs.CV) [Total: 65]
- [cs.CL](#cs.CL) [Total: 47]
- [eess.IV](#eess.IV) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents](https://arxiv.org/abs/2507.10562)
*Hari Masoor*

Main category: cs.AI

TL;DR: SAMEP is a protocol enabling secure, persistent, and searchable memory sharing among AI agents, improving collaboration and reducing redundancy.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations in AI agent memory sharing, ensuring persistence, security, and semantic searchability.

Method: Uses a distributed memory repository with vector-based semantic search, cryptographic controls (AES-256-GCM), and standardized APIs.

Result: Shows 73% less redundant computation, 89% better context relevance, and full regulatory compliance.

Conclusion: SAMEP enables persistent, collaborative AI ecosystems with strong security and privacy.

Abstract: Current AI agent architectures suffer from ephemeral memory limitations,
preventing effective collaboration and knowledge sharing across sessions and
agent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a
novel framework that enables persistent, secure, and semantically searchable
memory sharing among AI agents. Our protocol addresses three critical
challenges: (1) persistent context preservation across agent sessions, (2)
secure multi-agent collaboration with fine-grained access control, and (3)
efficient semantic discovery of relevant historical context. SAMEP implements a
distributed memory repository with vector-based semantic search, cryptographic
access controls (AES-256-GCM), and standardized APIs compatible with existing
agent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness
across diverse domains including multi-agent software development, healthcare
AI with HIPAA compliance, and multi-modal processing pipelines. Experimental
results show 73% reduction in redundant computations, 89% improvement in
context relevance scores, and complete compliance with regulatory requirements
including audit trail generation. SAMEP enables a new paradigm of persistent,
collaborative AI agent ecosystems while maintaining security and privacy
guarantees.

</details>


### [2] [AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems](https://arxiv.org/abs/2507.10566)
*Hung Ming Liu*

Main category: cs.AI

TL;DR: The paper challenges traditional inductive biases in MARL for emergent communication, proposing the AIM framework with VQ-VAE, showing natural semantic compression and convergence without external biases.


<details>
  <summary>Details</summary>
Motivation: Address the "Joint Exploration Dilemma" and "Communication Vacuum Equilibrium" in MARL by questioning the necessity of artificial inductive biases for communication emergence.

Method: Uses the AIM framework with VQ-VAE to allow agents to develop endogenous symbol systems, enabling spontaneous semantic compression and Nash equilibrium-driven convergence.

Result: AIM achieves effective symbolic communication without external biases, demonstrating stronger generality and efficiency than traditional methods.

Conclusion: The findings suggest new directions for integrating symbolism and connectionism, with future work exploring HQ-VAE and RL pre-training.

Abstract: In Decentralized Multi-Agent Reinforcement Learning (MARL), the development
of Emergent Communication has long been constrained by the ``Joint Exploration
Dilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' .
Traditional methods address this by introducing inductive biases to facilitate
communication emergence . This study fundamentally questions whether such
artificial inductive biases are, in fact, over-engineering. Through experiments
with the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized
Variational Autoencoder (VQ-VAE), we demonstrate that when agents possess an
endogenous symbol system, their neural representations naturally exhibit
spontaneous semantic compression and Nash equilibrium-driven semantic
convergence, achieving effective symbolic communication without external
inductive biases. This aligns with recent neuroscience findings suggesting that
the human brain does not directly use human language for internal thought , and
resonates with research on ``soft thinking'' capabilities in Large Language
Models (LLMs) . Compared to traditional explicit communication methods, AIM
demonstrates stronger generality and efficiency. The interpretable analysis
toolkit developed in this study confirms that symbol usage exhibits a
significant power-law distribution, leading to three major theoretical
insights: the ``Neural Communication Hypothesis'', the ``Tool-First
Principle'', and the ``Semantic Interpretability Paradigm''. Future research
will explore the integration of Hierarchical Quantized Variational Autoencoders
(HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the
potential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This
discovery offers new avenues for bridging symbolism and connectionism.

</details>


### [3] [Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning](https://arxiv.org/abs/2507.10571)
*Konstantinos I. Roumeliotis,Ranjan Sapkota,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.AI

TL;DR: A modular AI framework integrates multimodal agents with a reasoning orchestrator and RAG for trust-aware zero-shot visual classification, improving accuracy by 77.94% in apple leaf disease diagnosis.


<details>
  <summary>Details</summary>
Motivation: Addressing trust challenges in zero-shot AI settings by combining multimodal agents with a reasoning orchestrator and RAG for reliable performance.

Method: Proposes a framework with three configurations: zero-shot with confidence orchestration, fine-tuned agents, and trust-calibrated orchestration with CLIP-based retrieval and re-evaluation.

Result: Achieves 85.63% accuracy in zero-shot setting, with GPT-4o showing better calibration and Qwen-2.5-VL displaying overconfidence.

Conclusion: The system separates perception from meta-reasoning, offering scalable, interpretable AI for trust-critical domains, with open-source release for reproducibility.

Abstract: Modern Artificial Intelligence (AI) increasingly relies on multi-agent
architectures that blend visual and language understanding. Yet, a pressing
challenge remains: How can we trust these agents especially in zero-shot
settings with no fine-tuning? We introduce a novel modular Agentic AI visual
classification framework that integrates generalist multimodal agents with a
non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)
module. Applied to apple leaf disease diagnosis, we benchmark three
configurations: (I) zero-shot with confidence-based orchestration, (II)
fine-tuned agents with improved performance, and (III) trust-calibrated
orchestration enhanced by CLIP-based image retrieval and re-evaluation loops.
Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator
modulates trust across agents. Our results demonstrate a 77.94\% accuracy
improvement in the zero-shot setting using trust-aware orchestration and RAG,
achieving 85.63\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL
displayed overconfidence. Furthermore, image-RAG grounded predictions with
visually similar cases, enabling correction of agent overconfidence via
iterative re-evaluation. The proposed system separates perception (vision
agents) from meta-reasoning (orchestrator), enabling scalable and interpretable
multi-agent AI. This blueprint is extensible to diagnostics, biology, and other
trust-critical domains. All models, prompts, results, and system components
including the complete software source code are openly released to support
reproducibility, transparency, and community benchmarking at Github:
https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust

</details>


### [4] [Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning](https://arxiv.org/abs/2507.10624)
*Zheng Zhang*

Main category: cs.AI

TL;DR: LLMs exhibit fluency but struggle with symbolic reasoning, arithmetic, and logic due to a gap between comprehension and competence, termed the 'split-brain syndrome.'


<details>
  <summary>Details</summary>
Motivation: To diagnose why LLMs fail in tasks requiring precise reasoning despite their surface fluency.

Method: Controlled experiments and architectural analysis to identify the gap between comprehension and competence.

Result: LLMs articulate correct principles but fail in execution due to dissociated instruction and action pathways.

Conclusion: LLMs lack scaffolding for compositional reasoning; future models need metacognitive control and grounded execution.

Abstract: Large Language Models (LLMs) display striking surface fluency yet
systematically fail at tasks requiring symbolic reasoning, arithmetic accuracy,
and logical consistency. This paper offers a structural diagnosis of such
failures, revealing a persistent gap between \textit{comprehension} and
\textit{competence}. Through controlled experiments and architectural analysis,
we demonstrate that LLMs often articulate correct principles without reliably
applying them--a failure rooted not in knowledge access, but in computational
execution. We term this phenomenon the computational \textit{split-brain
syndrome}, where instruction and action pathways are geometrically and
functionally dissociated. This core limitation recurs across domains, from
mathematical operations to relational inferences, and explains why model
behavior remains brittle even under idealized prompting. We argue that LLMs
function as powerful pattern completion engines, but lack the architectural
scaffolding for principled, compositional reasoning. Our findings delineate the
boundary of current LLM capabilities and motivate future models with
metacognitive control, principle lifting, and structurally grounded execution.
This diagnosis also clarifies why mechanistic interpretability findings may
reflect training-specific pattern coordination rather than universal
computational principles, and why the geometric separation between instruction
and execution pathways suggests limitations in neural introspection and
mechanistic analysis.

</details>


### [5] [Enhancing the Capabilities of Large Language Models for API calls through Knowledge Graphs](https://arxiv.org/abs/2507.10630)
*Ye Yang,Xue Xiao,Ping Yin,Taotao Xie*

Main category: cs.AI

TL;DR: KG2data integrates knowledge graphs, LLMs, and tool-use technologies to improve API call accuracy and domain-specific query handling in meteorology, outperforming existing systems.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with domain-specific knowledge and API call accuracy in knowledge-intensive fields like meteorology.

Method: KG2data combines knowledge graphs, LLMs, ReAct agents, and tool-use technologies, evaluated via a virtual API for accuracy metrics.

Result: KG2data outperforms RAG2data and chat2data in API call accuracy (1.43%, 0%, 88.57%) and handles complex queries better.

Conclusion: KG2data offers a scalable, adaptable solution for knowledge-based question answering and data analysis in high-knowledge domains.

Abstract: API calls by large language models (LLMs) offer a cutting-edge approach for
data analysis. However, their ability to effectively utilize tools via API
calls remains underexplored in knowledge-intensive domains like meteorology.
This paper introduces KG2data, a system that integrates knowledge graphs, LLMs,
ReAct agents, and tool-use technologies to enable intelligent data acquisition
and query handling in the meteorological field. Using a virtual API, we
evaluate API call accuracy across three metrics: name recognition failure,
hallucination failure, and call correctness. KG2data achieves superior
performance (1.43%, 0%, 88.57%) compared to RAG2data (16%, 10%, 72.14%) and
chat2data (7.14%, 8.57%, 71.43%). KG2data differs from typical LLM-based
systems by addressing their limited access to domain-specific knowledge, which
hampers performance on complex or terminology-rich queries. By using a
knowledge graph as persistent memory, our system enhances content retrieval,
complex query handling, domain-specific reasoning, semantic relationship
resolution, and heterogeneous data integration. It also mitigates the high cost
of fine-tuning LLMs, making the system more adaptable to evolving domain
knowledge and API structures. In summary, KG2data provides a novel solution for
intelligent, knowledge-based question answering and data analysis in domains
with high knowledge demands.

</details>


### [6] [From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents](https://arxiv.org/abs/2507.10644)
*Tatiana Petrova,Aleksandr Puzikov,Boris Bliznukov,Radu State*

Main category: cs.AI

TL;DR: The paper provides a comprehensive evolutionary overview of the Web of Agents (WoA), linking modern protocols to historical standards and introducing a taxonomy for comparison. It highlights a paradigm shift in intelligence locus and calls for addressing socio-technical challenges.


<details>
  <summary>Details</summary>
Motivation: To unify fragmented research on WoA, bridging gaps between modern LLM-powered frameworks and legacy domains like MAS and the Semantic Web.

Method: Introduces a four-axis taxonomy (semantic foundation, communication paradigm, locus of intelligence, discovery mechanism) to analyze agent architectures across generations.

Result: Reveals a clear evolutionary lineage in agent architectures and identifies a paradigm shift in intelligence locus, foundational to modern Agentic AI.

Conclusion: New protocols alone are insufficient; future research must address socio-technical challenges like decentralized identity, economic models, security, and governance for a robust WoA ecosystem.

Abstract: The concept of the Web of Agents (WoA), which transforms the static,
document-centric Web into an environment of autonomous agents acting on users'
behalf, has attracted growing interest as large language models (LLMs) become
more capable. However, research in this area is still fragmented across
different communities. Contemporary surveys catalog the latest LLM-powered
frameworks, while the rich histories of Multi-Agent Systems (MAS) and the
Semantic Web are often treated as separate, legacy domains. This fragmentation
obscures the intellectual lineage of modern systems and hinders a holistic
understanding of the field's trajectory. We present the first comprehensive
evolutionary overview of the WoA. We show that modern protocols like A2A and
the MCP, are direct evolutionary responses to the well-documented limitations
of earlier standards like FIPA standards and OWL-based semantic agents. To
systematize this analysis, we introduce a four-axis taxonomy (semantic
foundation, communication paradigm, locus of intelligence, discovery
mechanism). This framework provides a unified analytical lens for comparing
agent architectures across all generations, revealing a clear line of descent
where others have seen a disconnect. Our analysis identifies a paradigm shift
in the 'locus of intelligence': from being encoded in external data (Semantic
Web) or the platform (MAS) to being embedded within the agent's core model
(LLM). This shift is foundational to modern Agentic AI, enabling the scalable
and adaptive systems the WoA has long envisioned. We conclude that while new
protocols are essential, they are insufficient for building a robust, open,
trustworthy ecosystem. Finally, we argue that the next research frontier lies
in solving persistent socio-technical challenges, and we map out a new agenda
focused on decentralized identity, economic models, security, and governance
for the emerging WoA.

</details>


### [7] [Parsing Musical Structure to Enable Meaningful Variations](https://arxiv.org/abs/2507.10740)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.AI

TL;DR: A rule-based method for music generation by mutating grammars derived from tunes, analyzing changes over iterations.


<details>
  <summary>Details</summary>
Motivation: To explore how tunes evolve through systematic grammar mutations and assess the musicality of results.

Method: Parse tunes into grammars using Sequitur, apply random mutations, expand grammars to generate new tunes, and analyze changes.

Result: New tunes are created with measurable changes in structure, complexity, and length; mutation effects are quantified.

Conclusion: The approach effectively generates related tunes, with insights into mutation impacts and musical quality.

Abstract: This paper presents a novel rule-based approach for generating music by
varying existing tunes. We parse each tune to find the Pathway Assembly (PA) [
1], that is a structure representing all repetitions in the tune. The Sequitur
algorithm [2 ] is used for this. The result is a grammar. We then carry out
mutation on the grammar, rather than on a tune directly. There are potentially
19 types of mutations such as adding, removing, swapping or reversing parts of
the grammar that can be applied to the grammars. The system employs one of the
mutations randomly in this step to automatically manipulate the grammar.
Following the mutation, we need to expand the grammar which returns a new tune.
The output after 1 or more mutations will be a new tune related to the original
tune. Our study examines how tunes change gradually over the course of multiple
mutations. Edit distances, structural complexity and length of the tunes are
used to show how a tune is changed after multiple mutations. In addition, the
size of effect of each mutation type is analyzed. As a final point, we review
the musical aspect of the output tunes. It should be noted that the study only
focused on generating new pitch sequences. The study is based on an Irish
traditional tune dataset and a list of integers has been used to represent each
tune's pitch values.

</details>


### [8] [AI and the Net-Zero Journey: Energy Demand, Emissions, and the Potential for Transition](https://arxiv.org/abs/2507.10750)
*Pandu Devarakota,Nicolas Tsesmetzis,Faruk O. Alpak,Apurva Gala,Detlef Hohl*

Main category: cs.AI

TL;DR: The paper examines AI's energy consumption and GHG emissions, projecting near-term increases but long-term potential for CO2 reduction through automation and optimization.


<details>
  <summary>Details</summary>
Motivation: To assess whether AI will have a net positive, neutral, or negative impact on CO2 emissions by 2035, considering its growing role in various sectors.

Method: Analysis of energy consumption scenarios for data centers, including near-term (up to 2030) and long-term (2035+) projections, and evaluation of AI's potential to optimize energy-related workflows.

Result: Near-term AI growth may increase CO2 emissions due to high energy demands, but long-term AI could significantly reduce emissions by optimizing processes across industries.

Conclusion: AI may initially strain resources and increase emissions, but its long-term potential for climate mitigation outweighs these early challenges.

Abstract: Thanks to the availability of massive amounts of data, computing resources,
and advanced algorithms, AI has entered nearly every sector. This has sparked
significant investment and interest, particularly in building data centers with
the necessary hardware and software to develop and operate AI models and
AI-based workflows. In this technical review article, we present energy
consumption scenarios of data centers and impact on GHG emissions, considering
both near-term projections (up to 2030) and long-term outlook (2035 and
beyond). We address the quintessential question of whether AI will have a net
positive, neutral, or negative impact on CO2 emissions by 2035. Additionally,
we discuss AI's potential to automate, create efficient and disruptive
workflows across various fields related to energy production, supply and
consumption. In the near-term scenario, the growing demand for AI will likely
strain computing resources, lead to increase in electricity consumption and
therefore associated CO2 emissions. This is due to the power-hungry nature of
big data centers and the requirements for training and running of large and
complex AI models, as well as the penetration of AI assistant search and
applications for public use. However, the long-term outlook could be more
promising. AI has the potential to be a game-changer in CO2 reduction. Its
ability to further automate and optimize processes across industries, from
energy production to logistics, could significantly decrease our carbon
footprint. This positive impact is anticipated to outweigh the initial
emissions bump, creating value for businesses and society in areas where
traditional solutions have fallen short. In essence, AI might cause some
initial growing pains for the environment, but it has the potential to support
climate mitigation efforts.

</details>


### [9] [IoT Malware Network Traffic Detection using Deep Learning and GraphSAGE Models](https://arxiv.org/abs/2507.10758)
*Nikesh Prajapati,Bimal Karki,Saroj Gopali,Akbar Siami Namin*

Main category: cs.AI

TL;DR: The paper evaluates deep learning and graph-based models for detecting IoT malicious attacks, with BERT achieving the highest accuracy (99.94%) and performance metrics. Multi-Head Attention and GraphSAGE also showed promise but with trade-offs in processing time and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting malicious attacks in IoT systems by leveraging temporal and sequential patterns in network traffic.

Method: Utilized GraphSAGE, BERT, TCN, Multi-Head Attention, BI-LSTM, and LSTM models to analyze temporal patterns and feature significance in IoT traffic.

Result: BERT outperformed other models with 99.94% accuracy and high precision, recall, F1-score, and AUC-ROC. Multi-Head Attention provided interpretable results, while GraphSAGE had the shortest training time but lower accuracy.

Conclusion: BERT is highly effective for IoT malicious attack detection due to its ability to capture temporal dependencies, though other models like Multi-Head Attention and GraphSAGE offer trade-offs in interpretability and efficiency.

Abstract: This paper intends to detect IoT malicious attacks through deep learning
models and demonstrates a comprehensive evaluation of the deep learning and
graph-based models regarding malicious network traffic detection. The models
particularly are based on GraphSAGE, Bidirectional encoder representations from
transformers (BERT), Temporal Convolutional Network (TCN) as well as Multi-Head
Attention, together with Bidirectional Long Short-Term Memory (BI-LSTM)
Multi-Head Attention and BI-LSTM and LSTM models. The chosen models
demonstrated great performance to model temporal patterns and detect feature
significance. The observed performance are mainly due to the fact that IoT
system traffic patterns are both sequential and diverse, leaving a rich set of
temporal patterns for the models to learn. Experimental results showed that
BERT maintained the best performance. It achieved 99.94% accuracy rate
alongside high precision and recall, F1-score and AUC-ROC score of 99.99% which
demonstrates its capabilities through temporal dependency capture. The
Multi-Head Attention offered promising results by providing good detection
capabilities with interpretable results. On the other side, the Multi-Head
Attention model required significant processing time like BI-LSTM variants. The
GraphSAGE model achieved good accuracy while requiring the shortest training
time but yielded the lowest accuracy, precision, and F1 score compared to the
other models

</details>


### [10] [Detecting AI Assistance in Abstract Complex Tasks](https://arxiv.org/abs/2507.10761)
*Tyler King,Nikolos Gurney,John H. Miller,Volkan Ustun*

Main category: cs.AI

TL;DR: The paper proposes using neural networks to detect AI assistance in abstract tasks by preprocessing data into neural network-friendly formats, including image and time-series formulations.


<details>
  <summary>Details</summary>
Motivation: Detecting AI assistance is crucial as AI becomes ubiquitous, but abstract task data poses challenges for humans and traditional methods.

Method: Four neural network-friendly image formulations and a time-series formulation are created. Three classical deep learning architectures and a parallel CNN-RNN model are benchmarked.

Result: Common models can effectively classify abstract task data when preprocessed appropriately, with temporal and spatial encoding improving performance.

Conclusion: Preprocessing and combining spatial and temporal data enhance AI assistance detection in abstract tasks, demonstrating the method's generalizability.

Abstract: Detecting assistance from artificial intelligence is increasingly important
as they become ubiquitous across complex tasks such as text generation, medical
diagnosis, and autonomous driving. Aid detection is challenging for humans,
especially when looking at abstract task data. Artificial neural networks excel
at classification thanks to their ability to quickly learn from and process
large amounts of data -- assuming appropriate preprocessing. We posit detecting
help from AI as a classification task for such models. Much of the research in
this space examines the classification of complex but concrete data classes,
such as images. Many AI assistance detection scenarios, however, result in data
that is not machine learning-friendly. We demonstrate that common models can
effectively classify such data when it is appropriately preprocessed. To do so,
we construct four distinct neural network-friendly image formulations along
with an additional time-series formulation that explicitly encodes the
exploration/exploitation of users, which allows for generalizability to other
abstract tasks. We benchmark the quality of each image formulation across three
classical deep learning architectures, along with a parallel CNN-RNN
architecture that leverages the additional time series to maximize testing
performance, showcasing the importance of encoding temporal and spatial
quantities for detecting AI aid in abstract tasks.

</details>


### [11] [Uncertainty-Informed Scheduling of Decision Points for Intelligent Mobile Health Interventions](https://arxiv.org/abs/2507.10798)
*Asim H. Gazi,Bhanu T. Gullapalli,Daiqi Gao,Benjamin M. Marlin,Vivek Shetty,Susan A. Murphy*

Main category: cs.AI

TL;DR: SigmaScheduling dynamically schedules decision points in mHealth interventions based on uncertainty in predicted behavior times, improving intervention timeliness for habitual behaviors like toothbrushing.


<details>
  <summary>Details</summary>
Motivation: Current fixed-interval scheduling for mHealth decision points is ineffective for individuals with irregular routines, often missing the target behavior window.

Method: Proposes SigmaScheduling, which adjusts decision points dynamically—closer to predicted behavior times when timing is predictable, earlier when uncertain.

Result: In a trial with 68 participants, SigmaScheduling ensured decision points preceded brushing events in ≥70% of cases, improving intervention opportunities.

Conclusion: SigmaScheduling enhances precision in mHealth, especially for time-sensitive habitual behaviors, advancing JITAI effectiveness.

Abstract: Timely decision making is critical to the effectiveness of mobile health
(mHealth) interventions. At predefined timepoints called "decision points,"
intelligent mHealth systems such as just-in-time adaptive interventions
(JITAIs) estimate an individual's biobehavioral context from sensor or survey
data and determine whether and how to intervene. For interventions targeting
habitual behavior (e.g., oral hygiene), effectiveness often hinges on
delivering support shortly before the target behavior is likely to occur.
Current practice schedules decision points at a fixed interval (e.g., one hour)
before user-provided behavior times, and the fixed interval is kept the same
for all individuals. However, this one-size-fits-all approach performs poorly
for individuals with irregular routines, often scheduling decision points after
the target behavior has already occurred, rendering interventions ineffective.
In this paper, we propose SigmaScheduling, a method to dynamically schedule
decision points based on uncertainty in predicted behavior times. When behavior
timing is more predictable, SigmaScheduling schedules decision points closer to
the predicted behavior time; when timing is less certain, SigmaScheduling
schedules decision points earlier, increasing the likelihood of timely
intervention. We evaluated SigmaScheduling using real-world data from 68
participants in a 10-week trial of Oralytics, a JITAI designed to improve daily
toothbrushing. SigmaScheduling increased the likelihood that decision points
preceded brushing events in at least 70% of cases, preserving opportunities to
intervene and impact behavior. Our results indicate that SigmaScheduling can
advance precision mHealth, particularly for JITAIs targeting time-sensitive,
habitual behaviors such as oral hygiene or dietary habits.

</details>


### [12] [Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case](https://arxiv.org/abs/2507.10803)
*JaMor Hairston,Ritvik Ranjan,Sahithi Lakamana,Anthony Spadaro,Selen Bozkurt,Jeanmarie Perrone,Abeed Sarker*

Main category: cs.AI

TL;DR: LLMs can automate thematic analysis in qualitative research, with GPT-4o performing best using few-shot prompting.


<details>
  <summary>Details</summary>
Motivation: To evaluate if LLMs can replicate expert-driven thematic analysis of social media data, addressing challenges in inductive thematic analysis.

Method: Evaluated five LLMs on Reddit datasets using binary classifications with zero-, single-, and few-shot prompting, measuring accuracy, precision, recall, and F1-score.

Result: GPT-4o with two-shot prompting achieved 90.9% accuracy and F1-score of 0.71, closely matching expert classifications for high-prevalence themes.

Conclusion: Few-shot LLM-based approaches can automate thematic analyses, providing a scalable supplement for qualitative research.

Abstract: Background Large language models (LLMs) face challenges in inductive thematic
analysis, a task requiring deep interpretive and domain-specific expertise. We
evaluated the feasibility of using LLMs to replicate expert-driven thematic
analysis of social media data. Methods Using two temporally non-intersecting
Reddit datasets on xylazine (n=286 and n=686, for model optimization and
validation, respectively) with twelve expert-derived themes, we evaluated five
LLMs against expert coding. We modeled the task as a series of binary
classifications, rather than a single, multi-label classification, employing
zero-, single-, and few-shot prompting strategies and measuring performance via
accuracy, precision, recall, and F1-score. Results On the validation set,
GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:
0.71). For high-prevalence themes, model-derived thematic distributions closely
mirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:
16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based
approaches can automate thematic analyses, offering a scalable supplement for
qualitative research. Keywords: thematic analysis, large language models,
natural language processing, qualitative analysis, social media, prompt
engineering, public health

</details>


### [13] [AF-XRAY: Visual Explanation and Resolution of Ambiguity in Legal Argumentation Frameworks](https://arxiv.org/abs/2507.10831)
*Yilin Xia,Heng Zheng,Shawn Bowers,Bertram Ludäscher*

Main category: cs.AI

TL;DR: AF-XRAY is an open-source toolkit for analyzing and visualizing argumentation frameworks (AFs) in legal reasoning, addressing ambiguity and aiding non-experts.


<details>
  <summary>Details</summary>
Motivation: To tackle challenges in identifying ambiguity and explaining argument acceptance in legal reasoning for non-experts.

Method: AF-XRAY introduces layered visualizations, attack edge classification, overlay visualizations, and critical attack set identification.

Result: The tool transforms ambiguous scenarios into grounded solutions, revealing causes of ambiguity and enabling exploration of alternative resolutions.

Conclusion: AF-XRAY supports teleological legal reasoning by showing how assumptions impact conclusions, demonstrated with real-world cases.

Abstract: Argumentation frameworks (AFs) provide formal approaches for legal reasoning,
but identifying sources of ambiguity and explaining argument acceptance remains
challenging for non-experts. We present AF-XRAY, an open-source toolkit for
exploring, analyzing, and visualizing abstract AFs in legal reasoning. AF-XRAY
introduces: (i) layered visualizations based on game-theoretic argument length
revealing well-founded derivation structures; (ii) classification of attack
edges by semantic roles (primary, secondary, blunders); (iii) overlay
visualizations of alternative 2-valued solutions on ambiguous 3-valued grounded
semantics; and (iv) identification of critical attack sets whose suspension
resolves undecided arguments. Through systematic generation of critical attack
sets, AF-XRAY transforms ambiguous scenarios into grounded solutions, enabling
users to pinpoint specific causes of ambiguity and explore alternative
resolutions. We use real-world legal cases (e.g., Wild Animals as modeled by
Bench-Capon) to show that our tool supports teleological legal reasoning by
revealing how different assumptions lead to different justified conclusions.

</details>


### [14] [NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization](https://arxiv.org/abs/2507.10894)
*Zongtao He,Liuyi Wang,Lu Chen,Chengju Liu,Qijun Chen*

Main category: cs.AI

TL;DR: NavComposer generates high-quality navigation instructions by decomposing and recomposing semantic entities, while NavInstrCritic evaluates them without expert annotations.


<details>
  <summary>Details</summary>
Motivation: Expert-provided navigation instructions are scarce, and synthesized ones often lack quality, limiting large-scale research.

Method: NavComposer decomposes semantic entities (actions, scenes, objects) and recomposes them into instructions. NavInstrCritic evaluates instructions on contrastive matching, semantic consistency, and linguistic diversity.

Result: The framework produces rich, accurate instructions and provides a holistic evaluation, enabling scalable research.

Conclusion: NavComposer and NavInstrCritic offer a scalable, annotation-free solution for language-guided navigation research.

Abstract: Language-guided navigation is a cornerstone of embodied AI, enabling agents
to interpret language instructions and navigate complex environments. However,
expert-provided instructions are limited in quantity, while synthesized
annotations often lack quality, making them insufficient for large-scale
research. To address this, we propose NavComposer, a novel framework for
automatically generating high-quality navigation instructions. NavComposer
explicitly decomposes semantic entities such as actions, scenes, and objects,
and recomposes them into natural language instructions. Its modular
architecture allows flexible integration of state-of-the-art techniques, while
the explicit use of semantic entities enhances both the richness and accuracy
of instructions. Moreover, it operates in a data-agnostic manner, supporting
adaptation to diverse navigation trajectories without domain-specific training.
Complementing NavComposer, we introduce NavInstrCritic, a comprehensive
annotation-free evaluation system that assesses navigation instructions on
three dimensions: contrastive matching, semantic consistency, and linguistic
diversity. NavInstrCritic provides a holistic evaluation of instruction
quality, addressing limitations of traditional metrics that rely heavily on
expert annotations. By decoupling instruction generation and evaluation from
specific navigation agents, our method enables more scalable and generalizable
research. Extensive experiments provide direct and practical evidence for the
effectiveness of our method.

</details>


### [15] [Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation](https://arxiv.org/abs/2507.10911)
*Yicong Wu,Ting Chen,Irit Hochberg,Zhoujian Sun,Ruth Edry,Zhengxing Huang,Mor Peleg*

Main category: cs.AI

TL;DR: The study explores using an LLM-based multi-agent system (MAS) for therapy recommendations in multimorbidity patients, finding single-agent systems perform comparably to multidisciplinary teams (MDTs) but with some limitations.


<details>
  <summary>Details</summary>
Motivation: Therapy recommendations for chronic patients with multimorbidity are complex due to treatment conflicts, and existing systems lack scalability. The study aims to leverage LLM-based MAS to simulate MDT collaboration for safer recommendations.

Method: The study designed a single agent and a MAS framework to simulate MDT decision-making. Systems were evaluated on therapy planning tasks using benchmark cases, comparing MAS with single-agent approaches and real-world benchmarks.

Result: Single-agent systems performed as well as MDTs, with some models providing correct but incomplete recommendations and unnecessary medications, leading to conflicts.

Conclusion: LLM-based MAS shows promise for therapy recommendations, but improvements are needed to address incompleteness and unnecessary medications.

Abstract: Therapy recommendation for chronic patients with multimorbidity is
challenging due to risks of treatment conflicts. Existing decision support
systems face scalability limitations. Inspired by the way in which general
practitioners (GP) manage multimorbidity patients, occasionally convening
multidisciplinary team (MDT) collaboration, this study investigated the
feasibility and value of using a Large Language Model (LLM)-based multi-agent
system (MAS) for safer therapy recommendations. We designed a single agent and
a MAS framework simulating MDT decision-making by enabling discussion among LLM
agents to resolve medical conflicts. The systems were evaluated on therapy
planning tasks for multimorbidity patients using benchmark cases. We compared
MAS performance with single-agent approaches and real-world benchmarks. An
important contribution of our study is the definition of evaluation metrics
that go beyond the technical precision and recall and allow the inspection of
clinical goals met and medication burden of the proposed advices to a gold
standard benchmark. Our results show that with current LLMs, a single agent GP
performs as well as MDTs. The best-scoring models provide correct
recommendations that address all clinical goals, yet the advices are
incomplete. Some models also present unnecessary medications, resulting in
unnecessary conflicts between medication and conditions or drug-drug
interactions.

</details>


### [16] [Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization](https://arxiv.org/abs/2507.10923)
*Yuhao Wang,Keyan Ding,Kehua Feng,Zeyuan Wang,Ming Qin,Xiaotong Li,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: A Knowledge-guided Preference Optimization (KPO) framework is proposed to mitigate risks of harmful protein sequence generation by protein language models, ensuring biosafety while maintaining functionality.


<details>
  <summary>Details</summary>
Motivation: Protein language models can generate harmful sequences, posing biosafety and ethical risks.

Method: KPO integrates prior knowledge via a Protein Safety Knowledge Graph, uses graph pruning, and reinforcement learning to minimize harmful sequence generation.

Result: KPO reduces hazardous sequence likelihood while preserving functionality.

Conclusion: KPO provides a robust safety framework for generative models in biotechnology.

Abstract: Protein language models have emerged as powerful tools for sequence
generation, offering substantial advantages in functional optimization and
denovo design. However, these models also present significant risks of
generating harmful protein sequences, such as those that enhance viral
transmissibility or evade immune responses. These concerns underscore critical
biosafety and ethical challenges. To address these issues, we propose a
Knowledge-guided Preference Optimization (KPO) framework that integrates prior
knowledge via a Protein Safety Knowledge Graph. This framework utilizes an
efficient graph pruning strategy to identify preferred sequences and employs
reinforcement learning to minimize the risk of generating harmful proteins.
Experimental results demonstrate that KPO effectively reduces the likelihood of
producing hazardous sequences while maintaining high functionality, offering a
robust safety assurance framework for applying generative models in
biotechnology.

</details>


### [17] [Modeling Habitat Shifts: Integrating Convolutional Neural Networks and Tabular Data for Species Migration Prediction](https://arxiv.org/abs/2507.10993)
*Emir Durakovic,Min-Hong Shih*

Main category: cs.AI

TL;DR: The paper proposes a hybrid model combining CNNs and tabular data to predict bird species presence in shifting habitats due to climate change, achieving 85% accuracy.


<details>
  <summary>Details</summary>
Motivation: Climate change is causing habitat shifts, necessitating accurate models to track bird species presence in new locations.

Method: Uses CNNs for spatial features from satellite imagery and tabular data for environmental features (temperature, precipitation, elevation).

Result: The hybrid model achieves 85% accuracy in predicting bird distribution.

Conclusion: The scalable and reliable method aids in understanding bird migration amid climate-induced habitat shifts.

Abstract: Due to climate-induced changes, many habitats are experiencing range shifts
away from their traditional geographic locations (Piguet, 2011). We propose a
solution to accurately model whether bird species are present in a specific
habitat through the combination of Convolutional Neural Networks (CNNs)
(O'Shea, 2015) and tabular data. Our approach makes use of satellite imagery
and environmental features (e.g., temperature, precipitation, elevation) to
predict bird presence across various climates. The CNN model captures spatial
characteristics of landscapes such as forestation, water bodies, and
urbanization, whereas the tabular method uses ecological and geographic data.
Both systems predict the distribution of birds with an average accuracy of 85%,
offering a scalable but reliable method to understand bird migration.

</details>


### [18] [Personalized Exercise Recommendation with Semantically-Grounded Knowledge Tracing](https://arxiv.org/abs/2507.11060)
*Yilmazcan Ozyurt,Tunaberk Almaci,Stefan Feuerriegel,Mrinmaya Sachan*

Main category: cs.AI

TL;DR: ExRec is a framework for personalized exercise recommendation using semantically-grounded knowledge tracing, addressing gaps in semantic content and structured learning progression. It combines KT models with RL, enhanced by model-based value estimation, and shows effectiveness in real-world math learning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing exercise recommendation methods often ignore semantic content and structured learning progression, limiting personalization.

Method: ExRec uses an end-to-end pipeline: annotating KCs, learning semantic representations, training KT models, and optimizing RL methods, including a tailored MVE approach.

Result: Validated across four real-world math tasks, ExRec generalizes to unseen questions and produces interpretable learning trajectories.

Conclusion: KT-guided RL holds promise for effective educational personalization, as demonstrated by ExRec.

Abstract: We introduce ExRec, a general framework for personalized exercise
recommendation with semantically-grounded knowledge tracing. Our method builds
on the observation that existing exercise recommendation approaches simulate
student performance via knowledge tracing (KT) but they often overlook two key
aspects: (a) the semantic content of questions and (b) the sequential,
structured progression of student learning. To address this, our ExRec presents
an end-to-end pipeline, from annotating the KCs of questions and learning their
semantic representations to training KT models and optimizing several
reinforcement learning (RL) methods. Moreover, we improve standard
Q-learning-based continuous RL methods via a tailored model-based value
estimation (MVE) approach that directly leverages the components of KT model in
estimating cumulative knowledge improvement. We validate the effectiveness of
our ExRec using various RL methods across four real-world tasks with different
educational goals in online math learning. We further show that ExRec
generalizes robustly to new, unseen questions and that it produces
interpretable student learning trajectories. Together, our findings highlight
the promise of KT-guided RL for effective personalization in education.

</details>


### [19] [Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander](https://arxiv.org/abs/2507.11079)
*Li Wang,Qizhen Wu,Lei Chen*

Main category: cs.AI

TL;DR: A vision-language model-based commander is proposed for autonomous multi-agent tactical decisions, combining scene understanding and strategic reasoning for high adaptability and interpretability, achieving an 80% win rate.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based and reinforcement learning methods lack adaptability and interpretability in complex battlefield environments, necessitating a more intelligent perception-to-decision approach.

Method: Integrates a vision-language model for scene understanding and a lightweight large language model for strategic reasoning within a shared semantic space.

Result: Achieves an 80% win rate in simulations, outperforming baseline models.

Conclusion: The proposed method effectively bridges perception and decision-making, mimicking human commander cognition with high performance and interpretability.

Abstract: In multiple unmanned ground vehicle confrontations, autonomously evolving
multi-agent tactical decisions from situational awareness remain a significant
challenge. Traditional handcraft rule-based methods become vulnerable in the
complicated and transient battlefield environment, and current reinforcement
learning methods mainly focus on action manipulation instead of strategic
decisions due to lack of interpretability. Here, we propose a vision-language
model-based commander to address the issue of intelligent
perception-to-decision reasoning in autonomous confrontations. Our method
integrates a vision language model for scene understanding and a lightweight
large language model for strategic reasoning, achieving unified perception and
decision within a shared semantic space, with strong adaptability and
interpretability. Unlike rule-based search and reinforcement learning methods,
the combination of the two modules establishes a full-chain process, reflecting
the cognitive process of human commanders. Simulation and ablation experiments
validate that the proposed approach achieves a win rate of over 80% compared
with baseline models.

</details>


### [20] [Function-to-Style Guidance of LLMs for Code Translation](https://arxiv.org/abs/2507.11083)
*Longhui Zhang,Bin Wang,Jiahao Wang,Xiaofeng Zhao,Min Zhang,Hao Yang,Meishan Zhang,Yu Li,Jing Li,Jun Yu,Min Zhang*

Main category: cs.AI

TL;DR: F2STrans improves LLM-based code translation by focusing on correctness (functional learning) and readability (style learning), outperforming larger models like GPT-4.


<details>
  <summary>Details</summary>
Motivation: Ensuring correctness and readability in LLM-based code translation is challenging, limiting real-world adoption.

Method: F2STrans uses functional learning (correctness) and style learning (readability) with high-quality code pairs and style examples.

Result: F2STrans outperforms Qwen-32B and GPT-4 in 20 code translation scenarios.

Conclusion: F2STrans enhances LLM performance in code translation, addressing correctness and readability effectively.

Abstract: Large language models (LLMs) have made significant strides in code
translation tasks. However, ensuring both the correctness and readability of
translated code remains a challenge, limiting their effective adoption in
real-world software development. In this work, we propose F2STrans, a
function-to-style guiding paradigm designed to progressively improve the
performance of LLMs in code translation. Our approach comprises two key stages:
(1) Functional learning, which optimizes translation correctness using
high-quality source-target code pairs mined from online programming platforms,
and (2) Style learning, which improves translation readability by incorporating
both positive and negative style examples. Additionally, we introduce a novel
code translation benchmark that includes up-to-date source code, extensive test
cases, and manually annotated ground-truth translations, enabling comprehensive
functional and stylistic evaluations. Experiments on both our new benchmark and
existing datasets demonstrate that our approach significantly improves code
translation performance. Notably, our approach enables Qwen-1.5B to outperform
prompt-enhanced Qwen-32B and GPT-4 on average across 20 diverse code
translation scenarios.

</details>


### [21] [AI Agent Architecture for Decentralized Trading of Alternative Assets](https://arxiv.org/abs/2507.11117)
*Ailiya Borjigin,Cong He,Charles CC Lee,Wei Zhou*

Main category: cs.AI

TL;DR: GoldMine OS uses AI agents to automate and secure tokenization of physical gold into blockchain stablecoins, achieving fast, resilient, and scalable decentralized trading.


<details>
  <summary>Details</summary>
Motivation: Bridging physical asset custody with blockchain while ensuring compliance, liquidity, and risk management for decentralized trading of alternative assets like gold.

Method: Combines on-chain smart contracts for risk controls with off-chain AI agents (Compliance, Token Issuance, Market Making, Risk Control) for decision-making. Evaluated via simulation and pilot deployment.

Result: Prototype achieves 1.2s token issuance, tight liquidity (spreads <0.5%), resilience to attacks, and scales to 5000 TPS with 10k users.

Conclusion: AI agent-based decentralized exchanges can meet performance and safety requirements, democratizing access to illiquid assets with transparent governance.

Abstract: Decentralized trading of real-world alternative assets (e.g., gold) requires
bridging physical asset custody with blockchain systems while meeting strict
requirements for compliance, liquidity, and risk management. We present
GoldMine OS, a research oriented architecture that employs multiple specialized
AI agents to automate and secure the tokenization and exchange of physical gold
into a blockchain based stablecoin ("OZ"). Our approach combines on chain smart
contracts for critical risk controls with off chain AI agents for decision
making, blending the transparency and reliability of blockchains with the
flexibility of AI driven automation. We describe four cooperative agents
(Compliance, Token Issuance, Market Making, and Risk Control) and a
coordinating core, and evaluate the system through simulation and a controlled
pilot deployment. In experiments the prototype delivers on demand token
issuance in under 1.2 s, more than 100 times faster than manual workflows. The
Market Making agent maintains tight liquidity with spreads often below 0.5
percent even under volatile conditions. Fault injection tests show resilience:
an oracle price spoofing attack is detected and mitigated within 10 s, and a
simulated vault mis reporting halts issuance immediately with minimal user
impact. The architecture scales to 5000 transactions per second with 10000
concurrent users in benchmarks. These results indicate that an AI agent based
decentralized exchange for alternative assets can satisfy rigorous performance
and safety requirements. We discuss broader implications for democratizing
access to traditionally illiquid assets and explain how our governance model --
multi signature agent updates and on chain community voting on risk parameters
-- provides ongoing transparency, adaptability, and formal assurance of system
integrity.

</details>


### [22] [Defining neurosymbolic AI](https://arxiv.org/abs/2507.11127)
*Lennert De Smet,Luc De Raedt*

Main category: cs.AI

TL;DR: The paper proposes a formal definition for neurosymbolic AI, unifying logical and neural representations through an integral computation of logical and belief functions.


<details>
  <summary>Details</summary>
Motivation: The field lacks a generally accepted formal definition of neurosymbolic AI, despite numerous systems.

Method: Introduces a formal definition of neurosymbolic inference as an integral over a product of logical and belief functions.

Result: The definition abstracts key ingredients of representative neurosymbolic AI systems.

Conclusion: The formal definition provides a unified framework for understanding neurosymbolic AI.

Abstract: Neurosymbolic AI focuses on integrating learning and reasoning, in
particular, on unifying logical and neural representations. Despite the
existence of an alphabet soup of neurosymbolic AI systems, the field is lacking
a generally accepted formal definition of what neurosymbolic models and
inference really are. We introduce a formal definition for neurosymbolic AI
that makes abstraction of its key ingredients. More specifically, we define
neurosymbolic inference as the computation of an integral over a product of a
logical and a belief function. We show that our neurosymbolic AI definition
makes abstraction of key representative neurosymbolic AI systems.

</details>


### [23] [Collaborative Trustworthiness for Good Decision Making in Autonomous Systems](https://arxiv.org/abs/2507.11135)
*Selma Saidi,Omar Laimona,Christoph Schmickler,Dirk Ziegenbein*

Main category: cs.AI

TL;DR: A collaborative approach for trustworthy decision-making in autonomous systems using quality attributes and BDDs for efficient aggregation and propagation.


<details>
  <summary>Details</summary>
Motivation: Ensuring safe and correct behavior of autonomous systems in dynamic environments is challenging, requiring trustworthy decision-making.

Method: Proposes a collaborative approach using quality attributes (e.g., perception quality) and BDDs for belief aggregation and propagation, with reduction rules for efficiency.

Result: Improved reliability and decision-making in autonomous systems by leveraging trustworthy data sharing and efficient computation.

Conclusion: The approach enhances trustworthiness and reliability in autonomous systems through collaborative data sharing and formal reasoning methods.

Abstract: Autonomous systems are becoming an integral part of many application domains,
like in the mobility sector. However, ensuring their safe and correct behaviour
in dynamic and complex environments remains a significant challenge, where
systems should autonomously make decisions e.g., about manoeuvring. We propose
in this paper a general collaborative approach for increasing the level of
trustworthiness in the environment of operation and improve reliability and
good decision making in autonomous system. In the presence of conflicting
information, aggregation becomes a major issue for trustworthy decision making
based on collaborative data sharing. Unlike classical approaches in the
literature that rely on consensus or majority as aggregation rule, we exploit
the fact that autonomous systems have different quality attributes like
perception quality. We use this criteria to determine which autonomous systems
are trustworthy and borrow concepts from social epistemology to define
aggregation and propagation rules, used for automated decision making. We use
Binary Decision Diagrams (BDDs) as formal models for beliefs aggregation and
propagation, and formulate reduction rules to reduce the size of the BDDs and
allow efficient computation structures for collaborative automated reasoning.

</details>


### [24] [Fine-grained Timing Analysis of Digital Integrated Circuits in Answer Set Programming](https://arxiv.org/abs/2507.11150)
*Alessandro Bertagnon,Marcello Dalpasso,Michele Favalli,Marco Gavanelli*

Main category: cs.AI

TL;DR: The paper addresses the challenge of computing the actual maximum delay in integrated circuits, using Answer Set Programming (ASP) for accurate results.


<details>
  <summary>Details</summary>
Motivation: Static Timing Analysis provides an upper bound for delay, leading to suboptimal processor speeds. The goal is to compute the actual maximum delay for better performance.

Method: The problem is modeled in Answer Set Programming (ASP) with non-trivial encodings, leveraging ASP's efficient solvers.

Result: Experimental results demonstrate ASP's viability for solving complex hardware design problems.

Conclusion: ASP is an effective tool for accurately determining maximum delays in integrated circuits, improving performance over traditional methods.

Abstract: In the design of integrated circuits, one critical metric is the maximum
delay introduced by combinational modules within the circuit. This delay is
crucial because it represents the time required to perform a computation: in an
Arithmetic-Logic Unit it represents the maximum time taken by the circuit to
perform an arithmetic operation. When such a circuit is part of a larger,
synchronous system, like a CPU, the maximum delay directly impacts the maximum
clock frequency of the entire system. Typically, hardware designers use Static
Timing Analysis to compute an upper bound of the maximum delay because it can
be determined in polynomial time. However, relying on this upper bound can lead
to suboptimal processor speeds, thereby missing performance opportunities. In
this work, we tackle the challenging task of computing the actual maximum
delay, rather than an approximate value. Since the problem is computationally
hard, we model it in Answer Set Programming (ASP), a logic language featuring
extremely efficient solvers. We propose non-trivial encodings of the problem
into ASP. Experimental results show that ASP is a viable solution to address
complex problems in hardware design.

</details>


### [25] [DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway Global-Local Fusion](https://arxiv.org/abs/2507.11229)
*Jin Li,Zezhong Ding,Xike Xie*

Main category: cs.AI

TL;DR: DuetGraph is a coarse-to-fine KG reasoning method that separates global and local information processing to prevent over-smoothing, improving reasoning quality and training efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing KG reasoning methods suffer from score over-smoothing, which blurs distinctions between correct and incorrect answers, reducing effectiveness.

Method: DuetGraph uses dual-pathway global-local fusion (message passing for local, attention for global) and coarse-to-fine optimization to partition entities into high- and low-score subsets.

Result: DuetGraph achieves SOTA performance with up to 8.7% better reasoning quality and 1.8x faster training efficiency.

Conclusion: DuetGraph effectively addresses over-smoothing and enhances KG reasoning, demonstrating significant improvements in performance and efficiency.

Abstract: Knowledge graphs (KGs) are vital for enabling knowledge reasoning across
various domains. Recent KG reasoning methods that integrate both global and
local information have achieved promising results. However, existing methods
often suffer from score over-smoothing, which blurs the distinction between
correct and incorrect answers and hinders reasoning effectiveness. To address
this, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with
dual-pathway global-local fusion. DuetGraph tackles over-smoothing by
segregating -- rather than stacking -- the processing of local (via message
passing) and global (via attention) information into two distinct pathways,
preventing mutual interference and preserving representational discrimination.
In addition, DuetGraph introduces a coarse-to-fine optimization, which
partitions entities into high- and low-score subsets. This strategy narrows the
candidate space and sharpens the score gap between the two subsets, which
alleviates over-smoothing and enhances inference quality. Extensive experiments
on various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA)
performance, with up to an 8.7% improvement in reasoning quality and a
1.8$\times$ acceleration in training efficiency.

</details>


### [26] [Taming Uncertainty via Automation: Observing, Analyzing, and Optimizing Agentic AI Systems](https://arxiv.org/abs/2507.11277)
*Dany Moshkovich,Sergey Zeltyn*

Main category: cs.AI

TL;DR: AgentOps is a framework for managing uncertainty in LLM-powered agentic systems, addressing the needs of developers, testers, SREs, and business users through a six-stage automation pipeline.


<details>
  <summary>Details</summary>
Motivation: Traditional observability practices are inadequate for the unique uncertainties in LLM-based agentic systems, necessitating a specialized framework.

Method: Introduces the AgentOps Automation Pipeline, a six-stage process for observing, analyzing, optimizing, and automating agentic AI operations.

Result: Provides a structured approach to manage uncertainty, enabling safe and adaptive operation of agentic systems.

Conclusion: AgentOps tames uncertainty in LLM-powered systems, ensuring their effective and self-improving operation.

Abstract: Large Language Models (LLMs) are increasingly deployed within agentic
systems-collections of interacting, LLM-powered agents that execute complex,
adaptive workflows using memory, tools, and dynamic planning. While enabling
powerful new capabilities, these systems also introduce unique forms of
uncertainty stemming from probabilistic reasoning, evolving memory states, and
fluid execution paths. Traditional software observability and operations
practices fall short in addressing these challenges.
  This paper introduces AgentOps: a comprehensive framework for observing,
analyzing, optimizing, and automating operation of agentic AI systems. We
identify distinct needs across four key roles-developers, testers, site
reliability engineers (SREs), and business users-each of whom engages with the
system at different points in its lifecycle. We present the AgentOps Automation
Pipeline, a six-stage process encompassing behavior observation, metric
collection, issue detection, root cause analysis, optimized recommendations,
and runtime automation. Throughout, we emphasize the critical role of
automation in managing uncertainty and enabling self-improving AI systems-not
by eliminating uncertainty, but by taming it to ensure safe, adaptive, and
effective operation.

</details>


### [27] [Opus: A Prompt Intention Framework for Complex Workflow Generation](https://arxiv.org/abs/2507.11288)
*Théo Fagnoni,Mahsun Altin,Chia En Chung,Phillip Kingston,Alan Tuning,Dana O. Mohamed,Inès Adnani*

Main category: cs.AI

TL;DR: The Opus Prompt Intention Framework improves workflow generation with LLMs by adding an intention capture layer, enhancing output quality and scalability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating logical and meaningful workflows from complex user queries using LLMs.

Method: Introduces an intermediate Intention Capture layer with Workflow Signals and Workflow Intentions, applied to LLM-driven workflow generation.

Result: Shows consistent improvements in semantic workflow similarity metrics on a benchmark of 1,000 multi-intent query-workflow pairs.

Conclusion: The framework significantly enhances workflow generation quality, especially for mixed intention elicitation, compared to direct generation.

Abstract: This paper introduces the Opus Prompt Intention Framework, designed to
improve complex Workflow Generation with instruction-tuned Large Language
Models (LLMs). We propose an intermediate Intention Capture layer between user
queries and Workflow Generation, implementing the Opus Workflow Intention
Framework, which consists of extracting Workflow Signals from user queries,
interpreting them into structured Workflow Intention objects, and generating
Workflows based on these Intentions. Our results show that this layer enables
LLMs to produce logical and meaningful outputs that scale reliably as query
complexity increases. On a synthetic benchmark of 1,000 multi-intent
query-Workflow(s) pairs, applying the Opus Prompt Intention Framework to
Workflow Generation yields consistent improvements in semantic Workflow
similarity metrics. In this paper, we introduce the Opus Prompt Intention
Framework by applying the concepts of Workflow Signal and Workflow Intention to
LLM-driven Workflow Generation. We present a reproducible, customizable
LLM-based Intention Capture system to extract Workflow Signals and Workflow
Intentions from user queries. Finally, we provide empirical evidence that the
proposed system significantly improves Workflow Generation quality compared to
direct generation from user queries, particularly in cases of Mixed Intention
Elicitation.

</details>


### [28] [Contestability in Quantitative Argumentation](https://arxiv.org/abs/2507.11323)
*Xiang Yin,Nico Potyka,Antonio Rago,Timotheus Kampik,Francesca Toni*

Main category: cs.AI

TL;DR: The paper proposes using Edge-Weighted Quantitative Bipolar Argumentation Frameworks (EW-QBAFs) to align AI decisions with human preferences, introducing gradient-based relation attribution explanations (G-RAEs) to adjust edge weights for contestability.


<details>
  <summary>Details</summary>
Motivation: To ensure AI-driven decisions align with human preferences, the paper explores EW-QBAFs, which have been underutilized for contestability.

Method: Introduces G-RAEs to quantify sensitivity of argument strength to edge weight changes and develops an iterative algorithm for weight adjustments.

Result: Experimental evaluation on synthetic EW-QBAFs shows the approach effectively achieves desired argument strength.

Conclusion: The proposed method successfully addresses the contestability problem in EW-QBAFs, providing interpretable guidance for aligning AI decisions with human preferences.

Abstract: Contestable AI requires that AI-driven decisions align with human
preferences. While various forms of argumentation have been shown to support
contestability, Edge-Weighted Quantitative Bipolar Argumentation Frameworks
(EW-QBAFs) have received little attention. In this work, we show how EW-QBAFs
can be deployed for this purpose. Specifically, we introduce the contestability
problem for EW-QBAFs, which asks how to modify edge weights (e.g., preferences)
to achieve a desired strength for a specific argument of interest (i.e., a
topic argument). To address this problem, we propose gradient-based relation
attribution explanations (G-RAEs), which quantify the sensitivity of the topic
argument's strength to changes in individual edge weights, thus providing
interpretable guidance for weight adjustments towards contestability. Building
on G-RAEs, we develop an iterative algorithm that progressively adjusts the
edge weights to attain the desired strength. We evaluate our approach
experimentally on synthetic EW-QBAFs that simulate the structural
characteristics of personalised recommender systems and multi-layer
perceptrons, and demonstrate that it can solve the problem effectively.

</details>


### [29] [CogDDN: A Cognitive Demand-Driven Navigation with Decision Optimization and Dual-Process Thinking](https://arxiv.org/abs/2507.11334)
*Yuehao Huang,Liang Liu,Shuangming Lei,Yukai Ma,Hao Su,Jianbiao Mei,Pengxiang Zhao,Yaqing Gu,Yong Liu,Jiajun Lv*

Main category: cs.AI

TL;DR: CogDDN is a VLM-based framework for demand-driven navigation, integrating fast and slow thinking systems to improve robot adaptability in unknown environments.


<details>
  <summary>Details</summary>
Motivation: Traditional data-driven DDN methods lack generalization in unseen scenarios, prompting the need for a cognitive approach.

Method: CogDDN uses semantic alignment of objects with instructions and a dual-process decision-making module (Heuristic and Analytic Processes) with CoT reasoning.

Result: CogDDN outperforms single-view camera-only methods by 15% in navigation accuracy on the AI2Thor simulator.

Conclusion: CogDDN enhances robot navigation by emulating human cognition, improving adaptability and performance in unstructured environments.

Abstract: Mobile robots are increasingly required to navigate and interact within
unknown and unstructured environments to meet human demands. Demand-driven
navigation (DDN) enables robots to identify and locate objects based on
implicit human intent, even when object locations are unknown. However,
traditional data-driven DDN methods rely on pre-collected data for model
training and decision-making, limiting their generalization capability in
unseen scenarios. In this paper, we propose CogDDN, a VLM-based framework that
emulates the human cognitive and learning mechanisms by integrating fast and
slow thinking systems and selectively identifying key objects essential to
fulfilling user demands. CogDDN identifies appropriate target objects by
semantically aligning detected objects with the given instructions.
Furthermore, it incorporates a dual-process decision-making module, comprising
a Heuristic Process for rapid, efficient decisions and an Analytic Process that
analyzes past errors, accumulates them in a knowledge base, and continuously
improves performance. Chain of Thought (CoT) reasoning strengthens the
decision-making process. Extensive closed-loop evaluations on the AI2Thor
simulator with the ProcThor dataset show that CogDDN outperforms single-view
camera-only methods by 15%, demonstrating significant improvements in
navigation accuracy and adaptability. The project page is available at
https://yuehaohuang.github.io/CogDDN/.

</details>


### [30] [Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces](https://arxiv.org/abs/2507.11352)
*Yunhao Yang,Neel P. Bhatt,Christian Ellis,Alvaro Velasquez,Zhangyang Wang,Ufuk Topcu*

Main category: cs.AI

TL;DR: A neurosymbolic framework combines natural-language dialogue with verifiable guarantees for logistics planning, improving accuracy and speed over traditional methods and LLMs.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of slow integer programming and unreliable large language models (LLMs) in logistics planning by ensuring safety, speed, and user alignment.

Method: Introduces a neurosymbolic framework that converts user requests into structured plans, quantifies uncertainty, and uses an interactive clarification loop for low-confidence cases.

Result: A lightweight model fine-tuned on 100 examples outperforms GPT-4.1 in zero-shot performance and reduces inference latency by nearly 50%.

Conclusion: The framework offers a practical solution for certifiable, real-time, and user-aligned logistics decision-making.

Abstract: Logistics operators, from battlefield coordinators rerouting airlifts ahead
of a storm to warehouse managers juggling late trucks, often face life-critical
decisions that demand both domain expertise and rapid and continuous
replanning. While popular methods like integer programming yield logistics
plans that satisfy user-defined logical constraints, they are slow and assume
an idealized mathematical model of the environment that does not account for
uncertainty. On the other hand, large language models (LLMs) can handle
uncertainty and promise to accelerate replanning while lowering the barrier to
entry by translating free-form utterances into executable plans, yet they
remain prone to misinterpretations and hallucinations that jeopardize safety
and cost. We introduce a neurosymbolic framework that pairs the accessibility
of natural-language dialogue with verifiable guarantees on goal interpretation.
It converts user requests into structured planning specifications, quantifies
its own uncertainty at the field and token level, and invokes an interactive
clarification loop whenever confidence falls below an adaptive threshold. A
lightweight model, fine-tuned on just 100 uncertainty-filtered examples,
surpasses the zero-shot performance of GPT-4.1 while cutting inference latency
by nearly 50%. These preliminary results highlight a practical path toward
certifiable, real-time, and user-aligned decision-making for complex logistics.

</details>


### [31] [Modeling Code: Is Text All You Need?](https://arxiv.org/abs/2507.11467)
*Daniel Nichols,Konstantinos Parasyris,Harshitha Menon,Brian R. Bartoldson,Giorgis Georgakoudis,Tal Ben-Nun,Abhinav Bhatele*

Main category: cs.AI

TL;DR: A novel approach combines the strengths of code-as-text modeling (using LLMs) and structured representations (like graphs) to enhance reasoning about code properties.


<details>
  <summary>Details</summary>
Motivation: Transformer-based LLMs excel in generative tasks but struggle with structured reasoning (e.g., control/data flow). Existing structured approaches lack LLMs' generative power and scalability.

Method: Introduces a hybrid approach integrating code-as-text modeling (LLMs) with structured representations (e.g., graphs) for better reasoning.

Result: Expected to improve reasoning about analytical code properties while retaining generative capabilities.

Conclusion: The proposed hybrid method aims to bridge the gap between generative and structured reasoning in code modeling.

Abstract: Code LLMs have become extremely popular recently for modeling source code
across a variety of tasks, such as generation, translation, and summarization.
However, transformer-based models are limited in their capabilities to reason
through structured, analytical properties of code, such as control and data
flow. Previous work has explored the modeling of these properties with
structured data and graph neural networks. However, these approaches lack the
generative capabilities and scale of modern LLMs. In this work, we introduce a
novel approach to combine the strengths of modeling both code as text and more
structured forms.

</details>


### [32] [Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety](https://arxiv.org/abs/2507.11473)
*Tomek Korbak,Mikita Balesni,Elizabeth Barnes,Yoshua Bengio,Joe Benton,Joseph Bloom,Mark Chen,Alan Cooney,Allan Dafoe,Anca Dragan,Scott Emmons,Owain Evans,David Farhi,Ryan Greenblatt,Dan Hendrycks,Marius Hobbhahn,Evan Hubinger,Geoffrey Irving,Erik Jenner,Daniel Kokotajlo,Victoria Krakovna,Shane Legg,David Lindner,David Luan,Aleksander Mądry,Julian Michael,Neel Nanda,Dave Orr,Jakub Pachocki,Ethan Perez,Mary Phuong,Fabien Roger,Joshua Saxe,Buck Shlegeris,Martín Soto,Eric Steinberger,Jasmine Wang,Wojciech Zaremba,Bowen Baker,Rohin Shah,Vlad Mikulik*

Main category: cs.AI

TL;DR: AI systems using human language for thought chains (CoT) can be monitored for misbehavior, though imperfectly. Further research and investment in CoT monitoring are recommended, alongside existing safety methods.


<details>
  <summary>Details</summary>
Motivation: To enhance AI safety by leveraging human-language-based thought chains for monitoring misbehavior.

Method: Monitoring chains of thought (CoT) in AI systems that "think" in human language.

Result: CoT monitoring is imperfect but promising, suggesting its integration with other safety methods.

Conclusion: Frontier model developers should consider CoT monitorability in their decisions, as it may be fragile.

Abstract: AI systems that "think" in human language offer a unique opportunity for AI
safety: we can monitor their chains of thought (CoT) for the intent to
misbehave. Like all other known AI oversight methods, CoT monitoring is
imperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows
promise and we recommend further research into CoT monitorability and
investment in CoT monitoring alongside existing safety methods. Because CoT
monitorability may be fragile, we recommend that frontier model developers
consider the impact of development decisions on CoT monitorability.

</details>


### [33] [Perspective-Aware AI in Extended Reality](https://arxiv.org/abs/2507.11479)
*Daniel Platnick,Matti Gruener,Marjan Alirezaie,Kent Larson,Dava J. Newman,Hossein Rahnama*

Main category: cs.AI

TL;DR: PAiR integrates Perspective-Aware AI with XR for adaptive, immersive experiences using user identity models called Chronicles.


<details>
  <summary>Details</summary>
Motivation: Current XR systems lack deep user modeling and cognitive context, limiting adaptive experiences.

Method: PAiR uses Chronicles (identity models from multimodal data) in a closed-loop system to link user states with XR environments.

Result: Implemented in Unity-based OpenDome, PAiR shows promise in two proof-of-concept scenarios.

Conclusion: PAiR advances human-AI interaction by embedding perspective-based identity models into XR.

Abstract: AI-enhanced Extended Reality (XR) aims to deliver adaptive, immersive
experiences-yet current systems fall short due to shallow user modeling and
limited cognitive context. We introduce Perspective-Aware AI in Extended
Reality (PAiR), a foundational framework for integrating Perspective-Aware AI
(PAi) with XR to enable interpretable, context-aware experiences grounded in
user identity. PAi is built on Chronicles: reasoning-ready identity models
learned from multimodal digital footprints that capture users' cognitive and
experiential evolution. PAiR employs these models in a closed-loop system
linking dynamic user states with immersive environments. We present PAiR's
architecture, detailing its modules and system flow, and demonstrate its
utility through two proof-of-concept scenarios implemented in the Unity-based
OpenDome engine. PAiR opens a new direction for human-AI interaction by
embedding perspective-based identity models into immersive systems.

</details>


### [34] [Illuminating the Three Dogmas of Reinforcement Learning under Evolutionary Light](https://arxiv.org/abs/2507.11482)
*Mani Hamidi,Terrence W. Deacon*

Main category: cs.AI

TL;DR: The paper critiques three core tenets of reinforcement learning (RL) and proposes an evolutionary-inspired framework to rethink them, addressing agency, learning objectives, and the reward hypothesis.


<details>
  <summary>Details</summary>
Motivation: To challenge and revise foundational assumptions in RL, drawing parallels with evolutionary theory to better model biological learning.

Method: The authors revisit each RL dogma, using evolutionary insights to reframe learning as adaptation, address multi-objective rewards, and explore agency. They also integrate thermodynamics from origins-of-life theory.

Result: The framework enriches RL theory by aligning it with evolutionary dynamics and biological learning, though agency remains unresolved without additional thermodynamic insights.

Conclusion: Evolutionary theory offers valuable perspectives for RL, but agency requires further integration with origins-of-life concepts to fully address its complexities.

Abstract: Three core tenets of reinforcement learning (RL)--concerning the definition
of agency, the objective of learning, and the scope of the reward
hypothesis--have been highlighted as key targets for conceptual revision, with
major implications for theory and application. We propose a framework, inspired
by open-ended evolutionary theory, to reconsider these three "dogmas." We
revisit each assumption and address related concerns raised alongside them. To
make our arguments relevant to RL as a model of biological learning, we first
establish that evolutionary dynamics can plausibly operate within living brains
over an individual's lifetime, and are not confined to cross-generational
processes. We begin by revisiting the second dogma, drawing on evolutionary
insights to enrich the "adaptation-rather-than-search" view of learning. We
then address the third dogma regarding the limits of the reward hypothesis,
using analogies from evolutionary fitness to illuminate the scalar reward vs.
multi-objective debate. After discussing practical implications for exploration
in RL, we turn to the first--and arguably most fundamental--issue: the absence
of a formal account of agency. We argue that unlike the other two problems, the
evolutionary paradigm alone cannot resolve the agency question, though it
gestures in a productive direction. We advocate integrating ideas from
origins-of-life theory, where the thermodynamics of sustenance and replication
offer promising foundations for understanding agency and resource-constrained
reinforcement learning in biological systems.

</details>


### [35] [DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering](https://arxiv.org/abs/2507.11527)
*Yinsheng Li,Zhen Dong,Yi Shao*

Main category: cs.AI

TL;DR: DrafterBench is a benchmark for evaluating LLM agents in technical drawing revision tasks in civil engineering, featuring 12 task types, 46 tools, and 1920 tasks.


<details>
  <summary>Details</summary>
Motivation: The need for systematic evaluation of LLM agents in industrial tasks, particularly civil engineering, due to the lack of benchmarks.

Method: Creation of DrafterBench, an open-source toolkit with diverse tasks and tools to assess LLM agents' capabilities in technical drawing revision.

Result: DrafterBench provides detailed accuracy and error analysis, identifying strengths and improvement areas for LLM agents in engineering tasks.

Conclusion: DrafterBench fills a gap in evaluating LLM agents for industrial applications, offering insights for future improvements in engineering automation.

Abstract: Large Language Model (LLM) agents have shown great potential for solving
real-world problems and promise to be a solution for tasks automation in
industry. However, more benchmarks are needed to systematically evaluate
automation agents from an industrial perspective, for example, in Civil
Engineering. Therefore, we propose DrafterBench for the comprehensive
evaluation of LLM agents in the context of technical drawing revision, a
representation task in civil engineering. DrafterBench contains twelve types of
tasks summarized from real-world drawing files, with 46 customized
functions/tools and 1920 tasks in total. DrafterBench is an open-source
benchmark to rigorously test AI agents' proficiency in interpreting intricate
and long-context instructions, leveraging prior knowledge, and adapting to
dynamic instruction quality via implicit policy awareness. The toolkit
comprehensively assesses distinct capabilities in structured data
comprehension, function execution, instruction following, and critical
reasoning. DrafterBench offers detailed analysis of task accuracy and error
statistics, aiming to provide deeper insight into agent capabilities and
identify improvement targets for integrating LLMs in engineering applications.
Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,
with the test set hosted at
https://huggingface.co/datasets/Eason666/DrafterBench.

</details>


### [36] [How Many Instructions Can LLMs Follow at Once?](https://arxiv.org/abs/2507.11538)
*Daniel Jaroslawicz,Brendan Whiting,Parth Shah,Karime Maamari*

Main category: cs.AI

TL;DR: IFScale benchmark evaluates LLMs' instruction-following performance at high densities, revealing degradation patterns and biases, with top models achieving only 68% accuracy at 500 instructions.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack evaluation of LLMs' performance at high instruction densities, critical for real-world applications.

Method: Introduces IFScale, a benchmark with 500 keyword-inclusion instructions for a business report task, testing 20 state-of-the-art models.

Result: Best models achieve 68% accuracy at max density; performance degrades with size, reasoning, and biases toward earlier instructions.

Conclusion: IFScale highlights tradeoffs in instruction-dense prompts, aiding real-world LLM system design, with open-sourced benchmark for further study.

Abstract: Production-grade LLM systems require robust adherence to dozens or even
hundreds of instructions simultaneously. However, the instruction-following
capabilities of LLMs at high instruction densities have not yet been
characterized, as existing benchmarks only evaluate models on tasks with a
single or few instructions. We introduce IFScale, a simple benchmark of 500
keyword-inclusion instructions for a business report writing task to measure
how instruction-following performance degrades as instruction density
increases. We evaluate 20 state-of-the-art models across seven major providers
and find that even the best frontier models only achieve 68% accuracy at the
max density of 500 instructions. Our analysis reveals model size and reasoning
capability to correlate with 3 distinct performance degradation patterns, bias
towards earlier instructions, and distinct categories of instruction-following
errors. Our insights can help inform design of instruction-dense prompts in
real-world applications and highlight important performance-latency tradeoffs.
We open-source the benchmark and all results for further analysis at
https://distylai.github.io/IFScale.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [37] [Tool-to-Tool Matching Analysis Based Difference Score Computation Methods for Semiconductor Manufacturing](https://arxiv.org/abs/2507.10564)
*Sameera Bharadwaja H.,Siddhrath Jandial,Shashank S. Agashe,Rajesh Kumar Reddy Moore,Youngkwan Kim*

Main category: cs.LG

TL;DR: Proposes novel tool-to-tool matching (TTTM) pipelines for semiconductor manufacturing, addressing limitations of traditional methods in heterogeneous settings. Achieves high correlation coefficients (>0.95 for variance, >0.5 for modes).


<details>
  <summary>Details</summary>
Motivation: Traditional TTTM methods rely on static data or golden references, which are hard to obtain and ineffective in heterogeneous equipment settings.

Method: Novel TTTM pipelines analyze variance and modes in data, hypothesizing mismatched equipment show higher variance/modes. Includes univariate and multivariate methods.

Result: Univariate method achieves >0.95 correlation with variance, >0.5 with modes. Multivariate method achieves >0.75 correlation with top univariate methods.

Conclusion: Proposed methods effectively address TTTM challenges in heterogeneous settings, with high correlation results and sensitivity analysis for robustness.

Abstract: We consider the problem of tool-to-tool matching (TTTM), also called, chamber
matching in the context of a semiconductor manufacturing equipment. Traditional
TTTM approaches utilize static configuration data or depend on a golden
reference which are difficult to obtain in a commercial manufacturing line.
Further, existing methods do not extend very well to a heterogeneous setting,
where equipment are of different make-and-model, sourced from different
equipment vendors. We propose novel TTTM analysis pipelines to overcome these
issues. We hypothesize that a mismatched equipment would have higher variance
and/or higher number of modes in the data. Our best univariate method achieves
a correlation coefficient >0.95 and >0.5 with the variance and number of modes,
respectively showing that the proposed methods are effective. Also, the best
multivariate method achieves a correlation coefficient >0.75 with the
top-performing univariate methods, showing its effectiveness. Finally, we
analyze the sensitivity of the multivariate algorithms to the algorithm
hyper-parameters.

</details>


### [38] [Enhancing Cross Entropy with a Linearly Adaptive Loss Function for Optimized Classification Performance](https://arxiv.org/abs/2507.10574)
*Jae Wan Shim*

Main category: cs.LG

TL;DR: A novel Linearly Adaptive Cross Entropy Loss function is proposed, outperforming standard cross entropy in classification tasks with improved accuracy while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance optimization in classification tasks with one-hot encoded labels by introducing a term dependent on the predicted probability of the true class.

Method: Derived from information theory, the loss function includes an additional term for the true class probability. Evaluated on a ResNet model using CIFAR-100.

Result: Consistently outperforms standard cross entropy in accuracy while maintaining similar efficiency.

Conclusion: The proposed loss function shows promise for future research in loss function design.

Abstract: We propose the Linearly Adaptive Cross Entropy Loss function. This is a novel
measure derived from the information theory. In comparison to the standard
cross entropy loss function, the proposed one has an additional term that
depends on the predicted probability of the true class. This feature serves to
enhance the optimization process in classification tasks involving one-hot
encoded class labels. The proposed one has been evaluated on a ResNet-based
model using the CIFAR-100 dataset. Preliminary results show that the proposed
one consistently outperforms the standard cross entropy loss function in terms
of classification accuracy. Moreover, the proposed one maintains simplicity,
achieving practically the same efficiency to the traditional cross entropy
loss. These findings suggest that our approach could broaden the scope for
future research into loss function design.

</details>


### [39] [An Adaptive Volatility-based Learning Rate Scheduler](https://arxiv.org/abs/2507.10575)
*Kieran Chai Kai Ren*

Main category: cs.LG

TL;DR: VolSched, a novel adaptive learning rate scheduler, dynamically adjusts LR using volatility metrics, improving model accuracy and generalization on CIFAR-100.


<details>
  <summary>Details</summary>
Motivation: Pre-defined and adaptive LR schedulers often lead to suboptimal generalization, prompting the need for a more dynamic approach.

Method: VolSched calculates long-term vs. short-term accuracy volatility to adjust LR, escaping plateaus and stabilizing training.

Result: VolSched improves top-1 accuracy by 1.4 and 1.3 points on ResNet-18 and ResNet-34, respectively, and finds flatter minima (38% flatter).

Conclusion: VolSched enhances exploration and generalization, outperforming baselines by dynamically adapting LR based on volatility.

Abstract: Effective learning rate (LR) scheduling is crucial for training deep neural
networks. However, popular pre-defined and adaptive schedulers can still lead
to suboptimal generalization. This paper introduces VolSched, a novel adaptive
LR scheduler inspired by the concept of volatility in stochastic processes like
Geometric Brownian Motion to dynamically adjust the learning rate. By
calculating the ratio between long-term and short-term accuracy volatility,
VolSched increases the LR to escape plateaus and decreases it to stabilize
training, allowing the model to explore the loss landscape more effectively. We
evaluate VolSched on the CIFAR-100 dataset against a strong baseline using a
standard augmentation pipeline. When paired with ResNet-18 and ResNet-34, our
scheduler delivers consistent performance gains, improving top-1 accuracy by
1.4 and 1.3 percentage points respectively. Analysis of the loss curves reveals
that VolSched promotes a longer exploration phase. A quantitative analysis of
the Hessian shows that VolSched finds a final solution that is 38% flatter than
the next-best baseline, allowing the model to obtain wider minima and hence
better generalization performance.

</details>


### [40] [Universal Approximation Theorem for a Single-Layer Transformer](https://arxiv.org/abs/2507.10581)
*Esmail Gumaan*

Main category: cs.LG

TL;DR: The paper explores the theoretical foundations of deep learning and Transformers, proving a universal approximation theorem for single-layer Transformers.


<details>
  <summary>Details</summary>
Motivation: Despite the success of deep learning and Transformers in various domains, their theoretical understanding remains limited. This paper aims to bridge that gap.

Method: The study reviews key mathematical concepts, analyzes the self-attention mechanism and backpropagation, and proves a universal approximation theorem for Transformers.

Result: The main result is a proof that a single-layer Transformer can approximate any continuous sequence-to-sequence mapping to arbitrary precision.

Conclusion: The findings advance the theoretical understanding of Transformers and help connect theory with practical applications.

Abstract: Deep learning employs multi-layer neural networks trained via the
backpropagation algorithm. This approach has achieved success across many
domains and relies on adaptive gradient methods such as the Adam optimizer.
Sequence modeling evolved from recurrent neural networks to attention-based
models, culminating in the Transformer architecture. Transformers have achieved
state-of-the-art performance in natural language processing (for example, BERT
and GPT-3) and have been applied in computer vision and computational biology.
However, theoretical understanding of these models remains limited. In this
paper, we examine the mathematical foundations of deep learning and
Transformers and present a novel theoretical result. We review key concepts
from linear algebra, probability, and optimization that underpin deep learning,
and we analyze the multi-head self-attention mechanism and the backpropagation
algorithm in detail. Our main contribution is a universal approximation theorem
for Transformers: we prove that a single-layer Transformer, comprising one
self-attention layer followed by a position-wise feed-forward network with ReLU
activation, can approximate any continuous sequence-to-sequence mapping on a
compact domain to arbitrary precision. We provide a formal statement and a
complete proof. Finally, we present case studies that demonstrate the practical
implications of this result. Our findings advance the theoretical understanding
of Transformer models and help bridge the gap between theory and practice.

</details>


### [41] [MH-FSF: A Unified Framework for Overcoming Benchmarking and Reproducibility Limitations in Feature Selection Evaluation](https://arxiv.org/abs/2507.10591)
*Vanderson Rocha,Diego Kreutz,Gabriel Canto,Hendrio Bragança,Eduardo Feitosa*

Main category: cs.LG

TL;DR: The MH-FSF framework is introduced to address reproducibility and benchmarking issues in feature selection research, offering a modular platform with 17 methods and evaluations on 10 Android malware datasets.


<details>
  <summary>Details</summary>
Motivation: Current feature selection research lacks reproducibility due to limited benchmarking and proprietary datasets, impacting performance and consistency.

Method: The MH-FSF framework provides implementations of 17 feature selection methods (11 classical, 6 domain-specific) and evaluates them on 10 public Android malware datasets.

Result: Performance variations are observed across balanced and imbalanced datasets, emphasizing the need for tailored preprocessing and selection criteria.

Conclusion: The MH-FSF framework enhances methodological consistency, broadens literature, and opens new research directions in feature selection, especially for Android malware detection.

Abstract: Feature selection is vital for building effective predictive models, as it
reduces dimensionality and emphasizes key features. However, current research
often suffers from limited benchmarking and reliance on proprietary datasets.
This severely hinders reproducibility and can negatively impact overall
performance. To address these limitations, we introduce the MH-FSF framework, a
comprehensive, modular, and extensible platform designed to facilitate the
reproduction and implementation of feature selection methods. Developed through
collaborative research, MH-FSF provides implementations of 17 methods (11
classical, 6 domain-specific) and enables systematic evaluation on 10 publicly
available Android malware datasets. Our results reveal performance variations
across both balanced and imbalanced datasets, highlighting the critical need
for data preprocessing and selection criteria that account for these
asymmetries. We demonstrate the importance of a unified platform for comparing
diverse feature selection techniques, fostering methodological consistency and
rigor. By providing this framework, we aim to significantly broaden the
existing literature and pave the way for new research directions in feature
selection, particularly within the context of Android malware detection.

</details>


### [42] [Extension OL-MDISF: Online Learning from Mix-Typed, Drifted, and Incomplete Streaming Features](https://arxiv.org/abs/2507.10594)
*Shengda Zhuo,Di Wu,Yi He,Shuqiang Huang,Xindong Wu*

Main category: cs.LG

TL;DR: OL-MDISF addresses challenges in online learning with mixed, drifted, and incomplete features using latent copula-based representation, drift detection, and pseudo-labeling.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle challenges in online learning: heterogeneous data, distribution shifts, and labeling constraints.

Method: OL-MDISF uses latent copula-based representation, ensemble entropy for drift detection, and structure-aware pseudo-labeling.

Result: Tested on 14 real-world datasets, the method shows effectiveness in handling drift and weak supervision.

Conclusion: The paper provides a reproducible benchmark for online learning with complex, weakly supervised streaming data.

Abstract: Online learning, where feature spaces can change over time, offers a flexible
learning paradigm that has attracted considerable attention. However, it still
faces three significant challenges. First, the heterogeneity of real-world data
streams with mixed feature types presents challenges for traditional parametric
modeling. Second, data stream distributions can shift over time, causing an
abrupt and substantial decline in model performance. Third, it is often
infeasible to label every data instance due to time and cost constraints. To
address these issues, we proposed OL-MDISF (Online Learning from Mix-typed,
Drifted, and Incomplete Streaming Features), which constructs a latent
copula-based representation for heterogeneous features, detects drifts via
ensemble entropy and latent mismatch, and performs structure-aware
pseudo-labeling.
  This companion paper serves as a standalone technical reference to OL-MDISF.
It provides a contextual discussion of related work in mixed-type modeling,
drift adaptation, and weak supervision, as well as a comprehensive set of
experiments across 14 real-world datasets under two types of drift scenarios.
These include CER trends, ablation studies, sensitivity analyses, and temporal
ensemble dynamics. We hope this document offers a reproducible benchmark for
online learning on complex, weakly supervised streaming data.

</details>


### [43] [Divide-Then-Rule: A Cluster-Driven Hierarchical Interpolator for Attribute-Missing Graphs](https://arxiv.org/abs/2507.10595)
*Yaowen Hu,Wenxuan Tu,Yue Liu,Miaomiao Li,Wenpeng Lu,Zhigang Luo,Xinwang Liu,Ping Chen*

Main category: cs.LG

TL;DR: Proposes DTRGC for clustering attribute-missing graphs by iteratively imputing missing attributes using neighborhood and clustering information, improving performance.


<details>
  <summary>Details</summary>
Motivation: Existing imputation methods for attribute-missing graphs often fail due to varying neighborhood information, leading to unreliable results.

Method: DTRGC uses Dynamic Cluster-Aware Feature Propagation (DCFP), Hierarchical Neighborhood-aware Imputation (HNAI), and Hop-wise Representation Enhancement (HRE) to iteratively impute and refine missing attributes.

Result: DTRGC significantly improves clustering performance on six graph datasets.

Conclusion: DTRGC effectively addresses attribute-missing graph clustering by leveraging neighborhood and clustering information.

Abstract: Deep graph clustering (DGC) for attribute-missing graphs is an unsupervised
task aimed at partitioning nodes with incomplete attributes into distinct
clusters. Addressing this challenging issue is vital for practical
applications. However, research in this area remains underexplored. Existing
imputation methods for attribute-missing graphs often fail to account for the
varying amounts of information available across node neighborhoods, leading to
unreliable results, especially for nodes with insufficient known neighborhood.
To address this issue, we propose a novel method named Divide-Then-Rule Graph
Completion (DTRGC). This method first addresses nodes with sufficient known
neighborhood information and treats the imputed results as new knowledge to
iteratively impute more challenging nodes, while leveraging clustering
information to correct imputation errors. Specifically, Dynamic Cluster-Aware
Feature Propagation (DCFP) initializes missing node attributes by adjusting
propagation weights based on the clustering structure. Subsequently,
Hierarchical Neighborhood-aware Imputation (HNAI) categorizes attribute-missing
nodes into three groups based on the completeness of their neighborhood
attributes. The imputation is performed hierarchically, prioritizing the groups
with nodes that have the most available neighborhood information. The cluster
structure is then used to refine the imputation and correct potential errors.
Finally, Hop-wise Representation Enhancement (HRE) integrates information
across multiple hops, thereby enriching the expressiveness of node
representations. Experimental results on six widely used graph datasets show
that DTRGC significantly improves the clustering performance of various DGC
methods under attribute-missing graphs.

</details>


### [44] [RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services](https://arxiv.org/abs/2507.10605)
*Fei Zhao,Chonggang Lu,Yue Wang,Zheyong Xie,Ziyan Liu,Haofu Qian,JianZhao Huang,Fangcheng Shi,Zijie Meng,Hongcheng Guo,Mingqian He,Xinze Lyu,Yiming Lu,Ziyang Xiang,Zheyu Ye,Chengqiang Lu,Zhe Xu,Yi Wu,Yao Hu,Yan Gao,Jun Fan,Xiaolong Jiang,Weiting Liu,Boyang Wang,Shaosheng Cao*

Main category: cs.LG

TL;DR: RedOne is a domain-specific LLM for SNS, outperforming single-task baselines by up to 14.02% in tasks and reducing harmful content exposure by 11.23%.


<details>
  <summary>Details</summary>
Motivation: Address challenges in SNS content management and interaction quality by overcoming limitations of isolated task-focused LLMs.

Method: Three-stage training: continued pretraining, supervised fine-tuning, and preference optimization using real-world data.

Result: Average 14.02% improvement in 8 SNS tasks, 7.56% in bilingual benchmarks, and real-world gains like 11.23% reduction in harmful content exposure.

Conclusion: RedOne is a robust, generalizable LLM for SNS, showing promise for diverse real-world applications.

Abstract: As a primary medium for modern information dissemination, social networking
services (SNS) have experienced rapid growth, which has proposed significant
challenges for platform content management and interaction quality improvement.
Recently, the development of large language models (LLMs) has offered potential
solutions but existing studies focus on isolated tasks, which not only
encounter diminishing benefit from the data scaling within individual scenarios
but also fail to flexibly adapt to diverse real-world context. To address these
challenges, we introduce RedOne, a domain-specific LLM designed to break the
performance bottleneck of single-task baselines and establish a comprehensive
foundation for the SNS. RedOne was developed through a three-stage training
strategy consisting of continue pretraining, supervised fine-tuning, and
preference optimization, using a large-scale real-world dataset. Through
extensive experiments, RedOne maintains strong general capabilities, and
achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56%
in SNS bilingual evaluation benchmark, compared with base models. Furthermore,
through online testing, RedOne reduced the exposure rate in harmful content
detection by 11.23% and improved the click page rate in post-view search by
14.95% compared with single-tasks finetuned baseline models. These results
establish RedOne as a robust domain-specific LLM for SNS, demonstrating
excellent generalization across various tasks and promising applicability in
real-world scenarios.

</details>


### [45] [DALI-PD: Diffusion-based Synthetic Layout Heatmap Generation for ML in Physical Design](https://arxiv.org/abs/2507.10606)
*Bing-Yue Wu,Vidya A. Chhabria*

Main category: cs.LG

TL;DR: DALI-PD is a scalable framework using diffusion models to generate synthetic layout heatmaps for ML in physical design, addressing dataset limitations.


<details>
  <summary>Details</summary>
Motivation: Limited availability of high-quality, large-scale datasets for ML in physical design due to computational cost and IP constraints.

Method: Uses a diffusion model to generate diverse layout heatmaps (power, IR drop, congestion, etc.) quickly.

Result: Created a dataset of 20,000+ layout configurations resembling real layouts, improving ML accuracy for tasks like IR drop prediction.

Conclusion: DALI-PD provides a scalable solution for synthetic dataset generation, enhancing ML research in physical design.

Abstract: Machine learning (ML) has demonstrated significant promise in various
physical design (PD) tasks. However, model generalizability remains limited by
the availability of high-quality, large-scale training datasets. Creating such
datasets is often computationally expensive and constrained by IP. While very
few public datasets are available, they are typically static, slow to generate,
and require frequent updates. To address these limitations, we present DALI-PD,
a scalable framework for generating synthetic layout heatmaps to accelerate ML
in PD research. DALI-PD uses a diffusion model to generate diverse layout
heatmaps via fast inference in seconds. The heatmaps include power, IR drop,
congestion, macro placement, and cell density maps. Using DALI-PD, we created a
dataset comprising over 20,000 layout configurations with varying macro counts
and placements. These heatmaps closely resemble real layouts and improve ML
accuracy on downstream ML tasks such as IR drop or congestion prediction.

</details>


### [46] [A Feed-Forward Artificial Intelligence Pipeline for Sustainable Desalination under Climate Uncertainties: UAE Insights](https://arxiv.org/abs/2507.10609)
*Obumneme Nwafor,Chioma Nwafor,Amro Zakaria,Nkechi Nwankwo*

Main category: cs.LG

TL;DR: The UAE's heavy reliance on energy-intensive desalination faces sustainability challenges due to climate factors like AOD. A two-stage predictive model forecasts AOD and desalination efficiency losses, achieving 98% accuracy, and proposes dust-aware control logic, integrated into a decision-support dashboard.


<details>
  <summary>Details</summary>
Motivation: Address the sustainability challenges of desalination in the UAE, exacerbated by climate uncertainties like AOD, which impact solar-powered systems.

Method: A pipelined two-stage predictive model: first forecasts AOD using satellite data, then predicts desalination efficiency losses. SHAP analysis identifies key degradation drivers. Dust-aware control logic adjusts operations based on predictions.

Result: 98% accuracy in predictions. SHAP reveals key degradation factors. Control logic optimizes desalination plant operations. An interactive dashboard supports decision-making.

Conclusion: The proposed framework effectively addresses desalination sustainability challenges, offering accurate predictions and adaptive control, packaged into a practical decision-support tool.

Abstract: The United Arab Emirates (UAE) relies heavily on seawater desalination to
meet over 90% of its drinking water needs. Desalination processes are highly
energy intensive and account for approximately 15% of the UAE's electricity
consumption, contributing to over 22% of the country's energy-related CO2
emissions. Moreover, these processes face significant sustainability challenges
in the face of climate uncertainties such as rising seawater temperatures,
salinity, and aerosol optical depth (AOD). AOD greatly affects the operational
and economic performance of solar-powered desalination systems through
photovoltaic soiling, membrane fouling, and water turbidity cycles.
  This study proposes a novel pipelined two-stage predictive modelling
architecture: the first stage forecasts AOD using satellite-derived time series
and meteorological data; the second stage uses the predicted AOD and other
meteorological factors to predict desalination performance efficiency losses.
The framework achieved 98% accuracy, and SHAP (SHapley Additive exPlanations)
was used to reveal key drivers of system degradation. Furthermore, this study
proposes a dust-aware rule-based control logic for desalination systems based
on predicted values of AOD and solar efficiency. This control logic is used to
adjust the desalination plant feed water pressure, adapt maintenance
scheduling, and regulate energy source switching.
  To enhance the practical utility of the research findings, the predictive
models and rule-based controls were packaged into an interactive dashboard for
scenario and predictive analytics. This provides a management decision-support
system for climate-adaptive planning.

</details>


### [47] [FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise](https://arxiv.org/abs/2507.10611)
*Mengwen Ye,Yingzi Huangfu,Shujian Gao,Wei Ren,Weifan Liu,Zekuan Yu*

Main category: cs.LG

TL;DR: FedGSCA is a novel FL framework addressing label noise in medical image classification by combining a Global Sample Selector and Client Adaptive Adjustment mechanism, outperforming existing methods in noisy scenarios.


<details>
  <summary>Details</summary>
Motivation: Label noise in FL due to inter-institutional data variability degrades model performance, and existing methods struggle with noise heterogeneity and data imbalance.

Method: FedGSCA uses a Global Sample Selector for noise aggregation and a Client Adaptive Adjustment mechanism (adaptive threshold pseudo-labeling and Robust Credal Labeling Loss) to manage noisy labels and class imbalances.

Result: FedGSCA outperforms state-of-the-art methods on real-world and synthetic datasets, excelling in extreme and heterogeneous noise conditions, and improves model stability.

Conclusion: FedGSCA is effective for noisy medical FL, enhancing robustness and generalizability, making it suitable for real-world applications.

Abstract: Federated Learning (FL) emerged as a solution for collaborative medical image
classification while preserving data privacy. However, label noise, which
arises from inter-institutional data variability, can cause training
instability and degrade model performance. Existing FL methods struggle with
noise heterogeneity and the imbalance in medical data. Motivated by these
challenges, we propose FedGSCA, a novel framework for enhancing robustness in
noisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates
noise knowledge from all clients, effectively addressing noise heterogeneity
and improving global model stability. Furthermore, we develop a Client Adaptive
Adjustment (CAA) mechanism that combines adaptive threshold pseudo-label
generation and Robust Credal Labeling Loss. CAA dynamically adjusts to class
distributions, ensuring the inclusion of minority samples and carefully
managing noisy labels by considering multiple plausible labels. This dual
approach mitigates the impact of noisy data and prevents overfitting during
local training, which improves the generalizability of the model. We evaluate
FedGSCA on one real-world colon slides dataset and two synthetic medical
datasets under various noise conditions, including symmetric, asymmetric,
extreme, and heterogeneous types. The results show that FedGSCA outperforms the
state-of-the-art methods, excelling in extreme and heterogeneous noise
scenarios. Moreover, FedGSCA demonstrates significant advantages in improving
model stability and handling complex noise, making it well-suited for
real-world medical federated learning scenarios.

</details>


### [48] [Sub-Scaling Laws: On the Role of Data Density and Training Strategies in LLMs](https://arxiv.org/abs/2507.10613)
*Zhengyu Chen,Siqi Wang,Teng Xiao,Yudong Wang,Shiqi Chen,Xunliang Cai,Junxian He,Jingang Wang*

Main category: cs.LG

TL;DR: The paper revisits scaling laws in NLP, identifying data quality and training strategies as key factors in sub-scaling. It proposes a new scaling law emphasizing data diversity and optimal resource allocation.


<details>
  <summary>Details</summary>
Motivation: To understand why large language models exhibit sub-scaling (decelerated performance improvements) despite increased size and data.

Method: Empirical analysis of over 400 models to study the impact of data density and resource allocation.

Result: High data density causes diminishing returns, and optimal resource allocation is critical for performance. A new sub-optimal scaling law is proposed.

Conclusion: Data quality and diversity, along with optimal resource allocation, are essential to mitigate sub-scaling and improve model performance.

Abstract: Traditional scaling laws in natural language processing suggest that
increasing model size and training data enhances performance. However, recent
studies reveal deviations, particularly in large language models, where
performance improvements decelerate, which is a phenomenon known as
sub-scaling. This paper revisits these scaling laws by examining the impact of
data quality and training strategies on model performance. Through extensive
empirical analysis of over 400 models, we identify high data density and
non-optimal resource allocation as key factors contributing to sub-scaling.
High data density leads to diminishing returns due to redundant information,
while optimal resource allocation is crucial for sustained performance
improvements. We propose a sub-optimal scaling law that better predicts
performance in sub-scaling regimes, highlighting the importance of data quality
and diversity.

</details>


### [49] [Fine-tuning Large Language Model for Automated Algorithm Design](https://arxiv.org/abs/2507.10614)
*Fei Liu,Rui Zhang,Xi Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: The paper explores fine-tuning LLMs for algorithm design, introducing a Diversity-Aware Rank-based sampling strategy and direct preference optimization to improve performance and generalization across tasks.


<details>
  <summary>Details</summary>
Motivation: Current methods use general-purpose LLMs for algorithm design, raising questions about the need for task-specific LLMs and their effectiveness.

Method: Proposes DAR sampling for balanced training data and direct preference optimization to align LLM outputs with task objectives, tested on Llama-3 models.

Result: Fine-tuned LLMs outperform general ones, with smaller models matching larger ones in some tasks, and show promising generalization to related tasks.

Conclusion: Task-specific LLMs enhance algorithm design performance, offering new research directions.

Abstract: The integration of large language models (LLMs) into automated algorithm
design has shown promising potential. A prevalent approach embeds LLMs within
search routines to iteratively generate and refine candidate algorithms.
However, most existing methods rely on off-the-shelf LLMs trained for general
coding tasks,leaving a key question open: Do we need LLMs specifically tailored
for algorithm design? If so, how can such LLMs be effectively obtained and how
well can they generalize across different algorithm design tasks? In this
paper, we take a first step toward answering these questions by exploring
fine-tuning of LLMs for algorithm design. We introduce a Diversity-Aware Rank
based (DAR) sampling strategy to balance training data diversity and quality,
then we leverage direct preference optimization to efficiently align LLM
outputs with task objectives. Our experiments, conducted on
Llama-3.2-1B-Instruct and Llama- 3.1-8B-Instruct, span three distinct algorithm
design tasks. Results suggest that finetuned LLMs can significantly outperform
their off-the-shelf counterparts with the smaller Llama-3.2-1B-Instruct and
match the larger Llama-3.1-8B-Instruct on the admissible set problem. Moreover,
we observe promising generalization: LLMs finetuned on specific algorithm
design tasks also improve performance on related tasks with varying settings.
These findings highlight the value of task-specific adaptation for LLMs in
algorithm design and open new avenues for future research.

</details>


### [50] [Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them](https://arxiv.org/abs/2507.10616)
*Neel Rajani,Aryo Pradipta Gema,Seraphina Goldfarb-Tarrant,Ivan Titov*

Main category: cs.LG

TL;DR: Comparative analysis of RL and SFT for LLM reasoning training shows RL has minor in-domain gains, while SFT causes more pronounced changes and out-of-domain degradation. Freezing parts of the model yields inconclusive results.


<details>
  <summary>Details</summary>
Motivation: To understand the training dynamics of RL and SFT for LLM reasoning tasks and their impact on model performance.

Method: Comparative analysis of RL and SFT on the same maths problems with the same model and similar hyperparameters, including parameter updates and freezing experiments.

Result: RL shows minor in-domain gains but slight degradation on knowledge benchmarks; SFT has more pronounced trends and affects mid-layer MLPs more. Freezing parts of the model yields mixed results.

Conclusion: RL amplifies existing capabilities, while SFT replaces old skills with new ones, though freezing experiments were inconclusive.

Abstract: Training large language models (LLMs) for reasoning via maths and code
datasets has become a major new focus in LLM post-training. Two particularly
popular approaches are reinforcement learning (RL) and supervised fine-tuning
(SFT), but their training dynamics are poorly understood. We present a
comparative analysis of RL and SFT on the same maths problems with the same
model and similar hyperparameters. We find that RL yields minor in-domain gains
on maths and slight degradation on knowledge-intensive benchmarks like MMLU,
while both trends are more pronounced in SFT. We also analyse model parameters
across checkpoints, observing that both algorithms modify query and key weights
the most. Meanwhile, SFT exhibits greater updates and also affects mid-layer
MLPs more, leading us to hypothesise that this may have caused the
out-of-domain degradation. We therefore investigate whether freezing parts of
the model during training can mitigate the reduced performance on
knowledge-intensive benchmarks. However, our results are inconclusive, with
benefits on GPQA:Diamond and degradation on other benchmarks. Taken together,
our observations provide a preliminary indication for why RL amplifies existing
capabilities, while SFT replaces old skills with new ones.

</details>


### [51] [Compute Requirements for Algorithmic Innovation in Frontier AI Models](https://arxiv.org/abs/2507.10618)
*Peter Barnett*

Main category: cs.LG

TL;DR: The paper investigates compute requirements for algorithmic innovations in large language model pretraining, analyzing 36 innovations in Llama 3 and DeepSeek-V3. It finds that compute caps may not significantly slow AI progress.


<details>
  <summary>Details</summary>
Motivation: To understand the compute resources needed for algorithmic innovations in pretraining and assess the impact of compute caps on AI progress.

Method: Catalog 36 pretraining algorithmic innovations, estimate their FLOP usage and hardware FLOP/s, and analyze the effect of compute caps.

Result: Compute requirements for innovations double yearly, but even stringent caps (e.g., GPT-2-level compute or 8 H100 GPUs) could allow half of the innovations.

Conclusion: Compute caps alone are unlikely to dramatically slow AI algorithmic progress, as many innovations can still occur under restrictive conditions.

Abstract: Algorithmic innovation in the pretraining of large language models has driven
a massive reduction in the total compute required to reach a given level of
capability. In this paper we empirically investigate the compute requirements
for developing algorithmic innovations. We catalog 36 pre-training algorithmic
innovations used in Llama 3 and DeepSeek-V3. For each innovation we estimate
both the total FLOP used in development and the FLOP/s of the hardware
utilized. Innovations using significant resources double in their requirements
each year. We then use this dataset to investigate the effect of compute caps
on innovation. Our analysis suggests that compute caps alone are unlikely to
dramatically slow AI algorithmic progress. Even stringent compute caps -- such
as capping total operations to the compute used to train GPT-2 or capping
hardware capacity to 8 H100 GPUs -- could still have allowed for half of the
cataloged innovations.

</details>


### [52] [Meta-Reinforcement Learning for Fast and Data-Efficient Spectrum Allocation in Dynamic Wireless Networks](https://arxiv.org/abs/2507.10619)
*Oluwaseyi Giwa,Tobi Awodunmila,Muhammad Ahmed Mohsin,Ahsan Bilal,Muhammad Ali Jamshed*

Main category: cs.LG

TL;DR: A meta-learning framework is proposed for dynamic spectrum allocation in 5G/6G networks, outperforming traditional DRL methods in throughput, safety, and adaptability.


<details>
  <summary>Details</summary>
Motivation: Traditional DRL is inefficient and risky for spectrum allocation due to high sample complexity and unsafe exploration.

Method: Three meta-learning architectures (MAML, RNN, attention-enhanced RNN) are compared to PPO in a simulated IAB environment.

Result: The attention-based meta-learning agent achieves 48 Mbps peak throughput (vs. PPO's 10 Mbps), reduces violations by 50%, and adapts quickly.

Conclusion: Meta-learning is effective and safer for intelligent control in wireless systems.

Abstract: The dynamic allocation of spectrum in 5G / 6G networks is critical to
efficient resource utilization. However, applying traditional deep
reinforcement learning (DRL) is often infeasible due to its immense sample
complexity and the safety risks associated with unguided exploration, which can
cause severe network interference. To address these challenges, we propose a
meta-learning framework that enables agents to learn a robust initial policy
and rapidly adapt to new wireless scenarios with minimal data. We implement
three meta-learning architectures, model-agnostic meta-learning (MAML),
recurrent neural network (RNN), and an attention-enhanced RNN, and evaluate
them against a non-meta-learning DRL algorithm, proximal policy optimization
(PPO) baseline, in a simulated dynamic integrated access/backhaul (IAB)
environment. Our results show a clear performance gap. The attention-based
meta-learning agent reaches a peak mean network throughput of 48 Mbps, while
the PPO baseline decreased drastically to 10 Mbps. Furthermore, our method
reduces SINR and latency violations by more than 50% compared to PPO. It also
shows quick adaptation, with a fairness index 0.7, showing better resource
allocation. This work proves that meta-learning is a very effective and safer
option for intelligent control in complex wireless systems.

</details>


### [53] [LLMs Meet Cross-Modal Time Series Analytics: Overview and Directions](https://arxiv.org/abs/2507.10620)
*Chenxi Liu,Hao Miao,Cheng Long,Yan Zhao,Ziyue Li,Panos Kalnis*

Main category: cs.LG

TL;DR: The paper provides an overview of LLM-based cross-modal time series analytics, classifying approaches into conversion, alignment, and fusion, and discusses applications and challenges.


<details>
  <summary>Details</summary>
Motivation: LLMs are promising for time series analytics but face a cross-modality gap due to their textual pre-training. This tutorial aims to bridge this gap and expand LLM applications in real-world problems.

Method: Introduces a taxonomy classifying approaches into conversion, alignment, and fusion, and discusses their applications in downstream tasks.

Result: Summarizes advancements and methodologies in LLM-based time series analytics, highlighting open challenges.

Conclusion: The tutorial aims to enhance practical LLM applications in cross-modal time series analytics, balancing effectiveness and efficiency, and outlines future research directions.

Abstract: Large Language Models (LLMs) have emerged as a promising paradigm for time
series analytics, leveraging their massive parameters and the shared sequential
nature of textual and time series data. However, a cross-modality gap exists
between time series and textual data, as LLMs are pre-trained on textual
corpora and are not inherently optimized for time series. In this tutorial, we
provide an up-to-date overview of LLM-based cross-modal time series analytics.
We introduce a taxonomy that classifies existing approaches into three groups
based on cross-modal modeling strategies, e.g., conversion, alignment, and
fusion, and then discuss their applications across a range of downstream tasks.
In addition, we summarize several open challenges. This tutorial aims to expand
the practical application of LLMs in solving real-world problems in cross-modal
time series analytics while balancing effectiveness and efficiency.
Participants will gain a thorough understanding of current advancements,
methodologies, and future research directions in cross-modal time series
analytics.

</details>


### [54] [Flows and Diffusions on the Neural Manifold](https://arxiv.org/abs/2507.10623)
*Daniel Saragih,Deyu Cao,Tejas Balaji*

Main category: cs.LG

TL;DR: The paper extends diffusion and flow-based generative models to weight space learning, leveraging optimization dynamics for structural priors. It introduces gradient flow matching for trajectory inference and explores architectural choices, showing improved weight generation and downstream performance.


<details>
  <summary>Details</summary>
Motivation: To apply successful generative models (like diffusion and flow-based) to weight space learning, using optimization dynamics as structural priors for better performance.

Method: Models gradient descent trajectories as inference problems, unifies trajectory techniques under gradient flow matching, and explores architectural choices like reward fine-tuning, autoencoders, and task-specific conditioning.

Result: Outperforms baselines in weight generation, improves downstream training initialization, and excels in detecting harmful covariate shifts.

Conclusion: The method effectively integrates generative models with weight space learning, offering practical benefits in performance and safety-critical applications.

Abstract: Diffusion and flow-based generative models have achieved remarkable success
in domains such as image synthesis, video generation, and natural language
modeling. In this work, we extend these advances to weight space learning by
leveraging recent techniques to incorporate structural priors derived from
optimization dynamics. Central to our approach is modeling the trajectory
induced by gradient descent as a trajectory inference problem. We unify several
trajectory inference techniques under the framework of gradient flow matching,
providing a theoretical framework for treating optimization paths as inductive
bias. We further explore architectural and algorithmic choices, including
reward fine-tuning by adjoint matching, the use of autoencoders for latent
weight representation, conditioning on task-specific context data, and adopting
informative source distributions such as Kaiming uniform. Experiments
demonstrate that our method matches or surpasses baselines in generating
in-distribution weights, improves initialization for downstream training, and
supports fine-tuning to enhance performance. Finally, we illustrate a practical
application in safety-critical systems: detecting harmful covariate shifts,
where our method outperforms the closest comparable baseline.

</details>


### [55] [Player-Team Heterogeneous Interaction Graph Transformer for Soccer Outcome Prediction](https://arxiv.org/abs/2507.10626)
*Lintao Wang,Shiwen Xu,Michael Horton,Joachim Gudmundsson,Zhiyong Wang*

Main category: cs.LG

TL;DR: HIGFormer, a graph-augmented transformer model, improves soccer match outcome prediction by capturing player and team interactions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook heterogeneous player and team interactions, crucial for accurate match outcome prediction.

Method: HIGFormer uses a multi-level interaction framework: Player Interaction Network, Team Interaction Network, and Match Comparison Transformer.

Result: Outperforms existing methods on the WyScout Open Access Dataset and aids in player performance evaluation.

Conclusion: HIGFormer offers a robust solution for soccer outcome prediction and insights for talent scouting and strategy analysis.

Abstract: Predicting soccer match outcomes is a challenging task due to the inherently
unpredictable nature of the game and the numerous dynamic factors influencing
results. While it conventionally relies on meticulous feature engineering, deep
learning techniques have recently shown a great promise in learning effective
player and team representations directly for soccer outcome prediction.
However, existing methods often overlook the heterogeneous nature of
interactions among players and teams, which is crucial for accurately modeling
match dynamics. To address this gap, we propose HIGFormer (Heterogeneous
Interaction Graph Transformer), a novel graph-augmented transformer-based deep
learning model for soccer outcome prediction. HIGFormer introduces a
multi-level interaction framework that captures both fine-grained player
dynamics and high-level team interactions. Specifically, it comprises (1) a
Player Interaction Network, which encodes player performance through
heterogeneous interaction graphs, combining local graph convolutions with a
global graph-augmented transformer; (2) a Team Interaction Network, which
constructs interaction graphs from a team-to-team perspective to model
historical match relationships; and (3) a Match Comparison Transformer, which
jointly analyzes both team and player-level information to predict match
outcomes. Extensive experiments on the WyScout Open Access Dataset, a
large-scale real-world soccer dataset, demonstrate that HIGFormer significantly
outperforms existing methods in prediction accuracy. Furthermore, we provide
valuable insights into leveraging our model for player performance evaluation,
offering a new perspective on talent scouting and team strategy analysis.

</details>


### [56] [GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning](https://arxiv.org/abs/2507.10628)
*Ziru Liu,Cheng Gong,Xinyu Fu,Yaofang Liu,Ran Chen,Shoubo Hu,Suiyun Zhang,Rui Liu,Qingfu Zhang,Dandan Tu*

Main category: cs.LG

TL;DR: RLVR improves LLMs in complex reasoning but faces training instability. GHPO, a difficulty-aware RL framework, dynamically adjusts task difficulty, balancing imitation and exploration, achieving 5% better performance on math benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address training instability and inefficiency in RLVR due to capacity-difficulty mismatch, especially for smaller LLMs.

Method: Introduces Guided Hybrid Policy Optimization (GHPO), which uses adaptive prompt refinement to balance imitation learning and exploration-based RL.

Result: GHPO achieves ~5% better performance on six math benchmarks, enhancing training stability and reasoning.

Conclusion: GHPO offers a scalable, efficient solution for robust reasoning models, outperforming existing RL and curriculum learning methods.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a powerful paradigm for facilitating the self-improvement of large language
models (LLMs), particularly in the domain of complex reasoning tasks. However,
prevailing on-policy RL methods often contend with significant training
instability and inefficiency. This is primarily due to a capacity-difficulty
mismatch, where the complexity of training data frequently outpaces the model's
current capabilities, leading to critically sparse reward signals and stalled
learning progress. This challenge is particularly acute for smaller, more
resource-efficient LLMs. To overcome this, we introduce the Guided Hybrid
Policy Optimization (GHPO), a novel difficulty-aware reinforcement learning
framework. GHPO dynamically calibrates task difficulty by employing adaptive
prompt refinement to provide targeted guidance. This unique approach adaptively
balances direct imitation learning for problems currently beyond the model's
reach with exploration-based reinforcement learning for more manageable tasks,
effectively creating a smooth and optimized learning curriculum. Extensive
experiments demonstrate that GHPO achieves an average performance gain of
approximately 5% across six challenging mathematics benchmarks, consistently
outperforming strong on-policy reinforcement learning and curriculum learning
baselines. Further analysis confirms that our framework significantly enhances
both training stability and final reasoning performance, thus offering a
scalable and efficient solution for developing powerful and robust reasoning
models.

</details>


### [57] [Scalable Unsupervised Segmentation via Random Fourier Feature-based Gaussian Process](https://arxiv.org/abs/2507.10632)
*Issei Saito,Masatoshi Nagano,Tomoaki Nakamura,Daichi Mochihashi,Koki Mimura*

Main category: cs.LG

TL;DR: RFF-GP-HSMM is a fast unsupervised time-series segmentation method using random Fourier features to reduce computational costs of GP-HSMM, achieving comparable performance with 278x speedup.


<details>
  <summary>Details</summary>
Motivation: The high computational cost of GP-HSMM due to kernel matrix inversion limits scalability for large datasets.

Method: Approximates Gaussian process with linear regression using random Fourier features (RFF), avoiding kernel matrix inversion.

Result: Achieves comparable segmentation performance to conventional methods with 278 times faster processing on 39,200 frames.

Conclusion: RFF-GP-HSMM offers a scalable and efficient alternative to GP-HSMM for large time-series datasets.

Abstract: In this paper, we propose RFF-GP-HSMM, a fast unsupervised time-series
segmentation method that incorporates random Fourier features (RFF) to address
the high computational cost of the Gaussian process hidden semi-Markov model
(GP-HSMM). GP-HSMM models time-series data using Gaussian processes, requiring
inversion of an N times N kernel matrix during training, where N is the number
of data points. As the scale of the data increases, matrix inversion incurs a
significant computational cost. To address this, the proposed method
approximates the Gaussian process with linear regression using RFF, preserving
expressive power while eliminating the need for inversion of the kernel matrix.
Experiments on the Carnegie Mellon University (CMU) motion-capture dataset
demonstrate that the proposed method achieves segmentation performance
comparable to that of conventional methods, with approximately 278 times faster
segmentation on time-series data comprising 39,200 frames.

</details>


### [58] [GeoHopNet: Hopfield-Augmented Sparse Spatial Attention for Dynamic UAV Site Location Problem](https://arxiv.org/abs/2507.10636)
*Jianing Zhi,Xinghua Li,Zidong Chen*

Main category: cs.LG

TL;DR: GeoHopNet introduces a Hopfield-augmented sparse spatial attention network for dynamic UAV site selection, improving efficiency and solution quality over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing computational bottlenecks in large-scale urban UAV site selection using traditional deep reinforcement learning and attention mechanisms.

Method: Proposes GeoHopNet with innovations: distance-biased multi-head attention, K-nearest neighbor sparse attention, Hopfield external memory, and memory regularization.

Result: Achieves high-quality solutions (0.22% optimality gap) in under 0.1 seconds for 1,000-node problems, outperforming baselines in speed and quality.

Conclusion: GeoHopNet significantly advances the scalability and efficiency of solving dynamic UAV site selection problems.

Abstract: The rapid development of urban low-altitude unmanned aerial vehicle (UAV)
economy poses new challenges for dynamic site selection of UAV landing points
and supply stations. Traditional deep reinforcement learning methods face
computational complexity bottlenecks, particularly with standard attention
mechanisms, when handling large-scale urban-level location problems. This paper
proposes GeoHopNet, a Hopfield-augmented sparse spatial attention network
specifically designed for dynamic UAV site location problems. Our approach
introduces four core innovations: (1) distance-biased multi-head attention
mechanism that explicitly encodes spatial geometric information; (2) K-nearest
neighbor sparse attention that reduces computational complexity from $O(N^2)$
to $O(NK)$; (3) a modern Hopfield external memory module; and (4) a memory
regularization strategy. Experimental results demonstrate that GeoHopNet
extends the boundary of solvable problem sizes. For large-scale instances with
1,000 nodes, where standard attention models become prohibitively slow (over 3
seconds per instance) and traditional solvers fail, GeoHopNet finds
high-quality solutions (0.22\% optimality gap) in under 0.1 seconds. Compared
to the state-of-the-art ADNet baseline on 100-node instances, our method
improves solution quality by 22.2\% and is 1.8$\times$ faster.

</details>


### [59] [A Simple Baseline for Stable and Plastic Neural Networks](https://arxiv.org/abs/2507.10637)
*É. Künzel,A. Jaziri,V. Ramesh*

Main category: cs.LG

TL;DR: RDBP introduces ReLUDown and Decreasing Backpropagation to balance plasticity and stability in continual learning, outperforming state-of-the-art methods with lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing continual learning approaches often imbalance plasticity and stability, limiting their effectiveness.

Method: Combines ReLUDown (activation modification) and Decreasing Backpropagation (gradient-scheduling) to prevent neuron dormancy and catastrophic updates.

Result: Matches or exceeds state-of-the-art methods on the Continual ImageNet benchmark with reduced computational cost.

Conclusion: RDBP offers a practical, efficient solution for continual learning and sets a benchmark for future research.

Abstract: Continual learning in computer vision requires that models adapt to a
continuous stream of tasks without forgetting prior knowledge, yet existing
approaches often tip the balance heavily toward either plasticity or stability.
We introduce RDBP, a simple, low-overhead baseline that unites two
complementary mechanisms: ReLUDown, a lightweight activation modification that
preserves feature sensitivity while preventing neuron dormancy, and Decreasing
Backpropagation, a biologically inspired gradient-scheduling scheme that
progressively shields early layers from catastrophic updates. Evaluated on the
Continual ImageNet benchmark, RDBP matches or exceeds the plasticity and
stability of state-of-the-art methods while reducing computational cost. RDBP
thus provides both a practical solution for real-world continual learning and a
clear benchmark against which future continual learning strategies can be
measured.

</details>


### [60] [ZClassifier: Temperature Tuning and Manifold Approximation via KL Divergence on Logit Space](https://arxiv.org/abs/2507.10638)
*Shim Soon Yong*

Main category: cs.LG

TL;DR: ZClassifier replaces deterministic logits with Gaussian-distributed logits, unifying uncertainty calibration and latent control via KL divergence minimization, improving robustness and calibration.


<details>
  <summary>Details</summary>
Motivation: To address temperature scaling and manifold approximation in classification, enabling better uncertainty calibration and latent control.

Method: Uses diagonal Gaussian-distributed logits, minimizing KL divergence between predicted Gaussians and a unit isotropic Gaussian.

Result: Outperforms softmax classifiers in robustness, calibration, and latent separation on CIFAR-10 and CIFAR-100.

Conclusion: ZClassifier provides a principled probabilistic framework for classification and classifier-guided generation.

Abstract: We introduce a novel classification framework, ZClassifier, that replaces
conventional deterministic logits with diagonal Gaussian-distributed logits.
Our method simultaneously addresses temperature scaling and manifold
approximation by minimizing the Kullback-Leibler (KL) divergence between the
predicted Gaussian distributions and a unit isotropic Gaussian. This unifies
uncertainty calibration and latent control in a principled probabilistic
manner, enabling a natural interpretation of class confidence and geometric
consistency. Experiments on CIFAR-10 and CIFAR-100 show that ZClassifier
improves over softmax classifiers in robustness, calibration, and latent
separation. We also demonstrate its effectiveness for classifier-guided
generation by interpreting logits as Gaussian semantic potentials.

</details>


### [61] [First-of-its-kind AI model for bioacoustic detection using a lightweight associative memory Hopfield neural network](https://arxiv.org/abs/2507.10642)
*Andrew Gascoyne,Wendy Lomas*

Main category: cs.LG

TL;DR: An AI model using Hopfield neural networks for bioacoustic analysis is proposed, addressing data scarcity, environmental impact, and hardware demands. It's fast, lightweight, and accurate.


<details>
  <summary>Details</summary>
Motivation: To tackle challenges in bioacoustic analysis, such as limited training data, high energy consumption, and hardware requirements, by developing a sustainable and efficient AI model.

Method: The model employs associative memory via a transparent Hopfield neural network, requiring minimal training data (one signal per target sound) and offering rapid processing.

Result: The model achieves 86% precision, processes 10,384 bat recordings in 5.4s, uses only 144.09MB RAM, and matches expert manual identification.

Conclusion: The proposed model is a fast, lightweight, sustainable, and accurate solution for bioacoustic analysis, with potential for broad application.

Abstract: A growing issue within conservation bioacoustics is the task of analysing the
vast amount of data generated from the use of passive acoustic monitoring
devices. In this paper, we present an alternative AI model which has the
potential to help alleviate this problem. Our model formulation addresses the
key issues encountered when using current AI models for bioacoustic analysis,
namely the: limited training data available; environmental impact, particularly
in energy consumption and carbon footprint of training and implementing these
models; and associated hardware requirements. The model developed in this work
uses associative memory via a transparent, explainable Hopfield neural network
to store signals and detect similar signals which can then be used to classify
species. Training is rapid ($3$\,ms), as only one representative signal is
required for each target sound within a dataset. The model is fast, taking only
$5.4$\,s to pre-process and classify all $10384$ publicly available bat
recordings, on a standard Apple MacBook Air. The model is also lightweight with
a small memory footprint of $144.09$\,MB of RAM usage. Hence, the low
computational demands make the model ideal for use on a variety of standard
personal devices with potential for deployment in the field via edge-processing
devices. It is also competitively accurate, with up to $86\%$ precision on the
dataset used to evaluate the model. In fact, we could not find a single case of
disagreement between model and manual identification via expert field guides.
Although a dataset of bat echolocation calls was chosen to demo this
first-of-its-kind AI model, trained on only two representative calls, the model
is not species specific. In conclusion, we propose an equitable AI model that
has the potential to be a game changer for fast, lightweight, sustainable,
transparent, explainable and accurate bioacoustic analysis.

</details>


### [62] [A Group Theoretic Analysis of the Symmetries Underlying Base Addition and Their Learnability by Neural Networks](https://arxiv.org/abs/2507.10678)
*Cutter Dawes,Simon Segert,Kamesh Krishnamurthy,Jonathan D. Cohen*

Main category: cs.LG

TL;DR: The paper explores radical generalization in neural networks by analyzing base addition and alternative carry functions, revealing their impact on learning efficiency and symmetry discovery.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of designing neural networks capable of efficient radical generalization, particularly through symmetry functions, using base addition as a case study.

Method: Group theoretic analysis of base addition, introduction of alternative carry functions, and training neural networks with different carries to study learning efficacy.

Result: Simple neural networks can achieve radical generalization with appropriate input format and carry function, with learning speed tied to carry structure.

Conclusion: The findings highlight the importance of carry function structure in symmetry learning, offering insights for cognitive science and machine learning.

Abstract: A major challenge in the use of neural networks both for modeling human
cognitive function and for artificial intelligence is the design of systems
with the capacity to efficiently learn functions that support radical
generalization. At the roots of this is the capacity to discover and implement
symmetry functions. In this paper, we investigate a paradigmatic example of
radical generalization through the use of symmetry: base addition. We present a
group theoretic analysis of base addition, a fundamental and defining
characteristic of which is the carry function -- the transfer of the remainder,
when a sum exceeds the base modulus, to the next significant place. Our
analysis exposes a range of alternative carry functions for a given base, and
we introduce quantitative measures to characterize these. We then exploit
differences in carry functions to probe the inductive biases of neural networks
in symmetry learning, by training neural networks to carry out base addition
using different carries, and comparing efficacy and rate of learning as a
function of their structure. We find that even simple neural networks can
achieve radical generalization with the right input format and carry function,
and that learning speed is closely correlated with carry function structure. We
then discuss the relevance this has for cognitive science and machine learning.

</details>


### [63] [A Simple Approximate Bayesian Inference Neural Surrogate for Stochastic Petri Net Models](https://arxiv.org/abs/2507.10714)
*Bright Kwaku Manu,Trevor Reckell,Beckett Sterner,Petar Jevtic*

Main category: cs.LG

TL;DR: A neural-surrogate framework using a 1D Convolutional Residual Network is introduced for parameter estimation in Stochastic Petri Nets (SPNs) with covariate-dependent rates, outperforming traditional Bayesian methods in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Parameter estimation in SPNs is challenging, especially with covariate-dependent rates and unavailable explicit likelihoods, necessitating a robust, data-driven solution.

Method: A lightweight 1D Convolutional Residual Network is trained on Gillespie-simulated SPN data to predict rate-function coefficients from noisy, partially observed trajectories, using Monte Carlo dropout for uncertainty.

Result: The surrogate achieves RMSE = 0.108 on synthetic SPNs with 20% missing events and runs faster than Bayesian methods.

Conclusion: Neural-surrogate frameworks enable accurate, robust, and real-time parameter recovery in complex, partially observed discrete-event systems.

Abstract: Stochastic Petri Nets (SPNs) are an increasingly popular tool of choice for
modeling discrete-event dynamics in areas such as epidemiology and systems
biology, yet their parameter estimation remains challenging in general and in
particular when transition rates depend on external covariates and explicit
likelihoods are unavailable. We introduce a neural-surrogate
(neural-network--based approximation of the posterior distribution) framework
that predicts the coefficients of known covariate-dependent rate functions
directly from noisy, partially observed token trajectories. Our model employs a
lightweight 1D Convolutional Residual Network trained end-to-end on
Gillespie-simulated SPN realizations, learning to invert system dynamics under
realistic conditions of event dropout. During inference, Monte Carlo dropout
provides calibrated uncertainty bounds together with point estimates. On
synthetic SPNs with 20% missing events, our surrogate recovers rate-function
coefficients with an RMSE = 0.108 and substantially runs faster than
traditional Bayesian approaches. These results demonstrate that data-driven,
likelihood-free surrogates can enable accurate, robust, and real-time parameter
recovery in complex, partially observed discrete-event systems.

</details>


### [64] [Distributionally Robust Optimization with Adversarial Data Contamination](https://arxiv.org/abs/2507.10718)
*Shuyao Li,Ilias Diakonikolas,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: The paper introduces a method to address outliers and distributional uncertainty in Wasserstein-1 DRO for generalized linear models, achieving an estimation error of O(√ϵ) with contaminated data.


<details>
  <summary>Details</summary>
Motivation: To tackle the dual challenges of data contamination and distributional uncertainty in DRO, ensuring robust decision-making.

Method: A novel framework integrating robustness against data contamination and distributional shifts, with an efficient algorithm inspired by robust statistics.

Result: Proves an estimation error of O(√ϵ) for the true DRO objective value using contaminated data under bounded covariance.

Conclusion: The work provides the first rigorous guarantees for learning under data contamination and distributional shifts, with efficient computation.

Abstract: Distributionally Robust Optimization (DRO) provides a framework for
decision-making under distributional uncertainty, yet its effectiveness can be
compromised by outliers in the training data. This paper introduces a
principled approach to simultaneously address both challenges. We focus on
optimizing Wasserstein-1 DRO objectives for generalized linear models with
convex Lipschitz loss functions, where an $\epsilon$-fraction of the training
data is adversarially corrupted. Our primary contribution lies in a novel
modeling framework that integrates robustness against training data
contamination with robustness against distributional shifts, alongside an
efficient algorithm inspired by robust statistics to solve the resulting
optimization problem. We prove that our method achieves an estimation error of
$O(\sqrt{\epsilon})$ for the true DRO objective value using only the
contaminated data under the bounded covariance assumption. This work
establishes the first rigorous guarantees, supported by efficient computation,
for learning under the dual challenges of data contamination and distributional
shifts.

</details>


### [65] [Ground-Compose-Reinforce: Tasking Reinforcement Learning Agents through Formal Language](https://arxiv.org/abs/2507.10741)
*Andrew C. Li,Toryn Q. Klassen,Andrew Wang,Parand A. Alamdari,Sheila A. McIlraith*

Main category: cs.LG

TL;DR: A neurosymbolic framework, Ground-Compose-Reinforce, is proposed for grounding formal language in perception and action, enabling efficient learning and generalization with limited data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of grounding language in complex perception and action without manual design or massive datasets.

Method: Uses a neurosymbolic framework combining data-driven learning and compositional formal language semantics to ground language and elicit behaviors via RL agents.

Result: Achieves reliable mapping of formal language instructions to behaviors with limited data, outperforming end-to-end data-driven approaches.

Conclusion: The framework efficiently grounds language and generalizes to arbitrary compositions, demonstrating its effectiveness in situated agent tasks.

Abstract: Grounding language in complex perception (e.g. pixels) and action is a key
challenge when building situated agents that can interact with humans via
language. In past works, this is often solved via manual design of the language
grounding or by curating massive datasets relating language to elements of the
environment. We propose Ground-Compose-Reinforce, a neurosymbolic framework for
grounding formal language from data, and eliciting behaviours by directly
tasking RL agents through this language. By virtue of data-driven learning, our
framework avoids the manual design of domain-specific elements like reward
functions or symbol detectors. By virtue of compositional formal language
semantics, our framework achieves data-efficient grounding and generalization
to arbitrary language compositions. Experiments on an image-based gridworld and
a MuJoCo robotics domain show that our approach reliably maps formal language
instructions to behaviours with limited data while end-to-end, data-driven
approaches fail.

</details>


### [66] [A Benchmarking Framework for AI models in Automotive Aerodynamics](https://arxiv.org/abs/2507.10747)
*Kaustubh Tangsali,Rishikesh Ranade,Mohammad Amin Nabian,Alexey Kamenev,Peter Sharpe,Neil Ashton,Ram Cherukuri,Sanjay Choudhry*

Main category: cs.LG

TL;DR: A benchmarking framework for AI models in automotive aerodynamics is introduced, focusing on accuracy, performance, scalability, and generalization. It evaluates three models and supports extensibility for future research.


<details>
  <summary>Details</summary>
Motivation: To standardize and improve the assessment of AI models for automotive aerodynamics, enhancing transparency and consistency in performance evaluation.

Method: The framework uses the NVIDIA PhysicsNeMo-CFD platform to evaluate AI models (DoMINO, X-MeshGraphNet, FIGConvNet) on the DrivAerML dataset, with guidelines for adding models and datasets.

Result: The framework provides a standardized way to compare AI models, demonstrating its utility through evaluations of surface and volumetric flow field predictions.

Conclusion: The framework aims to accelerate research and innovation by enabling better selection and refinement of AI-driven aerodynamic modeling approaches.

Abstract: In this paper, we introduce a benchmarking framework within the open-source
NVIDIA PhysicsNeMo-CFD framework designed to systematically assess the
accuracy, performance, scalability, and generalization capabilities of AI
models for automotive aerodynamics predictions. The open extensible framework
enables incorporation of a diverse set of metrics relevant to the
Computer-Aided Engineering (CAE) community. By providing a standardized
methodology for comparing AI models, the framework enhances transparency and
consistency in performance assessment, with the overarching goal of improving
the understanding and development of these models to accelerate research and
innovation in the field. To demonstrate its utility, the framework includes
evaluation of both surface and volumetric flow field predictions on three AI
models: DoMINO, X-MeshGraphNet, and FIGConvNet using the DrivAerML dataset. It
also includes guidelines for integrating additional models and datasets, making
it extensible for physically consistent metrics. This benchmarking study aims
to enable researchers and industry professionals in selecting, refining, and
advancing AI-driven aerodynamic modeling approaches, ultimately fostering the
development of more efficient, accurate, and interpretable solutions in
automotive aerodynamics

</details>


### [67] [Spatial Reasoners for Continuous Variables in Any Domain](https://arxiv.org/abs/2507.10768)
*Bart Pogodzinski,Christopher Wewer,Bernt Schiele,Jan Eric Lenssen*

Main category: cs.LG

TL;DR: Spatial Reasoners is a framework for spatial reasoning using generative denoising models, simplifying research with easy-to-use interfaces.


<details>
  <summary>Details</summary>
Motivation: To address the high effort required for generative reasoning with denoising models due to diverse formulations and strategies.

Method: Provides interfaces for variable mapping, generative model paradigms, and inference strategies.

Result: The framework is openly available, facilitating research in generative spatial reasoning.

Conclusion: Spatial Reasoners streamlines the use of denoising models for spatial reasoning tasks.

Abstract: We present Spatial Reasoners, a software framework to perform spatial
reasoning over continuous variables with generative denoising models. Denoising
generative models have become the de-facto standard for image generation, due
to their effectiveness in sampling from complex, high-dimensional
distributions. Recently, they have started being explored in the context of
reasoning over multiple continuous variables. Providing infrastructure for
generative reasoning with such models requires a high effort, due to a wide
range of different denoising formulations, samplers, and inference strategies.
Our presented framework aims to facilitate research in this area, providing
easy-to-use interfaces to control variable mapping from arbitrary data domains,
generative model paradigms, and inference strategies. Spatial Reasoners are
openly available at https://spatialreasoners.github.io/

</details>


### [68] [A Generalizable Physics-Enhanced State Space Model for Long-Term Dynamics Forecasting in Complex Environments](https://arxiv.org/abs/2507.10792)
*Yuchen Wang,Hongjue Zhao,Haohong Lin,Enze Xu,Lifang He,Huajie Shao*

Main category: cs.LG

TL;DR: Phy-SSM integrates partial physics knowledge into state space models for long-term dynamic forecasting in noisy, irregularly sampled environments, outperforming baselines in real-world tasks.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of long-term forecasting in complex environments with noisy, irregular data by leveraging SSMs and physics knowledge for better generalization.

Method: Decompose partially known system dynamics into known and unknown state matrices, integrate into Phy-SSM unit, and use physics state regularization for alignment with system dynamics.

Result: Superior performance in long-term interpolation and extrapolation tasks across vehicle motion, drone state, and COVID-19 forecasting.

Conclusion: Phy-SSM effectively combines physics knowledge with SSMs, enhancing long-term forecasting in complex scenarios, as validated by real-world experiments.

Abstract: This work aims to address the problem of long-term dynamic forecasting in
complex environments where data are noisy and irregularly sampled. While recent
studies have introduced some methods to improve prediction performance, these
approaches still face a significant challenge in handling long-term
extrapolation tasks under such complex scenarios. To overcome this challenge,
we propose Phy-SSM, a generalizable method that integrates partial physics
knowledge into state space models (SSMs) for long-term dynamics forecasting in
complex environments. Our motivation is that SSMs can effectively capture
long-range dependencies in sequential data and model continuous dynamical
systems, while the incorporation of physics knowledge improves generalization
ability. The key challenge lies in how to seamlessly incorporate partially
known physics into SSMs. To achieve this, we decompose partially known system
dynamics into known and unknown state matrices, which are integrated into a
Phy-SSM unit. To further enhance long-term prediction performance, we introduce
a physics state regularization term to make the estimated latent states align
with system dynamics. Besides, we theoretically analyze the uniqueness of the
solutions for our method. Extensive experiments on three real-world
applications, including vehicle motion prediction, drone state prediction, and
COVID-19 epidemiology forecasting, demonstrate the superior performance of
Phy-SSM over the baselines in both long-term interpolation and extrapolation
tasks. The code is available at https://github.com/511205787/Phy_SSM-ICML2025.

</details>


### [69] [Multi-Armed Sampling Problem and the End of Exploration](https://arxiv.org/abs/2507.10797)
*Mohammad Pedramfar,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: The paper introduces multi-armed sampling as a counterpart to multi-armed bandits, focusing on the exploration-exploitation trade-off in sampling. It defines regret notions, proposes an optimal algorithm, and connects sampling and bandit problems via a temperature parameter.


<details>
  <summary>Details</summary>
Motivation: To rigorously study the exploration-exploitation trade-off in sampling, contrasting it with optimization, and to unify sampling and bandit problems.

Method: Defines regret notions, establishes lower bounds, and proposes an optimal algorithm. Introduces a continuous family of problems with a temperature parameter.

Result: Demonstrates that sampling doesn't require exploration, unlike optimization. Connects findings to neural samplers and RLHF.

Conclusion: The framework and findings provide foundational insights for sampling studies, particularly in entropy-regularized RL and RLHF.

Abstract: This paper introduces the framework of multi-armed sampling, as the sampling
counterpart to the optimization problem of multi-arm bandits. Our primary
motivation is to rigorously examine the exploration-exploitation trade-off in
the context of sampling. We systematically define plausible notions of regret
for this framework and establish corresponding lower bounds. We then propose a
simple algorithm that achieves these optimal regret bounds. Our theoretical
results demonstrate that in contrast to optimization, sampling does not require
exploration. To further connect our findings with those of multi-armed bandits,
we define a continuous family of problems and associated regret measures that
smoothly interpolates and unifies multi-armed sampling and multi-armed bandit
problems using a temperature parameter. We believe the multi-armed sampling
framework, and our findings in this setting can have a foundational role in the
study of sampling including recent neural samplers, akin to the role of
multi-armed bandits in reinforcement learning. In particular, our work sheds
light on the need for exploration and the convergence properties of algorithm
for entropy-regularized reinforcement learning, fine-tuning of pretrained
models and reinforcement learning with human feedback (RLHF).

</details>


### [70] [Uncovering Causal Relation Shifts in Event Sequences under Out-of-Domain Interventions](https://arxiv.org/abs/2507.10809)
*Kazi Tasnim Zinat,Yun Zhou,Xiang Lyu,Yawei Wang,Zhicheng Liu,Panpan Xu*

Main category: cs.LG

TL;DR: A new causal framework is proposed to handle out-of-domain interventions in temporal sequences, improving ATE estimation and model fit.


<details>
  <summary>Details</summary>
Motivation: Existing causal inference methods ignore out-of-domain interventions, which can significantly alter causal dynamics in real-world settings.

Method: A Transformer-based neural network is designed to integrate out-of-domain intervention information, handling long-range dependencies and local patterns.

Result: The method outperforms baselines in ATE estimation and goodness-of-fit on simulated and real-world datasets.

Conclusion: The proposed framework effectively captures causal relation shifts under out-of-domain interventions, enhancing causal inference in temporal processes.

Abstract: Inferring causal relationships between event pairs in a temporal sequence is
applicable in many domains such as healthcare, manufacturing, and
transportation. Most existing work on causal inference primarily focuses on
event types within the designated domain, without considering the impact of
exogenous out-of-domain interventions. In real-world settings, these
out-of-domain interventions can significantly alter causal dynamics. To address
this gap, we propose a new causal framework to define average treatment effect
(ATE), beyond independent and identically distributed (i.i.d.) data in classic
Rubin's causal framework, to capture the causal relation shift between events
of temporal process under out-of-domain intervention. We design an unbiased ATE
estimator, and devise a Transformer-based neural network model to handle both
long-range temporal dependencies and local patterns while integrating
out-of-domain intervention information into process modeling. Extensive
experiments on both simulated and real-world datasets demonstrate that our
method outperforms baselines in ATE estimation and goodness-of-fit under
out-of-domain-augmented point processes.

</details>


### [71] [Semantic Context for Tool Orchestration](https://arxiv.org/abs/2507.10820)
*Robert Müller*

Main category: cs.LG

TL;DR: The paper shows Semantic Context (SC) is key for tool orchestration, introducing SC-LinUCB for lower regret, validating SC's role in LLMs, and proposing the FiReAct pipeline for large-scale tool orchestration.


<details>
  <summary>Details</summary>
Motivation: To improve tool orchestration by leveraging Semantic Context (SC) for better adaptability and efficiency in dynamic environments.

Method: 1. Theoretical foundation with SC-LinUCB (contextual bandits). 2. Empirical validation with LLMs. 3. FiReAct pipeline for large-scale tool orchestration.

Result: SC-LinUCB reduces regret; SC enhances LLM performance in static and dynamic settings; FiReAct scales to 10,000+ tools.

Conclusion: SC is essential for efficient, adaptive, and scalable tool orchestration, supported by theory and experiments.

Abstract: This paper demonstrates that Semantic Context (SC), leveraging descriptive
tool information, is a foundational component for robust tool orchestration.
Our contributions are threefold. First, we provide a theoretical foundation
using contextual bandits, introducing SC-LinUCB and proving it achieves lower
regret and adapts favourably in dynamic action spaces. Second, we provide
parallel empirical validation with Large Language Models, showing that SC is
critical for successful in-context learning in both static (efficient learning)
and non-stationary (robust adaptation) settings. Third, we propose the FiReAct
pipeline, and demonstrate on a benchmark with over 10,000 tools that SC-based
retrieval enables an LLM to effectively orchestrate over a large action space.
These findings provide a comprehensive guide to building more sample-efficient,
adaptive, and scalable orchestration agents.

</details>


### [72] [From Small to Large: A Graph Convolutional Network Approach for Solving Assortment Optimization Problems](https://arxiv.org/abs/2507.10834)
*Guokai Li,Pin Gao,Stefanus Jasin,Zizhuo Wang*

Main category: cs.LG

TL;DR: The paper proposes using Graph Convolutional Networks (GCNs) to solve constrained assortment optimization efficiently, achieving high performance on large-scale instances.


<details>
  <summary>Details</summary>
Motivation: Assortment optimization is NP-hard and challenging due to its combinatorial and non-linear nature. Existing methods lack efficiency and scalability.

Method: Develop a graph representation of the problem, train a GCN to learn optimal patterns, and propose two inference policies.

Result: GCN-based policies achieve 90%+ optimality on large instances (up to 2,000 products) quickly, outperforming heuristics.

Conclusion: GCNs offer a scalable and efficient solution for assortment optimization, even in model-free settings with unknown choice models.

Abstract: Assortment optimization involves selecting a subset of substitutable products
(subject to certain constraints) to maximize the expected revenue. It is a
classic problem in revenue management and finds applications across various
industries. However, the problem is usually NP-hard due to its combinatorial
and non-linear nature. In this work, we explore how graph concolutional
networks (GCNs) can be leveraged to efficiently solve constrained assortment
optimization under the mixed multinomial logit choice model. We first develop a
graph representation of the assortment problem, then train a GCN to learn the
patterns of optimal assortments, and lastly propose two inference policies
based on the GCN's output. Due to the GCN's inherent ability to generalize
across inputs of varying sizes, we can use a GCN trained on small-scale
instances to facilitate large-scale instances. Extensive numerical experiments
demonstrate that given a GCN trained on small-scale instances (e.g., with 20
products), the proposed policies can achieve superior performance (90%+
optimality) on large-scale instances (with up to 2,000 products) within
seconds, which outperform existing heuristic policies in both performance and
efficiency. Furthermore, we extend our framework to a model-free setting where
the underlying choice model is unknown but transaction data is available. We
also conduct numerical experiments to demonstrate the effectiveness and
efficiency of our proposed policies in this setting.

</details>


### [73] [Collaboration Promotes Group Resilience in Multi-Agent RL](https://arxiv.org/abs/2111.06614)
*Ilai Shraga,Guy Azran,Matthias Gerstgrasser,Ofir Abu,Jeffrey S. Rosenschein,Sarah Keren*

Main category: cs.LG

TL;DR: The paper introduces 'group resilience' in multi-agent RL, showing collaboration enhances adaptability to environmental changes.


<details>
  <summary>Details</summary>
Motivation: To address the lack of resilience frameworks in multi-agent RL settings.

Method: Formalized group resilience, tested collaboration protocols in MARL.

Result: Collaborative approaches outperformed non-collaborative ones in resilience.

Conclusion: Collaboration is key to achieving group resilience in MARL.

Abstract: To effectively operate in various dynamic scenarios, RL agents must be
resilient to unexpected changes in their environment. Previous work on this
form of resilience has focused on single-agent settings. In this work, we
introduce and formalize a multi-agent variant of resilience, which we term
group resilience. We further hypothesize that collaboration with other agents
is key to achieving group resilience; collaborating agents adapt better to
environmental perturbations in multi-agent reinforcement learning (MARL)
settings. We test our hypothesis empirically by evaluating different
collaboration protocols and examining their effect on group resilience. Our
experiments show that all the examined collaborative approaches achieve higher
group resilience than their non-collaborative counterparts.

</details>


### [74] [Offline Reinforcement Learning with Wasserstein Regularization via Optimal Transport Maps](https://arxiv.org/abs/2507.10843)
*Motoki Omura,Yusuke Mukuta,Kazuki Ota,Takayuki Osa,Tatsuya Harada*

Main category: cs.LG

TL;DR: The paper proposes a Wasserstein distance-based method for offline RL to address distributional shift, using ICNNs for stable learning, and shows competitive performance on D4RL.


<details>
  <summary>Details</summary>
Motivation: Offline RL is useful where data collection is costly, but suffers from distributional shift. Existing methods use density ratios, but Wasserstein distance offers robustness to out-of-distribution data.

Method: Uses Wasserstein distance with ICNNs to model optimal transport maps, avoiding adversarial training.

Result: Demonstrates comparable or superior performance to existing methods on D4RL.

Conclusion: The proposed method effectively mitigates distributional shift in offline RL with stable learning.

Abstract: Offline reinforcement learning (RL) aims to learn an optimal policy from a
static dataset, making it particularly valuable in scenarios where data
collection is costly, such as robotics. A major challenge in offline RL is
distributional shift, where the learned policy deviates from the dataset
distribution, potentially leading to unreliable out-of-distribution actions. To
mitigate this issue, regularization techniques have been employed. While many
existing methods utilize density ratio-based measures, such as the
$f$-divergence, for regularization, we propose an approach that utilizes the
Wasserstein distance, which is robust to out-of-distribution data and captures
the similarity between actions. Our method employs input-convex neural networks
(ICNNs) to model optimal transport maps, enabling the computation of the
Wasserstein distance in a discriminator-free manner, thereby avoiding
adversarial training and ensuring stable learning. Our approach demonstrates
comparable or superior performance to widely used existing methods on the D4RL
benchmark dataset. The code is available at
https://github.com/motokiomura/Q-DOT .

</details>


### [75] [Visually grounded emotion regulation via diffusion models and user-driven reappraisal](https://arxiv.org/abs/2507.10861)
*Edoardo Pinzuti,Oliver Tüscher,André Ferreira Castro*

Main category: cs.LG

TL;DR: The paper introduces a visually augmented cognitive reappraisal method using text-to-image diffusion models to transform spoken reappraisals into supportive visualizations, showing reduced negative affect in experiments.


<details>
  <summary>Details</summary>
Motivation: Standard cognitive reappraisal methods are often too abstract or verbal, limiting effectiveness for individuals with trauma or depression. The study aims to enhance reappraisal through visual feedback.

Method: A system integrates stable diffusion models to transform spoken reappraisals into emotionally congruent visualizations. A within-subject experiment (N=20) tested AI-assisted reappraisal against non-AI and control conditions.

Result: AI-assisted reappraisal significantly reduced negative affect compared to other conditions. Sentiment alignment between reappraisals and generated images correlated with affective relief.

Conclusion: Generative visual input supports cognitive reappraisal, offering new directions for AI in affective computing and therapy.

Abstract: Cognitive reappraisal is a key strategy in emotion regulation, involving
reinterpretation of emotionally charged stimuli to alter affective responses.
Despite its central role in clinical and cognitive science, real-world
reappraisal interventions remain cognitively demanding, abstract, and primarily
verbal. This reliance on higher-order cognitive and linguistic processes is
often impaired in individuals with trauma or depression, limiting the
effectiveness of standard approaches. Here, we propose a novel, visually based
augmentation of cognitive reappraisal by integrating large-scale text-to-image
diffusion models into the emotional regulation process. Specifically, we
introduce a system in which users reinterpret emotionally negative images via
spoken reappraisals, which are transformed into supportive, emotionally
congruent visualizations using stable diffusion models with a fine-tuned
IP-adapter. This generative transformation visually instantiates users'
reappraisals while maintaining structural similarity to the original stimuli,
externalizing and reinforcing regulatory intent. To test this approach, we
conducted a within-subject experiment (N = 20) using a modified cognitive
emotion regulation (CER) task. Participants reappraised or described aversive
images from the International Affective Picture System (IAPS), with or without
AI-generated visual feedback. Results show that AI-assisted reappraisal
significantly reduced negative affect compared to both non-AI and control
conditions. Further analyses reveal that sentiment alignment between
participant reappraisals and generated images correlates with affective relief,
suggesting that multimodal coherence enhances regulatory efficacy. These
findings demonstrate that generative visual input can support cogitive
reappraisal and open new directions at the intersection of generative AI,
affective computing, and therapeutic technology.

</details>


### [76] [GALDS: A Graph-Autoencoder-based Latent Dynamics Surrogate model to predict neurite material transport](https://arxiv.org/abs/2507.10871)
*Tsung Yeh Hsieh,Yongjie Jessica Zhang*

Main category: cs.LG

TL;DR: The paper introduces GALDS, a Graph-Autoencoder-based Latent Dynamics Surrogate model, to efficiently simulate material transport in neural trees, achieving high accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Neurons' complex geometries and material transport processes are computationally challenging to simulate. Traditional methods are inefficient, but neuron trees' properties allow for optimization.

Method: GALDS uses a graph autoencoder for latent representations of geometry, velocity, and concentration, combined with a Neural ODE-inspired dynamic model for predictions.

Result: GALDS achieves 3% mean relative error, <8% max error, and 10x speed improvement on unseen and abnormal transport cases.

Conclusion: GALDS offers an efficient, accurate solution for simulating material transport in neural trees, outperforming traditional methods.

Abstract: Neurons exhibit intricate geometries within their neurite networks, which
play a crucial role in processes such as signaling and nutrient transport.
Accurate simulation of material transport in the networks is essential for
understanding these biological phenomena but poses significant computational
challenges because of the complex tree-like structures involved. Traditional
approaches are time-intensive and resource-demanding, yet the inherent
properties of neuron trees, which consists primarily of pipes with steady-state
parabolic velocity profiles and bifurcations, provide opportunities for
computational optimization. To address these challenges, we propose a
Graph-Autoencoder-based Latent Dynamics Surrogate (GALDS) model, which is
specifically designed to streamline the simulation of material transport in
neural trees. GALDS employs a graph autoencoder to encode latent
representations of the network's geometry, velocity fields, and concentration
profiles. These latent space representations are then assembled into a global
graph, which is subsequently used to predict system dynamics in the latent
space via a trained graph latent space system dynamic model, inspired by the
Neural Ordinary Differential Equations (Neural ODEs) concept. The integration
of an autoencoder allows for the use of smaller graph neural network models
with reduced training data requirements. Furthermore, the Neural ODE component
effectively mitigates the issue of error accumulation commonly encountered in
recurrent neural networks. The effectiveness of the GALDS model is demonstrated
through results on eight unseen geometries and four abnormal transport
examples, where our approach achieves mean relative error of 3% with maximum
relative error <8% and demonstrates a 10-fold speed improvement compared to
previous surrogate model approaches.

</details>


### [77] [Domain-Adaptive Small Language Models for Structured Tax Code Prediction](https://arxiv.org/abs/2507.10880)
*Souvik Nath,Sumit Wadhwa,Luiz Perez*

Main category: cs.LG

TL;DR: The paper proposes a domain-adaptive small language model (SLM) with an encoder-decoder architecture for predicting hierarchical tax codes like HSN or SAC, outperforming flat classifiers and other architectures.


<details>
  <summary>Details</summary>
Motivation: Accurate tax code prediction is crucial for compliance, but current methods struggle with hierarchical dependencies in unstructured data.

Method: Uses an encoder-decoder SLM to sequentially generate tax codes, capturing hierarchical dependencies.

Result: The SLM outperforms flat classifiers, decoder-only, and encoder-only architectures for structured tax code prediction.

Conclusion: The approach is scalable to other tax codes and demonstrates the potential of SLMs in unexplored NLP domains.

Abstract: Every day, multinational firms process thousands of transactions, each of
which must adhere to tax regulations that vary by jurisdiction and are often
nuanced. The determination of product and service tax codes, such as HSN or SAC
is a major use case in Tax compliance. An accurate determination of such codes
is imperative to avoid any tax penalties. This paper proposes a domain-adaptive
small language model (SLM) with an encoder-decoder architecture for the
enhanced prediction of product and service tax codes. In this approach, we
address the problem of predicting hierarchical tax code sequences using
unstructured product and services data. We employ an SLM based upon
encoder-decoder architecture as this enables sequential generation of tax codes
to capture the hierarchical dependencies present within the tax codes. Our
experiments demonstrate that encoder-decoder SLMs can be successfully applied
to the sequential prediction of structured tax codes, a domain that remains
comparatively unexplored in current NLP research. In this paper, we demonstrate
the superior performance of the domain-adaptive encoder-decoder SLMs over flat
classifiers when applied to the Harmonized System of Nomenclature (HSN), and
achieve superior results compared to decoder-only and encoder-only
architectures for structured sequence generation tasks. This approach can also
be scaled to other government-mandated tax commodity codes, such as United
Nations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura
Comum do Mercosul (NCM).

</details>


### [78] [Learning from Imperfect Data: Robust Inference of Dynamic Systems using Simulation-based Generative Model](https://arxiv.org/abs/2507.10884)
*Hyunwoo Cho,Hyeontae Jo,Hyung Ju Hwang*

Main category: cs.LG

TL;DR: SiGMoID is a simulation-based generative model for robust inference of nonlinear dynamic systems from noisy or sparse data, combining physics-informed neural networks and Wasserstein GANs.


<details>
  <summary>Details</summary>
Motivation: Inferring nonlinear dynamic models from imperfect data is challenging, especially with noisy, sparse, or partially observable datasets.

Method: Integrates physics-informed neural networks with hyper-networks for ODE solving and Wasserstein GANs for parameter estimation.

Result: SiGMoID effectively quantifies noise, estimates parameters, and infers unobserved components, validated in realistic experiments.

Conclusion: SiGMoID is broadly applicable across domains, enabling accurate inference of full system dynamics.

Abstract: System inference for nonlinear dynamic models, represented by ordinary
differential equations (ODEs), remains a significant challenge in many fields,
particularly when the data are noisy, sparse, or partially observable. In this
paper, we propose a Simulation-based Generative Model for Imperfect Data
(SiGMoID) that enables precise and robust inference for dynamic systems. The
proposed approach integrates two key methods: (1) physics-informed neural
networks with hyper-networks that constructs an ODE solver, and (2) Wasserstein
generative adversarial networks that estimates ODE parameters by effectively
capturing noisy data distributions. We demonstrate that SiGMoID quantifies data
noise, estimates system parameters, and infers unobserved system components.
Its effectiveness is validated validated through realistic experimental
examples, showcasing its broad applicability in various domains, from
scientific research to engineered systems, and enabling the discovery of full
system dynamics.

</details>


### [79] [How to Protect Models against Adversarial Unlearning?](https://arxiv.org/abs/2507.10886)
*Patryk Jasiorski,Marek Klonowski,Michał Woźniak*

Main category: cs.LG

TL;DR: The paper explores adversarial unlearning in AI models, where malicious requests degrade performance, and proposes a method to protect model performance from such effects.


<details>
  <summary>Details</summary>
Motivation: AI models require unlearning for legal compliance (e.g., GDPR, AI Act), toxic content removal, debiasing, and adapting to data changes, but unlearning can harm performance.

Method: Investigates adversarial unlearning, where adversaries exploit unlearning requests to degrade model performance, and analyzes factors like model backbone and data selection strategies.

Result: Identifies dependencies of adversarial unlearning on model and data selection, and introduces a method to mitigate performance deterioration from unlearning.

Conclusion: Proposes a protective method against adversarial unlearning, addressing both spontaneous and malicious unlearning impacts on model performance.

Abstract: AI models need to be unlearned to fulfill the requirements of legal acts such
as the AI Act or GDPR, and also because of the need to remove toxic content,
debiasing, the impact of malicious instances, or changes in the data
distribution structure in which a model works. Unfortunately, removing
knowledge may cause undesirable side effects, such as a deterioration in model
performance. In this paper, we investigate the problem of adversarial
unlearning, where a malicious party intentionally sends unlearn requests to
deteriorate the model's performance maximally. We show that this phenomenon and
the adversary's capabilities depend on many factors, primarily on the backbone
model itself and strategy/limitations in selecting data to be unlearned. The
main result of this work is a new method of protecting model performance from
these side effects, both in the case of unlearned behavior resulting from
spontaneous processes and adversary actions.

</details>


### [80] [Outbound Modeling for Inventory Management](https://arxiv.org/abs/2507.10890)
*Riccardo Savorgnan,Udaya Ghai,Carson Eisenach,Dean Foster*

Main category: cs.LG

TL;DR: The paper addresses forecasting warehouse unit drain and shipping costs for RL-based inventory planning, proposing a probabilistic model and validation scheme.


<details>
  <summary>Details</summary>
Motivation: Accurate modeling of warehouse drain and shipping costs is crucial for RL-driven inventory planning, but existing methods are non-differentiable and inefficient.

Method: A probabilistic forecasting model is developed to predict joint distributions of drain and shipping costs, conditioned on inventory and demand. A validation scheme tests robustness in RL environments.

Result: Preliminary results show the model's accuracy in in-distribution settings.

Conclusion: The proposed model and validation scheme offer a scalable and differentiable solution for RL-based inventory planning.

Abstract: We study the problem of forecasting the number of units fulfilled (or
``drained'') from each inventory warehouse to meet customer demand, along with
the associated outbound shipping costs. The actual drain and shipping costs are
determined by complex production systems that manage the planning and execution
of customers' orders fulfillment, i.e. from where and how to ship a unit to be
delivered to a customer. Accurately modeling these processes is critical for
regional inventory planning, especially when using Reinforcement Learning (RL)
to develop control policies. For the RL usecase, a drain model is incorporated
into a simulator to produce long rollouts, which we desire to be
differentiable. While simulating the calls to the internal software systems can
be used to recover this transition, they are non-differentiable and too slow
and costly to run within an RL training environment. Accordingly, we frame this
as a probabilistic forecasting problem, modeling the joint distribution of
outbound drain and shipping costs across all warehouses at each time period,
conditioned on inventory positions and exogenous customer demand. To ensure
robustness in an RL environment, the model must handle out-of-distribution
scenarios that arise from off-policy trajectories. We propose a validation
scheme that leverages production systems to evaluate the drain model on
counterfactual inventory states induced by RL policies. Preliminary results
demonstrate the model's accuracy within the in-distribution setting.

</details>


### [81] [Class-Proportional Coreset Selection for Difficulty-Separable Data](https://arxiv.org/abs/2507.10904)
*Elisa Tsai,Haizhong Zheng,Atul Prakash*

Main category: cs.LG

TL;DR: The paper introduces class-difficulty separability in coreset selection, proposes the CDSC measure, and develops class-proportional methods to improve data efficiency and performance in high-stakes domains.


<details>
  <summary>Details</summary>
Motivation: Existing coreset methods assume class-wise homogeneity in data difficulty, neglecting variations across classes, which can degrade performance in domains like network intrusion detection and medical imaging.

Method: The authors formalize class-difficulty separability with the CDSC measure and introduce class-proportional variants of sampling strategies.

Result: Class-proportional methods outperform class-agnostic ones, showing minimal performance drops even at extreme pruning rates (e.g., 99%).

Conclusion: Explicitly modeling class-difficulty separability leads to more effective and robust data pruning, especially in critical applications.

Abstract: High-quality training data is essential for building reliable and efficient
machine learning systems. One-shot coreset selection addresses this by pruning
the dataset while maintaining or even improving model performance, often
relying on training-dynamics-based data difficulty scores. However, most
existing methods implicitly assume class-wise homogeneity in data difficulty,
overlooking variation in data difficulty across different classes.
  In this work, we challenge this assumption by showing that, in domains such
as network intrusion detection and medical imaging, data difficulty often
clusters by class. We formalize this as class-difficulty separability and
introduce the Class Difficulty Separability Coefficient (CDSC) as a
quantitative measure. We demonstrate that high CDSC values correlate with
performance degradation in class-agnostic coreset methods, which tend to
overrepresent easy majority classes while neglecting rare but informative ones.
  To address this, we introduce class-proportional variants of multiple
sampling strategies. Evaluated on five diverse datasets spanning security and
medical domains, our methods consistently achieve state-of-the-art data
efficiency. For instance, on CTU-13, at an extreme 99% pruning rate, a
class-proportional variant of Coverage-centric Coreset Selection (CCS-CP) shows
remarkable stability, with accuracy dropping only 2.58%, precision 0.49%, and
recall 0.19%. In contrast, the class-agnostic CCS baseline, the next best
method, suffers sharper declines of 7.59% in accuracy, 4.57% in precision, and
4.11% in recall.
  We further show that aggressive pruning enhances generalization in noisy,
imbalanced, and large-scale datasets. Our results underscore that explicitly
modeling class-difficulty separability leads to more effective, robust, and
generalizable data pruning, particularly in high-stakes scenarios.

</details>


### [82] [Diffusion Decoding for Peptide De Novo Sequencing](https://arxiv.org/abs/2507.10955)
*Chi-en Amy Tai,Alexander Wong*

Main category: cs.LG

TL;DR: The paper explores diffusion decoders for peptide de novo sequencing, finding they improve amino acid recall but not peptide precision/recall compared to autoregressive models.


<details>
  <summary>Details</summary>
Motivation: Traditional autoregressive decoders in deep learning (e.g., Casanovo) suffer from cascading errors and underutilize high-confidence regions.

Method: Three diffusion decoder designs, knapsack beam search, and various loss functions were tested.

Result: The best diffusion decoder with DINOISER loss improved amino acid recall by 0.373 over Casanovo, but peptide metrics remained unchanged.

Conclusion: Diffusion decoders show promise for enhancing sensitivity in peptide de novo sequencing.

Abstract: Peptide de novo sequencing is a method used to reconstruct amino acid
sequences from tandem mass spectrometry data without relying on existing
protein sequence databases. Traditional deep learning approaches, such as
Casanovo, mainly utilize autoregressive decoders and predict amino acids
sequentially. Subsequently, they encounter cascading errors and fail to
leverage high-confidence regions effectively. To address these issues, this
paper investigates using diffusion decoders adapted for the discrete data
domain. These decoders provide a different approach, allowing sequence
generation to start from any peptide segment, thereby enhancing prediction
accuracy. We experiment with three different diffusion decoder designs,
knapsack beam search, and various loss functions. We find knapsack beam search
did not improve performance metrics and simply replacing the transformer
decoder with a diffusion decoder lowered performance. Although peptide
precision and recall were still 0, the best diffusion decoder design with the
DINOISER loss function obtained a statistically significant improvement in
amino acid recall by 0.373 compared to the baseline autoregressive
decoder-based Casanovo model. These findings highlight the potential of
diffusion decoders to not only enhance model sensitivity but also drive
significant advancements in peptide de novo sequencing.

</details>


### [83] [Physics-Informed Neural Networks For Semiconductor Film Deposition: A Review](https://arxiv.org/abs/2507.10983)
*Tao Han,Zahra Taheri,Hyunwoong Ko*

Main category: cs.LG

TL;DR: The paper reviews ML applications, especially Physics-Informed Neural Networks (PINNs), for improving semiconductor film deposition processes, identifying trends, gaps, and proposing future research directions.


<details>
  <summary>Details</summary>
Motivation: To address challenges in semiconductor film deposition (e.g., control, quality) using ML, particularly PINNs, for better precision and efficiency.

Method: Thematic analysis of ML applications in film deposition, focusing on PINNs, their integration of physical laws, and neural network architectures.

Result: Identified key trends, limitations, and gaps in current ML methodologies, proposing PINNs as a solution for enhanced interpretability and robustness.

Conclusion: PINNs offer significant potential for advancing film deposition processes, with proposed research directions to improve precision and scalability in semiconductor manufacturing.

Abstract: Semiconductor manufacturing relies heavily on film deposition processes, such
as Chemical Vapor Deposition and Physical Vapor Deposition. These complex
processes require precise control to achieve film uniformity, proper adhesion,
and desired functionality. Recent advancements in Physics-Informed Neural
Networks (PINNs), an innovative machine learning (ML) approach, have shown
significant promise in addressing challenges related to process control,
quality assurance, and predictive modeling within semiconductor film deposition
and other manufacturing domains. This paper provides a comprehensive review of
ML applications targeted at semiconductor film deposition processes. Through a
thematic analysis, we identify key trends, existing limitations, and research
gaps, offering insights into both the advantages and constraints of current
methodologies. Our structured analysis aims to highlight the potential
integration of these ML techniques to enhance interpretability, accuracy, and
robustness in film deposition processes. Additionally, we examine
state-of-the-art PINN methods, discussing strategies for embedding physical
knowledge, governing laws, and partial differential equations into advanced
neural network architectures tailored for semiconductor manufacturing. Based on
this detailed review, we propose novel research directions that integrate the
strengths of PINNs to significantly advance film deposition processes. The
contributions of this study include establishing a clear pathway for future
research in integrating physics-informed ML frameworks, addressing existing
methodological gaps, and ultimately improving precision, scalability, and
operational efficiency within semiconductor manufacturing.

</details>


### [84] [StellarF: A Lora-Adapter Integrated Large Model Framework for Stellar Flare Forecasting with Historical & Statistical Data](https://arxiv.org/abs/2507.10986)
*Tianyu Su,Zhiqiang Zou,Ali Luo,Xiao Kong,Qingyu Lu,Min Li*

Main category: cs.LG

TL;DR: StellarF introduces a parameter-efficient model for stellar flare forecasting using LoRA and Adapter techniques, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Addressing the sparsity of flare events and lack of domain-specific large-scale models in stellar flare forecasting.

Method: Combines flare statistical and historical record modules for multi-scale pattern recognition, tested on Kepler and TESS datasets.

Result: StellarF outperforms existing methods, demonstrating superior performance.

Conclusion: Establishes a novel framework for astrophysical research and cross-disciplinary applications.

Abstract: Stellar flare forecasting, a critical research frontier in astronomy, offers
profound insights into stellar activity. However, the field is constrained by
both the sparsity of recorded flare events and the absence of domain-specific
large-scale predictive models. To address these challenges, this study
introduces StellarF (Stellar Flare Forecasting), a novel large model that
leverages Low-Rank (LoRA) and Adapter techniques to parameter-efficient
learning for stellar flare forecasting. At its core, StellarF integrates an
flare statistical information module with a historical flare record module,
enabling multi-scale pattern recognition from observational data. Extensive
experiments on our self-constructed datasets (derived from Kepler and TESS
light curves) demonstrate that StellarF achieves state-of-the-art performance
compared to existing methods. The proposed prediction paradigm establishes a
novel methodological framework for advancing astrophysical research and
cross-disciplinary applications.

</details>


### [85] [High-Throughput Distributed Reinforcement Learning via Adaptive Policy Synchronization](https://arxiv.org/abs/2507.10990)
*Rodney Lafuente-Mercado*

Main category: cs.LG

TL;DR: ClusterEnv introduces a lightweight, learner-agnostic interface for distributed RL, decoupling simulation from training using the DETACH pattern and addressing policy staleness with AAPS.


<details>
  <summary>Details</summary>
Motivation: Existing RL frameworks entangle simulation, learning, and orchestration, limiting modularity and reusability.

Method: ClusterEnv uses the DETACH pattern to offload simulation to remote workers and proposes AAPS for adaptive policy synchronization.

Result: Experiments show AAPS achieves high sample efficiency with fewer weight updates.

Conclusion: ClusterEnv integrates seamlessly into RL pipelines, supports diverse methods, and improves efficiency.

Abstract: Scaling reinforcement learning (RL) workloads often requires distributing
environment simulation across compute clusters. Existing frameworks entangle
simulation, learning logic, and orchestration into monolithic systems, limiting
modularity and reusability. We present ClusterEnv, a lightweight,
learner-agnostic interface for distributed environment execution that mirrors
the Gymnasium API. ClusterEnv introduces the DETACH pattern, which decouples
simulation from training by offloading reset() and step() operations to remote
workers while keeping learning centralized. To address policy staleness in
distributed execution, we propose Adaptive Actor Policy Synchronization (AAPS),
a divergence-triggered update mechanism that reduces synchronization overhead
without sacrificing performance. ClusterEnv integrates cleanly into existing RL
pipelines, supports both on-policy and off-policy methods, and requires minimal
code changes. Experiments on discrete control tasks demonstrate that AAPS
achieves high sample efficiency with significantly fewer weight updates. Source
code is available at https://github.com/rodlaf/ClusterEnv.

</details>


### [86] [Misalignment from Treating Means as Ends](https://arxiv.org/abs/2507.10995)
*Henrik Marklund,Alex Infanger,Benjamin Van Roy*

Main category: cs.LG

TL;DR: Reward functions in reinforcement learning often mix terminal and instrumental goals, leading to misalignment and poor performance when optimized.


<details>
  <summary>Details</summary>
Motivation: To highlight how human-specified or learned reward functions can conflate terminal (end goals) and instrumental (means to goals) objectives, causing severe misalignment in reinforcement learning.

Method: Formulated a simple example demonstrating how slight conflation of goals leads to poor performance when optimizing misspecified rewards. Analyzed environments sensitive to such conflation.

Result: Showed that even minor goal conflation results in significant misalignment, with poor performance under true reward functions.

Conclusion: Identified a critical issue in reward learning and reinforcement learning, emphasizing the need to distinguish between terminal and instrumental goals to avoid misalignment.

Abstract: Reward functions, learned or manually specified, are rarely perfect. Instead
of accurately expressing human goals, these reward functions are often
distorted by human beliefs about how best to achieve those goals. Specifically,
these reward functions often express a combination of the human's terminal
goals -- those which are ends in themselves -- and the human's instrumental
goals -- those which are means to an end. We formulate a simple example in
which even slight conflation of instrumental and terminal goals results in
severe misalignment: optimizing the misspecified reward function results in
poor performance when measured by the true reward function. This example
distills the essential properties of environments that make reinforcement
learning highly sensitive to conflation of instrumental and terminal goals. We
discuss how this issue can arise with a common approach to reward learning and
how it can manifest in real environments.

</details>


### [87] [Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data](https://arxiv.org/abs/2507.10998)
*Zhipeng He,Alexander Stevens,Chun Ouyang,Johannes De Smedt,Alistair Barros,Catarina Moreira*

Main category: cs.LG

TL;DR: A framework using a mixed-input VAE for generating imperceptible adversarial examples on tabular data, ensuring statistical consistency and outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of adversarial attacks on tabular data, which lacks intuitive similarity metrics and often produces detectable deviations from original distributions.

Method: Proposes a latent space perturbation framework with a mixed-input VAE, integrating categorical and numerical features into a unified latent manifold for consistent perturbations.

Result: Achieves lower outlier rates and more consistent performance across datasets and model architectures, with a focus on in-distribution success rate (IDSR).

Conclusion: VAE-based attacks are effective for realistic adversarial examples on tabular data, emphasizing the importance of on-manifold perturbations and reconstruction quality.

Abstract: Adversarial attacks on tabular data present fundamental challenges distinct
from image or text domains due to the heterogeneous nature of mixed categorical
and numerical features. Unlike images where pixel perturbations maintain visual
similarity, tabular data lacks intuitive similarity metrics, making it
difficult to define imperceptible modifications. Additionally, traditional
gradient-based methods prioritise $\ell_p$-norm constraints, often producing
adversarial examples that deviate from the original data distributions, making
them detectable. We propose a latent space perturbation framework using a
mixed-input Variational Autoencoder (VAE) to generate imperceptible adversarial
examples. The proposed VAE integrates categorical embeddings and numerical
features into a unified latent manifold, enabling perturbations that preserve
statistical consistency. We specify In-Distribution Success Rate (IDSR) to
measure the proportion of adversarial examples that remain statistically
indistinguishable from the input distribution. Evaluation across six publicly
available datasets and three model architectures demonstrates that our method
achieves substantially lower outlier rates and more consistent performance
compared to traditional input-space attacks and other VAE-based methods adapted
from image domain approaches. Our comprehensive analysis includes
hyperparameter sensitivity, sparsity control mechanisms, and generative
architectural comparisons, revealing that VAE-based attacks depend critically
on reconstruction quality but offer superior practical utility when sufficient
training data is available. This work highlights the importance of on-manifold
perturbations for realistic adversarial attacks on tabular data, offering a
robust approach for practical deployment. The source code can be accessed
through https://github.com/ZhipengHe/VAE-TabAttack.

</details>


### [88] [First-Order Error Matters: Accurate Compensation for Quantized Large Language Models](https://arxiv.org/abs/2507.11017)
*Xingyu Zheng,Haotong Qin,Yuye Li,Jiakai Wang,Jinyang Guo,Michele Magno,Xianglong Liu*

Main category: cs.LG

TL;DR: FOEM is a novel post-training quantization method for LLMs that addresses flaws in existing methods by incorporating first-order gradient terms, improving accuracy with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing PTQ methods assume negligible first-order terms in quantization error, but this assumption is flawed due to accumulated deviations. FOEM aims to correct this.

Method: FOEM explicitly includes first-order gradient terms, approximates gradients efficiently, and uses precomputed Cholesky factors for Hessian submatrix inversion.

Result: FOEM outperforms GPTQ, reducing perplexity by 89.6% for Llama3-8B and improving MMLU accuracy for Llama3-70B from 51.7% to 74.9%. It also integrates well with advanced techniques.

Conclusion: FOEM effectively addresses quantization error compensation, achieving near full-precision performance and compatibility with other advanced methods.

Abstract: Post-training quantization (PTQ) offers an efficient approach to compressing
large language models (LLMs), significantly reducing memory access and
computational costs. Existing compensation-based weight calibration methods
often rely on a second-order Taylor expansion to model quantization error,
under the assumption that the first-order term is negligible in well-trained
full-precision models. However, we reveal that the progressive compensation
process introduces accumulated first-order deviations between latent weights
and their full-precision counterparts, making this assumption fundamentally
flawed. To address this, we propose FOEM, a novel PTQ method that explicitly
incorporates first-order gradient terms to improve quantization error
compensation. FOEM approximates gradients by directly computing the difference
between latent and full-precision weights, avoiding the high cost and limited
generalization of backpropagation-based gradient computation. This approach
introduces minimal additional computational overhead. Moreover, FOEM leverages
precomputed Cholesky factors to efficiently recover the inverse of Hessian
submatrices in real time. Extensive experiments across a wide range of models
and benchmarks demonstrate that FOEM consistently outperforms the classical
GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of
Llama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from
51.7% to 74.9%, approaching the full-precision performance of 78.6%.
Furthermore, FOEM can be seamlessly integrated with advanced techniques such as
GPTAQ and SpinQuant, yielding additional improvements under the challenging
W4A4KV4 setting, and further narrowing the accuracy gap with full-precision
baselines beyond what current state-of-the-art methods achieve. The code is
available at https://github.com/Xingyu-Zheng/FOEM.

</details>


### [89] [AdaMuon: Adaptive Muon Optimizer](https://arxiv.org/abs/2507.11005)
*Chongjie Si,Debing Zhang,Wei Shen*

Main category: cs.LG

TL;DR: AdaMuon is an adaptive learning-rate framework enhancing Muon with per-parameter modulation and RMS-aligned rescaling, outperforming Muon in convergence and stability without extra tuning.


<details>
  <summary>Details</summary>
Motivation: To improve upon Muon's efficiency and adaptivity in large-scale model training while maintaining simplicity.

Method: Augments Muon with two modules: per-parameter second-moment modulation and RMS-aligned rescaling.

Result: Consistently outperforms Muon in convergence speed and training stability across various model scales.

Conclusion: AdaMuon is a robust, tunable-free enhancement to Muon, suitable for seamless integration into existing pipelines.

Abstract: We propose AdaMuon, an adaptive learning-rate framework built upon the
recently validated Muon optimizer, which has demonstrated substantial
efficiency gains over AdamW in large-scale model training. AdaMuon augments
Muon with two mutually dependent modules: (1) a per-parameter second-moment
modulation that captures orthogonal gradient updates to ensure update-level
adaptivity, and (2) a RMS-aligned rescaling that regulates the overall update
magnitude by aligning it with the intrinsic structure of the parameter space.
Empirical results on multiple model scales and learning-rate regimes confirm
that AdaMuon consistently outperforms the original Muon, delivering higher
acceleration in convergence while maintaining training stability. Our method
introduces no additional tuning burden and can be seamlessly integrated into
existing Muon training pipelines.

</details>


### [90] [AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air](https://arxiv.org/abs/2507.11515)
*Shiyi Yang,Xiaoxue Yu,Rongpeng Li,Jianhang Zhu,Zhifeng Zhao,Honggang Zhang*

Main category: cs.LG

TL;DR: AirLLM introduces a hierarchical diffusion policy framework for efficient LoRA adaptation in LLMs, combining PPO and DDIM to optimize rank configurations and reduce transmission costs.


<details>
  <summary>Details</summary>
Motivation: Addressing inefficiencies in existing LoRA approaches for remote fine-tuning of LLMs due to fixed rank configurations and high transmission costs.

Method: Uses a PPO agent for coarse-grained decisions and DDIM for refining rank vectors, optimized alternately under CFG.

Result: AirLLM improves fine-tuning performance and reduces transmission costs under varying signal-to-noise ratios.

Conclusion: AirLLM demonstrates scalable and efficient remote fine-tuning via reinforcement-driven, diffusion-refined rank adaptation.

Abstract: Operating Large Language Models (LLMs) on edge devices is increasingly
challenged by limited communication bandwidth and strained computational and
memory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.
Nevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ
fixed or heuristic rank configurations, and the subsequent over-the-air
transmission of all LoRA parameters could be rather inefficient. To address
this limitation, we develop AirLLM, a hierarchical diffusion policy framework
for communication-aware LoRA adaptation. Specifically, AirLLM models the rank
configuration as a structured action vector that spans all LoRA-inserted
projections. To solve the underlying high-dimensional sequential
decision-making problem, a Proximal Policy Optimization (PPO) agent generates
coarse-grained decisions by jointly observing wireless states and linguistic
complexity, which are then refined via Denoising Diffusion Implicit Models
(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The
two modules are optimized alternatively, with the DDIM trained under the
Classifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.
Experiments under varying signal-to-noise ratios demonstrate that AirLLM
consistently enhances fine-tuning performance while significantly reducing
transmission costs, highlighting the effectiveness of reinforcement-driven,
diffusion-refined rank adaptation for scalable and efficient remote fine-tuning
over the air.

</details>


### [91] [Leveraging Advanced Machine Learning to Predict Turbulence Dynamics from Temperature Observations at an Experimental Prescribed Fire](https://arxiv.org/abs/2507.11012)
*Dipak Dulal,Joseph J. Charney,Michael R. Gallagher,Pitambar Acharya,Carmeliza Navasca,Nicholas S. Skowronski*

Main category: cs.LG

TL;DR: Machine learning models predict turbulent kinetic energy (TKE) from temperature data in a fire environment, revealing new insights into fire dynamics.


<details>
  <summary>Details</summary>
Motivation: To explore the relationship between temperature and TKE in fire environments, aiding fire research and management.

Method: Used Deep Neural Networks, Random Forest Regressor, Gradient Boosting, and Gaussian Process Regressor on 10 Hz temperature and turbulence data from a prescribed burn.

Result: Achieved accurate TKE predictions despite weak correlations, with regression models performing particularly well.

Conclusion: Demonstrates a novel numerical approach for fire research, highlighting machine learning's potential in analyzing complex fire environment data.

Abstract: This study explores the potential for predicting turbulent kinetic energy
(TKE) from more readily acquired temperature data using temperature profiles
and turbulence data collected concurrently at 10 Hz during a small experimental
prescribed burn in the New Jersey Pine Barrens. Machine learning models,
including Deep Neural Networks, Random Forest Regressor, Gradient Boosting, and
Gaussian Process Regressor, were employed to assess the potential to predict
TKE from temperature perturbations and explore temporal and spatial dynamics of
correlations. Data visualization and correlation analyses revealed patterns and
relationships between thermocouple temperatures and TKE, providing insight into
the underlying dynamics. More accurate predictions of TKE were achieved by
employing various machine learning models despite a weak correlation between
the predictors and the target variable. The results demonstrate significant
success, particularly from regression models, in accurately predicting the TKE.
The findings of this study demonstrate a novel numerical approach to
identifying new relationships between temperature and airflow processes in and
around the fire environment. These relationships can help refine our
understanding of combustion environment processes and the coupling and
decoupling of fire environment processes necessary for improving fire
operations strategy and fire and smoke model predictions. The findings of this
study additionally highlight the valuable role of machine learning techniques
in analyzing the complex large datasets of the fire environments, showcasing
their potential to advance fire research and management practices.

</details>


### [92] [Relative Entropy Pathwise Policy Optimization](https://arxiv.org/abs/2507.11019)
*Claas Voelcker,Axel Brunnbauer,Marcel Hussing,Michal Nauman,Pieter Abbeel,Eric Eaton,Radu Grosu,Amir-massoud Farahmand,Igor Gilitschenski*

Main category: cs.LG

TL;DR: The paper introduces REPPO, an on-policy algorithm combining pathwise policy gradients' efficiency with on-policy learning's simplicity, addressing high variance and unstable training in score-function methods.


<details>
  <summary>Details</summary>
Motivation: Score-function policy gradients suffer from high variance, while pathwise policy gradients require accurate action-conditioned value functions. The paper aims to bridge this gap for stable, efficient on-policy learning.

Method: Proposes REPPO, balancing stochastic policies for exploration with constrained updates, and optimizing value function learning.

Result: REPPO shows strong performance with reduced sample needs, faster training, lower memory use, and hyperparameter robustness in benchmarks.

Conclusion: REPPO effectively combines pathwise policy gradients' benefits with on-policy learning, offering a practical solution for stable and efficient training.

Abstract: Score-function policy gradients have delivered strong results in
game-playing, robotics and language-model fine-tuning. Yet its high-variance
often undermines training stability. On the other hand, pathwise policy
gradients alleviate the training variance, but are reliable only when driven by
an accurate action-conditioned value function which is notoriously hard to
train without relying on past off-policy data. In this paper, we discuss how to
construct a value-gradient driven, on-policy algorithm that allow training
Q-value models purely from on-policy data, unlocking the possibility of using
pathwise policy updates in the context of on-policy learning. We show how to
balance stochastic policies for exploration with constrained policy updates for
stable training, and evaluate important architectural components that
facilitate accurate value function learning. Building on these insights, we
propose Relative Entropy Pathwise Policy Optimization (REPPO), an efficient
on-policy algorithm that combines the sample-efficiency of pathwise policy
gradients with the simplicity and minimal memory footprint of standard
on-policy learning. We demonstrate that REPPO provides strong empirical
performance at decreased sample requirements, wall-clock time, memory footprint
as well as high hyperparameter robustness in a set of experiments on two
standard GPU-parallelized benchmarks.

</details>


### [93] [GATE: Graph Attention Neural Networks with Real-Time Edge Construction for Robust Indoor Localization using Mobile Embedded Devices](https://arxiv.org/abs/2507.11053)
*Danish Gufran,Sudeep Pasricha*

Main category: cs.LG

TL;DR: GATE, a novel framework using adaptive graph representations and attention mechanisms, improves Wi-Fi RSS fingerprinting for indoor localization, outperforming existing methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Current DL models for indoor localization assume Euclidean space for RSS fingerprinting, ignoring spatial relationships and non-uniform noise, leading to poor generalization across devices. GNNs help but struggle with noise and blind spots.

Method: GATE introduces adaptive graph representations, AHV for message passing, MDHV to address GNN blind spots, and RTEC for dynamic graph adaptation, modeling non-Euclidean RSS noise.

Result: GATE achieves 1.6x to 4.72x lower mean localization errors and 1.85x to 4.57x lower worst-case errors compared to state-of-the-art frameworks.

Conclusion: GATE effectively addresses challenges in indoor localization, offering superior accuracy and adaptability in diverse environments with heterogeneous devices.

Abstract: Accurate indoor localization is crucial for enabling spatial context in smart
environments and navigation systems. Wi-Fi Received Signal Strength (RSS)
fingerprinting is a widely used indoor localization approach due to its
compatibility with mobile embedded devices. Deep Learning (DL) models improve
accuracy in localization tasks by learning RSS variations across locations, but
they assume fingerprint vectors exist in a Euclidean space, failing to
incorporate spatial relationships and the non-uniform distribution of
real-world RSS noise. This results in poor generalization across heterogeneous
mobile devices, where variations in hardware and signal processing distort RSS
readings. Graph Neural Networks (GNNs) can improve upon conventional DL models
by encoding indoor locations as nodes and modeling their spatial and signal
relationships as edges. However, GNNs struggle with non-Euclidean noise
distributions and suffer from the GNN blind spot problem, leading to degraded
accuracy in environments with dense access points (APs). To address these
challenges, we propose GATE, a novel framework that constructs an adaptive
graph representation of fingerprint vectors while preserving an indoor
state-space topology, modeling the non-Euclidean structure of RSS noise to
mitigate environmental noise and address device heterogeneity. GATE introduces
1) a novel Attention Hyperspace Vector (AHV) for enhanced message passing, 2) a
novel Multi-Dimensional Hyperspace Vector (MDHV) to mitigate the GNN blind
spot, and 3) an new Real-Time Edge Construction (RTEC) approach for dynamic
graph adaptation. Extensive real-world evaluations across multiple indoor
spaces with varying path lengths, AP densities, and heterogeneous devices
demonstrate that GATE achieves 1.6x to 4.72x lower mean localization errors and
1.85x to 4.57x lower worst-case errors compared to state-of-the-art indoor
localization frameworks.

</details>


### [94] [A Distance Metric for Mixed Integer Programming Instances](https://arxiv.org/abs/2507.11063)
*Gwen Maudet,Grégoire Danoy*

Main category: cs.LG

TL;DR: The paper introduces a mathematical distance metric for MILP instances to improve similarity comparison, outperforming existing methods in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Current similarity metrics for MILP instances lack precision and rely on labeled data, limiting their effectiveness. A robust metric could enhance solver guidance and instance set evaluation.

Method: The proposed metric discretizes right-hand sides, weights, and variables, using an Earth mover's distance-inspired approach for constraint comparisons. It includes exact and greedy variants.

Result: The greedy variant achieves near-identical accuracy to the exact version but is 200 times faster. It outperforms non-learned baselines and rivals supervised classifiers.

Conclusion: The new metric effectively identifies instance classes and subclasses, offering a scalable and unsupervised solution for MILP instance comparison.

Abstract: Mixed-integer linear programming (MILP) is a powerful tool for addressing a
wide range of real-world problems, but it lacks a clear structure for comparing
instances. A reliable similarity metric could establish meaningful
relationships between instances, enabling more effective evaluation of instance
set heterogeneity and providing better guidance to solvers, particularly when
machine learning is involved. Existing similarity metrics often lack precision
in identifying instance classes or rely heavily on labeled data, which limits
their applicability and generalization. To bridge this gap, this paper
introduces the first mathematical distance metric for MILP instances, derived
directly from their mathematical formulations. By discretizing right-hand
sides, weights, and variables into classes, the proposed metric draws
inspiration from the Earth mover's distance to quantify mismatches in
weight-variable distributions for constraint comparisons. This approach
naturally extends to enable instance-level comparisons. We evaluate both an
exact and a greedy variant of our metric under various parameter settings,
using the StrIPLIB dataset. Results show that all components of the metric
contribute to class identification, and that the greedy version achieves
accuracy nearly identical to the exact formulation while being nearly 200 times
faster. Compared to state-of-the-art baselines, including feature-based,
image-based, and neural network models, our unsupervised method consistently
outperforms all non-learned approaches and rivals the performance of a
supervised classifier on class and subclass grouping tasks.

</details>


### [95] [LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly Detection](https://arxiv.org/abs/2507.11071)
*Isaiah Thompson Ocansey,Ritwik Bhattacharya,Tanmay Sen*

Main category: cs.LG

TL;DR: The paper proposes parameter-efficient finetuning (LoRA and adapter-based methods) for log anomaly detection, outperforming traditional methods with 18-19% higher accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based or deep learning methods struggle with large, complex log sequences, necessitating more efficient anomaly detection for system maintenance.

Method: Uses LoRA and adapter-based finetuning on tiny LLMs, tested on the Thunderbird dataset.

Result: LoRA-based finetuning achieves 97.76%-98.83% accuracy, a significant improvement over LogBert's 79.37%.

Conclusion: Parameter-efficient finetuning, especially LoRA, is highly effective for log anomaly detection, offering substantial performance gains.

Abstract: Log anomaly detection using traditional rule based or deep learning based
methods is often challenging due to the large volume and highly complex nature
of log sequence. So effective way of detection of anomalous sequence of logs is
crucial for system maintenance and development. This paper proposes parameter
efficient finetuning specifically low rank adaptation (LoRA) and adapter based
approaches for finding contextual anomalies in sequence of logs in large log
data set. It compares different tiny large language models (LLMs) on the
Thunderbird dataset. The results show that LoRA based finetuning provides
substantial performance improvements of 18 to 19 percentage over LogBert based
full finetuning approach, achieving accuracy scores between 97.76% and 98.83%
compared to 79.37%.

</details>


### [96] [Real-Time Bayesian Detection of Drift-Evasive GNSS Spoofing in Reinforcement Learning Based UAV Deconfliction](https://arxiv.org/abs/2507.11173)
*Deepak Kumar Panda,Weisi Guo*

Main category: cs.LG

TL;DR: The paper proposes a Bayesian online change point detection (BOCPD) method to detect subtle drift-evasive spoofing attacks on UAVs, outperforming traditional techniques.


<details>
  <summary>Details</summary>
Motivation: UAVs rely on GNSS for navigation but are vulnerable to spoofing attacks that evade conventional detection. Rapid detection is crucial for resilience.

Method: Uses BOCPD to monitor temporal shifts in RL critic network value estimates for detecting behavioral deviations.

Result: The method achieves higher accuracy and lower false rates compared to existing spoofing detectors.

Conclusion: The temporal value-based framework enhances UAV resilience against stealthy spoofing attacks.

Abstract: Autonomous unmanned aerial vehicles (UAVs) rely on global navigation
satellite system (GNSS) pseudorange measurements for accurate real-time
localization and navigation. However, this dependence exposes them to
sophisticated spoofing threats, where adversaries manipulate pseudoranges to
deceive UAV receivers. Among these, drift-evasive spoofing attacks subtly
perturb measurements, gradually diverting the UAVs trajectory without
triggering conventional signal-level anti-spoofing mechanisms. Traditional
distributional shift detection techniques often require accumulating a
threshold number of samples, causing delays that impede rapid detection and
timely response. Consequently, robust temporal-scale detection methods are
essential to identify attack onset and enable contingency planning with
alternative sensing modalities, improving resilience against stealthy
adversarial manipulations. This study explores a Bayesian online change point
detection (BOCPD) approach that monitors temporal shifts in value estimates
from a reinforcement learning (RL) critic network to detect subtle behavioural
deviations in UAV navigation. Experimental results show that this temporal
value-based framework outperforms conventional GNSS spoofing detectors,
temporal semi-supervised learning frameworks, and the Page-Hinkley test,
achieving higher detection accuracy and lower false-positive and false-negative
rates for drift-evasive spoofing attacks.

</details>


### [97] [Gradient Regularization-based Neural Granger Causality](https://arxiv.org/abs/2507.11178)
*Meiliang Liu,Huiwen Dong,Xiaoxiao Yang,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.LG

TL;DR: The paper proposes GRNGC, a neural Granger causality model using gradient regularization, reducing computational costs and improving flexibility.


<details>
  <summary>Details</summary>
Motivation: Existing neural Granger causality models are computationally expensive and limited in capturing complex interactions.

Method: GRNGC applies $L_{1}$ regularization to gradients between input and output, requiring only one prediction model and supporting diverse architectures.

Result: GRNGC outperforms baselines in simulations and real-world datasets, reducing computational overhead.

Conclusion: GRNGC is a flexible, efficient solution for inferring Granger causality and reconstructing gene regulatory networks.

Abstract: With the advancement of deep learning technologies, various neural
network-based Granger causality models have been proposed. Although these
models have demonstrated notable improvements, several limitations remain. Most
existing approaches adopt the component-wise architecture, necessitating the
construction of a separate model for each time series, which results in
substantial computational costs. In addition, imposing the sparsity-inducing
penalty on the first-layer weights of the neural network to extract causal
relationships weakens the model's ability to capture complex interactions. To
address these limitations, we propose Gradient Regularization-based Neural
Granger Causality (GRNGC), which requires only one time series prediction model
and applies $L_{1}$ regularization to the gradient between model's input and
output to infer Granger causality. Moreover, GRNGC is not tied to a specific
time series forecasting model and can be implemented with diverse architectures
such as KAN, MLP, and LSTM, offering enhanced flexibility. Numerical
simulations on DREAM, Lorenz-96, fMRI BOLD, and CausalTime show that GRNGC
outperforms existing baselines and significantly reduces computational
overhead. Meanwhile, experiments on real-world DNA, Yeast, HeLa, and bladder
urothelial carcinoma datasets further validate the model's effectiveness in
reconstructing gene regulatory networks.

</details>


### [98] [Mixture of Experts in Large Language Models](https://arxiv.org/abs/2507.11181)
*Danyang Zhang,Junhao Song,Ziqian Bi,Yingfang Yuan,Tianyang Wang,Joe Yeong,Junfeng Hao*

Main category: cs.LG

TL;DR: A review of Mixture-of-Experts (MoE) architecture in large language models, showcasing its performance benefits with low computational cost, and discussing key mechanisms, applications, and challenges.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of MoE in enhancing large language models by analyzing its theoretical and practical aspects, while addressing challenges for future improvements.

Method: Systematic analysis of MoE's theoretical foundations, architectural designs, gating/routing mechanisms, hierarchical/sparse configurations, meta-learning, and real-world applications.

Result: MoE offers superior model capacity, efficient scaling, and improved task-specific performance, but requires expert diversity and accurate calibration for optimal effectiveness.

Conclusion: The review highlights MoE's advantages and challenges, providing a roadmap for future research and innovation in its architecture and applications.

Abstract: This paper presents a comprehensive review of the Mixture-of-Experts (MoE)
architecture in large language models, highlighting its ability to
significantly enhance model performance while maintaining minimal computational
overhead. Through a systematic analysis spanning theoretical foundations, core
architectural designs, and large language model (LLM) applications, we examine
expert gating and routing mechanisms, hierarchical and sparse MoE
configurations, meta-learning approaches, multimodal and multitask learning
scenarios, real-world deployment cases, and recent advances and challenges in
deep learning. Our analysis identifies key advantages of MoE, including
superior model capacity compared to equivalent Bayesian approaches, improved
task-specific performance, and the ability to scale model capacity efficiently.
We also underscore the importance of ensuring expert diversity, accurate
calibration, and reliable inference aggregation, as these are essential for
maximizing the effectiveness of MoE architectures. Finally, this review
outlines current research limitations, open challenges, and promising future
directions, providing a foundation for continued innovation in MoE architecture
and its applications.

</details>


### [99] [Quantized Rank Reduction: A Communications-Efficient Federated Learning Scheme for Network-Critical Applications](https://arxiv.org/abs/2507.11183)
*Dimitrios Kritsiolis,Constantine Kotropoulos*

Main category: cs.LG

TL;DR: A communication-efficient federated learning scheme using low-rank approximation and quantization to reduce network load while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of high communication overhead in federated learning by minimizing the exchange of model updates.

Method: Proposes a scheme combining low-rank approximation of neural network gradients and quantization to reduce data transmission.

Result: Significantly reduces network load with minimal impact on model accuracy.

Conclusion: The proposed method effectively balances communication efficiency and model performance in federated learning.

Abstract: Federated learning is a machine learning approach that enables multiple
devices (i.e., agents) to train a shared model cooperatively without exchanging
raw data. This technique keeps data localized on user devices, ensuring privacy
and security, while each agent trains the model on their own data and only
shares model updates. The communication overhead is a significant challenge due
to the frequent exchange of model updates between the agents and the central
server. In this paper, we propose a communication-efficient federated learning
scheme that utilizes low-rank approximation of neural network gradients and
quantization to significantly reduce the network load of the decentralized
learning process with minimal impact on the model's accuracy.

</details>


### [100] [An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular Disease Detection and Risk Assessment](https://arxiv.org/abs/2507.11185)
*Md. Emon Akter Sourov,Md. Sabbir Hossen,Pabon Shaha,Mohammad Minoar Hossain,Md Sadiq Iqbal*

Main category: cs.LG

TL;DR: Machine learning improves heart disease diagnosis and risk prediction using classification and regression models, with Random Forest and Linear Regression performing best.


<details>
  <summary>Details</summary>
Motivation: Traditional heart disease diagnostic methods are often inaccurate, especially in resource-limited regions, necessitating better solutions.

Method: A framework combining classification and regression models was applied to the Heart Disease dataset (1,035 cases), using SMOTE for class imbalance and generating 100,000 synthetic data points.

Result: Random Forest achieved 97.2% accuracy (real data) and 97.6% (synthetic data); Linear Regression had R2 values of 0.992 (real) and 0.984 (synthetic).

Conclusion: Machine learning can revolutionize heart disease diagnosis and risk prediction, aiding early intervention and clinical decisions.

Abstract: Heart disease remains a major global health concern, particularly in regions
with limited access to medical resources and diagnostic facilities. Traditional
diagnostic methods often fail to accurately identify and manage heart disease
risks, leading to adverse outcomes. Machine learning has the potential to
significantly enhance the accuracy, efficiency, and speed of heart disease
diagnosis. In this study, we proposed a comprehensive framework that combines
classification models for heart disease detection and regression models for
risk prediction. We employed the Heart Disease dataset, which comprises 1,035
cases. To address the issue of class imbalance, the Synthetic Minority
Oversampling Technique (SMOTE) was applied, resulting in the generation of an
additional 100,000 synthetic data points. Performance metrics, including
accuracy, precision, recall, F1-score, R2, MSE, RMSE, and MAE, were used to
evaluate the model's effectiveness. Among the classification models, Random
Forest emerged as the standout performer, achieving an accuracy of 97.2% on
real data and 97.6% on synthetic data. For regression tasks, Linear Regression
demonstrated the highest R2 values of 0.992 and 0.984 on real and synthetic
datasets, respectively, with the lowest error metrics. Additionally,
Explainable AI techniques were employed to enhance the interpretability of the
models. This study highlights the potential of machine learning to
revolutionize heart disease diagnosis and risk prediction, thereby facilitating
early intervention and enhancing clinical decision-making.

</details>


### [101] [Striking the Perfect Balance: Preserving Privacy While Boosting Utility in Collaborative Medical Prediction Platforms](https://arxiv.org/abs/2507.11187)
*Shao-Bo Lin,Xiaotong Liu,Yao Wang*

Main category: cs.LG

TL;DR: The paper proposes a privacy-preserving one-shot distributed learning framework for collaborative medical prediction, addressing privacy attacks and ensuring optimal prediction performance.


<details>
  <summary>Details</summary>
Motivation: Concerns about privacy (attribute and model extraction attacks) and low prediction quality hinder patient and doctor participation in online collaborative medical prediction platforms.

Method: A privacy-preserving mechanism integrated into a one-shot distributed learning framework, validated through simulations and real-world data.

Result: The framework achieves optimal prediction performance under specific privacy requirements, as demonstrated theoretically and experimentally.

Conclusion: The proposed solution effectively balances privacy and performance in collaborative medical prediction.

Abstract: Online collaborative medical prediction platforms offer convenience and
real-time feedback by leveraging massive electronic health records. However,
growing concerns about privacy and low prediction quality can deter patient
participation and doctor cooperation. In this paper, we first clarify the
privacy attacks, namely attribute attacks targeting patients and model
extraction attacks targeting doctors, and specify the corresponding privacy
principles. We then propose a privacy-preserving mechanism and integrate it
into a novel one-shot distributed learning framework, aiming to simultaneously
meet both privacy requirements and prediction performance objectives. Within
the framework of statistical learning theory, we theoretically demonstrate that
the proposed distributed learning framework can achieve the optimal prediction
performance under specific privacy requirements. We further validate the
developed privacy-preserving collaborative medical prediction platform through
both toy simulations and real-world data experiments.

</details>


### [102] [Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with Data on the Sphere?](https://arxiv.org/abs/2507.11228)
*Si Yi Meng,Baptiste Goujaud,Antonio Orvieto,Christopher De Sa*

Main category: cs.LG

TL;DR: The paper explores if equal-magnitude data ensures global convergence in logistic regression under any step size below the stability threshold. It confirms this for 1D but finds cycling in higher dimensions.


<details>
  <summary>Details</summary>
Motivation: To understand conditions for global convergence in logistic regression, especially with large step sizes, and to inspire further research on cycling behavior.

Method: Analyzes gradient descent (GD) on logistic regression, focusing on datasets with equal-magnitude data and step sizes below the stability threshold.

Result: Proves global convergence in 1D but identifies cycling behavior in higher dimensions.

Conclusion: Equal-magnitude data ensures global convergence in 1D, but cycling persists in higher dimensions, calling for further research on realistic datasets and sufficient conditions.

Abstract: Gradient descent (GD) on logistic regression has many fascinating properties.
When the dataset is linearly separable, it is known that the iterates converge
in direction to the maximum-margin separator regardless of how large the step
size is. In the non-separable case, however, it has been shown that GD can
exhibit a cycling behaviour even when the step sizes is still below the
stability threshold $2/\lambda$, where $\lambda$ is the largest eigenvalue of
the Hessian at the solution. This short paper explores whether restricting the
data to have equal magnitude is a sufficient condition for global convergence,
under any step size below the stability threshold. We prove that this is true
in a one dimensional space, but in higher dimensions cycling behaviour can
still occur. We hope to inspire further studies on quantifying how common these
cycles are in realistic datasets, as well as finding sufficient conditions to
guarantee global convergence with large step sizes.

</details>


### [103] [Generative Click-through Rate Prediction with Applications to Search Advertising](https://arxiv.org/abs/2507.11246)
*Lingwei Kong,Lu Wang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.LG

TL;DR: A novel model combines generative and discriminative approaches for CTR prediction, using a two-stage training process to enhance precision. It shows strong results in experiments and is deployed on a major e-commerce platform.


<details>
  <summary>Details</summary>
Motivation: To leverage generative models' expressive power for improving CTR prediction accuracy beyond traditional discriminative methods.

Method: Two-stage training: 1) Generative pre-training for next-item prediction, 2) Fine-tuning within a discriminative CTR framework.

Result: Effective performance in experiments and successful deployment on a large e-commerce platform.

Conclusion: The hybrid model enhances CTR prediction and has practical industrial applications.

Abstract: Click-Through Rate (CTR) prediction models are integral to a myriad of
industrial settings, such as personalized search advertising. Current methods
typically involve feature extraction from users' historical behavior sequences
combined with product information, feeding into a discriminative model that is
trained on user feedback to estimate CTR. With the success of models such as
GPT, the potential for generative models to enrich expressive power beyond
discriminative models has become apparent. In light of this, we introduce a
novel model that leverages generative models to enhance the precision of CTR
predictions in discriminative models. To reconcile the disparate data
aggregation needs of both model types, we design a two-stage training process:
1) Generative pre-training for next-item prediction with the given item
category in user behavior sequences; 2) Fine-tuning the well-trained generative
model within a discriminative CTR prediction framework. Our method's efficacy
is substantiated through extensive experiments on a new dataset, and its
significant utility is further corroborated by online A/B testing results.
Currently, the model is deployed on one of the world's largest e-commerce
platforms, and we intend to release the associated code and dataset in the
future.

</details>


### [104] [LyAm: Robust Non-Convex Optimization for Stable Learning in Noisy Environments](https://arxiv.org/abs/2507.11262)
*Elmira Mirzabeigi,Sepehr Rezaee,Kourosh Parand*

Main category: cs.LG

TL;DR: LyAm is a novel optimizer combining Adam with Lyapunov stability to improve deep learning convergence and robustness.


<details>
  <summary>Details</summary>
Motivation: Noisy gradients and unstable convergence in deep neural networks hinder performance and generalization.

Method: LyAm integrates Adam's adaptive moment estimation with Lyapunov-based stability mechanisms to dynamically adjust learning rates.

Result: LyAm outperforms state-of-the-art optimizers in accuracy, convergence speed, and stability on datasets like CIFAR-10 and CIFAR-100.

Conclusion: LyAm is a robust optimizer for deep learning, offering theoretical guarantees and practical performance improvements.

Abstract: Training deep neural networks, particularly in computer vision tasks, often
suffers from noisy gradients and unstable convergence, which hinder performance
and generalization. In this paper, we propose LyAm, a novel optimizer that
integrates Adam's adaptive moment estimation with Lyapunov-based stability
mechanisms. LyAm dynamically adjusts the learning rate using Lyapunov stability
theory to enhance convergence robustness and mitigate training noise. We
provide a rigorous theoretical framework proving the convergence guarantees of
LyAm in complex, non-convex settings. Extensive experiments on like as CIFAR-10
and CIFAR-100 show that LyAm consistently outperforms state-of-the-art
optimizers in terms of accuracy, convergence speed, and stability, establishing
it as a strong candidate for robust deep learning optimization.

</details>


### [105] [Turning Sand to Gold: Recycling Data to Bridge On-Policy and Off-Policy Learning via Causal Bound](https://arxiv.org/abs/2507.11269)
*Tal Fiskus,Uri Shaham*

Main category: cs.LG

TL;DR: The paper introduces a novel method using the Neyman-Rubin potential outcomes framework in DRL to improve sample efficiency by reducing the experience replay buffer size and computational demands, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: DRL agents require extensive training steps and large experience replay buffers, leading to high computational costs. The paper aims to address these inefficiencies.

Method: Leverages the Neyman-Rubin framework to establish a causal bound on factual loss, storing past value network outputs in the replay buffer to utilize otherwise discarded data.

Result: Achieves up to 2,427% higher reward ratio and reduces buffer size by 96%, improving sample efficiency with minimal cost.

Conclusion: The proposed method significantly enhances DRL performance and efficiency, making it more practical for resource-intensive tasks.

Abstract: Deep reinforcement learning (DRL) agents excel in solving complex
decision-making tasks across various domains. However, they often require a
substantial number of training steps and a vast experience replay buffer,
leading to significant computational and resource demands. To address these
challenges, we introduce a novel theoretical result that leverages the
Neyman-Rubin potential outcomes framework into DRL. Unlike most methods that
focus on bounding the counterfactual loss, we establish a causal bound on the
factual loss, which is analogous to the on-policy loss in DRL. This bound is
computed by storing past value network outputs in the experience replay buffer,
effectively utilizing data that is usually discarded. Extensive experiments
across the Atari 2600 and MuJoCo domains on various agents, such as DQN and
SAC, achieve up to 2,427% higher reward ratio, outperforming the same agents
without our proposed term, and reducing the experience replay buffer size by up
to 96%, significantly improving sample efficiency at negligible cost.

</details>


### [106] [Fast Last-Iterate Convergence of SGD in the Smooth Interpolation Regime](https://arxiv.org/abs/2507.11274)
*Amit Attia,Matan Schliserman,Uri Sherman,Tomer Koren*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study population convergence guarantees of stochastic gradient descent
(SGD) for smooth convex objectives in the interpolation regime, where the noise
at optimum is zero or near zero. The behavior of the last iterate of SGD in
this setting -- particularly with large (constant) stepsizes -- has received
growing attention in recent years due to implications for the training of
over-parameterized models, as well as to analyzing forgetting in continual
learning and to understanding the convergence of the randomized Kaczmarz method
for solving linear systems. We establish that after $T$ steps of SGD on
$\beta$-smooth convex loss functions with stepsize $\eta \leq 1/\beta$, the
last iterate exhibits expected excess risk $\widetilde{O}(1/(\eta
T^{1-\beta\eta/2}) + \eta T^{\beta\eta/2} \sigma_\star^2)$, where
$\sigma_\star^2$ denotes the variance of the stochastic gradients at the
optimum. In particular, for a well-tuned stepsize we obtain a near optimal
$\widetilde{O}(1/T + \sigma_\star/\sqrt{T})$ rate for the last iterate,
extending the results of Varre et al. (2021) beyond least squares regression;
and when $\sigma_\star=0$ we obtain a rate of $O(1/\sqrt{T})$ with
$\eta=1/\beta$, improving upon the best-known $O(T^{-1/4})$ rate recently
established by Evron et al. (2025) in the special case of realizable linear
regression.

</details>


### [107] [Guiding LLM Decision-Making with Fairness Reward Models](https://arxiv.org/abs/2507.11344)
*Zara Hall,Melanie Subbiah,Thomas P Zollo,Kathleen McKeown,Richard Zemel*

Main category: cs.LG

TL;DR: A framework for training a Fairness Reward Model (FRM) is proposed to mitigate bias in LLM reasoning for high-stakes decisions, improving fairness without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the amplification of unfair bias in LLM reasoning for high-stakes decisions like bail or loans, ensuring trustworthy use.

Method: Train a Fairness Reward Model (FRM) using weakly supervised, LLM-annotated examples to score and prioritize equitable reasoning chains.

Result: The FRM generalizes across tasks, domains, and model families, improving fairness while maintaining or surpassing baseline accuracy.

Conclusion: The FRM framework enables fairer and trustworthy LLM reasoning in high-stakes decision-making.

Abstract: Large language models are increasingly used to support high-stakes decisions,
potentially influencing who is granted bail or receives a loan. Naive
chain-of-thought sampling can improve average decision accuracy, but has also
been shown to amplify unfair bias. To address this challenge and enable the
trustworthy use of reasoning models in high-stakes decision-making, we propose
a framework for training a generalizable Fairness Reward Model (FRM). Our model
assigns a fairness score to LLM reasoning, enabling the system to down-weight
biased trajectories and favor equitable ones when aggregating decisions across
reasoning chains. We show that a single Fairness Reward Model, trained on
weakly supervised, LLM-annotated examples of biased versus unbiased reasoning,
transfers across tasks, domains, and model families without additional
fine-tuning. Applied to real-world decision-making tasks including recidivism
prediction and social media moderation, we show that our approach consistently
improves fairness while matching, or even surpassing, baseline accuracy.

</details>


### [108] [Neurosymbolic Reasoning Shortcuts under the Independence Assumption](https://arxiv.org/abs/2507.11357)
*Emile van Krieken,Pasquale Minervini,Edoardo Ponti,Antonio Vergari*

Main category: cs.LG

TL;DR: The paper challenges the independence assumption in neurosymbolic predictors, showing it limits uncertainty modeling and causes reasoning shortcuts.


<details>
  <summary>Details</summary>
Motivation: To address skepticism about the impact of the independence assumption in neurosymbolic systems and demonstrate its limitations.

Method: Formal analysis of the independence assumption's effects on uncertainty representation and reasoning shortcuts.

Result: Independence assumption prevents models from representing uncertainty over certain concept combinations, leading to reasoning shortcuts.

Conclusion: The independence assumption in neurosymbolic predictors hinders proper uncertainty modeling and awareness of reasoning shortcuts.

Abstract: The ubiquitous independence assumption among symbolic concepts in
neurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors
use it to speed up probabilistic reasoning. Recent works like van Krieken et
al. (2024) and Marconato et al. (2024) argued that the independence assumption
can hinder learning of NeSy predictors and, more crucially, prevent them from
correctly modelling uncertainty. There is, however, scepticism in the NeSy
community around the scenarios in which the independence assumption actually
limits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle
this question by formally showing that assuming independence among symbolic
concepts entails that a model can never represent uncertainty over certain
concept combinations. Thus, the model fails to be aware of reasoning shortcuts,
i.e., the pathological behaviour of NeSy predictors that predict correct
downstream tasks but for the wrong reasons.

</details>


### [109] [Local Pairwise Distance Matching for Backpropagation-Free Reinforcement Learning](https://arxiv.org/abs/2507.11367)
*Daniel Tanneberg*

Main category: cs.LG

TL;DR: A novel RL method trains neural networks layer-wise using local signals during forward pass, eliminating backpropagation and activations storage, achieving competitive performance and improved stability.


<details>
  <summary>Details</summary>
Motivation: Backpropagation in RL requires storing activations and suffers from vanishing/exploding gradients, degrading learning performance and stability.

Method: Proposes local, layer-wise losses using multi-dimensional scaling distances, with optional reward-driven guidance, trained during forward pass.

Result: Competitive performance to BP-based methods, enhanced stability, consistency, and better performance in challenging environments.

Conclusion: The method offers a viable alternative to BP in RL, improving stability and performance without backward passes.

Abstract: Training neural networks with reinforcement learning (RL) typically relies on
backpropagation (BP), necessitating storage of activations from the forward
pass for subsequent backward updates. Furthermore, backpropagating error
signals through multiple layers often leads to vanishing or exploding
gradients, which can degrade learning performance and stability. We propose a
novel approach that trains each layer of the neural network using local signals
during the forward pass in RL settings. Our approach introduces local,
layer-wise losses leveraging the principle of matching pairwise distances from
multi-dimensional scaling, enhanced with optional reward-driven guidance. This
method allows each hidden layer to be trained using local signals computed
during forward propagation, thus eliminating the need for backward passes and
storing intermediate activations. Our experiments, conducted with policy
gradient methods across common RL benchmarks, demonstrate that this
backpropagation-free method achieves competitive performance compared to their
classical BP-based counterpart. Additionally, the proposed method enhances
stability and consistency within and across runs, and improves performance
especially in challenging environments.

</details>


### [110] [Step-wise Policy for Rare-tool Knowledge (SPaRK): Offline RL that Drives Diverse Tool Use in LLMs](https://arxiv.org/abs/2507.11371)
*Gabriel Bo,Koa Chang,Justin Gu*

Main category: cs.LG

TL;DR: SPaRK is a reinforcement learning framework for LLMs that promotes diverse tool usage via a dual-objective reward system, achieving competitive performance and higher tool diversity.


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning in LLMs by encouraging systematic exploration of diverse tools beyond conventional methods.

Method: Uses step-wise RL with a dual-objective reward (answer quality and tool diversity), offline PPO on synthetic MMLU-Pro data, and a rarity-first exploitation strategy with GPT-4o scoring.

Result: Competitive performance on 14 MMLU-Pro categories with higher tool selection entropy than baselines.

Conclusion: Explicit tool diversity can improve reasoning without accuracy loss.

Abstract: We present Step-wise Policy for Rare-tool Knowledge (SPaRK), a novel
reinforcement learning framework that teaches large language models to explore
diverse tool usage patterns beyond conventional high-temperature sampling.
Building on recent advances in step-wise reinforcement learning, we introduce a
dual-objective reward system that simultaneously optimizes for answer quality
and tool diversity, training a Llama-3.1 8B model through offline PPO on
synthetically generated trajectories from the MMLU-Pro dataset. Our approach
uniquely employs a rarity-first exploitation strategy where a GPT-4o judge
scores candidate actions across eight distinct tools plus chain-of-thought
reasoning, with the policy favoring less-frequently used but still viable tools
to encourage systematic exploration. Empirical results demonstrate that SPaRK
achieves competitive performance across 14 MMLU-Pro categories while exhibiting
significantly higher entropy in tool selection compared to both baseline and
supervised fine-tuning approaches, suggesting that algorithmic exploration
through explicit tool diversity can enhance reasoning capabilities without
sacrificing accuracy.

</details>


### [111] [A Neural Network Model of Complementary Learning Systems: Pattern Separation and Completion for Continual Learning](https://arxiv.org/abs/2507.11393)
*James P Jun,Vijay Marupudi,Raj Sanjay Shah,Sashank Varma*

Main category: cs.LG

TL;DR: The paper introduces a neurally plausible continual learning model combining VAEs and MHNs to mitigate catastrophic forgetting, achieving ~90% accuracy on Split-MNIST.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting in neural networks by mimicking human memory systems (CLS theory) for pattern separation and completion.

Method: Combine variational autoencoders (VAEs) for pattern completion and Modern Hopfield networks (MHNs) for pattern separation into a continual learning model.

Result: Achieves ~90% accuracy on Split-MNIST, reducing forgetting; VAEs handle pattern completion, MHNs drive pattern separation.

Conclusion: The model provides a scalable template for memory consolidation and continual learning in biological and artificial systems.

Abstract: Learning new information without forgetting prior knowledge is central to
human intelligence. In contrast, neural network models suffer from catastrophic
forgetting: a significant degradation in performance on previously learned
tasks when acquiring new information. The Complementary Learning Systems (CLS)
theory offers an explanation for this human ability, proposing that the brain
has distinct systems for pattern separation (encoding distinct memories) and
pattern completion (retrieving complete memories from partial cues). To capture
these complementary functions, we leverage the representational generalization
capabilities of variational autoencoders (VAEs) and the robust memory storage
properties of Modern Hopfield networks (MHNs), combining them into a neurally
plausible continual learning model. We evaluate this model on the Split-MNIST
task, a popular continual learning benchmark, and achieve close to
state-of-the-art accuracy (~90%), substantially reducing forgetting.
Representational analyses empirically confirm the functional dissociation: the
VAE underwrites pattern completion, while the MHN drives pattern separation. By
capturing pattern separation and completion in scalable architectures, our work
provides a functional template for modeling memory consolidation,
generalization, and continual learning in both biological and artificial
systems.

</details>


### [112] [Robust-Multi-Task Gradient Boosting](https://arxiv.org/abs/2507.11411)
*Seyedsaman Emami,Gonzalo Martínez-Muñoz,Daniel Hernández-Lobato*

Main category: cs.LG

TL;DR: R-MTGB is a robust multi-task gradient boosting framework that handles outlier tasks while promoting knowledge transfer among related tasks, improving overall performance.


<details>
  <summary>Details</summary>
Motivation: Real-world multi-task learning often includes outlier tasks that degrade performance, necessitating a method to handle task heterogeneity.

Method: R-MTGB uses three blocks: learning shared patterns, partitioning tasks into outliers/non-outliers, and fine-tuning task-specific predictors.

Result: R-MTGB successfully isolates outliers, transfers knowledge, and reduces prediction errors, achieving performance gains across tasks.

Conclusion: R-MTGB is robust, adaptable, and reliable in challenging multi-task learning environments.

Abstract: Multi-task learning (MTL) has shown effectiveness in exploiting shared
information across tasks to improve generalization. MTL assumes tasks share
similarities that can improve performance. In addition, boosting algorithms
have demonstrated exceptional performance across diverse learning problems,
primarily due to their ability to focus on hard-to-learn instances and
iteratively reduce residual errors. This makes them a promising approach for
learning multi-task problems. However, real-world MTL scenarios often involve
tasks that are not well-aligned (known as outlier or adversarial tasks), which
do not share beneficial similarities with others and can, in fact, deteriorate
the performance of the overall model. To overcome this challenge, we propose
Robust-Multi-Task Gradient Boosting (R-MTGB), a novel boosting framework that
explicitly models and adapts to task heterogeneity during training. R-MTGB
structures the learning process into three sequential blocks: (1) learning
shared patterns, (2) partitioning tasks into outliers and non-outliers with
regularized parameters, and (3) fine-tuning task-specific predictors. This
architecture enables R-MTGB to automatically detect and penalize outlier tasks
while promoting effective knowledge transfer among related tasks. Our method
integrates these mechanisms seamlessly within gradient boosting, allowing
robust handling of noisy or adversarial tasks without sacrificing accuracy.
Extensive experiments on both synthetic benchmarks and real-world datasets
demonstrate that our approach successfully isolates outliers, transfers
knowledge, and consistently reduces prediction errors for each task
individually, and achieves overall performance gains across all tasks. These
results highlight robustness, adaptability, and reliable convergence of R-MTGB
in challenging MTL environments.

</details>


### [113] [Toward Improving fNIRS Classification: A Study on Activation Functions in Deep Neural Architectures](https://arxiv.org/abs/2507.11436)
*Behtom Adeli,John McLinden,Pankaj Pandey,Ming Shao,Yalda Shahriari*

Main category: cs.LG

TL;DR: The study evaluates activation functions for fNIRS classification, finding symmetrical functions like Tanh and Abs(x) outperform ReLU in certain architectures.


<details>
  <summary>Details</summary>
Motivation: The impact of activation functions on deep learning performance in fNIRS is underexplored, despite challenges like nonlinearity and low SNR.

Method: Tested conventional and field-specific activation functions on multiple DL architectures (fNIRSNet, AbsoluteNet, MDNN, shallowConvNet) using standardized preprocessing and training.

Result: Symmetrical activation functions (Tanh, Abs(x)) outperformed ReLU in some architectures; Modified Absolute Function (MAF) further supported symmetry's role.

Conclusion: Proper activation function selection, aligned with fNIRS signal characteristics, is crucial for performance gains.

Abstract: Activation functions are critical to the performance of deep neural networks,
particularly in domains such as functional near-infrared spectroscopy (fNIRS),
where nonlinearity, low signal-to-noise ratio (SNR), and signal variability
poses significant challenges to model accuracy. However, the impact of
activation functions on deep learning (DL) performance in the fNIRS domain
remains underexplored and lacks systematic investigation in the current
literature. This study evaluates a range of conventional and field-specific
activation functions for fNIRS classification tasks using multiple deep
learning architectures, including the domain-specific fNIRSNet, AbsoluteNet,
MDNN, and shallowConvNet (as the baseline), all tested on a single dataset
recorded during an auditory task. To ensure fair a comparison, all networks
were trained and tested using standardized preprocessing and consistent
training parameters. The results show that symmetrical activation functions
such as Tanh and the Absolute value function Abs(x) can outperform commonly
used functions like the Rectified Linear Unit (ReLU), depending on the
architecture. Additionally, a focused analysis of the role of symmetry was
conducted using a Modified Absolute Function (MAF), with results further
supporting the effectiveness of symmetrical activation functions on performance
gains. These findings underscore the importance of selecting proper activation
functions that align with the signal characteristics of fNIRS data.

</details>


### [114] [Data Augmentation in Time Series Forecasting through Inverted Framework](https://arxiv.org/abs/2507.11439)
*Hongming Tan,Ting Chen,Ruochong Jin,Wai Kin Chan*

Main category: cs.LG

TL;DR: The paper introduces DAIF, a novel data augmentation method for the iTransformer model in MTS forecasting, addressing its limitations in capturing temporal interdependency and noise from nonsignificant variable correlations.


<details>
  <summary>Details</summary>
Motivation: The iTransformer model, while effective for MTS forecasting, has limitations in capturing temporal interdependency and introduces noise in cases of nonsignificant variable correlations.

Method: The authors propose DAIF, featuring two strategies: Frequency Filtering and Cross-variation Patching, designed for the inverted framework of iTransformer.

Result: Experiments show DAIF effectively improves the performance of iTransformer across multiple datasets.

Conclusion: DAIF successfully addresses the limitations of the iTransformer framework, enhancing its effectiveness in MTS forecasting.

Abstract: Currently, iTransformer is one of the most popular and effective models for
multivariate time series (MTS) forecasting. Thanks to its inverted framework,
iTransformer effectively captures multivariate correlation. However, the
inverted framework still has some limitations. It diminishes temporal
interdependency information, and introduces noise in cases of nonsignificant
variable correlation. To address these limitations, we introduce a novel data
augmentation method on inverted framework, called DAIF. Unlike previous data
augmentation methods, DAIF stands out as the first real-time augmentation
specifically designed for the inverted framework in MTS forecasting. We first
define the structure of the inverted sequence-to-sequence framework, then
propose two different DAIF strategies, Frequency Filtering and Cross-variation
Patching to address the existing challenges of the inverted framework.
Experiments across multiple datasets and inverted models have demonstrated the
effectiveness of our DAIF.

</details>


### [115] [LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis Assessment in Rectal Cancer](https://arxiv.org/abs/2507.11457)
*Yaoxian Dong,Yifan Gao,Haoyue Li,Yanfen Cui,Xin Gao*

Main category: cs.LG

TL;DR: LRMR, an LLM-driven framework, improves preoperative LN metastasis assessment in rectal cancer by combining visual analysis and relational ranking, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Conventional MRI and AI models lack interpretability and patient-level context, limiting their clinical trust and diagnostic performance.

Method: LRMR uses a two-stage LLM approach: multimodal LLM generates structured reports of LN features, and text-based LLM ranks patients by risk through pairwise comparisons.

Result: LRMR achieved AUC 0.7917 and F1-score 0.7200, outperforming baselines like ResNet50 (AUC 0.7708). Ablation studies confirmed the importance of both stages.

Conclusion: Decoupling visual perception and cognitive reasoning in a two-stage LLM framework provides an interpretable and effective paradigm for LN metastasis assessment.

Abstract: Accurate preoperative assessment of lymph node (LN) metastasis in rectal
cancer guides treatment decisions, yet conventional MRI evaluation based on
morphological criteria shows limited diagnostic performance. While some
artificial intelligence models have been developed, they often operate as black
boxes, lacking the interpretability needed for clinical trust. Moreover, these
models typically evaluate nodes in isolation, overlooking the patient-level
context. To address these limitations, we introduce LRMR, an LLM-Driven
Relational Multi-node Ranking framework. This approach reframes the diagnostic
task from a direct classification problem into a structured reasoning and
ranking process. The LRMR framework operates in two stages. First, a multimodal
large language model (LLM) analyzes a composite montage image of all LNs from a
patient, generating a structured report that details ten distinct radiological
features. Second, a text-based LLM performs pairwise comparisons of these
reports between different patients, establishing a relative risk ranking based
on the severity and number of adverse features. We evaluated our method on a
retrospective cohort of 117 rectal cancer patients. LRMR achieved an area under
the curve (AUC) of 0.7917 and an F1-score of 0.7200, outperforming a range of
deep learning baselines, including ResNet50 (AUC 0.7708). Ablation studies
confirmed the value of our two main contributions: removing the relational
ranking stage or the structured prompting stage led to a significant
performance drop, with AUCs falling to 0.6875 and 0.6458, respectively. Our
work demonstrates that decoupling visual perception from cognitive reasoning
through a two-stage LLM framework offers a powerful, interpretable, and
effective new paradigm for assessing lymph node metastasis in rectal cancer.

</details>


### [116] [D3FL: Data Distribution and Detrending for Robust Federated Learning in Non-linear Time-series Data](https://arxiv.org/abs/2507.11471)
*Harsha Varun Marisetty,Manik Gupta,Yogesh Simmhan*

Main category: cs.LG

TL;DR: The paper explores federated learning (FL) for non-linear, non-stationary IoT time-series data, comparing it to centralized methods and evaluating detrending techniques.


<details>
  <summary>Details</summary>
Motivation: Centralized IoT data analysis causes delays and high costs; FL offers a decentralized alternative, but data variations impact accuracy.

Method: Synthetic and real-world datasets with non-linear distributions were used to train LSTM models in FL and centralized setups, testing detrending techniques.

Result: FL underperforms centralized methods for non-linear data, but detrending improves FL accuracy.

Conclusion: Detrending enhances FL performance for non-linear time-series data, though FL still lags behind centralized approaches.

Abstract: With advancements in computing and communication technologies, the Internet
of Things (IoT) has seen significant growth. IoT devices typically collect data
from various sensors, such as temperature, humidity, and energy meters. Much of
this data is temporal in nature. Traditionally, data from IoT devices is
centralized for analysis, but this approach introduces delays and increased
communication costs. Federated learning (FL) has emerged as an effective
alternative, allowing for model training across distributed devices without the
need to centralize data. In many applications, such as smart home energy and
environmental monitoring, the data collected by IoT devices across different
locations can exhibit significant variation in trends and seasonal patterns.
Accurately forecasting such non-stationary, non-linear time-series data is
crucial for applications like energy consumption estimation and weather
forecasting. However, these data variations can severely impact prediction
accuracy. The key contributions of this paper are: (1) Investigating how
non-linear, non-stationary time-series data distributions, like generalized
extreme value (gen-extreme) and log norm distributions, affect FL performance.
(2) Analyzing how different detrending techniques for non-linear time-series
data influence the forecasting model's performance in a FL setup. We generated
several synthetic time-series datasets using non-linear data distributions and
trained an LSTM-based forecasting model using both centralized and FL
approaches. Additionally, we evaluated the impact of detrending on real-world
datasets with non-linear time-series data distributions. Our experimental
results show that: (1) FL performs worse than centralized approaches when
dealing with non-linear data distributions. (2) The use of appropriate
detrending techniques improves FL performance, reducing loss across different
data distributions.

</details>


### [117] [Exploring the robustness of TractOracle methods in RL-based tractography](https://arxiv.org/abs/2507.11486)
*Jeremi Levesque,Antoine Théberge,Maxime Descoteaux,Pierre-Marc Jodoin*

Main category: cs.LG

TL;DR: The paper explores extensions of the TractOracle-RL framework for tractography, introducing Iterative Reward Training (IRT) to improve accuracy and anatomical validity. Results show RL methods with oracle feedback outperform traditional techniques.


<details>
  <summary>Details</summary>
Motivation: To enhance tractography accuracy by integrating RL advancements and anatomical priors, reducing false positives and improving reliability.

Method: Extends TractOracle-RL with four RL-based methods and introduces IRT, a training scheme using bundle filtering for iterative refinement. Evaluated on five diffusion MRI datasets.

Result: RL methods with oracle feedback consistently outperform traditional tractography, showing higher accuracy and anatomical validity.

Conclusion: Combining RL frameworks with oracle guidance, especially via IRT, leads to robust and reliable tractography across diverse datasets.

Abstract: Tractography algorithms leverage diffusion MRI to reconstruct the fibrous
architecture of the brain's white matter. Among machine learning approaches,
reinforcement learning (RL) has emerged as a promising framework for
tractography, outperforming traditional methods in several key aspects.
TractOracle-RL, a recent RL-based approach, reduces false positives by
incorporating anatomical priors into the training process via a reward-based
mechanism. In this paper, we investigate four extensions of the original
TractOracle-RL framework by integrating recent advances in RL, and we evaluate
their performance across five diverse diffusion MRI datasets. Results
demonstrate that combining an oracle with the RL framework consistently leads
to robust and reliable tractography, regardless of the specific method or
dataset used. We also introduce a novel RL training scheme called Iterative
Reward Training (IRT), inspired by the Reinforcement Learning from Human
Feedback (RLHF) paradigm. Instead of relying on human input, IRT leverages
bundle filtering methods to iteratively refine the oracle's guidance throughout
training. Experimental results show that RL methods trained with oracle
feedback significantly outperform widely used tractography techniques in terms
of accuracy and anatomical validity.

</details>


### [118] [A parametric activation function based on Wendland RBF](https://arxiv.org/abs/2507.11493)
*Majid Darehmiraki*

Main category: cs.LG

TL;DR: A novel parametric activation function using Wendland RBFs is introduced for deep learning, combining smoothness and compact support to outperform traditional functions like ReLU in certain tasks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of traditional activation functions (ReLU, sigmoid, tanh) by leveraging Wendland RBFs' properties (compact support, smoothness) for better performance and stability.

Method: Proposes an enhanced Wendland activation with linear and exponential terms, analyzed theoretically and tested on synthetic tasks (sine wave) and benchmarks (MNIST, Fashion-MNIST).

Result: Superior accuracy in regression tasks, improved gradient propagation, and stability, with competitive performance on benchmarks.

Conclusion: Wendland activations bridge RBF theory and deep learning, offering better generalization and potential for hybrid architectures.

Abstract: This paper introduces a novel parametric activation function based on
Wendland radial basis functions (RBFs) for deep neural networks. Wendland RBFs,
known for their compact support, smoothness, and positive definiteness in
approximation theory, are adapted to address limitations of traditional
activation functions like ReLU, sigmoid, and tanh. The proposed enhanced
Wendland activation combines a standard Wendland component with linear and
exponential terms, offering tunable locality, improved gradient propagation,
and enhanced stability during training. Theoretical analysis highlights its
mathematical properties, including smoothness and adaptability, while empirical
experiments on synthetic tasks (e.g., sine wave approximation) and benchmark
datasets (MNIST, Fashion-MNIST) demonstrate competitive performance. Results
show that the Wendland-based activation achieves superior accuracy in certain
scenarios, particularly in regression tasks, while maintaining computational
efficiency. The study bridges classical RBF theory with modern deep learning,
suggesting that Wendland activations can mitigate overfitting and improve
generalization through localized, smooth transformations. Future directions
include hybrid architectures and domain-specific adaptations.

</details>


### [119] [Langevin Flows for Modeling Neural Latent Dynamics](https://arxiv.org/abs/2507.11531)
*Yue Song,T. Anderson Keller,Yisong Yue,Pietro Perona,Max Welling*

Main category: cs.LG

TL;DR: LangevinFlow, a physics-inspired VAE, models neural dynamics using Langevin equation, outperforming benchmarks in accuracy and prediction.


<details>
  <summary>Details</summary>
Motivation: To capture intrinsic and external influences in neural dynamics, leveraging physical priors like inertia and stochastic forces.

Method: Uses a sequential VAE with Langevin dynamics, a recurrent encoder, Transformer decoder, and oscillator-based potential function.

Result: Outperforms benchmarks on synthetic and real datasets (NLB), achieving high accuracy in firing rates and behavioral decoding.

Conclusion: LangevinFlow provides a flexible, high-performing framework for modeling neural dynamics and unobserved influences.

Abstract: Neural populations exhibit latent dynamical structures that drive
time-evolving spiking activities, motivating the search for models that capture
both intrinsic network dynamics and external unobserved influences. In this
work, we introduce LangevinFlow, a sequential Variational Auto-Encoder where
the time evolution of latent variables is governed by the underdamped Langevin
equation. Our approach incorporates physical priors -- such as inertia,
damping, a learned potential function, and stochastic forces -- to represent
both autonomous and non-autonomous processes in neural systems. Crucially, the
potential function is parameterized as a network of locally coupled
oscillators, biasing the model toward oscillatory and flow-like behaviors
observed in biological neural populations. Our model features a recurrent
encoder, a one-layer Transformer decoder, and Langevin dynamics in the latent
space. Empirically, our method outperforms state-of-the-art baselines on
synthetic neural populations generated by a Lorenz attractor, closely matching
ground-truth firing rates. On the Neural Latents Benchmark (NLB), the model
achieves superior held-out neuron likelihoods (bits per spike) and forward
prediction accuracy across four challenging datasets. It also matches or
surpasses alternative methods in decoding behavioral metrics such as hand
velocity. Overall, this work introduces a flexible, physics-inspired,
high-performing framework for modeling complex neural population dynamics and
their unobserved influences.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [120] [CWNet: Causal Wavelet Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.10689)
*Tongshun Zhang,Pingping Liu,Yubing Lu,Mengen Cai,Zijian Zhang,Zhe Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: CWNet introduces a causal reasoning approach for low-light image enhancement, combining wavelet transforms and causal principles for superior performance.


<details>
  <summary>Details</summary>
Motivation: Traditional LLIE methods lack semantic and feature-specific adjustments. CWNet addresses this by incorporating causal reasoning and wavelet transforms.

Method: CWNet uses causal reasoning (global metric learning and local CLIP semantic loss) and a wavelet-based backbone for frequency optimization.

Result: CWNet outperforms state-of-the-art methods across diverse datasets.

Conclusion: CWNet's causal and wavelet-based approach effectively enhances low-light images, setting a new benchmark for LLIE.

Abstract: Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on
uniform brightness adjustment, often neglecting instance-level semantic
information and the inherent characteristics of different features. To address
these limitations, we propose CWNet (Causal Wavelet Network), a novel
architecture that leverages wavelet transforms for causal reasoning.
Specifically, our approach comprises two key components: 1) Inspired by the
concept of intervention in causality, we adopt a causal reasoning perspective
to reveal the underlying causal relationships in low-light enhancement. From a
global perspective, we employ a metric learning strategy to ensure causal
embeddings adhere to causal principles, separating them from non-causal
confounding factors while focusing on the invariance of causal factors. At the
local level, we introduce an instance-level CLIP semantic loss to precisely
maintain causal factor consistency. 2) Based on our causal analysis, we present
a wavelet transform-based backbone network that effectively optimizes the
recovery of frequency information, ensuring precise enhancement tailored to the
specific attributes of wavelet transforms. Extensive experiments demonstrate
that CWNet significantly outperforms current state-of-the-art methods across
multiple datasets, showcasing its robust performance across diverse scenes.
Code is available at https://github.com/bywlzts/CWNet-Causal-Wavelet-Network.

</details>


### [121] [Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines](https://arxiv.org/abs/2507.10737)
*Jiayuan Chen,Thai-Hoang Pham,Yuanlong Wang,Ping Zhang*

Main category: cs.CV

TL;DR: A novel framework integrates biological knowledge to improve microscopy image profiling for de novo cell lines by disentangling perturbation-specific and cell line-specific features.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of robust perturbation screening for de novo cell lines due to morphological and biological heterogeneity.

Method: Integrates external biological knowledge (protein interaction data, transcriptomic features) into pretraining to disentangle representations.

Result: Improves generalization of imaging models to de novo cell lines, validated on RxRx datasets.

Conclusion: The framework enhances microscopy image profiling, proving effective for phenotype-based drug discovery.

Abstract: High-throughput screening techniques, such as microscopy imaging of cellular
responses to genetic and chemical perturbations, play a crucial role in drug
discovery and biomedical research. However, robust perturbation screening for
\textit{de novo} cell lines remains challenging due to the significant
morphological and biological heterogeneity across cell lines. To address this,
we propose a novel framework that integrates external biological knowledge into
existing pretraining strategies to enhance microscopy image profiling models.
Our approach explicitly disentangles perturbation-specific and cell
line-specific representations using external biological information.
Specifically, we construct a knowledge graph leveraging protein interaction
data from STRING and Hetionet databases to guide models toward
perturbation-specific features during pretraining. Additionally, we incorporate
transcriptomic features from single-cell foundation models to capture cell
line-specific representations. By learning these disentangled features, our
method improves the generalization of imaging models to \textit{de novo} cell
lines. We evaluate our framework on the RxRx database through one-shot
fine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from
the RxRx19a dataset. Experimental results demonstrate that our method enhances
microscopy image profiling for \textit{de novo} cell lines, highlighting its
effectiveness in real-world phenotype-based drug discovery applications.

</details>


### [122] [Auditing Facial Emotion Recognition Datasets for Posed Expressions and Racial Bias](https://arxiv.org/abs/2507.10755)
*Rina Khan,Catherine Stinson*

Main category: cs.CV

TL;DR: The study audits FER datasets, revealing mislabeled posed images and racial bias in model predictions, impacting real-world performance and ethics.


<details>
  <summary>Details</summary>
Motivation: Address performance drops in FER algorithms for spontaneous expressions and racial bias, linked to dataset collection practices.

Method: Audit two FER datasets by sampling images to classify as spontaneous or posed, and test model performance across races and skin tones.

Result: Found mislabeled posed images in 'in-the-wild' datasets and racial bias in FER models, skewing predictions for non-white/dark-skinned individuals.

Conclusion: Dataset inaccuracies and model biases undermine FER reliability and ethics, risking harm in real-world applications.

Abstract: Facial expression recognition (FER) algorithms classify facial expressions
into emotions such as happy, sad, or angry. An evaluative challenge facing FER
algorithms is the fall in performance when detecting spontaneous expressions
compared to posed expressions. An ethical (and evaluative) challenge facing FER
algorithms is that they tend to perform poorly for people of some races and
skin colors. These challenges are linked to the data collection practices
employed in the creation of FER datasets. In this study, we audit two
state-of-the-art FER datasets. We take random samples from each dataset and
examine whether images are spontaneous or posed. In doing so, we propose a
methodology for identifying spontaneous or posed images. We discover a
significant number of images that were posed in the datasets purporting to
consist of in-the-wild images. Since performance of FER models vary between
spontaneous and posed images, the performance of models trained on these
datasets will not represent the true performance if such models were to be
deployed in in-the-wild applications. We also observe the skin color of
individuals in the samples, and test three models trained on each of the
datasets to predict facial expressions of people from various races and skin
tones. We find that the FER models audited were more likely to predict people
labeled as not white or determined to have dark skin as showing a negative
emotion such as anger or sadness even when they were smiling. This bias makes
such models prone to perpetuate harm in real life applications.

</details>


### [123] [FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching](https://arxiv.org/abs/2507.10770)
*Ionuţ Grigore,Călin-Adrian Popa,Claudiu Leoveanu-Condrei*

Main category: cs.CV

TL;DR: A novel method eliminates descriptor use in interest point matching, reducing memory usage while slightly lowering accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional methods rely on descriptors for matching interest points, which increases memory and computational overhead.

Method: Interest points are inherently associated during detection, bypassing descriptor computation and matching.

Result: Matching accuracy is marginally lower, but memory usage is drastically reduced compared to descriptor-based methods.

Conclusion: The proposed method offers a memory-efficient alternative to descriptor-based matching, suitable for localization systems.

Abstract: The extraction and matching of interest points are fundamental to many
geometric computer vision tasks. Traditionally, matching is performed by
assigning descriptors to interest points and identifying correspondences based
on descriptor similarity. This work introduces a technique where interest
points are inherently associated during detection, eliminating the need for
computing, storing, transmitting, or matching descriptors. Although the
matching accuracy is marginally lower than that of conventional approaches, our
method completely eliminates the need for descriptors, leading to a drastic
reduction in memory usage for localization systems. We assess its effectiveness
by comparing it against both classical handcrafted methods and modern learned
approaches.

</details>


### [124] [A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers](https://arxiv.org/abs/2507.10775)
*Jeffrey Joan Sam,Janhavi Sathe,Nikhil Chigali,Naman Gupta,Radhey Ruparel,Yicheng Jiang,Janmajay Singh,James W. Berck,Arko Barman*

Main category: cs.CV

TL;DR: A new dataset of 64k annotated spacecraft images was created for autonomous inspection systems, addressing the scarcity of such data. YOLOv8 and YOLOv11 models were fine-tuned, achieving high performance under real-world constraints.


<details>
  <summary>Details</summary>
Motivation: Spacecraft face damage risks in outer space, and repairs are costly. Autonomous inspection systems can mitigate these issues, but lack of annotated data hinders development.

Method: Created a dataset using real spacecraft models and synthetic backgrounds, adding noise/distortion. Fine-tuned YOLOv8 and YOLOv11 models for segmentation under hardware/time constraints.

Result: Models achieved a Dice score of 0.92, Hausdorff distance of 0.69, and 0.5-second inference time.

Conclusion: The dataset and models provide a reliable benchmark for real-time spacecraft inspection, enabling cost-effective autonomous systems.

Abstract: Spacecraft deployed in outer space are routinely subjected to various forms
of damage due to exposure to hazardous environments. In addition, there are
significant risks to the subsequent process of in-space repairs through human
extravehicular activity or robotic manipulation, incurring substantial
operational costs. Recent developments in image segmentation could enable the
development of reliable and cost-effective autonomous inspection systems. While
these models often require large amounts of training data to achieve
satisfactory results, publicly available annotated spacecraft segmentation data
are very scarce. Here, we present a new dataset of nearly 64k annotated
spacecraft images that was created using real spacecraft models, superimposed
on a mixture of real and synthetic backgrounds generated using NASA's TTALOS
pipeline. To mimic camera distortions and noise in real-world image
acquisition, we also added different types of noise and distortion to the
images. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to
generate performance benchmarks for the dataset under well-defined hardware and
inference time constraints to mimic real-world image segmentation challenges
for real-time onboard applications in space on NASA's inspector spacecraft. The
resulting models, when tested under these constraints, achieved a Dice score of
0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second.
The dataset and models for performance benchmark are available at
https://github.com/RiceD2KLab/SWiM.

</details>


### [125] [Warehouse Spatial Question Answering with LLM Agent](https://arxiv.org/abs/2507.10778)
*Hsiang-Wei Huang,Jen-Hao Cheng,Kuang-Ming Chen,Cheng-Yen Yang,Bahaa Alattar,Yi-Ru Lin,Pyongkun Kim,Sangwon Kim,Kwangju Kim,Chung-I Huang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: A data-efficient LLM agent system is proposed to enhance spatial reasoning in MLLMs for complex indoor warehouse tasks.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with spatial understanding, requiring large-scale finetuning. This work aims to improve efficiency.

Method: An LLM agent system integrates spatial reasoning tools and API interactions for complex spatial question answering.

Result: High accuracy and efficiency achieved in object retrieval, counting, and distance estimation on the AI City Challenge dataset.

Conclusion: The system demonstrates strong spatial reasoning capabilities for warehouse scenarios, with code publicly available.

Abstract: Spatial understanding has been a challenging task for existing Multi-modal
Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM
finetuning to enhance MLLM's spatial understanding ability. In this paper, we
present a data-efficient approach. We propose a LLM agent system with strong
and advanced spatial reasoning ability, which can be used to solve the
challenging spatial question answering task in complex indoor warehouse
scenarios. Our system integrates multiple tools that allow the LLM agent to
conduct spatial reasoning and API tools interaction to answer the given
complicated spatial question. Extensive evaluations on the 2025 AI City
Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that
our system achieves high accuracy and efficiency in tasks such as object
retrieval, counting, and distance estimation. The code is available at:
https://github.com/hsiangwei0903/SpatialAgent

</details>


### [126] [ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference](https://arxiv.org/abs/2507.10800)
*Ali Hojjat,Janek Haberer,Soren Pirk,Olaf Landsiedel*

Main category: cs.CV

TL;DR: ThinkingViT is a nested Vision Transformer (ViT) that dynamically adjusts computation based on input complexity, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Fixed computational budgets in Vision Transformers limit scalability across hardware, and nested models inefficiently allocate compute uniformly.

Method: ThinkingViT uses progressive thinking stages and Token Recycling to dynamically activate attention heads and terminate early if predictions are certain.

Result: ThinkingViT outperforms nested baselines by up to 2.0 p.p. in accuracy at the same throughput and 2.9 p.p. at equal GMACs on ImageNet-1K.

Conclusion: ThinkingViT offers an efficient, scalable solution for Vision Transformers, serving as a plugin upgrade with improved performance.

Abstract: Vision Transformers deliver state-of-the-art performance, yet their fixed
computational budget prevents scalable deployment across heterogeneous
hardware. Recent nested Transformer architectures mitigate this by embedding
nested subnetworks within a single model to enable scalable inference. However,
these models allocate the same amount of compute to all inputs, regardless of
their complexity, which leads to inefficiencies. To address this, we introduce
ThinkingViT, a nested ViT architecture that employs progressive thinking stages
to dynamically adjust inference computation based on input difficulty.
ThinkingViT initiates inference by activating a small subset of the most
important attention heads and terminates early if predictions reach sufficient
certainty. Otherwise, it activates additional attention heads and re-evaluates
the input. At the core of ThinkingViT is our Token Recycling mechanism, which
conditions each subsequent inference stage on the embeddings from the previous
stage, enabling progressive improvement. Due to its backbone-preserving design,
ThinkingViT also serves as a plugin upgrade for vanilla ViT. Experiments show
that ThinkingViT surpasses nested baselines by up to 2.0 percentage points
(p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs
on ImageNet-1K. The source code is available at
https://github.com/ds-kiel/ThinkingViT.

</details>


### [127] [LLM-Guided Agentic Object Detection for Open-World Understanding](https://arxiv.org/abs/2507.10844)
*Furkan Mumcu,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.CV

TL;DR: Proposes LLM-guided agentic object detection (LAOD) for label-free, zero-shot detection by generating scene-specific object names via LLM and using an open-vocabulary detector. Introduces new metrics (CAAP, SNAP) and validates performance on LVIS, COCO, and COCO-OOD.


<details>
  <summary>Details</summary>
Motivation: Traditional object detection lacks flexibility for novel objects. Open-World and Open-Vocabulary methods have limitations (missing semantic labels or dependency on user prompts). LAOD aims for autonomy and adaptability.

Method: Uses an LLM to generate object names dynamically, paired with an open-vocabulary detector for localization. Introduces CAAP and SNAP metrics for evaluation.

Result: Validated on LVIS, COCO, and COCO-OOD, showing strong performance in detecting and naming novel objects.

Conclusion: LAOD enhances autonomy and adaptability for open-world object detection, addressing limitations of prior methods.

Abstract: Object detection traditionally relies on fixed category sets, requiring
costly re-training to handle novel objects. While Open-World and
Open-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD
lacks semantic labels for unknowns, and OVOD depends on user prompts, limiting
autonomy. We propose an LLM-guided agentic object detection (LAOD) framework
that enables fully label-free, zero-shot detection by prompting a Large
Language Model (LLM) to generate scene-specific object names. These are passed
to an open-vocabulary detector for localization, allowing the system to adapt
its goals dynamically. We introduce two new metrics, Class-Agnostic Average
Precision (CAAP) and Semantic Naming Average Precision (SNAP), to separately
evaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD
validate our approach, showing strong performance in detecting and naming novel
objects. Our method offers enhanced autonomy and adaptability for open-world
understanding.

</details>


### [128] [Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization](https://arxiv.org/abs/2507.10846)
*Casey Wall,Longwei Wang,Rodrigue Rizk,KC Santosh*

Main category: cs.CV

TL;DR: Winsor-CAM improves Grad-CAM by aggregating info across all CNN layers using Winsorization for clearer, human-tunable saliency maps.


<details>
  <summary>Details</summary>
Motivation: Enhancing CNN interpretability for high-stakes applications by addressing Grad-CAM's limitations in layer aggregation and noise handling.

Method: Winsor-CAM applies Winsorization to attenuate outliers and aggregates attributions across all layers, with a user-tunable threshold.

Result: Outperforms Grad-CAM in interpretability and localization metrics (e.g., IoU, center-of-mass alignment) on PASCAL VOC 2012.

Conclusion: Winsor-CAM advances trustworthy AI with multi-layer, human-controllable explanations.

Abstract: Interpreting the decision-making process of Convolutional Neural Networks
(CNNs) is critical for deploying models in high-stakes domains.
Gradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method
for visual explanations, yet it typically focuses on the final convolutional
layer or na\"ively averages across layers, strategies that can obscure
important semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a
novel, human-tunable extension of Grad-CAM that generates robust and coherent
saliency maps by aggregating information across all convolutional layers. To
mitigate the influence of noisy or extreme attribution values, Winsor-CAM
applies Winsorization, a percentile-based outlier attenuation technique. A
user-controllable threshold allows for semantic-level tuning, enabling flexible
exploration of model behavior across representational hierarchies. Evaluations
on standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the
PASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable
heatmaps and achieves superior performance in localization metrics, including
intersection-over-union and center-of-mass alignment, when compared to Grad-CAM
and uniform layer-averaging baselines. Winsor-CAM advances the goal of
trustworthy AI by offering interpretable, multi-layer insights with
human-in-the-loop control.

</details>


### [129] [Sparse Fine-Tuning of Transformers for Generative Tasks](https://arxiv.org/abs/2507.10855)
*Wei Chen,Jingxi Yu,Zichen Miao,Qiang Qiu*

Main category: cs.CV

TL;DR: A sparse coding-inspired fine-tuning framework for transformers improves interpretability and task adaptation by representing updates as sparse combinations of feature dictionary atoms.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning methods lack interpretability in how models adapt to new tasks due to dense parameter updates.

Method: Introduces a framework where fine-tuned features are sparse combinations of feature dictionary atoms, with coefficients indicating importance.

Result: Enhances image editing performance and outperforms baselines in text-to-image concept customization.

Conclusion: The sparse coding approach improves interpretability and efficiency in fine-tuning transformers for downstream tasks.

Abstract: Large pre-trained transformers have revolutionized artificial intelligence
across various domains, and fine-tuning remains the dominant approach for
adapting these models to downstream tasks due to the cost of training from
scratch. However, in existing fine-tuning methods, the updated representations
are formed as a dense combination of modified parameters, making it challenging
to interpret their contributions and understand how the model adapts to new
tasks. In this work, we introduce a fine-tuning framework inspired by sparse
coding, where fine-tuned features are represented as a sparse combination of
basic elements, i.e., feature dictionary atoms. The feature dictionary atoms
function as fundamental building blocks of the representation, and tuning atoms
allows for seamless adaptation to downstream tasks. Sparse coefficients then
serve as indicators of atom importance, identifying the contribution of each
atom to the updated representation. Leveraging the atom selection capability of
sparse coefficients, we first demonstrate that our method enhances image
editing performance by improving text alignment through the removal of
unimportant feature dictionary atoms. Additionally, we validate the
effectiveness of our approach in the text-to-image concept customization task,
where our method efficiently constructs the target concept using a sparse
combination of feature dictionary atoms, outperforming various baseline
fine-tuning methods.

</details>


### [130] [A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n](https://arxiv.org/abs/2507.10864)
*Saadat Behzadi,Danial Sharifrazi,Bita Mesbahzadeh,Javad Hassannataj Joloudarid,Roohallah Alizadehsani*

Main category: cs.CV

TL;DR: A lightweight framework combining LOF for noise filtering and YOLO-v11n for polyp detection achieves high accuracy and efficiency, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Timely and accurate polyp detection is vital for colorectal cancer prevention, requiring efficient and robust AI solutions.

Method: LOF removes noisy data; YOLO-v11n processes cleaned data with 5-fold cross-validation and augmentation for robust training.

Result: High performance: precision 95.83%, recall 91.85%, F1-score 93.48%, mAP@0.5 96.48%, mAP@0.5:0.95 77.75%.

Conclusion: The method is effective for real-time clinical use, highlighting the importance of data preprocessing and model efficiency.

Abstract: Objectives: Timely and accurate detection of colorectal polyps plays a
crucial role in diagnosing and preventing colorectal cancer, a major cause of
mortality worldwide. This study introduces a new, lightweight, and efficient
framework for polyp detection that combines the Local Outlier Factor (LOF)
algorithm for filtering noisy data with the YOLO-v11n deep learning model.
  Study design: An experimental study leveraging deep learning and outlier
removal techniques across multiple public datasets.
  Methods: The proposed approach was tested on five diverse and publicly
available datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.
Since these datasets originally lacked bounding box annotations, we converted
their segmentation masks into suitable detection labels. To enhance the
robustness and generalizability of our model, we apply 5-fold cross-validation
and remove anomalous samples using the LOF method configured with 30 neighbors
and a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a
fast and resource-efficient object detection architecture optimized for
real-time applications. We train the model using a combination of modern
augmentation strategies to improve detection accuracy under diverse conditions.
  Results: Our approach significantly improves polyp localization performance,
achieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5
of 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,
our model demonstrates enhanced accuracy and efficiency.
  Conclusions: These results suggest that the proposed method is well-suited
for real-time colonoscopy support in clinical settings. Overall, the study
underscores how crucial data preprocessing and model efficiency are when
designing effective AI systems for medical imaging.

</details>


### [131] [Trexplorer Super: Topologically Correct Centerline Tree Tracking of Tubular Objects in CT Volumes](https://arxiv.org/abs/2507.10881)
*Roman Naeem,David Hagerman,Jennifer Alvén,Lennart Svensson,Fredrik Kahl*

Main category: cs.CV

TL;DR: Trexplorer Super improves upon Trexplorer for 3D centerline tracking in medical images, addressing duplicate branches and premature termination. It outperforms SOTA models on new synthetic and real datasets.


<details>
  <summary>Details</summary>
Motivation: Accurate tracking of tubular tree structures like blood vessels is vital for medical tasks, but existing models like Trexplorer have limitations.

Method: Trexplorer Super introduces novel advancements to enhance tracking performance. Three datasets (one synthetic, two real) are developed for evaluation.

Result: Trexplorer Super outperforms SOTA models on all datasets, though synthetic performance doesn't guarantee real-data success.

Conclusion: Trexplorer Super advances centerline tracking, with datasets and code publicly available for further research.

Abstract: Tubular tree structures, such as blood vessels and airways, are essential in
human anatomy and accurately tracking them while preserving their topology is
crucial for various downstream tasks. Trexplorer is a recurrent model designed
for centerline tracking in 3D medical images but it struggles with predicting
duplicate branches and terminating tracking prematurely. To address these
issues, we present Trexplorer Super, an enhanced version that notably improves
performance through novel advancements. However, evaluating centerline tracking
models is challenging due to the lack of public datasets. To enable thorough
evaluation, we develop three centerline datasets, one synthetic and two real,
each with increasing difficulty. Using these datasets, we conduct a
comprehensive evaluation of existing state-of-the-art (SOTA) models and compare
them with our approach. Trexplorer Super outperforms previous SOTA models on
every dataset. Our results also highlight that strong performance on synthetic
data does not necessarily translate to real datasets. The code and datasets are
available at https://github.com/RomStriker/Trexplorer-Super.

</details>


### [132] [Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency](https://arxiv.org/abs/2507.10893)
*Minjong Cheon,Eunhan Goo,Su-Hyeon Shin,Muhammad Ahmed,Hyungjun Kim*

Main category: cs.CV

TL;DR: A lightweight CNN-based model, KAI-a, achieves competitive accuracy in weather forecasting with reduced computational demands compared to Transformer-based models.


<details>
  <summary>Details</summary>
Motivation: To address the high training complexity and resource demands of Transformer-based weather forecast models while maintaining accuracy.

Method: Introduces a modernized CNN-based model with scale-invariant architecture and InceptionNeXt-based blocks, trained on ERA5 daily dataset with 67 atmospheric variables.

Result: KAI-a matches state-of-the-art performance in medium-range forecasting and excels in capturing extreme events like the 2018 European heatwave.

Conclusion: KAI-a offers a practical, resource-efficient alternative to Transformer-based models for global weather forecasting.

Abstract: Recently, AI-based weather forecast models have achieved impressive advances.
These models have reached accuracy levels comparable to traditional NWP
systems, marking a significant milestone in data-driven weather prediction.
However, they mostly leverage Transformer-based architectures, which often
leads to high training complexity and resource demands due to the massive
parameter sizes. In this study, we introduce a modernized CNN-based model for
global weather forecasting that delivers competitive accuracy while
significantly reducing computational requirements. To present a systematic
modernization roadmap, we highlight key architectural enhancements across
multiple design scales from an earlier CNN-based approach. KAI-a incorporates a
scale-invariant architecture and InceptionNeXt-based blocks within a
geophysically-aware design, tailored to the structure of Earth system data.
Trained on the ERA5 daily dataset with 67 atmospheric variables, the model
contains about 7 million parameters and completes training in just 12 hours on
a single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the
performance of state-of-the-art models in medium-range weather forecasting,
while offering a significantly lightweight design. Furthermore, case studies on
the 2018 European heatwave and the East Asian summer monsoon demonstrate
KAI-a's robust skill in capturing extreme events, reinforcing its practical
utility.

</details>


### [133] [Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition](https://arxiv.org/abs/2507.10895)
*Xiaocong Zeng,Craig Michoski,Yan Pang,Dongyang Kuang*

Main category: cs.CV

TL;DR: The paper proposes two regularization strategies, LVL and LGCL, to address Timescale Dependent Label Inconsistency (TsDLI) in EEG-based emotion recognition, improving model generalization and explainability.


<details>
  <summary>Details</summary>
Motivation: To mitigate TsDLI and enhance model performance in EEG emotion recognition by incorporating mathematical principles into regularization.

Method: Introduces Local Variation Loss (LVL) and Local-Global Consistency Loss (LGCL) using bounded variation and commute-time distances in a graph theoretic framework.

Result: Outperforms state-of-the-art baselines on DREAMER and DEAP datasets, with LVL achieving the best aggregate performance.

Conclusion: The proposed methods offer a principled trade-off between interpretability and predictive power, effectively addressing TsDLI.

Abstract: In this work, we address the often-overlooked issue of Timescale Dependent
Label Inconsistency (TsDLI) in training neural network models for EEG-based
human emotion recognition. To mitigate TsDLI and enhance model generalization
and explainability, we propose two novel regularization strategies: Local
Variation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods
incorporate classical mathematical principles--specifically, functions of
bounded variation and commute-time distances--within a graph theoretic
framework. Complementing our regularizers, we introduce a suite of new
evaluation metrics that better capture the alignment between temporally local
predictions and their associated global emotion labels. We validate our
approach through comprehensive experiments on two widely used EEG emotion
datasets, DREAMER and DEAP, across a range of neural architectures including
LSTM and transformer-based models. Performance is assessed using five distinct
metrics encompassing both quantitative accuracy and qualitative consistency.
Results consistently show that our proposed methods outperform state-of-the-art
baselines, delivering superior aggregate performance and offering a principled
trade-off between interpretability and predictive power under label
inconsistency. Notably, LVL achieves the best aggregate rank across all
benchmarked backbones and metrics, while LGCL frequently ranks the second,
highlighting the effectiveness of our framework.

</details>


### [134] [GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization](https://arxiv.org/abs/2507.10935)
*Shaowen Tong,Zimin Xia,Alexandre Alahi,Xuming He,Yujiao Shi*

Main category: cs.CV

TL;DR: GeoDistill is a weakly supervised framework for cross-view localization using teacher-student learning with FoV-based masking, improving accuracy without costly annotations.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on expensive ground-truth pose annotations, limiting scalability. GeoDistill aims to reduce this dependency while enhancing localization robustness.

Method: Uses teacher-student learning with FoV-based masking. The teacher localizes panoramic images, while the student learns from limited FoV images, aligning predictions to focus on key features.

Result: Improves localization accuracy and reduces uncertainty, even with limited FoV images. Introduces a novel orientation estimation network.

Conclusion: GeoDistill offers a scalable, efficient solution for cross-view localization, validated by improved performance across frameworks.

Abstract: Cross-view localization, the task of estimating a camera's
3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with
satellite images, is crucial for large-scale outdoor applications like
autonomous navigation and augmented reality. Existing methods often rely on
fully supervised learning, which requires costly ground-truth pose annotations.
In this work, we propose GeoDistill, a Geometry guided weakly supervised self
distillation framework that uses teacher-student learning with Field-of-View
(FoV)-based masking to enhance local feature learning for robust cross-view
localization. In GeoDistill, the teacher model localizes a panoramic image,
while the student model predicts locations from a limited FoV counterpart
created by FoV-based masking. By aligning the student's predictions with those
of the teacher, the student focuses on key features like lane lines and ignores
textureless regions, such as roads. This results in more accurate predictions
and reduced uncertainty, regardless of whether the query images are panoramas
or limited FoV images. Our experiments show that GeoDistill significantly
improves localization performance across different frameworks. Additionally, we
introduce a novel orientation estimation network that predicts relative
orientation without requiring precise planar position ground truth. GeoDistill
provides a scalable and efficient solution for real-world cross-view
localization challenges. Code and model can be found at
https://github.com/tongshw/GeoDistill.

</details>


### [135] [Graph Aggregation Prototype Learning for Semantic Change Detection in Remote Sensing](https://arxiv.org/abs/2507.10938)
*Zhengyi Xu,Haoran Wu,Wen Jiang,Jie Geng*

Main category: cs.CV

TL;DR: The paper proposes GAPL-SCD, a method for Semantic Change Detection (SCD) in remote sensing, addressing multi-task optimization challenges with adaptive weight allocation and gradient rotation. It introduces graph aggregation prototype learning and feature fusion modules, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: SCD provides detailed semantic insights into changes in multi-temporal remote sensing data but faces challenges like negative transfer due to conflicting tasks.

Method: GAPL-SCD uses multi-task joint optimization, graph aggregation prototype learning, adaptive weight allocation, and gradient rotation. It includes self-query multi-level feature interaction and bi-temporal feature fusion.

Result: Experiments on SECOND and Landsat-SCD datasets show GAPL-SCD outperforms existing methods in accuracy and robustness.

Conclusion: The proposed framework effectively addresses SCD challenges, improving performance through multi-task optimization and advanced feature representation.

Abstract: Semantic change detection (SCD) extends the binary change detection task to
provide not only the change locations but also the detailed "from-to"
categories in multi-temporal remote sensing data. Such detailed semantic
insights into changes offer considerable advantages for a wide array of
applications. However, since SCD involves the simultaneous optimization of
multiple tasks, the model is prone to negative transfer due to task-specific
learning difficulties and conflicting gradient flows. To address this issue, we
propose Graph Aggregation Prototype Learning for Semantic Change Detection in
remote sensing(GAPL-SCD). In this framework, a multi-task joint optimization
method is designed to optimize the primary task of semantic segmentation and
change detection, along with the auxiliary task of graph aggregation prototype
learning. Adaptive weight allocation and gradient rotation methods are used to
alleviate the conflict between training tasks and improve multi-task learning
capabilities. Specifically, the graph aggregation prototype learning module
constructs an interaction graph using high-level features. Prototypes serve as
class proxies, enabling category-level domain alignment across time points and
reducing interference from irrelevant changes. Additionally, the proposed
self-query multi-level feature interaction and bi-temporal feature fusion
modules further enhance multi-scale feature representation, improving
performance in complex scenes. Experimental results on the SECOND and
Landsat-SCD datasets demonstrate that our method achieves state-of-the-art
performance, with significant improvements in accuracy and robustness for SCD
task.

</details>


### [136] [Robust ID-Specific Face Restoration via Alignment Learning](https://arxiv.org/abs/2507.10943)
*Yushun Fang,Lu Liu,Xiang Gao,Qiang Hu,Ning Cao,Jianghe Cui,Gang Chen,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: RIDFR introduces a robust ID-specific face restoration framework using diffusion models to address identity uncertainty in degraded inputs.


<details>
  <summary>Details</summary>
Motivation: Current face restoration methods struggle with identity fidelity due to obscured inputs and stochastic processes.

Method: RIDFR uses a pre-trained diffusion model with two parallel conditioning modules (Content and Identity Injection) and Alignment Learning to align restoration results.

Result: The framework outperforms state-of-the-art methods, achieving high-quality, ID-specific results with strong robustness.

Conclusion: RIDFR effectively restores faces with high identity fidelity, addressing key challenges in face restoration.

Abstract: The latest developments in Face Restoration have yielded significant
advancements in visual quality through the utilization of diverse diffusion
priors. Nevertheless, the uncertainty of face identity introduced by
identity-obscure inputs and stochastic generative processes remains unresolved.
To address this challenge, we present Robust ID-Specific Face Restoration
(RIDFR), a novel ID-specific face restoration framework based on diffusion
models. Specifically, RIDFR leverages a pre-trained diffusion model in
conjunction with two parallel conditioning modules. The Content Injection
Module inputs the severely degraded image, while the Identity Injection Module
integrates the specific identity from a given image. Subsequently, RIDFR
incorporates Alignment Learning, which aligns the restoration results from
multiple references with the same identity in order to suppress the
interference of ID-irrelevant face semantics (e.g. pose, expression, make-up,
hair style). Experiments demonstrate that our framework outperforms the
state-of-the-art methods, reconstructing high-quality ID-specific results with
high identity fidelity and demonstrating strong robustness.

</details>


### [137] [Women Sport Actions Dataset for Visual Classification Using Small Scale Training Data](https://arxiv.org/abs/2507.10969)
*Palash Ray,Mahuya Sasmal,Asish Bera*

Main category: cs.CV

TL;DR: The paper introduces a new dataset, WomenSports, for women's sports action classification and proposes a CNN with channel attention for improved feature extraction, achieving 89.15% accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack sufficient representation of women's sports actions with diverse variations, limiting research in this area.

Method: A convolutional neural network (CNN) with a channel attention mechanism is proposed for deep feature extraction.

Result: The method achieves 89.15% top-1 classification accuracy on the WomenSports dataset and performs well on other datasets.

Conclusion: The WomenSports dataset and proposed CNN method address a gap in women's sports action classification, demonstrating high accuracy and generalizability.

Abstract: Sports action classification representing complex body postures and
player-object interactions is an emerging area in image-based sports analysis.
Some works have contributed to automated sports action recognition using
machine learning techniques over the past decades. However, sufficient image
datasets representing women sports actions with enough intra- and inter-class
variations are not available to the researchers. To overcome this limitation,
this work presents a new dataset named WomenSports for women sports
classification using small-scale training data. This dataset includes a variety
of sports activities, covering wide variations in movements, environments, and
interactions among players. In addition, this study proposes a convolutional
neural network (CNN) for deep feature extraction. A channel attention scheme
upon local contextual regions is applied to refine and enhance feature
representation. The experiments are carried out on three different sports
datasets and one dance dataset for generalizing the proposed algorithm, and the
performances on these datasets are noteworthy. The deep learning method
achieves 89.15% top-1 classification accuracy using ResNet-50 on the proposed
WomenSports dataset, which is publicly available for research at Mendeley Data.

</details>


### [138] [Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection](https://arxiv.org/abs/2507.10977)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: The paper introduces a wavelet attention-like backbone and a ray-based encoder for efficient and accurate human-object interaction (HOI) detection, addressing limitations in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing HOI detectors are inefficient and resource-intensive, lacking reliable predictions for middle-order interactions.

Method: Proposes a wavelet backbone for feature aggregation and a ray-based encoder for multi-scale attention, optimizing computational efficiency.

Result: Demonstrates improved performance on benchmark datasets like ImageNet and HICO-DET.

Conclusion: The proposed architecture enhances HOI detection efficiency and accuracy, with code made publicly available.

Abstract: Human-object interaction (HOI) detection is essential for accurately
localizing and characterizing interactions between humans and objects,
providing a comprehensive understanding of complex visual scenes across various
domains. However, existing HOI detectors often struggle to deliver reliable
predictions efficiently, relying on resource-intensive training methods and
inefficient architectures. To address these challenges, we conceptualize a
wavelet attention-like backbone and a novel ray-based encoder architecture
tailored for HOI detection. Our wavelet backbone addresses the limitations of
expressing middle-order interactions by aggregating discriminative features
from the low- and high-order interactions extracted from diverse convolutional
filters. Concurrently, the ray-based encoder facilitates multi-scale attention
by optimizing the focus of the decoder on relevant regions of interest and
mitigating computational overhead. As a result of harnessing the attenuated
intensity of learnable ray origins, our decoder aligns query embeddings with
emphasized regions of interest for accurate predictions. Experimental results
on benchmark datasets, including ImageNet and HICO-DET, showcase the potential
of our proposed architecture. The code is publicly available at
[https://github.com/henry-pay/RayEncoder].

</details>


### [139] [Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction](https://arxiv.org/abs/2507.10978)
*Ayush Gupta,Siyuan Huang,Rama Chellappa*

Main category: cs.CV

TL;DR: RG-Gait improves gait recognition under occlusion by modeling occluded gait as a residual deviation from holistic gait, enhancing performance without losing accuracy on holistic inputs.


<details>
  <summary>Details</summary>
Motivation: Addressing the impracticality of paired occluded-holistic data collection and the trade-off between occlusion handling and holistic recognition in gait re-identification.

Method: Proposes RG-Gait, a residual learning approach where occluded gait is treated as a residual deviation from holistic gait, integrated adaptively by the network.

Result: Demonstrates improved performance on occluded sequences while retaining accuracy on holistic inputs, validated on Gait3D, GREW, and BRIAR datasets.

Conclusion: Residual learning is effective for occluded gait recognition with holistic retention, offering a practical solution to real-world challenges.

Abstract: Gait is becoming popular as a method of person re-identification because of
its ability to identify people at a distance. However, most current works in
gait recognition do not address the practical problem of occlusions. Among
those which do, some require paired tuples of occluded and holistic sequences,
which are impractical to collect in the real world. Further, these approaches
work on occlusions but fail to retain performance on holistic inputs. To
address these challenges, we propose RG-Gait, a method for residual correction
for occluded gait recognition with holistic retention. We model the problem as
a residual learning task, conceptualizing the occluded gait signature as a
residual deviation from the holistic gait representation. Our proposed network
adaptively integrates the learned residual, significantly improving performance
on occluded gait sequences without compromising the holistic recognition
accuracy. We evaluate our approach on the challenging Gait3D, GREW and BRIAR
datasets and show that learning the residual can be an effective technique to
tackle occluded gait recognition with holistic retention.

</details>


### [140] [SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition](https://arxiv.org/abs/2507.10999)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: SpaRTAN is a lightweight CNN architecture that improves spatial and channel-wise feature processing, achieving competitive performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Address the simplicity bias and redundancy issues in CNNs and transformers by enhancing spatial and channel-wise information processing.

Method: Uses kernels with varying receptive fields and a wave-based channel aggregation module to capture multi-order spatial features and reduce redundancies.

Result: Achieves 77.7% accuracy on ImageNet-1k with 3.8M parameters and 50.0% AP on COCO with 21.5M parameters, surpassing benchmarks.

Conclusion: SpaRTAN offers an efficient and effective design for visual recognition tasks, balancing performance and parameter efficiency.

Abstract: The resurgence of convolutional neural networks (CNNs) in visual recognition
tasks, exemplified by ConvNeXt, has demonstrated their capability to rival
transformer-based architectures through advanced training methodologies and
ViT-inspired design principles. However, both CNNs and transformers exhibit a
simplicity bias, favoring straightforward features over complex structural
representations. Furthermore, modern CNNs often integrate MLP-like blocks akin
to those in transformers, but these blocks suffer from significant information
redundancies, necessitating high expansion ratios to sustain competitive
performance. To address these limitations, we propose SpaRTAN, a lightweight
architectural design that enhances spatial and channel-wise information
processing. SpaRTAN employs kernels with varying receptive fields, controlled
by kernel size and dilation factor, to capture discriminative multi-order
spatial features effectively. A wave-based channel aggregation module further
modulates and reinforces pixel interactions, mitigating channel-wise
redundancies. Combining the two modules, the proposed network can efficiently
gather and dynamically contextualize discriminative features. Experimental
results in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable
parameter efficiency while maintaining competitive performance. In particular,
on the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M
parameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver
strong performance through an efficient design. On the COCO benchmark, it
achieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M
parameters. The code is publicly available at
[https://github.com/henry-pay/SpaRTAN].

</details>


### [141] [Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering for Zero-shot Anomaly Detection](https://arxiv.org/abs/2507.11003)
*Yuhu Bai,Jiangning Zhang,Yunkang Cao,Guangyuan Lu,Qingdong He,Xiangtai Li,Guanzhong Tian*

Main category: cs.CV

TL;DR: FiSeCLIP enhances zero-shot anomaly detection (ZSAD) by combining feature matching and cross-modal alignment with CLIP, using batch-based testing and text filtering for noisy features, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of zero-shot anomaly detection (ZSAD) by leveraging CLIP's capabilities without training, while improving accuracy through feature filtering and local semantic correlation.

Method: FiSeCLIP uses batch-based testing with mutual references, filters noisy features using text information, and restores CLIP's local semantic correlation for fine-grained anomaly detection.

Result: FiSeCLIP outperforms SOTA AdaCLIP by +4.6%/+5.7% in segmentation metrics (AU-ROC/F1-max) on MVTec-AD.

Conclusion: FiSeCLIP provides a stronger baseline for ZSAD, demonstrating the effectiveness of combining feature matching, cross-modal alignment, and local semantic restoration.

Abstract: With the advent of vision-language models (e.g., CLIP) in zero- and few-shot
settings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in
recent research, where the rare classes are essential and expected in many
applications. This study introduces \textbf{FiSeCLIP} for ZSAD with
training-free \textbf{CLIP}, combining the feature matching with the
cross-modal alignment. Testing with the entire dataset is impractical, while
batch-based testing better aligns with real industrial needs, and images within
a batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes
other images in the same batch as reference information for the current image.
However, the lack of labels for these references can introduce ambiguity, we
apply text information to \textbf{fi}lter out noisy features. In addition, we
further explore CLIP's inherent potential to restore its local
\textbf{se}mantic correlation, adapting it for fine-grained anomaly detection
tasks to enable a more accurate filtering process. Our approach exhibits
superior performance for both anomaly classification and segmentation on
anomaly detection benchmarks, building a stronger baseline for the direction,
e.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by
+4.6\%$\uparrow$/+5.7\%$\uparrow$ in segmentation metrics AU-ROC/$F_1$-max.

</details>


### [142] [Semantically Informed Salient Regions Guided Radiology Report Generation](https://arxiv.org/abs/2507.11015)
*Zeyi Hou,Zeqiang Wei,Ruixin Yan,Ning Lang,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: Proposes SISRNet, a method for generating accurate radiology reports by focusing on medically salient regions to address data bias in chest X-rays.


<details>
  <summary>Details</summary>
Motivation: Existing methods produce fluent but inaccurate reports due to data bias in radiology images, limiting clinical applicability.

Method: SISRNet identifies and focuses on salient regions with critical medical characteristics using fine-grained cross-modal semantics.

Result: Outperforms peers on IU-Xray and MIMIC-CXR datasets, generating clinically accurate reports.

Conclusion: SISRNet effectively mitigates data bias and improves report accuracy, enhancing clinical utility.

Abstract: Recent advances in automated radiology report generation from chest X-rays
using deep learning algorithms have the potential to significantly reduce the
arduous workload of radiologists. However, due to the inherent massive data
bias in radiology images, where abnormalities are typically subtle and sparsely
distributed, existing methods often produce fluent yet medically inaccurate
reports, limiting their applicability in clinical practice. To address this
issue effectively, we propose a Semantically Informed Salient Regions-guided
(SISRNet) report generation method. Specifically, our approach explicitly
identifies salient regions with medically critical characteristics using
fine-grained cross-modal semantics. Then, SISRNet systematically focuses on
these high-information regions during both image modeling and report
generation, effectively capturing subtle abnormal findings, mitigating the
negative impact of data bias, and ultimately generating clinically accurate
reports. Compared to its peers, SISRNet demonstrates superior performance on
widely used IU-Xray and MIMIC-CXR datasets.

</details>


### [143] [Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schrödinger Bridge with Conditional Diffusion](https://arxiv.org/abs/2507.11025)
*Sung Ho Kang,Hyun-Cheol Park*

Main category: cs.CV

TL;DR: A novel CBCT-to-MDCT translation framework using Schrodinger Bridge and human-guided diffusion outperforms GANs and diffusion models, ensuring anatomical fidelity and clinical preference alignment with minimal steps.


<details>
  <summary>Details</summary>
Motivation: To improve CBCT-to-MDCT translation by integrating GAN priors and human feedback, ensuring anatomical accuracy and clinical relevance.

Method: Combines Schrodinger Bridge with GAN-derived priors and human-guided conditional diffusion, using classifier-free guidance and tournament-based preference selection.

Result: Superior performance in RMSE, SSIM, LPIPS, and Dice metrics, with only 10 sampling steps, outperforming prior methods.

Conclusion: The framework is effective and efficient for real-time, preference-aligned medical image translation.

Abstract: We present a novel framework for CBCT-to-MDCT translation, grounded in the
Schrodinger Bridge (SB) formulation, which integrates GAN-derived priors with
human-guided conditional diffusion. Unlike conventional GANs or diffusion
models, our approach explicitly enforces boundary consistency between CBCT
inputs and pseudo targets, ensuring both anatomical fidelity and perceptual
controllability. Binary human feedback is incorporated via classifier-free
guidance (CFG), effectively steering the generative process toward clinically
preferred outcomes. Through iterative refinement and tournament-based
preference selection, the model internalizes human preferences without relying
on a reward model. Subtraction image visualizations reveal that the proposed
method selectively attenuates shade artifacts in key anatomical regions while
preserving fine structural detail. Quantitative evaluations further demonstrate
superior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical
datasets -- outperforming prior GAN- and fine-tuning-based feedback methods --
while requiring only 10 sampling steps. These findings underscore the
effectiveness and efficiency of our framework for real-time, preference-aligned
medical image translation.

</details>


### [144] [Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2507.11030)
*Sunghyun Park,Jungsoo Lee,Shubhankar Borse,Munawar Hayat,Sungha Choi,Kyuwoong Hwang,Fatih Porikli*

Main category: cs.CV

TL;DR: The paper introduces personalized open-vocabulary semantic segmentation (OVSS) to recognize user-specific concepts like 'my mug cup' among similar objects. It proposes a text prompt tuning-based method with 'negative mask proposal' to reduce false predictions and visual embedding injection to enhance performance, maintaining original OVSS capabilities.


<details>
  <summary>Details</summary>
Motivation: Current OVSS fails to segment user-specific personal texts (e.g., 'my mug cup') among similar objects, limiting its practical utility.

Method: A text prompt tuning-based plug-in method with 'negative mask proposal' to reduce false predictions and visual embedding injection to enrich text prompts.

Result: The method improves personalized OVSS performance on new benchmarks (FSS$^\text{per}$, CUB$^\text{per}$, ADE$^\text{per}$) without degrading original OVSS.

Conclusion: The proposed approach effectively addresses personalized OVSS challenges, enhancing segmentation of user-specific concepts while preserving general OVSS performance.

Abstract: While open-vocabulary semantic segmentation (OVSS) can segment an image into
semantic regions based on arbitrarily given text descriptions even for classes
unseen during training, it fails to understand personal texts (e.g., `my mug
cup') for segmenting regions of specific interest to users. This paper
addresses challenges like recognizing `my mug cup' among `multiple mug cups'.
To overcome this challenge, we introduce a novel task termed
\textit{personalized open-vocabulary semantic segmentation} and propose a text
prompt tuning-based plug-in method designed to recognize personal visual
concepts using a few pairs of images and masks, while maintaining the
performance of the original OVSS. Based on the observation that reducing false
predictions is essential when applying text prompt tuning to this task, our
proposed method employs `negative mask proposal' that captures visual concepts
other than the personalized concept. We further improve the performance by
enriching the representation of text prompts by injecting visual embeddings of
the personal concept into them. This approach enhances personalized OVSS
without compromising the original OVSS performance. We demonstrate the
superiority of our method on our newly established benchmarks for this task,
including FSS$^\text{per}$, CUB$^\text{per}$, and ADE$^\text{per}$.

</details>


### [145] [Efficient Dual-domain Image Dehazing with Haze Prior Perception](https://arxiv.org/abs/2507.11035)
*Lirong Zheng,Yanshan Li,Rui Yu,Kaihao Zhang*

Main category: cs.CV

TL;DR: DGFDNet is a dual-domain dehazing network combining spatial and frequency domains, using dark channel priors and multi-level gating for efficient, high-quality dehazing.


<details>
  <summary>Details</summary>
Motivation: Existing methods are computationally expensive and struggle with complex haze conditions due to weak coupling between spatial and frequency domains.

Method: Proposes DGFDNet with HAFM for adaptive frequency modulation and MGAM for multi-scale feature fusion, plus PCGB for iterative prior refinement.

Result: Achieves state-of-the-art performance on four benchmark datasets with robustness and real-time efficiency.

Conclusion: DGFDNet effectively addresses dehazing challenges by integrating spatial and frequency domains, offering superior performance and practicality.

Abstract: Transformer-based models exhibit strong global modeling capabilities in
single-image dehazing, but their high computational cost limits real-time
applicability. Existing methods predominantly rely on spatial-domain features
to capture long-range dependencies, which are computationally expensive and
often inadequate under complex haze conditions. While some approaches introduce
frequency-domain cues, the weak coupling between spatial and frequency branches
limits the overall performance. To overcome these limitations, we propose the
Dark Channel Guided Frequency-aware Dehazing Network (DGFDNet), a novel
dual-domain framework that performs physically guided degradation alignment
across spatial and frequency domains. At its core, the DGFDBlock comprises two
key modules: 1) the Haze-Aware Frequency Modulator (HAFM), which generates a
pixel-level haze confidence map from dark channel priors to adaptively enhance
haze-relevant frequency components, thereby achieving global degradation-aware
spectral modulation; 2) the Multi-level Gating Aggregation Module (MGAM), which
fuses multi-scale features through diverse convolutional kernels and hybrid
gating mechanisms to recover fine structural details. Additionally, a Prior
Correction Guidance Branch (PCGB) incorporates a closed-loop feedback
mechanism, enabling iterative refinement of the prior by intermediate dehazed
features and significantly improving haze localization accuracy, especially in
challenging outdoor scenes. Extensive experiments on four benchmark haze
datasets demonstrate that DGFDNet achieves state-of-the-art performance with
superior robustness and real-time efficiency. Code is available at:
https://github.com/Dilizlr/DGFDNet.

</details>


### [146] [A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion](https://arxiv.org/abs/2507.11037)
*Jie-Wen Li,Zi-Han Ye,Qingyuan Zhou,Jiayi Song,Ying He,Ben Fei,Wen-Ming Chen*

Main category: cs.CV

TL;DR: FootGait3D is a multi-view dataset of high-resolution ankle-foot point clouds for gait analysis, designed to evaluate 3D point cloud completion methods under occlusion.


<details>
  <summary>Details</summary>
Motivation: Accurate ankle-foot surface geometry data during gait is challenging to collect due to occlusions and viewing limitations, necessitating a specialized dataset.

Method: FootGait3D includes 8,403 point cloud frames from 46 subjects, captured using a five-camera system, with complete and partial views for evaluation.

Result: The dataset enables benchmarking of point cloud completion methods and supports biomechanics, clinical gait analysis, and robotics.

Conclusion: FootGait3D advances detailed foot modeling research and is publicly available for further applications.

Abstract: The kinematics analysis of foot-ankle complex during gait is essential for
advancing biomechanical research and clinical assessment. Collecting accurate
surface geometry data from the foot and ankle during dynamic gait conditions is
inherently challenging due to swing foot occlusions and viewing limitations.
Thus, this paper introduces FootGait3D, a novel multi-view dataset of
high-resolution ankle-foot surface point clouds captured during natural gait.
Different from existing gait datasets that typically target whole-body or
lower-limb motion, FootGait3D focuses specifically on the detailed modeling of
the ankle-foot region, offering a finer granularity of motion data. To address
this, FootGait3D consists of 8,403 point cloud frames collected from 46
subjects using a custom five-camera depth sensing system. Each frame includes a
complete 5-view reconstruction of the foot and ankle (serving as ground truth)
along with partial point clouds obtained from only four, three, or two views.
This structured variation enables rigorous evaluation of 3D point cloud
completion methods under varying occlusion levels and viewpoints. Our dataset
is designed for shape completion tasks, facilitating the benchmarking of
state-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and
multi-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the
challenge of recovering the full foot geometry from occluded inputs. FootGait3D
has significant potential to advance research in biomechanics and multi-segment
foot modeling, offering a valuable testbed for clinical gait analysis,
prosthetic design, and robotics applications requiring detailed 3D models of
the foot during motion. The dataset is now available at
https://huggingface.co/datasets/ljw285/FootGait3D.

</details>


### [147] [Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery](https://arxiv.org/abs/2507.11040)
*Nicolas Drapier,Aladine Chetouani,Aurélien Chateigner*

Main category: cs.CV

TL;DR: GLOD is a transformer-based architecture for satellite object detection, using Swin Transformer and novel blocks for upsampling and multi-scale fusion, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of object detection in high-resolution satellite imagery by leveraging transformer-first architectures and multi-scale feature integration.

Method: Replaces CNN backbones with Swin Transformer, introduces UpConvMixer blocks for upsampling, and Fusion Blocks for multi-scale integration. Uses asymmetric fusion with CBAM attention and a multi-path head design.

Result: Achieves 32.95% on xView, outperforming SOTA by 11.46%.

Conclusion: GLOD is effective for satellite imagery, combining spatial priors and computational efficiency for superior object detection.

Abstract: We present GLOD, a transformer-first architecture for object detection in
high-resolution satellite imagery. GLOD replaces CNN backbones with a Swin
Transformer for end-to-end feature extraction, combined with novel UpConvMixer
blocks for robust upsampling and Fusion Blocks for multi-scale feature
integration. Our approach achieves 32.95\% on xView, outperforming SOTA methods
by 11.46\%. Key innovations include asymmetric fusion with CBAM attention and a
multi-path head design capturing objects across scales. The architecture is
optimized for satellite imagery challenges, leveraging spatial priors while
maintaining computational efficiency.

</details>


### [148] [Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation](https://arxiv.org/abs/2507.11055)
*Shuchang Ye,Usman Naseem,Mingyuan Meng,Jinman Kim*

Main category: cs.CV

TL;DR: ProLearn introduces a prototype-driven framework to reduce reliance on paired image-text data for medical language-guided segmentation, improving performance when text is scarce.


<details>
  <summary>Details</summary>
Motivation: The reliance on paired image-text data limits the use of language-guided segmentation in datasets lacking reports and restricts its clinical applicability.

Method: ProLearn uses a Prototype-driven Semantic Approximation (PSA) module to approximate semantic guidance from text, enabling segmentation without paired reports.

Result: ProLearn outperforms state-of-the-art methods on datasets like QaTa-COV19, MosMedData+, and Kvasir-SEG when text is limited.

Conclusion: ProLearn effectively reduces textual reliance, enhancing the practicality of language-guided segmentation in clinical settings.

Abstract: Medical language-guided segmentation, integrating textual clinical reports as
auxiliary guidance to enhance image segmentation, has demonstrated significant
improvements over unimodal approaches. However, its inherent reliance on paired
image-text input, which we refer to as ``textual reliance", presents two
fundamental limitations: 1) many medical segmentation datasets lack paired
reports, leaving a substantial portion of image-only data underutilized for
training; and 2) inference is limited to retrospective analysis of cases with
paired reports, limiting its applicability in most clinical scenarios where
segmentation typically precedes reporting. To address these limitations, we
propose ProLearn, the first Prototype-driven Learning framework for
language-guided segmentation that fundamentally alleviates textual reliance. At
its core, in ProLearn, we introduce a novel Prototype-driven Semantic
Approximation (PSA) module to enable approximation of semantic guidance from
textual input. PSA initializes a discrete and compact prototype space by
distilling segmentation-relevant semantics from textual reports. Once
initialized, it supports a query-and-respond mechanism which approximates
semantic guidance for images without textual input, thereby alleviating textual
reliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG
demonstrate that ProLearn outperforms state-of-the-art language-guided methods
when limited text is available.

</details>


### [149] [Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling](https://arxiv.org/abs/2507.11061)
*Hayeon Kim,Ji Ha Jang,Se Young Chun*

Main category: cs.CV

TL;DR: RoMaP is a framework for precise local 3D Gaussian editing, addressing challenges in multi-view segmentation and SDS loss ambiguity with 3D-GALP and a regularized SDS loss.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with precise local 3D edits due to inconsistent multi-view segmentations and ambiguous SDS loss, limiting high-quality part-level modifications.

Method: RoMaP introduces 3D-GALP for robust 3D mask generation using SH coefficients and a regularized SDS loss with L1 anchor loss (SLaMP) and additional regularizers.

Result: RoMaP achieves state-of-the-art local 3D editing on Gaussian scenes, enabling precise and flexible part-level modifications.

Conclusion: RoMaP advances local 3D Gaussian editing by improving segmentation consistency and SDS loss robustness, facilitating high-quality part-level edits.

Abstract: Recent advances in 3D neural representations and instance-level editing
models have enabled the efficient creation of high-quality 3D content. However,
achieving precise local 3D edits remains challenging, especially for Gaussian
Splatting, due to inconsistent multi-view 2D part segmentations and inherently
ambiguous nature of Score Distillation Sampling (SDS) loss. To address these
limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that
enables precise and drastic part-level modifications. First, we introduce a
robust 3D mask generation module with our 3D-Geometry Aware Label Prediction
(3D-GALP), which uses spherical harmonics (SH) coefficients to model
view-dependent label variations and soft-label property, yielding accurate and
consistent part segmentations across viewpoints. Second, we propose a
regularized SDS loss that combines the standard SDS loss with additional
regularizers. In particular, an L1 anchor loss is introduced via our Scheduled
Latent Mixing and Part (SLaMP) editing method, which generates high-quality
part-edited 2D images and confines modifications only to the target region
while preserving contextual coherence. Additional regularizers, such as
Gaussian prior removal, further improve flexibility by allowing changes beyond
the existing context, and robust 3D masking prevents unintended edits.
Experimental results demonstrate that our RoMaP achieves state-of-the-art local
3D editing on both reconstructed and generated Gaussian scenes and objects
qualitatively and quantitatively, making it possible for more robust and
flexible part-level 3D Gaussian editing.

</details>


### [150] [Joint angle model based learning to refine kinematic human pose estimation](https://arxiv.org/abs/2507.11075)
*Chang Peng,Yifei Zhou,Huifeng Xi,Shiqing Huang,Chuangye Chen,Jianming Yang,Bao Yang,Zhenyu Jiang*

Main category: cs.CV

TL;DR: The paper introduces a joint angle-based refinement (JAR) method to improve marker-free human pose estimation (HPE) by correcting errors and smoothing trajectories using a bidirectional recurrent network trained on high-quality datasets.


<details>
  <summary>Details</summary>
Motivation: Current HPE methods suffer from keypoint recognition errors and trajectory fluctuations due to inaccurate manually annotated datasets.

Method: Proposes joint angle-based modeling, approximating joint angle variations with Fourier series, and using a bidirectional recurrent network to refine HRNet outputs.

Result: JAR outperforms state-of-the-art HPE refinement networks, especially in challenging cases like figure skating and breaking.

Conclusion: The joint angle-based approach effectively enhances HPE accuracy and robustness in kinematic scenarios.

Abstract: Marker-free human pose estimation (HPE) has found increasing applications in
various fields. Current HPE suffers from occasional errors in keypoint
recognition and random fluctuation in keypoint trajectories when analyzing
kinematic human poses. The performance of existing deep learning-based models
for HPE refinement is considerably limited by inaccurate training datasets in
which the keypoints are manually annotated. This paper proposed a novel method
to overcome the difficulty through joint angle-based modeling. The key
techniques include: (i) A joint angle-based model of human pose, which is
robust to describe kinematic human poses; (ii) Approximating temporal variation
of joint angles through high order Fourier series to get reliable "ground
truth"; (iii) A bidirectional recurrent network is designed as a
post-processing module to refine the estimation of well-established HRNet.
Trained with the high-quality dataset constructed using our method, the network
demonstrates outstanding performance to correct wrongly recognized joints and
smooth their spatiotemporal trajectories. Tests show that joint angle-based
refinement (JAR) outperforms the state-of-the-art HPE refinement network in
challenging cases like figure skating and breaking.

</details>


### [151] [GKNet: Graph-based Keypoints Network for Monocular Pose Estimation of Non-cooperative Spacecraft](https://arxiv.org/abs/2507.11077)
*Weizhao Ma,Dong Zhou,Yuhui Hu,Zipeng He*

Main category: cs.CV

TL;DR: The paper proposes GKNet, a graph-based keypoints network for monocular pose estimation of non-cooperative spacecraft, addressing challenges like structural symmetry and occlusion. It also introduces the SKD dataset for validation.


<details>
  <summary>Details</summary>
Motivation: Accurate monocular pose estimation is crucial for on-orbit service tasks, but current keypoint detectors struggle with symmetry and occlusion.

Method: GKNet leverages geometric constraints of keypoints graph. The SKD dataset (90,000 simulated images) is introduced for validation.

Result: GKNet outperforms state-of-the-art detectors in accuracy and effectiveness.

Conclusion: GKNet and the SKD dataset advance spacecraft pose estimation, with code and data publicly available.

Abstract: Monocular pose estimation of non-cooperative spacecraft is significant for
on-orbit service (OOS) tasks, such as satellite maintenance, space debris
removal, and station assembly. Considering the high demands on pose estimation
accuracy, mainstream monocular pose estimation methods typically consist of
keypoint detectors and PnP solver. However, current keypoint detectors remain
vulnerable to structural symmetry and partial occlusion of non-cooperative
spacecraft. To this end, we propose a graph-based keypoints network for the
monocular pose estimation of non-cooperative spacecraft, GKNet, which leverages
the geometric constraint of keypoints graph. In order to better validate
keypoint detectors, we present a moderate-scale dataset for the spacecraft
keypoint detection, named SKD, which consists of 3 spacecraft targets, 90,000
simulated images, and corresponding high-precise keypoint annotations.
Extensive experiments and an ablation study have demonstrated the high accuracy
and effectiveness of our GKNet, compared to the state-of-the-art spacecraft
keypoint detectors. The code for GKNet and the SKD dataset is available at
https://github.com/Dongzhou-1996/GKNet.

</details>


### [152] [Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification](https://arxiv.org/abs/2507.11081)
*Chang Peng,Bao Yang,Meiqi Li,Ge Zhang,Hui Sun,Zhenyu Jiang*

Main category: cs.CV

TL;DR: A novel cross-verification strategy using YOLO models improves RSD recognition in GPR images, achieving 98.6% recall and reducing inspection labor by 90%.


<details>
  <summary>Details</summary>
Motivation: Manual RSD recognition in GPR images is labor-intensive and expertise-dependent. Deep learning solutions are limited by dataset scarcity and network capability.

Method: Constructed a 3D GPR dataset (2134 samples) and proposed a cross-verification strategy based on YOLO model sensitivity to different RSD types.

Result: Achieved over 98.6% recall in field tests. Integrated into an online system, reducing inspection labor by 90%.

Conclusion: The approach significantly enhances RSD recognition efficiency and accuracy, demonstrating practical utility in road inspection.

Abstract: Ground penetrating radar (GPR) has become a rapid and non-destructive
solution for road subsurface distress (RSD) detection. However, RSD recognition
from GPR images is labor-intensive and heavily relies on inspectors' expertise.
Deep learning offers the possibility for automatic RSD recognition, but its
current performance is limited by two factors: Scarcity of high-quality dataset
for network training and insufficient capability of network to distinguish RSD.
In this study, a rigorously validated 3D GPR dataset containing 2134 samples of
diverse types was constructed through field scanning. Based on the finding that
the YOLO model trained with one of the three scans of GPR images exhibits
varying sensitivity to specific type of RSD, we proposed a novel
cross-verification strategy with outstanding accuracy in RSD recognition,
achieving recall over 98.6% in field tests. The approach, integrated into an
online RSD detection system, can reduce the labor of inspection by around 90%.

</details>


### [153] [Atmos-Bench: 3D Atmospheric Structures for Climate Insight](https://arxiv.org/abs/2507.11085)
*Tianchi Xu*

Main category: cs.CV

TL;DR: Atmos-Bench introduces the first 3D atmospheric benchmark and a novel FourCastX network for improved atmospheric structure recovery, outperforming existing methods without auxiliary inputs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for atmospheric structure recovery rely on simplified physics and lack standardized benchmarks, introducing uncertainties. Atmos-Bench aims to address these gaps.

Method: The study uses a FourCastX network to generate high-quality 3D scattering volumes from satellite LiDAR data, embedding physical constraints for energy consistency.

Result: The method achieves consistent improvements on the Atmos-Bench dataset across 355 nm and 532 nm bands, outperforming baseline models.

Conclusion: Atmos-Bench sets a new standard for 3D atmospheric recovery, enhancing climate understanding and forecasting.

Abstract: Atmospheric structure, represented by backscatter coefficients (BC) recovered
from satellite LiDAR attenuated backscatter (ATB), provides a volumetric view
of clouds, aerosols, and molecules, playing a critical role in human
activities, climate understanding, and extreme weather forecasting. Existing
methods often rely on auxiliary inputs and simplified physics-based
approximations, and lack a standardized 3D benchmark for fair evaluation.
However, such approaches may introduce additional uncertainties and
insufficiently capture realistic radiative transfer and atmospheric
scattering-absorption effects. To bridge these gaps, we present Atmos-Bench:
the first 3D atmospheric benchmark, along with a novel FourCastX:
Frequency-enhanced Spatio-Temporal Mixture-of-Experts Network that (a)
generates 921,600 image slices from 3D scattering volumes simulated at 532 nm
and 355 nm by coupling WRF with an enhanced COSP simulator over 384 land-ocean
time steps, yielding high-quality voxel-wise references; (b) embeds ATB-BC
physical constraints into the model architecture, promoting energy consistency
during restoration; (c) achieves consistent improvements on the Atmos-Bench
dataset across both 355 nm and 532 nm bands, outperforming state-of-the-art
baseline models without relying on auxiliary inputs. Atmos-Bench establishes a
new standard for satellite-based 3D atmospheric structure recovery and paves
the way for deeper climate insight.

</details>


### [154] [A Survey on Interpretability in Visual Recognition](https://arxiv.org/abs/2507.11099)
*Qiyang Wan,Chengzhi Gao,Ruiping Wang,Xilin Chen*

Main category: cs.CV

TL;DR: This paper reviews interpretability in visual recognition models, proposing a human-centered taxonomy and discussing evaluation metrics and future opportunities.


<details>
  <summary>Details</summary>
Motivation: To understand and deploy visual recognition models in critical applications, advancing interpretability research.

Method: Systematic review and taxonomy of interpretable methods categorized by Intent, Object, Presentation, and Methodology.

Result: A coherent taxonomy and summary of evaluation metrics, with insights into future research enabled by new technologies.

Conclusion: The paper organizes existing research and inspires further investigations into model interpretability.

Abstract: In recent years, visual recognition methods have advanced significantly,
finding applications across diverse fields. While researchers seek to
understand the mechanisms behind the success of these models, there is also a
growing impetus to deploy them in critical areas like autonomous driving and
medical diagnostics to better diagnose failures, which promotes the development
of interpretability research. This paper systematically reviews existing
research on the interpretability of visual recognition models and proposes a
taxonomy of methods from a human-centered perspective. The proposed taxonomy
categorizes interpretable recognition methods based on Intent, Object,
Presentation, and Methodology, thereby establishing a systematic and coherent
set of grouping criteria for these XAI methods. Additionally, we summarize the
requirements for evaluation metrics and explore new opportunities enabled by
recent technologies, such as large multimodal models. We aim to organize
existing research in this domain and inspire future investigations into the
interpretability of visual recognition models.

</details>


### [155] [KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model](https://arxiv.org/abs/2507.11102)
*Jie Yang,Wang Zeng,Sheng Jin,Lumin Xu,Wentao Liu,Chen Qian,Zhen Li,Ruimao Zhang*

Main category: cs.CV

TL;DR: KptLLM++ is a multimodal large language model designed for fine-grained keypoint comprehension, achieving state-of-the-art performance through a novel identify-then-detect paradigm and extensive training on 500K diverse samples.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with fine-grained semantic information like keypoints, which are crucial for applications such as image analysis and behavior recognition.

Method: KptLLM++ uses an identify-then-detect paradigm with structured chain-of-thought reasoning, integrating diverse input modalities guided by user instructions.

Result: The model achieves remarkable accuracy and generalization, outperforming benchmarks in keypoint detection.

Conclusion: KptLLM++ is a transformative solution for fine-grained image understanding and human-AI collaboration.

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has revolutionized
image understanding by bridging textual and visual modalities. However, these
models often struggle with capturing fine-grained semantic information, such as
the precise identification and analysis of object keypoints. Keypoints, as
structure-aware, pixel-level, and compact representations of objects,
particularly articulated ones, play a crucial role in applications such as
fine-grained image analysis, object retrieval, and behavior recognition. In
this paper, we propose KptLLM++, a novel multimodal large language model that
specifically designed for generic keypoint comprehension through the
integration of diverse input modalities guided by user-defined instructions. By
unifying keypoint detection across varied contexts, KptLLM++ establishes itself
as an advanced interface, fostering more effective human-AI collaboration. The
model is built upon a novel identify-then-detect paradigm, which first
interprets keypoint semantics and subsequently localizes their precise
positions through a structured chain-of-thought reasoning mechanism. To push
the boundaries of performance, we have scaled up the training dataset to over
500K samples, encompassing diverse objects, keypoint categories, image styles,
and scenarios with complex occlusions. This extensive scaling enables KptLLM++
to unlock its potential, achieving remarkable accuracy and generalization.
Comprehensive experiments on multiple keypoint detection benchmarks demonstrate
its state-of-the-art performance, underscoring its potential as a unified
solution for fine-grained image understanding and its transformative
implications for human-AI interaction.

</details>


### [156] [Jellyfish Species Identification: A CNN Based Artificial Neural Network Approach](https://arxiv.org/abs/2507.11116)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha,Mostofa Kamal Nasir*

Main category: cs.CV

TL;DR: A deep learning framework for jellyfish species detection achieves 98% accuracy using MobileNetV3 and hybrid classifiers.


<details>
  <summary>Details</summary>
Motivation: Jellyfish proliferation impacts marine ecosystems, necessitating accurate species identification for ecological monitoring.

Method: Combines MobileNetV3, ResNet50, EfficientNetV2-B0, VGG16 with traditional ML and Feedforward Neural Network classifiers, using softmax for CNN-based classification.

Result: MobileNetV3 with Artificial Neural Network achieved 98% accuracy, outperforming other models.

Conclusion: Deep learning and hybrid frameworks effectively address biodiversity challenges in marine species detection.

Abstract: Jellyfish, a diverse group of gelatinous marine organisms, play a crucial
role in maintaining marine ecosystems but pose significant challenges for
biodiversity and conservation due to their rapid proliferation and ecological
impact. Accurate identification of jellyfish species is essential for
ecological monitoring and management. In this study, we proposed a deep
learning framework for jellyfish species detection and classification using an
underwater image dataset. The framework integrates advanced feature extraction
techniques, including MobileNetV3, ResNet50, EfficientNetV2-B0, and VGG16,
combined with seven traditional machine learning classifiers and three
Feedforward Neural Network classifiers for precise species identification.
Additionally, we activated the softmax function to directly classify jellyfish
species using the convolutional neural network models. The combination of the
Artificial Neural Network with MobileNetV3 is our best-performing model,
achieving an exceptional accuracy of 98%, significantly outperforming other
feature extractor-classifier combinations. This study demonstrates the efficacy
of deep learning and hybrid frameworks in addressing biodiversity challenges
and advancing species detection in marine environments.

</details>


### [157] [Try Harder: Hard Sample Generation and Learning for Clothes-Changing Person Re-ID](https://arxiv.org/abs/2507.11119)
*Hankun Liu,Yujian Zhao,Guanglin Niu*

Main category: cs.CV

TL;DR: Proposes a multimodal-guided framework (HSGL) for defining, generating, and optimizing hard samples in clothing-changing person Re-ID, improving robustness and performance.


<details>
  <summary>Details</summary>
Motivation: Hard samples in CC-ReID lack explicit definitions and hinder model robustness. Addressing this gap can enhance learning strategies and performance.

Method: HSGL includes Dual-Granularity Hard Sample Generation (DGHSG) for synthesizing diverse hard samples and Hard Sample Adaptive Learning (HSAL) for hardness-aware optimization.

Result: Achieves state-of-the-art performance on PRCC and LTCC datasets, accelerating convergence and improving discriminative capability.

Conclusion: HSGL demonstrates the effectiveness of multimodal-guided hard sample generation and learning for robust CC-ReID.

Abstract: Hard samples pose a significant challenge in person re-identification (ReID)
tasks, particularly in clothing-changing person Re-ID (CC-ReID). Their inherent
ambiguity or similarity, coupled with the lack of explicit definitions, makes
them a fundamental bottleneck. These issues not only limit the design of
targeted learning strategies but also diminish the model's robustness under
clothing or viewpoint changes. In this paper, we propose a novel
multimodal-guided Hard Sample Generation and Learning (HSGL) framework, which
is the first effort to unify textual and visual modalities to explicitly
define, generate, and optimize hard samples within a unified paradigm. HSGL
comprises two core components: (1) Dual-Granularity Hard Sample Generation
(DGHSG), which leverages multimodal cues to synthesize semantically consistent
samples, including both coarse- and fine-grained hard positives and negatives
for effectively increasing the hardness and diversity of the training data. (2)
Hard Sample Adaptive Learning (HSAL), which introduces a hardness-aware
optimization strategy that adjusts feature distances based on textual semantic
labels, encouraging the separation of hard positives and drawing hard negatives
closer in the embedding space to enhance the model's discriminative capability
and robustness to hard samples. Extensive experiments on multiple CC-ReID
benchmarks demonstrate the effectiveness of our approach and highlight the
potential of multimodal-guided hard sample generation and learning for robust
CC-ReID. Notably, HSAL significantly accelerates the convergence of the
targeted learning procedure and achieves state-of-the-art performance on both
PRCC and LTCC datasets. The code is available at
https://github.com/undooo/TryHarder-ACMMM25.

</details>


### [158] [MMOne: Representing Multiple Modalities in One Scene](https://arxiv.org/abs/2507.11129)
*Zhifeng Gu,Bing Wang*

Main category: cs.CV

TL;DR: MMOne is a framework for multimodal scene representation, addressing modality conflicts (property and granularity disparities) through modality modeling and decomposition, enhancing representation capability and scalability.


<details>
  <summary>Details</summary>
Motivation: Humans use multimodal cues to understand the world, but modality conflicts (property and granularity disparities) hinder effective scene representation.

Method: Proposes MMOne with a modality modeling module (using modality indicators) and a multimodal decomposition mechanism to separate and disentangle multimodal information.

Result: Enhances representation for each modality and scales to additional modalities, as shown in experiments.

Conclusion: MMOne effectively addresses modality conflicts, offering a compact and efficient multimodal scene representation.

Abstract: Humans perceive the world through multimodal cues to understand and interact
with the environment. Learning a scene representation for multiple modalities
enhances comprehension of the physical world. However, modality conflicts,
arising from inherent distinctions among different modalities, present two
critical challenges: property disparity and granularity disparity. To address
these challenges, we propose a general framework, MMOne, to represent multiple
modalities in one scene, which can be readily extended to additional
modalities. Specifically, a modality modeling module with a novel modality
indicator is proposed to capture the unique properties of each modality.
Additionally, we design a multimodal decomposition mechanism to separate
multi-modal Gaussians into single-modal Gaussians based on modality
differences. We address the essential distinctions among modalities by
disentangling multimodal information into shared and modality-specific
components, resulting in a more compact and efficient multimodal scene
representation. Extensive experiments demonstrate that our method consistently
enhances the representation capability for each modality and is scalable to
additional modalities. The code is available at
https://github.com/Neal2020GitHub/MMOne.

</details>


### [159] [RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images](https://arxiv.org/abs/2507.11143)
*Lam Pham,Cam Le,Hieu Tang,Khang Truong,Truong Nguyen,Jasmin Lampert,Alexander Schindler,Martin Boyer,Son Phan*

Main category: cs.CV

TL;DR: A deep-learning-based model for automatic landslide observation using remote sensing images, achieving high F1 and mIoU scores on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Frequent landslide disasters due to extreme weather and human activities, coupled with challenges in manual observation, drive the need for automated solutions.

Method: Proposes an end-to-end neural network for landslide detection and segmentation using remote sensing images.

Result: Achieved F1 scores of 98.23 and 93.83 for detection, and mIoU scores of 63.74 and 76.88 for segmentation on benchmark datasets.

Conclusion: The model shows potential for real-life landslide observation systems.

Abstract: In recent years, landslide disasters have reported frequently due to the
extreme weather events of droughts, floods , storms, or the consequence of
human activities such as deforestation, excessive exploitation of natural
resources. However, automatically observing landslide is challenging due to the
extremely large observing area and the rugged topography such as mountain or
highland. This motivates us to propose an end-to-end deep-learning-based model
which explores the remote sensing images for automatically observing landslide
events. By considering remote sensing images as the input data, we can obtain
free resource, observe large and rough terrains by time. To explore the remote
sensing images, we proposed a novel neural network architecture which is for
two tasks of landslide detection and landslide segmentation. We evaluated our
proposed model on three different benchmark datasets of LandSlide4Sense, Bijie,
and Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,
93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU
scores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,
Nepal datasets. These experimental results prove potential to integrate our
proposed model into real-life landslide observation systems.

</details>


### [160] [Assessing Color Vision Test in Large Vision-language Models](https://arxiv.org/abs/2507.11153)
*Hongfei Ye,Bin Chen,Wenxi Liu,Yu Zhang,Zhao Li,Dandan Ni,Hongyang Chen*

Main category: cs.CV

TL;DR: The paper explores color vision abilities in large vision-language models, introduces a testing task, and proposes fine-tuning strategies to improve performance.


<details>
  <summary>Details</summary>
Motivation: The color vision capabilities of large vision-language models are understudied, prompting the need for a dedicated evaluation framework.

Method: A color vision testing task is defined, and a dataset with diverse question categories and difficulty levels is constructed. Error analysis is conducted, and fine-tuning strategies are proposed.

Result: The study identifies common errors in models and suggests methods to enhance their color vision performance.

Conclusion: Fine-tuning strategies can improve the color vision abilities of large vision-language models, addressing a critical gap in their evaluation.

Abstract: With the widespread adoption of large vision-language models, the capacity
for color vision in these models is crucial. However, the color vision
abilities of large visual-language models have not yet been thoroughly
explored. To address this gap, we define a color vision testing task for large
vision-language models and construct a dataset \footnote{Anonymous Github
Showing some of the data
https://anonymous.4open.science/r/color-vision-test-dataset-3BCD} that covers
multiple categories of test questions and tasks of varying difficulty levels.
Furthermore, we analyze the types of errors made by large vision-language
models and propose fine-tuning strategies to enhance their performance in color
vision tests.

</details>


### [161] [Clustering-Guided Multi-Layer Contrastive Representation Learning for Citrus Disease Classification](https://arxiv.org/abs/2507.11171)
*Jun Chen,Yonghua Yu,Weifu Li,Yaohui Chen,Hong Chen*

Main category: cs.CV

TL;DR: A novel self-supervised learning method (CMCRL) for citrus disease detection outperforms existing methods by 4.5%-30.1% accuracy, using unannotated samples and hierarchical feature learning.


<details>
  <summary>Details</summary>
Motivation: Citrus diseases cause significant yield losses, and accurate detection is crucial for targeted control. Current deep learning methods require extensive labeled data, which is costly.

Method: Proposes CMCRL, combining contrasting with cluster centroids and multi-layer contrastive training (MCT) to learn hierarchical features from unannotated samples.

Result: Achieves state-of-the-art performance on the CDD dataset, narrowing the gap with fully supervised methods and excelling in F1 score, precision, and recall.

Conclusion: CMCRL offers a robust, efficient solution for citrus disease detection, reducing reliance on labeled data and addressing class imbalance.

Abstract: Citrus, as one of the most economically important fruit crops globally,
suffers severe yield depressions due to various diseases. Accurate disease
detection and classification serve as critical prerequisites for implementing
targeted control measures. Recent advancements in artificial intelligence,
particularly deep learning-based computer vision algorithms, have substantially
decreased time and labor requirements while maintaining the accuracy of
detection and classification. Nevertheless, these methods predominantly rely on
massive, high-quality annotated training examples to attain promising
performance. By introducing two key designs: contrasting with cluster centroids
and a multi-layer contrastive training (MCT) paradigm, this paper proposes a
novel clustering-guided self-supervised multi-layer contrastive representation
learning (CMCRL) algorithm. The proposed method demonstrates several advantages
over existing counterparts: (1) optimizing with massive unannotated samples;
(2) effective adaptation to the symptom similarity across distinct citrus
diseases; (3) hierarchical feature representation learning. The proposed method
achieves state-of-the-art performance on the public citrus image set CDD,
outperforming existing methods by 4.5\%-30.1\% accuracy. Remarkably, our method
narrows the performance gap with fully supervised counterparts (all samples are
labeled). Beyond classification accuracy, our method shows great performance on
other evaluation metrics (F1 score, precision, and recall), highlighting the
robustness against the class imbalance challenge.

</details>


### [162] [How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study](https://arxiv.org/abs/2507.11200)
*Che Liu,Jiazhen Pan,Weixiang Shen,Wenjia Bai,Daniel Rueckert,Rossella Arcucci*

Main category: cs.CV

TL;DR: Evaluation of Vision-Language Models (VLMs) in medical tasks reveals general-purpose models match or outperform medical-specific ones, but reasoning lags understanding, and reliability for clinical use remains unmet.


<details>
  <summary>Details</summary>
Motivation: To assess the competence of VLMs in medical tasks, given their increasing use in healthcare despite limited exploration of their medical task performance.

Method: Comprehensive evaluation of open-source general-purpose and medically specialized VLMs (3B to 72B parameters) across eight benchmarks, analyzing understanding and reasoning separately.

Result: General-purpose models match or surpass medical-specific ones; reasoning performance is lower than understanding; benchmark performance varies widely.

Conclusion: No model meets clinical reliability standards, highlighting the need for better multimodal alignment and rigorous evaluation.

Abstract: Vision-Language Models (VLMs) trained on web-scale corpora excel at natural
image tasks and are increasingly repurposed for healthcare; however, their
competence in medical tasks remains underexplored. We present a comprehensive
evaluation of open-source general-purpose and medically specialised VLMs,
ranging from 3B to 72B parameters, across eight benchmarks: MedXpert,
OmniMedVQA, PMC-VQA, PathVQA, MMMU, SLAKE, and VQA-RAD. To observe model
performance across different aspects, we first separate it into understanding
and reasoning components. Three salient findings emerge. First, large
general-purpose models already match or surpass medical-specific counterparts
on several benchmarks, demonstrating strong zero-shot transfer from natural to
medical images. Second, reasoning performance is consistently lower than
understanding, highlighting a critical barrier to safe decision support. Third,
performance varies widely across benchmarks, reflecting differences in task
design, annotation quality, and knowledge demands. No model yet reaches the
reliability threshold for clinical deployment, underscoring the need for
stronger multimodal alignment and more rigorous, fine-grained evaluation
protocols.

</details>


### [163] [A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition](https://arxiv.org/abs/2507.11202)
*Xinkui Zhao,Jinsong Shu,Yangyang Wu,Guanjie Cheng,Zihe Liu,Naibo Wang,Shuiguang Deng,Zhongle Xie,Jianwei Yin*

Main category: cs.CV

TL;DR: MCULoRA is a novel framework for incomplete multimodal learning, addressing gradient conflicts by decoupling shared and distinct modality information and dynamically adjusting training ratios.


<details>
  <summary>Details</summary>
Motivation: Existing MER methods struggle with incomplete modalities due to conflicting training gradients, degrading performance.

Method: MCULoRA uses two modules: MCLA for decoupling shared/distinct modality information and DPFT for dynamic training ratio adjustment.

Result: MCULoRA outperforms previous methods in downstream task accuracy across benchmark datasets.

Conclusion: MCULoRA effectively improves incomplete multimodal learning by optimizing modality combination training.

Abstract: Multimodal Emotion Recognition (MER) often encounters incomplete
multimodality in practical applications due to sensor failures or privacy
protection requirements. While existing methods attempt to address various
incomplete multimodal scenarios by balancing the training of each modality
combination through additional gradients, these approaches face a critical
limitation: training gradients from different modality combinations conflict
with each other, ultimately degrading the performance of the final prediction
model. In this paper, we propose a unimodal decoupled dynamic low-rank
adaptation method based on modality combinations, named MCULoRA, which is a
novel framework for the parameter-efficient training of incomplete multimodal
learning models. MCULoRA consists of two key modules, modality combination
aware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The
MCLA module effectively decouples the shared information from the distinct
characteristics of individual modality combinations. The DPFT module adjusts
the training ratio of modality combinations based on the separability of each
modality's representation space, optimizing the learning efficiency across
different modality combinations. Our extensive experimental evaluation in
multiple benchmark datasets demonstrates that MCULoRA substantially outperforms
previous incomplete multimodal learning approaches in downstream task accuracy.

</details>


### [164] [NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models](https://arxiv.org/abs/2507.11245)
*X. Feng,H. Yu,M. Wu,S. Hu,J. Chen,C. Zhu,J. Wu,X. Chu,K. Huang*

Main category: cs.CV

TL;DR: The paper introduces NarrLV, the first benchmark for evaluating narrative expression in long video generation models, using Temporal Narrative Atoms (TNAs) and a novel MLLM-based metric.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack focus on narrative richness in long videos, limiting evaluation of models' ability to express complex narratives.

Method: Proposes Temporal Narrative Atoms (TNAs) for measuring narrative richness, an automatic prompt generation pipeline, and an MLLM-based evaluation metric.

Result: NarrLV aligns with human judgments and reveals capability boundaries of current models in narrative expression.

Conclusion: NarrLV provides a comprehensive benchmark for assessing narrative capabilities in long video generation, addressing gaps in existing evaluation methods.

Abstract: With the rapid development of foundation video generation technologies, long
video generation models have exhibited promising research potential thanks to
expanded content creation space. Recent studies reveal that the goal of long
video generation tasks is not only to extend video duration but also to
accurately express richer narrative content within longer videos. However, due
to the lack of evaluation benchmarks specifically designed for long video
generation models, the current assessment of these models primarily relies on
benchmarks with simple narrative prompts (e.g., VBench). To the best of our
knowledge, our proposed NarrLV is the first benchmark to comprehensively
evaluate the Narrative expression capabilities of Long Video generation models.
Inspired by film narrative theory, (i) we first introduce the basic narrative
unit maintaining continuous visual presentation in videos as Temporal Narrative
Atom (TNA), and use its count to quantitatively measure narrative richness.
Guided by three key film narrative elements influencing TNA changes, we
construct an automatic prompt generation pipeline capable of producing
evaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based
on the three progressive levels of narrative content expression, we design an
effective evaluation metric using the MLLM-based question generation and
answering framework. (iii) Finally, we conduct extensive evaluations on
existing long video generation models and the foundation generation models.
Experimental results demonstrate that our metric aligns closely with human
judgments. The derived evaluation outcomes reveal the detailed capability
boundaries of current video generation models in narrative content expression.

</details>


### [165] [Fairness-Aware Grouping for Continuous Sensitive Variables: Application for Debiasing Face Analysis with respect to Skin Tone](https://arxiv.org/abs/2507.11247)
*Veronika Shilova,Emmanuel Malherbe,Giovanni Palma,Laurent Risser,Jean-Michel Loubes*

Main category: cs.CV

TL;DR: The paper proposes a fairness-based grouping method for continuous sensitive attributes to better identify discrimination patterns, validated on synthetic and real datasets, and applied for debiasing with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Current fairness assessments often overlook discrimination in continuous sensitive attributes (e.g., skin color) by relying on predefined groups. This limits the detection of nuanced discrimination patterns.

Method: The authors introduce a fairness-based grouping approach for continuous sensitive attributes, maximizing inter-group variance in discrimination to identify critical subgroups. The method is validated on synthetic datasets and applied to real datasets (CelebA, FFHQ) using skin tone predictions.

Result: The proposed method uncovers nuanced discrimination patterns, remains stable across datasets, and improves fairness in debiasing with minimal impact on accuracy.

Conclusion: The approach effectively addresses limitations of predefined groups, offering a robust and practical solution for fairness assessment and debiasing in industrial applications.

Abstract: Within a legal framework, fairness in datasets and models is typically
assessed by dividing observations into predefined groups and then computing
fairness measures (e.g., Disparate Impact or Equality of Odds with respect to
gender). However, when sensitive attributes such as skin color are continuous,
dividing into default groups may overlook or obscure the discrimination
experienced by certain minority subpopulations. To address this limitation, we
propose a fairness-based grouping approach for continuous (possibly
multidimensional) sensitive attributes. By grouping data according to observed
levels of discrimination, our method identifies the partition that maximizes a
novel criterion based on inter-group variance in discrimination, thereby
isolating the most critical subgroups.
  We validate the proposed approach using multiple synthetic datasets and
demonstrate its robustness under changing population distributions - revealing
how discrimination is manifested within the space of sensitive attributes.
Furthermore, we examine a specialized setting of monotonic fairness for the
case of skin color. Our empirical results on both CelebA and FFHQ, leveraging
the skin tone as predicted by an industrial proprietary algorithm, show that
the proposed segmentation uncovers more nuanced patterns of discrimination than
previously reported, and that these findings remain stable across datasets for
a given model. Finally, we leverage our grouping model for debiasing purpose,
aiming at predicting fair scores with group-by-group post-processing. The
results demonstrate that our approach improves fairness while having minimal
impact on accuracy, thus confirming our partition method and opening the door
for industrial deployment.

</details>


### [166] [MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection](https://arxiv.org/abs/2507.11252)
*Guanghao Wu,Chen Xu,Hai Song,Chong Wang,Qixing Zhang*

Main category: cs.CV

TL;DR: A framework for generating realistic forest fire smoke images using deep learning, addressing data scarcity and improving detection models.


<details>
  <summary>Details</summary>
Motivation: The scarcity of smoke image data from forest fires hinders detection. Current inpainting models struggle with quality and consistency.

Method: Uses pre-trained segmentation and multimodal models for masks/captions, introduces mask-guided architecture, and a new loss function (mask random difference loss). Also employs a multimodal LLM for filtering.

Result: Generated smoke images are realistic and diverse, enhancing forest fire smoke detection model performance.

Conclusion: The proposed framework effectively addresses data scarcity and improves smoke detection, with code publicly available.

Abstract: Smoke is the first visible indicator of a wildfire.With the advancement of
deep learning, image-based smoke detection has become a crucial method for
detecting and preventing forest fires. However, the scarcity of smoke image
data from forest fires is one of the significant factors hindering the
detection of forest fire smoke. Image generation models offer a promising
solution for synthesizing realistic smoke images. However, current inpainting
models exhibit limitations in generating high-quality smoke representations,
particularly manifesting as inconsistencies between synthesized smoke and
background contexts. To solve these problems, we proposed a comprehensive
framework for generating forest fire smoke images. Firstly, we employed the
pre-trained segmentation model and the multimodal model to obtain smoke masks
and image captions.Then, to address the insufficient utilization of masks and
masked images by inpainting models, we introduced a network architecture guided
by mask and masked image features. We also proposed a new loss function, the
mask random difference loss, which enhances the consistency of the generated
effects around the mask by randomly expanding and eroding the mask
edges.Finally, to generate a smoke image dataset using random masks for
subsequent detection tasks, we incorporated smoke characteristics and use a
multimodal large language model as a filtering tool to select diverse and
reasonable smoke images, thereby improving the quality of the synthetic
dataset. Experiments showed that our generated smoke images are realistic and
diverse, and effectively enhance the performance of forest fire smoke detection
models. Code is available at https://github.com/wghr123/MFGDiffusion.

</details>


### [167] [ViewSRD: 3D Visual Grounding via Structured Multi-View Decomposition](https://arxiv.org/abs/2507.11261)
*Ronggang Huang,Haoxin Yang,Yan Cai,Xuemiao Xu,Huaidong Zhang,Shengfeng He*

Main category: cs.CV

TL;DR: ViewSRD improves 3D visual grounding by decomposing complex queries into simpler statements and integrating multi-view features for better spatial reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with complex multi-anchor queries and perspective variations in spatial descriptions.

Method: ViewSRD uses Simple Relation Decoupling (SRD) to simplify queries, Multi-view Textual-Scene Interaction (Multi-TSI) for feature integration, and Textual-Scene Reasoning for unified predictions.

Result: ViewSRD outperforms state-of-the-art methods, especially in complex spatial queries.

Conclusion: The framework effectively addresses challenges in 3D visual grounding through structured decomposition and multi-view integration.

Abstract: 3D visual grounding aims to identify and localize objects in a 3D space based
on textual descriptions. However, existing methods struggle with disentangling
targets from anchors in complex multi-anchor queries and resolving
inconsistencies in spatial descriptions caused by perspective variations. To
tackle these challenges, we propose ViewSRD, a framework that formulates 3D
visual grounding as a structured multi-view decomposition process. First, the
Simple Relation Decoupling (SRD) module restructures complex multi-anchor
queries into a set of targeted single-anchor statements, generating a
structured set of perspective-aware descriptions that clarify positional
relationships. These decomposed representations serve as the foundation for the
Multi-view Textual-Scene Interaction (Multi-TSI) module, which integrates
textual and scene features across multiple viewpoints using shared, Cross-modal
Consistent View Tokens (CCVTs) to preserve spatial correlations. Finally, a
Textual-Scene Reasoning module synthesizes multi-view predictions into a
unified and robust 3D visual grounding. Experiments on 3D visual grounding
datasets show that ViewSRD significantly outperforms state-of-the-art methods,
particularly in complex queries requiring precise spatial differentiation.

</details>


### [168] [YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery](https://arxiv.org/abs/2507.11267)
*Aon Safdar,Usman Akram,Waseem Anwar,Basit Malik,Mian Ibad Ali*

Main category: cs.CV

TL;DR: The paper introduces YOLOatr, a modified YOLOv5s-based model for improved Automatic Target Recognition (ATR) in Thermal Infrared imagery, addressing domain-specific challenges and achieving 99.6% accuracy.


<details>
  <summary>Details</summary>
Motivation: ATR in Thermal Infrared imagery faces challenges like limited datasets, hardware constraints, and environmental variations, causing current deep learning models to underperform.

Method: Proposes YOLOatr, a modified YOLOv5s with optimized detection heads, feature fusion, and custom augmentation, tested on the DSIAC MWIR dataset.

Result: YOLOatr achieves state-of-the-art ATR performance of up to 99.6% accuracy.

Conclusion: The proposed YOLOatr model effectively addresses ATR challenges in Thermal Infrared imagery, outperforming existing methods.

Abstract: Automatic Target Detection (ATD) and Recognition (ATR) from Thermal Infrared
(TI) imagery in the defense and surveillance domain is a challenging computer
vision (CV) task in comparison to the commercial autonomous vehicle perception
domain. Limited datasets, peculiar domain-specific and TI modality-specific
challenges, i.e., limited hardware, scale invariance issues due to greater
distances, deliberate occlusion by tactical vehicles, lower sensor resolution
and resultant lack of structural information in targets, effects of weather,
temperature, and time of day variations, and varying target to clutter ratios
all result in increased intra-class variability and higher inter-class
similarity, making accurate real-time ATR a challenging CV task. Resultantly,
contemporary state-of-the-art (SOTA) deep learning architectures underperform
in the ATR domain. We propose a modified anchor-based single-stage detector,
called YOLOatr, based on a modified YOLOv5s, with optimal modifications to the
detection heads, feature fusion in the neck, and a custom augmentation profile.
We evaluate the performance of our proposed model on a comprehensive DSIAC MWIR
dataset for real-time ATR over both correlated and decorrelated testing
protocols. The results demonstrate that our proposed model achieves
state-of-the-art ATR performance of up to 99.6%.

</details>


### [169] [Tomato Multi-Angle Multi-Pose Dataset for Fine-Grained Phenotyping](https://arxiv.org/abs/2507.11279)
*Yujie Zhang,Sabine Struckmeyer,Andreas Kolb,Sven Reichardt*

Main category: cs.CV

TL;DR: TomatoMAP is a dataset for Solanum lycopersicum using IoT-based imaging, offering annotated images and AI validation for accurate plant phenotyping.


<details>
  <summary>Details</summary>
Motivation: Overcome observer bias and inconsistencies in traditional plant phenotyping methods.

Method: Developed TomatoMAP with standardized IoT imaging, manual annotations, and validated using deep learning models (MobileNetv3, YOLOv11, MaskRCNN).

Result: AI models trained on TomatoMAP achieve accuracy and speed comparable to human experts, confirmed by Cohen's Kappa and inter-rater agreement.

Conclusion: TomatoMAP enables reliable, automated fine-grained plant phenotyping, addressing limitations of traditional methods.

Abstract: Observer bias and inconsistencies in traditional plant phenotyping methods
limit the accuracy and reproducibility of fine-grained plant analysis. To
overcome these challenges, we developed TomatoMAP, a comprehensive dataset for
Solanum lycopersicum using an Internet of Things (IoT) based imaging system
with standardized data acquisition protocols. Our dataset contains 64,464 RGB
images that capture 12 different plant poses from four camera elevation angles.
Each image includes manually annotated bounding boxes for seven regions of
interest (ROIs), including leaves, panicle, batch of flowers, batch of fruits,
axillary shoot, shoot and whole plant area, along with 50 fine-grained growth
stage classifications based on the BBCH scale. Additionally, we provide 3,616
high-resolution image subset with pixel-wise semantic and instance segmentation
annotations for fine-grained phenotyping. We validated our dataset using a
cascading model deep learning framework combining MobileNetv3 for
classification, YOLOv11 for object detection, and MaskRCNN for segmentation.
Through AI vs. Human analysis involving five domain experts, we demonstrate
that the models trained on our dataset achieve accuracy and speed comparable to
the experts. Cohen's Kappa and inter-rater agreement heatmap confirm the
reliability of automated fine-grained phenotyping using our approach.

</details>


### [170] [Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers](https://arxiv.org/abs/2507.11287)
*An-Lun Liu,Yu-Wei Chao,Yi-Ting Chen*

Main category: cs.CV

TL;DR: The paper introduces task-oriented human grasp synthesis, using task-aware contact maps to improve grasp quality and task performance.


<details>
  <summary>Details</summary>
Motivation: Traditional grasp synthesis lacks task and context awareness, limiting its practicality.

Method: A two-stage pipeline: (1) constructs task-aware contact maps using scene and task info, (2) synthesizes grasps from these maps.

Result: Experiments show significant improvements in grasp quality and task performance over existing methods.

Conclusion: Task-aware contact maps are crucial for accurate, task-oriented human grasps.

Abstract: In this paper, we study task-oriented human grasp synthesis, a new grasp
synthesis task that demands both task and context awareness. At the core of our
method is the task-aware contact maps. Unlike traditional contact maps that
only reason about the manipulated object and its relation with the hand, our
enhanced maps take into account scene and task information. This comprehensive
map is critical for hand-object interaction, enabling accurate grasping poses
that align with the task. We propose a two-stage pipeline that first constructs
a task-aware contact map informed by the scene and task. In the subsequent
stage, we use this contact map to synthesize task-oriented human grasps. We
introduce a new dataset and a metric for the proposed task to evaluate our
approach. Our experiments validate the importance of modeling both scene and
task, demonstrating significant improvements over existing methods in both
grasp quality and task performance. See our project page for more details:
https://hcis-lab.github.io/TOHGS/

</details>


### [171] [Detección y Cuantificación de Erosión Fluvial con Visión Artificial](https://arxiv.org/abs/2507.11301)
*Paúl Maji,Marlon Túquerres,Stalin Valencia,Marcela Valenzuela,Christian Mejia-Escobar*

Main category: cs.CV

TL;DR: The paper proposes an AI-based method using YOLOv11 and a combined dataset of photos and LiDAR images to automatically detect and quantify fluvial erosion, achieving 70% accuracy. The EROSCAN web app is developed for practical use.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for detecting fluvial erosion are manual and require expertise. The study aims to automate this process for efficiency and accessibility.

Method: Uses YOLOv11, fine-tuned and trained with photos and LiDAR images, segmented and labeled via Roboflow.

Result: Achieves 70% accuracy in detecting erosion patterns and reliably estimates eroded areas in pixels and square meters.

Conclusion: The EROSCAN system provides an efficient, automated tool for erosion detection and area estimation, aiding risk management and planning.

Abstract: Fluvial erosion is a natural process that can generate significant impacts on
soil stability and strategic infrastructures. The detection and monitoring of
this phenomenon is traditionally addressed by photogrammetric methods and
analysis in geographic information systems. These tasks require specific
knowledge and intensive manual processing. This study proposes an artificial
intelligence-based approach for automatic identification of eroded zones and
estimation of their area. The state-of-the-art computer vision model YOLOv11,
adjusted by fine-tuning and trained with photographs and LiDAR images, is used.
This combined dataset was segmented and labeled using the Roboflow platform.
Experimental results indicate efficient detection of erosion patterns with an
accuracy of 70%, precise identification of eroded areas and reliable
calculation of their extent in pixels and square meters. As a final product,
the EROSCAN system has been developed, an interactive web application that
allows users to upload images and obtain automatic segmentations of fluvial
erosion, together with the estimated area. This tool optimizes the detection
and quantification of the phenomenon, facilitating decision making in risk
management and territorial planning.

</details>


### [172] [A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction](https://arxiv.org/abs/2507.11321)
*Haoxuan Qu,Yujun Cai,Hossein Rahmani,Ajay Kumar,Junsong Yuan,Jun Liu*

Main category: cs.CV

TL;DR: The paper proposes a novel framework for Gaussian Splatting (GS) that incorporates multiple types of geometrical primitives to improve surface reconstruction quality, addressing the limitation of existing GS methods that use only a single primitive type.


<details>
  <summary>Details</summary>
Motivation: Existing GS-based methods use a single type of splatting primitive (Gaussian ellipse or ellipsoid), which is insufficient for high-quality representation of complex 3D object surfaces.

Method: The framework introduces a compositional splatting strategy, a mixed-primitive-based initialization, and a vertex pruning mechanism to leverage multiple primitive types in the GS pipeline.

Result: Extensive experiments demonstrate the framework's efficacy and accurate surface reconstruction performance.

Conclusion: The proposed framework enhances GS by enabling the use of diverse primitives, improving surface reconstruction quality.

Abstract: Recently, Gaussian Splatting (GS) has received a lot of attention in surface
reconstruction. However, while 3D objects can be of complex and diverse shapes
in the real world, existing GS-based methods only limitedly use a single type
of splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent
object surfaces during their reconstruction. In this paper, we highlight that
this can be insufficient for object surfaces to be represented in high quality.
Thus, we propose a novel framework that, for the first time, enables Gaussian
Splatting to incorporate multiple types of (geometrical) primitives during its
surface reconstruction process. Specifically, in our framework, we first
propose a compositional splatting strategy, enabling the splatting and
rendering of different types of primitives in the Gaussian Splatting pipeline.
In addition, we also design our framework with a mixed-primitive-based
initialization strategy and a vertex pruning mechanism to further promote its
surface representation learning process to be well executed leveraging
different types of primitives. Extensive experiments show the efficacy of our
framework and its accurate surface reconstruction performance.

</details>


### [173] [MonoMVSNet: Monocular Priors Guided Multi-View Stereo Network](https://arxiv.org/abs/2507.11333)
*Jianfei Jiang,Qiankun Liu,Haochen Yu,Hongyuan Liu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: MonoMVSNet integrates monocular depth estimation into MVS to improve performance in challenging regions like textureless and reflective surfaces.


<details>
  <summary>Details</summary>
Motivation: Existing MVS methods struggle in challenging regions due to failed feature matching, while monocular depth estimation excels there.

Method: MonoMVSNet combines monocular features and depth with multi-view geometry using attention and dynamic depth candidate updates.

Result: Achieves state-of-the-art performance on DTU and Tanks-and-Temples datasets, ranking first on Intermediate and Advanced benchmarks.

Conclusion: MonoMVSNet effectively bridges the gap between monocular and multi-view depth estimation, enhancing robustness in challenging scenarios.

Abstract: Learning-based Multi-View Stereo (MVS) methods aim to predict depth maps for
a sequence of calibrated images to recover dense point clouds. However,
existing MVS methods often struggle with challenging regions, such as
textureless regions and reflective surfaces, where feature matching fails. In
contrast, monocular depth estimation inherently does not require feature
matching, allowing it to achieve robust relative depth estimation in these
regions. To bridge this gap, we propose MonoMVSNet, a novel monocular feature
and depth guided MVS network that integrates powerful priors from a monocular
foundation model into multi-view geometry. Firstly, the monocular feature of
the reference view is integrated into source view features by the attention
mechanism with a newly designed cross-view position encoding. Then, the
monocular depth of the reference view is aligned to dynamically update the
depth candidates for edge regions during the sampling procedure. Finally, a
relative consistency loss is further designed based on the monocular depth to
supervise the depth prediction. Extensive experiments demonstrate that
MonoMVSNet achieves state-of-the-art performance on the DTU and
Tanks-and-Temples datasets, ranking first on the Tanks-and-Temples Intermediate
and Advanced benchmarks. The source code is available at
https://github.com/JianfeiJ/MonoMVSNet.

</details>


### [174] [UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks](https://arxiv.org/abs/2507.11336)
*Peiran Wu,Yunze Liu,Zhengdong Zhu,Enmin Zhou,Shawn Shen*

Main category: cs.CV

TL;DR: The paper introduces UGC-VideoCap, a benchmark and model framework for omnimodal captioning of user-generated videos, addressing the lack of audio-visual integration in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing video captioning benchmarks and models are visual-centric, ignoring audio's role in scene dynamics and narrative context, limiting multimodal understanding.

Method: UGC-VideoCap includes 1000 TikTok videos annotated via a three-stage human-in-the-loop pipeline and 4000 QA pairs. The proposed UGC-VideoCaptioner(3B) model uses a two-stage training strategy (supervised fine-tuning and GRPO).

Result: The benchmark and model provide a high-quality, data-efficient solution for omnimodal video captioning in real-world UGC settings.

Conclusion: UGC-VideoCap and the proposed model advance multimodal video understanding by integrating audio and visual modalities effectively.

Abstract: Real-world user-generated videos, especially on platforms like TikTok, often
feature rich and intertwined audio visual content. However, existing video
captioning benchmarks and models remain predominantly visual centric,
overlooking the crucial role of audio in conveying scene dynamics, speaker
intent, and narrative context. This lack of omni datasets and lightweight,
capable models hampers progress in fine grained, multimodal video
understanding. To address these challenges, we introduce UGC-VideoCap, a new
benchmark and model framework specifically designed for detailed omnimodal
captioning of short form user-generated videos. Unlike prior datasets,
UGC-VideoCap emphasizes balanced integration of audio and visual modalities,
featuring 1000 TikTok videos annotated through a structured three stage
human-in-the-loop pipeline covering audio only, visual only, and joint audio
visual semantics. The benchmark also includes 4000 carefully crafted QA pairs
probing both unimodal and cross modal understanding. Alongside the dataset, we
propose UGC-VideoCaptioner(3B), a 3B parameter captioning model distilled from
Gemini 2.5 Flash. Using a novel two-stage training strategy supervised fine
tuning followed by Group Relative Policy Optimization (GRPO), our approach
enables efficient adaptation from limited data while maintaining competitive
performance. Together, our benchmark and model offer a high-quality foundation
and a data-efficient solution for advancing omnimodal video captioning in
unconstrained real-world UGC settings.

</details>


### [175] [Attributes Shape the Embedding Space of Face Recognition Models](https://arxiv.org/abs/2507.11372)
*Pierrick Leroy,Antonio Mastropietro,Marco Nurisso,Francesco Vaccarino*

Main category: cs.CV

TL;DR: The paper explores the geometric structure in face recognition embedding spaces, influenced by facial and image attributes, and introduces a physics-inspired metric to evaluate model invariance to these attributes.


<details>
  <summary>Details</summary>
Motivation: Despite progress in face recognition using deep neural networks, existing contrastive losses ignore interpretable attributes (e.g., hair color, image contrast). The study aims to analyze and quantify the dependence or invariance of models to such attributes.

Method: The authors propose a geometric approach and a physics-inspired alignment metric to evaluate attribute invariance. They test this on controlled models and widely used face recognition models fine-tuned with synthetic data for attribute augmentation.

Result: Findings show varying degrees of invariance across attributes, revealing model strengths and weaknesses and enhancing interpretability.

Conclusion: The study provides insights into attribute-based invariance in face recognition models, aiding deeper understanding and interpretability.

Abstract: Face Recognition (FR) tasks have made significant progress with the advent of
Deep Neural Networks, particularly through margin-based triplet losses that
embed facial images into high-dimensional feature spaces. During training,
these contrastive losses focus exclusively on identity information as labels.
However, we observe a multiscale geometric structure emerging in the embedding
space, influenced by interpretable facial (e.g., hair color) and image
attributes (e.g., contrast). We propose a geometric approach to describe the
dependence or invariance of FR models to these attributes and introduce a
physics-inspired alignment metric. We evaluate the proposed metric on
controlled, simplified models and widely used FR models fine-tuned with
synthetic data for targeted attribute augmentation. Our findings reveal that
the models exhibit varying degrees of invariance across different attributes,
providing insight into their strengths and weaknesses and enabling deeper
interpretability. Code available here:
https://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs

</details>


### [176] [Implementing Adaptations for Vision AutoRegressive Model](https://arxiv.org/abs/2507.11441)
*Kaif Shaikh,Antoni Kowalczuk,Franziska Boenisch,Adam Dziedzic*

Main category: cs.CV

TL;DR: VAR outperforms DMs in non-DP image generation adaptations but struggles with DP adaptations, highlighting a need for further research in private VAR adaptations.


<details>
  <summary>Details</summary>
Motivation: To explore and benchmark adaptation strategies for VAR, particularly for downstream tasks like medical data generation, and compare them to DM adaptations, including DP methods.

Method: Implemented and benchmarked various adaptation strategies for VAR, comparing them to state-of-the-art DM adaptation techniques.

Result: VAR performs better than DMs for non-DP adaptations but shows limitations in DP adaptations.

Conclusion: Further research is needed to improve DP adaptations for VAR, as current methods underperform compared to DMs.

Abstract: Vision AutoRegressive model (VAR) was recently introduced as an alternative
to Diffusion Models (DMs) in image generation domain. In this work we focus on
its adaptations, which aim to fine-tune pre-trained models to perform specific
downstream tasks, like medical data generation. While for DMs there exist many
techniques, adaptations for VAR remain underexplored. Similarly, differentially
private (DP) adaptations-ones that aim to preserve privacy of the adaptation
data-have been extensively studied for DMs, while VAR lacks such solutions. In
our work, we implement and benchmark many strategies for VAR, and compare them
to state-of-the-art DM adaptation strategies. We observe that VAR outperforms
DMs for non-DP adaptations, however, the performance of DP suffers, which
necessitates further research in private adaptations for VAR. Code is available
at https://github.com/sprintml/finetuning_var_dp.

</details>


### [177] [COLI: A Hierarchical Efficient Compressor for Large Images](https://arxiv.org/abs/2507.11443)
*Haoran Wang,Hanyu Pei,Yang Lyu,Kai Zhang,Li Li,Feng-Lei Fan*

Main category: cs.CV

TL;DR: COLI is a novel framework using Neural Representations for Videos (NeRV) to improve compression of large images, addressing speed and ratio issues in INR-based methods.


<details>
  <summary>Details</summary>
Motivation: Traditional compression methods lose critical details, and data-driven approaches lack generalizability. INRs offer promise but face speed and ratio challenges.

Method: COLI accelerates INR convergence via pretraining-finetuning, mixed-precision training, and parallelizable loss. It also introduces Hyper-Compression for better ratios.

Result: COLI achieves better PSNR and SSIM at lower bpp, with 4x faster NeRV training on medical imaging datasets.

Conclusion: COLI effectively addresses INR limitations, offering efficient, high-quality compression for large images.

Abstract: The escalating adoption of high-resolution, large-field-of-view imagery
amplifies the need for efficient compression methodologies. Conventional
techniques frequently fail to preserve critical image details, while
data-driven approaches exhibit limited generalizability. Implicit Neural
Representations (INRs) present a promising alternative by learning continuous
mappings from spatial coordinates to pixel intensities for individual images,
thereby storing network weights rather than raw pixels and avoiding the
generalization problem. However, INR-based compression of large images faces
challenges including slow compression speed and suboptimal compression ratios.
To address these limitations, we introduce COLI (Compressor for Large Images),
a novel framework leveraging Neural Representations for Videos (NeRV). First,
recognizing that INR-based compression constitutes a training process, we
accelerate its convergence through a pretraining-finetuning paradigm,
mixed-precision training, and reformulation of the sequential loss into a
parallelizable objective. Second, capitalizing on INRs' transformation of image
storage constraints into weight storage, we implement Hyper-Compression, a
novel post-training technique to substantially enhance compression ratios while
maintaining minimal output distortion. Evaluations across two medical imaging
datasets demonstrate that COLI consistently achieves competitive or superior
PSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while
accelerating NeRV training by up to 4 times.

</details>


### [178] [HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry Synthesis and Controllable Editing](https://arxiv.org/abs/2507.11474)
*Pan Du,Mingqi Xu,Xiaozhi Zhu,Jian-xun Wang*

Main category: cs.CV

TL;DR: HUG-VAS is a hierarchical NURBS generative model for vascular geometry synthesis, combining NURBS parameterization with diffusion-based modeling to create realistic aortic geometries.


<details>
  <summary>Details</summary>
Motivation: Traditional SSM methods are limited by linear assumptions, hindering their ability to model complex vascular topologies like multi-branch structures.

Method: HUG-VAS uses a hierarchical architecture with a denoising diffusion model for centerlines and a guided diffusion model for radial profiles, trained on 21 patient samples.

Result: The model generates anatomically accurate aortas with biomarker distributions matching the original dataset, supporting zero-shot conditional generation.

Conclusion: HUG-VAS bridges image-derived priors with generative shape modeling, enabling applications like segmentation, reconstruction, and device optimization.

Abstract: Accurate characterization of vascular geometry is essential for
cardiovascular diagnosis and treatment planning. Traditional statistical shape
modeling (SSM) methods rely on linear assumptions, limiting their expressivity
and scalability to complex topologies such as multi-branch vascular structures.
We introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular
geometry Synthesis, which integrates NURBS surface parameterization with
diffusion-based generative modeling to synthesize realistic, fine-grained
aortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates
anatomically faithful aortas with supra-aortic branches, yielding biomarker
distributions that closely match those of the original dataset. HUG-VAS adopts
a hierarchical architecture comprising a denoising diffusion model that
generates centerlines and a guided diffusion model that synthesizes radial
profiles conditioned on those centerlines, thereby capturing two layers of
anatomical variability. Critically, the framework supports zero-shot
conditional generation from image-derived priors, enabling practical
applications such as interactive semi-automatic segmentation, robust
reconstruction under degraded imaging conditions, and implantable device
optimization. To our knowledge, HUG-VAS is the first SSM framework to bridge
image-derived priors with generative shape modeling via a unified integration
of NURBS parameterization and hierarchical diffusion processes.

</details>


### [179] [C-FBI: A Combinatorial method using Convolutions for Circle Fitting in Blurry Images](https://arxiv.org/abs/2507.11476)
*Esteban Román Catafau,Torbjörn E. M. Nordling*

Main category: cs.CV

TL;DR: 3C-FBI is a robust circle detection algorithm combining combinatorial sampling and convolution-based density estimation, achieving high accuracy and real-time performance in degraded imaging conditions.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of robust circle detection and fitting in degraded imaging conditions, particularly for applications like medical imaging and industrial inspection.

Method: Combines combinatorial edge pixel sampling with convolution-based density estimation in parameter space.

Result: Achieves state-of-the-art accuracy (Jaccard index 0.896) and real-time performance (40.3 fps), outperforming classical methods like RCD. Maintains high accuracy even with outliers and low resolutions.

Conclusion: 3C-FBI is ideal for medical imaging, robotics, and industrial inspection due to its accuracy, speed, and robustness.

Abstract: This paper addresses the fundamental computer vision challenge of robust
circle detection and fitting in degraded imaging conditions. We present
Combinatorial Convolution-based Circle Fitting for Blurry Images (3C-FBI), an
algorithm that bridges the gap between circle detection and precise parametric
fitting by combining (1) efficient combinatorial edge pixel (edgel) sampling
and (2) convolution-based density estimation in parameter space.
  We evaluate 3C-FBI across three experimental frameworks: (1) real-world
medical data from Parkinson's disease assessments (144 frames from 36 videos),
(2) controlled synthetic data following established circle-fitting benchmarks,
and (3) systematic analysis across varying spatial resolutions and outlier
contamination levels. Results show that 3C-FBI achieves state-of-the-art
accuracy (Jaccard index 0.896) while maintaining real-time performance (40.3
fps), significantly outperforming classical methods like RCD (6.8 fps) on a
standard CPU (i7-10875H). It maintains near-perfect accuracy (Jaccard almost
1.0) at high resolutions (480x480) and reliable performance (Jaccard higher
than 0.95) down to 160x160 with up to 20% outliers.
  In extensive synthetic testing, 3C-FBI achieves a mean Jaccard Index of 0.989
across contamination levels, comparable to modern methods like Qi et al. (2024,
0.991), and surpassing RHT (0.964). This combination of accuracy, speed, and
robustness makes 3C-FBI ideal for medical imaging, robotics, and industrial
inspection under challenging conditions.

</details>


### [180] [COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation](https://arxiv.org/abs/2507.11488)
*Pakizar Shamoi,Nuray Toganas,Muragul Muratbekova,Elnara Kadyrgali,Adilet Yerkin,Ayan Igali,Malika Ziyada,Ayana Adilova,Aron Karatayev,Yerdauit Torekhan*

Main category: cs.CV

TL;DR: The paper introduces COLIBRI, a fuzzy color model aligning computational color representation with human perception, validated through large-scale experiments.


<details>
  <summary>Details</summary>
Motivation: Bridging the gap between computational color models and human visual perception, as current models like RGB, HSV, and LAB lack perceptual alignment.

Method: A three-phase approach: identifying distinguishable color stimuli, conducting a large-scale human categorization survey (n=2496), and using fuzzy logic to model perceptual uncertainty.

Result: COLIBRI outperforms traditional models (RGB, HSV, LAB) in aligning with human perception, with adaptable membership functions.

Conclusion: The model is impactful for design, AI, marketing, and HCI, offering perceptually accurate color representation.

Abstract: Colors are omnipresent in today's world and play a vital role in how humans
perceive and interact with their surroundings. However, it is challenging for
computers to imitate human color perception. This paper introduces the Human
Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based
Representation and Interpretation), designed to bridge the gap between
computational color representations and human visual perception. The proposed
model uses fuzzy sets and logic to create a framework for color categorization.
Using a three-phase experimental approach, the study first identifies
distinguishable color stimuli for hue, saturation, and intensity through
preliminary experiments, followed by a large-scale human categorization survey
involving more than 1000 human subjects. The resulting data are used to extract
fuzzy partitions and generate membership functions that reflect real-world
perceptual uncertainty. The model incorporates a mechanism for adaptation that
allows refinement based on feedback and contextual changes. Comparative
evaluations demonstrate the model's alignment with human perception compared to
traditional color models, such as RGB, HSV, and LAB. To the best of our
knowledge, no previous research has documented the construction of a model for
color attribute specification based on a sample of this size or a comparable
sample of the human population (n = 2496). Our findings are significant for
fields such as design, artificial intelligence, marketing, and human-computer
interaction, where perceptually relevant color representation is critical.

</details>


### [181] [CATVis: Context-Aware Thought Visualization](https://arxiv.org/abs/2507.11522)
*Tariq Mehmood,Hamza Ahmad,Muhammad Haroon Shakeel,Murtaza Taj*

Main category: cs.CV

TL;DR: A novel 5-stage framework decodes visual representations from EEG signals, outperforming state-of-the-art methods in accuracy and image quality.


<details>
  <summary>Details</summary>
Motivation: EEG-based BCIs struggle with decoding visual representations due to noise and complexity.

Method: A 5-stage framework involving EEG encoding, cross-modal alignment, caption refinement, weighted interpolation, and image generation.

Result: Outperforms SOTA by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy, and reduces Fréchet Inception Distance by 36.61%.

Conclusion: The framework enables high-quality, context-aware EEG-to-image generation with superior semantic alignment.

Abstract: EEG-based brain-computer interfaces (BCIs) have shown promise in various
applications, such as motor imagery and cognitive state monitoring. However,
decoding visual representations from EEG signals remains a significant
challenge due to their complex and noisy nature. We thus propose a novel
5-stage framework for decoding visual representations from EEG signals: (1) an
EEG encoder for concept classification, (2) cross-modal alignment of EEG and
text embeddings in CLIP feature space, (3) caption refinement via re-ranking,
(4) weighted interpolation of concept and caption embeddings for richer
semantics, and (5) image generation using a pre-trained Stable Diffusion model.
We enable context-aware EEG-to-image generation through cross-modal alignment
and re-ranking. Experimental results demonstrate that our method generates
high-quality images aligned with visual stimuli, outperforming SOTA approaches
by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and
reducing Fr\'echet Inception Distance by 36.61%, indicating superior semantic
alignment and image quality.

</details>


### [182] [CharaConsist: Fine-Grained Consistent Character Generation](https://arxiv.org/abs/2507.11533)
*Mengyu Wang,Henghui Ding,Jianing Peng,Yao Zhao,Yunpeng Chen,Yunchao Wei*

Main category: cs.CV

TL;DR: CharaConsist improves text-to-image generation consistency for foreground and background using point-tracking attention and adaptive token merge, addressing prior limitations.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail in maintaining consistent background details and struggle with identity/clothing consistency during large motion variations.

Method: Proposes CharaConsist with point-tracking attention, adaptive token merge, and decoupled foreground/background control.

Result: Achieves fine-grained consistency for both foreground and background, supporting continuous and discrete shots.

Conclusion: CharaConsist enhances text-to-image DiT models, offering high-quality outputs for broader real-world applications.

Abstract: In text-to-image generation, producing a series of consistent contents that
preserve the same identity is highly valuable for real-world applications.
Although a few works have explored training-free methods to enhance the
consistency of generated subjects, we observe that they suffer from the
following problems. First, they fail to maintain consistent background details,
which limits their applicability. Furthermore, when the foreground character
undergoes large motion variations, inconsistencies in identity and clothing
details become evident. To address these problems, we propose CharaConsist,
which employs point-tracking attention and adaptive token merge along with
decoupled control of the foreground and background. CharaConsist enables
fine-grained consistency for both foreground and background, supporting the
generation of one character in continuous shots within a fixed scene or in
discrete shots across different scenes. Moreover, CharaConsist is the first
consistent generation method tailored for text-to-image DiT model. Its ability
to maintain fine-grained consistency, combined with the larger capacity of
latest base model, enables it to produce high-quality visual outputs,
broadening its applicability to a wider range of real-world scenarios. The
source code has been released at https://github.com/Murray-Wang/CharaConsist

</details>


### [183] [Streaming 4D Visual Geometry Transformer](https://arxiv.org/abs/2507.11539)
*Dong Zhuo,Wenzhao Zheng,Jiahe Guo,Yuqi Wu,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: A streaming 4D visual geometry transformer is proposed for real-time 4D reconstruction from videos, using causal attention and implicit memory for efficiency.


<details>
  <summary>Details</summary>
Motivation: To enable interactive and real-time 4D spatial-temporal geometry reconstruction from videos, addressing the challenge of processing long sequences efficiently.

Method: Employs a causal transformer architecture with temporal causal attention and cached historical keys/values as implicit memory. Knowledge is distilled from a dense bidirectional transformer (VGGT) for training.

Result: Achieves competitive performance with increased inference speed in online scenarios, enabling scalable and interactive 4D vision systems.

Conclusion: The proposed model efficiently handles real-time 4D reconstruction while maintaining spatial consistency, paving the way for practical applications.

Abstract: Perceiving and reconstructing 4D spatial-temporal geometry from videos is a
fundamental yet challenging computer vision task. To facilitate interactive and
real-time applications, we propose a streaming 4D visual geometry transformer
that shares a similar philosophy with autoregressive large language models. We
explore a simple and efficient design and employ a causal transformer
architecture to process the input sequence in an online manner. We use temporal
causal attention and cache the historical keys and values as implicit memory to
enable efficient streaming long-term 4D reconstruction. This design can handle
real-time 4D reconstruction by incrementally integrating historical information
while maintaining high-quality spatial consistency. For efficient training, we
propose to distill knowledge from the dense bidirectional visual geometry
grounded transformer (VGGT) to our causal model. For inference, our model
supports the migration of optimized efficient attention operator (e.g.,
FlashAttention) from the field of large language models. Extensive experiments
on various 4D geometry perception benchmarks demonstrate that our model
increases the inference speed in online scenarios while maintaining competitive
performance, paving the way for scalable and interactive 4D vision systems.
Code is available at: https://github.com/wzzheng/StreamVGGT.

</details>


### [184] [Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation](https://arxiv.org/abs/2507.11540)
*Zhen Xu,Hongyu Zhou,Sida Peng,Haotong Lin,Haoyu Guo,Jiahao Shao,Peishan Yang,Qinglin Yang,Sheng Miao,Xingyi He,Yifan Wang,Yue Wang,Ruizhen Hu,Yiyi Liao,Xiaowei Zhou,Hujun Bao*

Main category: cs.CV

TL;DR: A survey on depth estimation in 3D computer vision, focusing on the evolution of deep learning methods and the potential of foundation models to overcome limitations of traditional and recent approaches.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of traditional hardware-based depth estimation (cost, resolution, environmental sensitivity) and challenges in vision-based methods (generalization, stability).

Method: Surveys deep learning architectures and paradigms for depth estimation (monocular, stereo, multi-view, monocular video) and explores large-scale datasets.

Result: Identifies key architectures and training strategies for robust depth foundation models with zero-shot generalization.

Conclusion: Highlights the potential of depth foundation models and provides insights for future research and applications.

Abstract: Depth estimation is a fundamental task in 3D computer vision, crucial for
applications such as 3D reconstruction, free-viewpoint rendering, robotics,
autonomous driving, and AR/VR technologies. Traditional methods relying on
hardware sensors like LiDAR are often limited by high costs, low resolution,
and environmental sensitivity, limiting their applicability in real-world
scenarios. Recent advances in vision-based methods offer a promising
alternative, yet they face challenges in generalization and stability due to
either the low-capacity model architectures or the reliance on domain-specific
and small-scale datasets. The emergence of scaling laws and foundation models
in other domains has inspired the development of "depth foundation models":
deep neural networks trained on large datasets with strong zero-shot
generalization capabilities. This paper surveys the evolution of deep learning
architectures and paradigms for depth estimation across the monocular, stereo,
multi-view, and monocular video settings. We explore the potential of these
models to address existing challenges and provide a comprehensive overview of
large-scale datasets that can facilitate their development. By identifying key
architectures and training strategies, we aim to highlight the path towards
robust depth foundation models, offering insights into their future research
and applications.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [185] [Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions](https://arxiv.org/abs/2507.10577)
*Logé Cécile,Ghori Rehan*

Main category: cs.CL

TL;DR: The paper introduces an AI-powered system with two agents, Truth Sleuth and Trend Bender, to fact-check YouTube videos and engage users in comments to combat misinformation.


<details>
  <summary>Details</summary>
Motivation: Misinformation spreads rapidly on platforms like YouTube, necessitating innovative solutions to fact-check and counteract misleading narratives.

Method: The system uses Retrieval-Augmented Generation (RAG) for fact-checking (Truth Sleuth) and generates persuasive comments (Trend Bender) to engage users and improve iteratively.

Result: Experiments show high accuracy in fact-checking and potential to influence user perspectives in real-world YouTube deployments.

Conclusion: AI-driven interventions can effectively combat misinformation and foster informed online discussions.

Abstract: Misinformation poses a significant threat in today's digital world, often
spreading rapidly through platforms like YouTube. This paper introduces a novel
approach to combating misinformation by developing an AI-powered system that
not only fact-checks claims made in YouTube videos but also actively engages
users in the comment section and challenge misleading narratives. Our system
comprises two main agents: Truth Sleuth and Trend Bender.
  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented
Generation (RAG) approach - drawing on sources like Wikipedia, Google Search,
Google FactCheck - to accurately assess their veracity and generates a nuanced
and comprehensive report. Through rigorous prompt engineering, Trend Bender
leverages this report along with a curated corpus of relevant articles to
generate insightful and persuasive comments designed to stimulate a productive
debate. With a carefully set up self-evaluation loop, this agent is able to
iteratively improve its style and refine its output.
  We demonstrate the system's capabilities through experiments on established
benchmark datasets and a real-world deployment on YouTube, showcasing its
potential to engage users and potentially influence perspectives. Our findings
highlight the high accuracy of our fact-checking agent, and confirm the
potential of AI-driven interventions in combating misinformation and fostering
a more informed online space.

</details>


### [186] [An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation](https://arxiv.org/abs/2507.10580)
*Vimaleswar A,Prabhu Nandan Sahu,Nilesh Kumar Sahu,Haroon R Lone*

Main category: cs.CL

TL;DR: EmoSApp is an offline, smartphone-based mental health app using fine-tuned LLMs for empathetic, coherent support without internet dependency.


<details>
  <summary>Details</summary>
Motivation: Address challenges like limited accessibility, connectivity, and privacy in digital mental health platforms by creating an offline solution.

Method: Fine-tuned and quantized LLaMA-3.2-1B-Instruct model on a custom mental health QA dataset, deployed on-device using Torchtune and Executorch.

Result: Qualitative evaluation shows coherent, empathetic responses; quantitative benchmarks confirm efficacy in low-resource settings.

Conclusion: EmoSApp exemplifies secure, portable AI-driven mental health solutions, setting a blueprint for future innovations.

Abstract: Mental health plays a crucial role in the overall well-being of an
individual. In recent years, digital platforms have been increasingly used to
expand mental health and emotional support. However, there are persistent
challenges related to limited user accessibility, internet connectivity, and
data privacy, which highlight the need for an offline, smartphone-based
solution. To address these challenges, we propose EmoSApp (Emotional Support
App): an entirely offline, smartphone-based conversational app designed for
mental health and emotional support. The system leverages Large Language Models
(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and
Executorch for resource-constrained devices, allowing all inferences to occur
on the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned
the LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of
14,582 mental-health QA pairs, along with the multi-turn conversational data.
  Through qualitative human evaluation with the student population, we
demonstrate that EmoSApp has the ability to respond coherently, empathetically,
maintain interactive dialogue, and provide relevant suggestions to user's
mental health problems. Additionally, quantitative evaluations on nine standard
commonsense and reasoning benchmarks demonstrate the efficacy of our
fine-tuned, quantized model in low-resource settings. By prioritizing on-device
deployment and specialized domain adaptation, EmoSApp serves as a blueprint for
future innovations in portable, secure, and highly tailored AI-driven mental
health solutions.

</details>


### [187] [Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis](https://arxiv.org/abs/2507.10582)
*Anders Ledberg,Anna Thalén*

Main category: cs.CL

TL;DR: A modular toolchain using open-weight LLMs processes sensitive, unstructured text for embedding-based analysis, ensuring privacy and standardization.


<details>
  <summary>Details</summary>
Motivation: To enable large-scale research in public health and social sciences by addressing privacy and heterogeneity challenges in unstructured text data.

Method: Uses LLM prompting for standardization, summarization, and translation, plus LLM-based redaction and NER for anonymization. Validated on Swedish court decisions.

Result: Effective anonymization and semantic retention, demonstrated via manual review, automated scanning, and predictive evaluation.

Conclusion: The toolchain facilitates privacy-conscious, large-scale textual analysis, expanding research possibilities in sensitive domains.

Abstract: Unstructured text from legal, medical, and administrative sources offers a
rich but underutilized resource for research in public health and the social
sciences. However, large-scale analysis is hampered by two key challenges: the
presence of sensitive, personally identifiable information, and significant
heterogeneity in structure and language. We present a modular toolchain that
prepares such text data for embedding-based analysis, relying entirely on
open-weight models that run on local hardware, requiring only a
workstation-level GPU and supporting privacy-sensitive research.
  The toolchain employs large language model (LLM) prompting to standardize,
summarize, and, when needed, translate texts to English for greater
comparability. Anonymization is achieved via LLM-based redaction, supplemented
with named entity recognition and rule-based methods to minimize the risk of
disclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court
decisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.
Each document is processed into an anonymized, standardized summary and
transformed into a document-level embedding. Validation, including manual
review, automated scanning, and predictive evaluation shows the toolchain
effectively removes identifying information while retaining semantic content.
As an illustrative application, we train a predictive model using embedding
vectors derived from a small set of manually labeled summaries, demonstrating
the toolchain's capacity for semi-automated content analysis at scale.
  By enabling structured, privacy-conscious analysis of sensitive documents,
our toolchain opens new possibilities for large-scale research in domains where
textual data was previously inaccessible due to privacy and heterogeneity
constraints.

</details>


### [188] [A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations](https://arxiv.org/abs/2507.10585)
*Isar Nejadgholi,Mona Omidyeganeh,Marc-Antoine Drouin,Jonathan Boisvert*

Main category: cs.CL

TL;DR: The paper proposes an updated taxonomy for Explainable AI (XAI) focused on Natural Language Explanations (NLEs) to improve AI governance.


<details>
  <summary>Details</summary>
Motivation: The rise of large language models necessitates clear methods to articulate and verify AI behavior, requiring structured approaches for stakeholders.

Method: The authors draw on XAI literature to create a taxonomy for NLEs across three dimensions: Context, Generation and Presentation, and Evaluation.

Result: The taxonomy offers a framework for researchers, auditors, and policymakers to design and evaluate NLEs for transparent AI systems.

Conclusion: The updated XAI taxonomy enhances the governance of AI systems by improving the clarity and effectiveness of NLEs.

Abstract: Effective AI governance requires structured approaches for stakeholders to
access and verify AI system behavior. With the rise of large language models,
Natural Language Explanations (NLEs) are now key to articulating model
behavior, which necessitates a focused examination of their characteristics and
governance implications. We draw on Explainable AI (XAI) literature to create
an updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:
(1) Context, including task, data, audience, and goals; (2) Generation and
Presentation, covering generation methods, inputs, interactivity, outputs, and
forms; and (3) Evaluation, focusing on content, presentation, and user-centered
properties, as well as the setting of the evaluation. This taxonomy provides a
framework for researchers, auditors, and policymakers to characterize, design,
and enhance NLEs for transparent AI systems.

</details>


### [189] [AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters](https://arxiv.org/abs/2507.10586)
*Kaushik Dwivedi,Padmanabh Patanjali Mishra*

Main category: cs.CL

TL;DR: AutoRAG-LoRA is a framework using LoRA-based adapters and KL-regularized training to reduce hallucinations in LLMs by grounding responses in retrieved evidence and incorporating a feedback loop for factual alignment.


<details>
  <summary>Details</summary>
Motivation: Address the issue of factual inaccuracies (hallucinations) in LLMs to improve trust in real-world applications.

Method: Combines automated prompt rewriting, hybrid retrieval, LoRA-based adapter tuning, and a hallucination detection module with feedback correction.

Result: Significantly reduces factual drift while maintaining model efficiency and modularity.

Conclusion: AutoRAG-LoRA effectively mitigates hallucinations in LLMs, enhancing their reliability for practical use.

Abstract: Large Language Models (LLMs) have demonstrated remarkable fluency across a
range of natural language tasks, yet remain vulnerable to hallucinations -
factual inaccuracies that undermine trust in real world deployment. We present
AutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that
tackles hallucination in large language models through lightweight LoRA-based
adapters and KL-regularized training. Our pipeline integrates automated prompt
rewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in
retrieved evidence. A hallucination detection module, using both
classifier-based and self-evaluation techniques, assigns confidence scores to
generated outputs, triggering an optional feedback correction loop. This loop
enforces factual alignment via contrastive KL loss and adapter fine tuning. We
demonstrate that AutoRAG-LoRA significantly reduces the factual drift while
preserving the efficiency and modularity of the model.

</details>


### [190] [Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing](https://arxiv.org/abs/2507.10587)
*Dennis Ulmer,Alexandra Lorson,Ivan Titov,Christian Hardmeier*

Main category: cs.CL

TL;DR: The paper discusses the need for LLMs to communicate uncertainty to users to enhance trust and mitigate harm, advocating for anthropomimetic uncertainty—emulating human-like communication.


<details>
  <summary>Details</summary>
Motivation: LLMs often output overly confident responses, undermining trust. Effective uncertainty communication can improve human-machine collaboration.

Method: The paper reviews human uncertainty communication, surveys NLP research, and analyzes biases in verbalized uncertainty.

Result: Highlights overlooked biases and proposes anthropomimetic uncertainty as a solution for intuitive, personalized communication.

Conclusion: Future NLP research should focus on human-like uncertainty communication to address unique human-machine interaction challenges.

Abstract: Human users increasingly rely on natural language interactions with large
language models (LLMs) in order to receive help on a large variety of tasks and
problems. However, the trustworthiness and perceived legitimacy of LLMs is
undermined by the fact that their output is frequently stated in very confident
terms, even when its accuracy is questionable. Therefore, there is a need to
signal the confidence of the language model to a user in order to reap the
benefits of human-machine collaboration and mitigate potential harms.
Verbalized uncertainty is the expression of confidence with linguistic means,
an approach that integrates perfectly into language-based interfaces.
Nevertheless, most recent research in natural language processing (NLP)
overlooks the nuances surrounding human uncertainty communication and the data
biases that influence machine uncertainty communication. We argue for
anthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty
communication requires a degree of linguistic authenticity and personalization
to the user, which could be achieved by emulating human communication. We
present a thorough overview over the research in human uncertainty
communication, survey ongoing research, and perform additional analyses to
demonstrate so-far overlooked biases in verbalized uncertainty. We conclude by
pointing out unique factors in human-machine communication of uncertainty and
deconstruct anthropomimetic uncertainty into future research directions for
NLP.

</details>


### [191] [PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification](https://arxiv.org/abs/2507.10596)
*Yogachandran Rahulamathavan,Misbah Farooq,Varuna De Silva*

Main category: cs.CL

TL;DR: PLEX is a perturbation-free method for explaining LLM predictions, using contextual embeddings and a Siamese network, achieving high agreement with LIME/SHAP while being much faster.


<details>
  <summary>Details</summary>
Motivation: LLMs lack interpretability, and existing XAI methods like LIME/SHAP are computationally expensive due to reliance on perturbations.

Method: PLEX uses contextual embeddings and a Siamese network to align with feature importance scores, eliminating the need for perturbations.

Result: PLEX shows >92% agreement with LIME/SHAP, accurately identifies influential words, and reduces computational overhead significantly.

Conclusion: PLEX provides efficient and accurate explanations for LLM-based text classification, outperforming traditional methods in speed and sometimes in performance.

Abstract: Large Language Models (LLMs) excel in text classification, but their
complexity hinders interpretability, making it difficult to understand the
reasoning behind their predictions. Explainable AI (XAI) methods like LIME and
SHAP offer local explanations by identifying influential words, but they rely
on computationally expensive perturbations. These methods typically generate
thousands of perturbed sentences and perform inferences on each, incurring a
substantial computational burden, especially with LLMs. To address this, we
propose \underline{P}erturbation-free \underline{L}ocal \underline{Ex}planation
(PLEX), a novel method that leverages the contextual embeddings extracted from
the LLM and a ``Siamese network" style neural network trained to align with
feature importance scores. This one-off training eliminates the need for
subsequent perturbations, enabling efficient explanations for any new sentence.
We demonstrate PLEX's effectiveness on four different classification tasks
(sentiment, fake news, fake COVID-19 news and depression), showing more than
92\% agreement with LIME and SHAP. Our evaluation using a ``stress test"
reveals that PLEX accurately identifies influential words, leading to a similar
decline in classification accuracy as observed with LIME and SHAP when these
words are removed. Notably, in some cases, PLEX demonstrates superior
performance in capturing the impact of key features. PLEX dramatically
accelerates explanation, reducing time and computational overhead by two and
four orders of magnitude, respectively. This work offers a promising solution
for explainable LLM-based text classification.

</details>


### [192] [Emergence of Hierarchical Emotion Organization in Large Language Models](https://arxiv.org/abs/2507.10599)
*Bo Zhao,Maya Okawa,Eric J. Bigelow,Rose Yu,Tomer Ullman,Ekdeep Singh Lubana,Hidenori Tanaka*

Main category: cs.CL

TL;DR: LLMs form hierarchical emotion trees aligning with human psychology, with larger models showing more complexity. Biases in emotion recognition exist, especially for underrepresented groups, mirroring human social perception.


<details>
  <summary>Details</summary>
Motivation: Understanding how LLMs model emotional states is crucial for ethical deployment, inspired by psychological frameworks like emotion wheels.

Method: Analyzed probabilistic dependencies between emotional states in LLM outputs and compared with human psychological models. Conducted human studies to validate findings.

Result: LLMs naturally form hierarchical emotion trees similar to human models, with larger models exhibiting more complexity. Systematic biases in emotion recognition were found, particularly for underrepresented groups.

Conclusion: The study highlights emergent emotional reasoning in LLMs and suggests using cognitively-grounded theories for better model evaluations.

Abstract: As large language models (LLMs) increasingly power conversational agents,
understanding how they model users' emotional states is critical for ethical
deployment. Inspired by emotion wheels -- a psychological framework that argues
emotions organize hierarchically -- we analyze probabilistic dependencies
between emotional states in model outputs. We find that LLMs naturally form
hierarchical emotion trees that align with human psychological models, and
larger models develop more complex hierarchies. We also uncover systematic
biases in emotion recognition across socioeconomic personas, with compounding
misclassifications for intersectional, underrepresented groups. Human studies
reveal striking parallels, suggesting that LLMs internalize aspects of social
perception. Beyond highlighting emergent emotional reasoning in LLMs, our
results hint at the potential of using cognitively-grounded theories for
developing better model evaluations.

</details>


### [193] [Language Models for Adult Service Website Text Analysis](https://arxiv.org/abs/2507.10743)
*Nickolas Freeman,Thanh Nguyen,Gregory Bott,Jason Parton,Collin Francel*

Main category: cs.CL

TL;DR: The paper explores language modeling for analyzing Adult Service Websites (ASWs) to combat sex trafficking, showing custom transformers outperform pre-trained models in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: ASWs are linked to sex trafficking, but analyzing their ad text is challenging due to obfuscation and poor grammar. Effective text analysis can aid victim identification.

Method: The study evaluates various language models (information retrieval, pre-trained transformers, custom transformers) on ASW text, focusing on efficiency and performance.

Result: Custom transformer models outperform BERT-base, RoBERTa, and ModernBERT in accuracy, recall, F1 score, and ROC AUC, and are efficient on consumer hardware.

Conclusion: Custom models advance ASW text analysis, enabling tasks like graph decomposition, ad clustering, and emoji analysis, with potential for broader applications in combating sex trafficking.

Abstract: Sex trafficking refers to the use of force, fraud, or coercion to compel an
individual to perform in commercial sex acts against their will. Adult service
websites (ASWs) have and continue to be linked to sex trafficking, offering a
platform for traffickers to advertise their victims. Thus, organizations
involved in the fight against sex trafficking often use ASW data when
attempting to identify potential sex trafficking victims. A critical challenge
in transforming ASW data into actionable insight is text analysis. Previous
research using ASW data has shown that ASW ad text is important for linking
ads. However, working with this text is challenging due to its extensive use of
emojis, poor grammar, and deliberate obfuscation to evade law enforcement
scrutiny. We conduct a comprehensive study of language modeling approaches for
this application area, including simple information retrieval methods,
pre-trained transformers, and custom transformer models. We demonstrate that
characteristics of ASW text data allow efficient custom transformer models to
be trained with relatively small GPU resources and used efficiently for
inference on consumer hardware. Our custom models outperform fine-tuned
variants of well-known encoder-only transformer models, including BERT-base,
RoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We
demonstrate the use of our best-performing custom configuration on three tasks
related to ASW data analysis: (i) decomposing the giant component in a graph
representation of ASW data, (ii) clustering ASW ad text, and (iii) using the
learned token embeddings to understand the use of emojis in the illicit context
we study. The models we develop represent a significant advancement in ASW text
analysis, which can be leveraged in a variety of downstream applications and
research.

</details>


### [194] [Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs](https://arxiv.org/abs/2507.10772)
*Michal Podstawski*

Main category: cs.CL

TL;DR: The paper explores using pretrained text embedding models to enhance semantic analysis in labeled property graphs, improving tasks like node classification and relation prediction without altering the graph structure.


<details>
  <summary>Details</summary>
Motivation: To leverage rich textual attributes in property graphs for better analytical tasks by incorporating semantic understanding.

Method: Integrates pretrained text embedding models to embed textual node and edge properties, maintaining the original graph structure.

Result: Demonstrates that textual semantics significantly improve accuracy and interpretability in property graph analysis.

Conclusion: Textual semantics, when embedded into property graphs, enhance analytical tasks without structural changes.

Abstract: Labeled property graphs often contain rich textual attributes that can
enhance analytical tasks when properly leveraged. This work explores the use of
pretrained text embedding models to enable efficient semantic analysis in such
graphs. By embedding textual node and edge properties, we support downstream
tasks including node classification and relation prediction with improved
contextual understanding. Our approach integrates language model embeddings
into the graph pipeline without altering its structure, demonstrating that
textual semantics can significantly enhance the accuracy and interpretability
of property graph analysis.

</details>


### [195] [Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers](https://arxiv.org/abs/2507.10787)
*Yilun Zhao,Chengye Wang,Chuhan Li,Arman Cohan*

Main category: cs.CL

TL;DR: MISS-QA is a new benchmark for evaluating models' ability to interpret schematic diagrams in scientific papers, revealing a performance gap between models and humans.


<details>
  <summary>Details</summary>
Motivation: To assess and improve models' comprehension of multimodal scientific literature, particularly schematic diagrams.

Method: Created MISS-QA with 1,500 expert-annotated examples from 465 papers, testing 18 multimodal models on diagram interpretation and question-answering.

Result: Significant performance gap between models and human experts; detailed error analysis provided insights into model limitations.

Conclusion: Highlights the need for enhancing models' multimodal comprehension in scientific literature.

Abstract: This paper introduces MISS-QA, the first benchmark specifically designed to
evaluate the ability of models to interpret schematic diagrams within
scientific literature. MISS-QA comprises 1,500 expert-annotated examples over
465 scientific papers. In this benchmark, models are tasked with interpreting
schematic diagrams that illustrate research overviews and answering
corresponding information-seeking questions based on the broader context of the
paper. We assess the performance of 18 frontier multimodal foundation models,
including o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant
performance gap between these models and human experts on MISS-QA. Our analysis
of model performance on unanswerable questions and our detailed error analysis
further highlight the strengths and limitations of current models, offering key
insights to enhance models in comprehending multimodal scientific literature.

</details>


### [196] [Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler](https://arxiv.org/abs/2507.10810)
*David M. Markowitz,Samuel Hardman Taylor*

Main category: cs.CL

TL;DR: The paper examines whether social approval (upvotes) on hate speech posts predicts future hate speech, finding no consistent relationship.


<details>
  <summary>Details</summary>
Motivation: To test Walther's (2024) social approval theory of online hate, specifically whether social approval reinforces hate speech.

Method: Analyzed over 110 million posts from Parler (2018-2021) to measure the relationship between upvotes and subsequent hate speech.

Result: No clear link between upvotes and future hate speech; mixed results for between-person effects.

Conclusion: Social approval may not consistently reinforce hate speech on niche platforms like Parler.

Abstract: In this paper, we explored how online hate is motivated by receiving social
approval from others. We specifically examined two central tenets of Walther's
(2024) social approval theory of online hate: (H1a) more signals of social
approval on hate messages predicts more subsequent hate messages, and (H1b) as
social approval increases, hate speech messages become more extreme. Using over
110 million posts from Parler (2018-2021), we observed that the number of
upvotes a person received on a hate speech post was unassociated with the
amount of hate speech in their next post and posts during the next week, month,
three months, and six months. Between-person effects revealed an average
negative relationship between social approval and hate speech production at the
post level, but this relationship was mixed at other time intervals. Social
approval reinforcement mechanisms of online hate may operate differently on
niche social media platforms.

</details>


### [197] [LLMs on Trial: Evaluating Judicial Fairness for Large Language Models](https://arxiv.org/abs/2507.10852)
*Yiran Hu,Zongyue Xue,Haitao Li,Siyuan Zheng,Qingjing Chen,Shaochun Wang,Xihan Zhang,Ning Zheng,Yun Liu,Qingyao Ai,Yiqun Liu,Charles L. A. Clarke,Weixing Shen*

Main category: cs.CL

TL;DR: The paper evaluates the judicial fairness of Large Language Models (LLMs) using a comprehensive framework, revealing pervasive biases and inconsistencies, and introduces a toolkit for future research.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in high-stakes fields like judiciary, but their fairness and implications for social justice remain underexplored.

Method: A framework with 65 labels and 161 values is constructed to measure LLM fairness. A dataset (JudiFair) of 177,100 case facts is compiled, and three metrics (inconsistency, bias, imbalanced inaccuracy) are developed to evaluate fairness across 16 LLMs.

Result: Experiments reveal severe judicial unfairness in LLMs, with pronounced biases on demographic labels. Adjusting temperature affects fairness, but model size, release date, and origin do not.

Conclusion: The study highlights LLM fairness issues in judicial contexts and provides a toolkit to aid future research in improving fairness.

Abstract: Large Language Models (LLMs) are increasingly used in high-stakes fields
where their decisions impact rights and equity. However, LLMs' judicial
fairness and implications for social justice remain underexplored. When LLMs
act as judges, the ability to fairly resolve judicial issues is a prerequisite
to ensure their trustworthiness. Based on theories of judicial fairness, we
construct a comprehensive framework to measure LLM fairness, leading to a
selection of 65 labels and 161 corresponding values. Applying this framework to
the judicial system, we compile an extensive dataset, JudiFair, comprising
177,100 unique case facts. To achieve robust statistical inference, we develop
three evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and
introduce a method to assess the overall fairness of multiple LLMs across
various labels. Through experiments with 16 LLMs, we uncover pervasive
inconsistency, bias, and imbalanced inaccuracy across models, underscoring
severe LLM judicial unfairness. Particularly, LLMs display notably more
pronounced biases on demographic labels, with slightly less bias on substance
labels compared to procedure ones. Interestingly, increased inconsistency
correlates with reduced biases, but more accurate predictions exacerbate
biases. While we find that adjusting the temperature parameter can influence
LLM fairness, model size, release date, and country of origin do not exhibit
significant effects on judicial fairness. Accordingly, we introduce a publicly
available toolkit containing all datasets and code, designed to support future
research in evaluating and improving LLM fairness.

</details>


### [198] [How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations](https://arxiv.org/abs/2507.10918)
*Ikumi Numaya,Shoji Moriya,Shiki Sato,Reina Akama,Jun Suzuki*

Main category: cs.CL

TL;DR: The paper explores the impact of stylistic similarity in dialogue systems, distinguishing between subjective (user-perceived) and objective (third-party annotated) similarity. It finds a strong correlation between subjective similarity and user preference, highlighting the need to differentiate these evaluations.


<details>
  <summary>Details</summary>
Motivation: Prior work overlooks the distinction between subjective and objective stylistic similarity in dialogue systems, despite its potential impact on user impressions.

Method: A novel dataset is introduced, combining user preferences, subjective stylistic similarity (user-perceived), and objective stylistic similarity (third-party annotated) in open-domain dialogues.

Result: Analysis shows a strong positive correlation between subjective stylistic similarity and user preference, with subjective and objective similarity differing significantly.

Conclusion: The study emphasizes the importance of distinguishing subjective and objective evaluations in dialogue systems to better understand their impact on user preferences.

Abstract: Recent advancements in dialogue generation have broadened the scope of
human-bot interactions, enabling not only contextually appropriate responses
but also the analysis of human affect and sensitivity. While prior work has
suggested that stylistic similarity between user and system may enhance user
impressions, the distinction between subjective and objective similarity is
often overlooked. To investigate this issue, we introduce a novel dataset that
includes users' preferences, subjective stylistic similarity based on users'
own perceptions, and objective stylistic similarity annotated by third party
evaluators in open-domain dialogue settings. Analysis using the constructed
dataset reveals a strong positive correlation between subjective stylistic
similarity and user preference. Furthermore, our analysis suggests an important
finding: users' subjective stylistic similarity differs from third party
objective similarity. This underscores the importance of distinguishing between
subjective and objective evaluations and understanding the distinct aspects
each captures when analyzing the relationship between stylistic similarity and
user preferences. The dataset presented in this paper is available online.

</details>


### [199] [HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training](https://arxiv.org/abs/2507.10920)
*Seungho Choi*

Main category: cs.CL

TL;DR: HanjaBridge improves Korean LLM performance by injecting Hanja meanings during training, enhancing contextual disambiguation and cross-lingual transfer without runtime costs.


<details>
  <summary>Details</summary>
Motivation: LLMs perform poorly in low-resource languages like Korean due to linguistic challenges like homophonous Sino-Korean words.

Method: Proposes HanjaBridge, a meaning-injection technique in continual pre-training, presenting all Hanja candidates for homographs and using token-level knowledge distillation.

Result: Achieves a 21% relative improvement on KoBALT, with strong cross-lingual transfer and no runtime cost.

Conclusion: HanjaBridge effectively addresses semantic ambiguity in Korean LLMs, improving performance and cross-lingual alignment.

Abstract: Large language models (LLMs) often show poor performance in low-resource
languages like Korean, partly due to unique linguistic challenges such as
homophonous Sino-Korean words that are indistinguishable in Hangul script. To
address this semantic ambiguity, we propose HanjaBridge, a novel
meaning-injection technique integrated into a continual pre-training (CPT)
framework. Instead of deterministically mapping a word to a single Hanja
(Chinese character), HanjaBridge presents the model with all possible Hanja
candidates for a given homograph, encouraging the model to learn contextual
disambiguation. This process is paired with token-level knowledge distillation
to prevent catastrophic forgetting. Experimental results show that HanjaBridge
significantly improves Korean language understanding, achieving a 21\% relative
improvement on the KoBALT benchmark. Notably, by reinforcing semantic alignment
between Korean and Chinese through shared Hanja, we observe a strong positive
cross-lingual transfer. Furthermore, these gains persist even when Hanja
augmentation is omitted at inference time, ensuring practical efficiency with
no additional run-time cost.

</details>


### [200] [Modeling Understanding of Story-Based Analogies Using Large Language Models](https://arxiv.org/abs/2507.10957)
*Kalit Inani,Keshav Kabra,Vijay Marupudi,Sashank Varma*

Main category: cs.CL

TL;DR: The study evaluates LLMs' analogical reasoning compared to humans, focusing on semantic representation and prompting for explanations, while assessing model size and architecture impacts.


<details>
  <summary>Details</summary>
Motivation: To determine how well LLMs align with human performance in detecting and mapping analogies, addressing gaps in robust human-like reasoning.

Method: Used a story-based analogical mapping task, analyzed semantic representations via sentence embeddings, and tested explicit prompting for explanations. Evaluated model size (8B vs. 70B) and architectures (GPT-4, LLaMA3).

Result: Assessed LLMs' ability to capture analogy similarities and dissimilarities, and the impact of prompting and model variations on reasoning performance.

Conclusion: Advances understanding of LLMs' analogical reasoning and their potential as models of human cognition.

Abstract: Recent advancements in Large Language Models (LLMs) have brought them closer
to matching human cognition across a variety of tasks. How well do these models
align with human performance in detecting and mapping analogies? Prior research
has shown that LLMs can extract similarities from analogy problems but lack
robust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the
current study focused on a story-based analogical mapping task and conducted a
fine-grained evaluation of LLM reasoning abilities compared to human
performance. First, it explored the semantic representation of analogies in
LLMs, using sentence embeddings to assess whether they capture the similarity
between the source and target texts of an analogy, and the dissimilarity
between the source and distractor texts. Second, it investigated the
effectiveness of explicitly prompting LLMs to explain analogies. Throughout, we
examine whether LLMs exhibit similar performance profiles to those observed in
humans by evaluating their reasoning at the level of individual analogies, and
not just at the level of overall accuracy (as prior studies have done). Our
experiments include evaluating the impact of model size (8B vs. 70B parameters)
and performance variation across state-of-the-art model architectures such as
GPT-4 and LLaMA3. This work advances our understanding of the analogical
reasoning abilities of LLMs and their potential as models of human reasoning.

</details>


### [201] [DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models](https://arxiv.org/abs/2507.10958)
*Anthony Miyaguchi,David Guecha,Yuwen Chiu,Sidharth Gaur*

Main category: cs.CL

TL;DR: The DS@GT team participated in eRisk 2025's conversational depression detection task using prompt-engineered LLMs to assess BDI-II criteria, achieving strong cross-model agreement and ranking second on the leaderboard.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of large language models (LLMs) in detecting depression through conversational cues, leveraging prompt engineering for structured assessments.

Method: Adopted a prompt-engineering strategy with diverse LLMs to conduct BDI-II-based assessments, producing structured JSON outputs and evaluating cross-model agreement due to lack of ground-truth labels.

Result: Achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27, ranking second on the official leaderboard.

Conclusion: The prompt design successfully aligned LLM outputs with BDI-II criteria, demonstrating potential for analyzing conversational cues in depression detection.

Abstract: This Working Note summarizes the participation of the DS@GT team in two eRisk
2025 challenges. For the Pilot Task on conversational depression detection with
large language-models (LLMs), we adopted a prompt-engineering strategy in which
diverse LLMs conducted BDI-II-based assessments and produced structured JSON
outputs. Because ground-truth labels were unavailable, we evaluated cross-model
agreement and internal consistency. Our prompt design methodology aligned model
outputs with BDI-II criteria and enabled the analysis of conversational cues
that influenced the prediction of symptoms. Our best submission, second on the
official leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.

</details>


### [202] [Teach Me Sign: Stepwise Prompting LLM for Sign Language Production](https://arxiv.org/abs/2507.10972)
*Zhaoyi An,Rei Kawakami*

Main category: cs.CL

TL;DR: TEAM-Sign fine-tunes an LLM to treat sign language as a natural language, using stepwise prompting to align sign and spoken language distributions and rules, showing effectiveness on How2Sign and Phoenix14T datasets.


<details>
  <summary>Details</summary>
Motivation: Sign language generation is complex and lacks the impact of LLMs due to unique rules. TEAM-Sign aims to bridge this gap by leveraging LLMs' reasoning and knowledge.

Method: Fine-tune an LLM to learn text-sign language correspondence, using stepwise prompting to extract inherent sign language knowledge for generation.

Result: Effective alignment of sign and spoken language distributions and grammatical rules on How2Sign and Phoenix14T datasets.

Conclusion: TEAM-Sign successfully leverages LLMs for sign language generation, addressing complexity and unique rules.

Abstract: Large language models, with their strong reasoning ability and rich
knowledge, have brought revolution to many tasks of AI, but their impact on
sign language generation remains limited due to its complexity and unique
rules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign
language as another natural language. By fine-tuning an LLM, we enable it to
learn the correspondence between text and sign language, and facilitate
generation. Considering the differences between sign and spoken language, we
employ a stepwise prompting strategy to extract the inherent sign language
knowledge within the LLM, thereby supporting the learning and generation
process. Experimental results on How2Sign and Phoenix14T datasets demonstrate
that our approach effectively leverages both the sign language knowledge and
reasoning capabilities of LLM to align the different distribution and
grammatical rules between sign and spoken language.

</details>


### [203] [Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection](https://arxiv.org/abs/2507.10996)
*Lin Tian,Johanne R. Trippas,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: The paper introduces a hierarchical LoRA adaptation method for Llama 3.1 8B to detect sexism in multilingual tweets, achieving efficient performance with minimal preprocessing and parameter usage.


<details>
  <summary>Details</summary>
Motivation: To address text-based sexism detection in English and Spanish tweets efficiently, leveraging hierarchical subtasks and multilingual training.

Method: Hierarchical LoRA adaptation applied to all linear transformations, with conditional adapter routing for subtasks (binary sexism, intention detection, categorization). Uses QLoRA 4-bit and unified multilingual training.

Result: Achieves 1.7-2.4% F1 improvements via cross-lingual transfer, with 75% faster training and 98% reduced storage. Performance: 0.6774 (binary), 0.4991 (intention), 0.6519 (multilabel).

Conclusion: Parameter-efficient fine-tuning with hierarchical LoRA is effective for multilingual sexism detection, balancing performance and resource efficiency.

Abstract: This paper presents our approach to EXIST 2025 Task 1, addressing text-based
sexism detection in English and Spanish tweets through hierarchical Low-Rank
Adaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter
routing that explicitly models label dependencies across three hierarchically
structured subtasks: binary sexism identification, source intention detection,
and multilabel sexism categorization. Unlike conventional LoRA applications
that target only attention layers, we apply adaptation to all linear
transformations, enhancing the model's capacity to capture task-specific
patterns. In contrast to complex data processing and ensemble approaches, we
show that straightforward parameter-efficient fine-tuning achieves strong
performance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each
subtask using unified multilingual training that leverages Llama 3.1's native
bilingual capabilities. The method requires minimal preprocessing and uses
standard supervised learning. Our multilingual training strategy eliminates the
need for separate language-specific models, achieving 1.7-2.4\% F1 improvements
through cross-lingual transfer. With only 1.67\% trainable parameters compared
to full fine-tuning, our approach reduces training time by 75\% and model
storage by 98\%, while achieving competitive performance across all subtasks
(ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection,
0.6519 for multilabel categorization).

</details>


### [204] [Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification](https://arxiv.org/abs/2507.11004)
*Yejun Yoon,Jaeyoon Jung,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

TL;DR: HerO 2 is an improved version of HerO, enhancing evidence quality, veracity prediction, and system performance, ranking second in the AVeriTeC shared task with high efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve upon the previous best-performing open-source model (HerO) for fact verification by enhancing evidence quality, optimizing veracity prediction, and integrating updated language models.

Method: Uses document summarization, answer reformulation, post-training quantization, and updated LM backbones.

Result: Ranked second in the AVeriTeC shared task with the shortest runtime among top systems.

Conclusion: HerO 2 demonstrates high efficiency and strong potential for real-world fact verification, with code publicly available.

Abstract: This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task
at the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the
best-performing open-source model from the previous year's challenge. It
improves evidence quality through document summarization and answer
reformulation, optimizes veracity prediction via post-training quantization
under computational constraints, and enhances overall system performance by
integrating updated language model (LM) backbones. HerO 2 ranked second on the
leaderboard while achieving the shortest runtime among the top three systems,
demonstrating both high efficiency and strong potential for real-world fact
verification. The code is available at https://github.com/ssu-humane/HerO2.

</details>


### [205] [Journalism-Guided Agentic In-Context Learning for News Stance Detection](https://arxiv.org/abs/2507.11049)
*Dahyun Lee,Jonghyeon Choi,Jiyoung Han,Kunwoo Park*

Main category: cs.CL

TL;DR: The paper introduces K-News-Stance, a Korean dataset for article-level stance detection, and JoA-ICL, a framework for stance detection in long-form news, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address gaps in stance detection research, particularly for long texts and low-resource languages, and mitigate filter bubbles in news recommendations.

Method: Proposes JoA-ICL, a journalism-guided framework using in-context learning to predict stances of key segments and aggregate them for article-level stance detection.

Result: JoA-ICL outperforms existing methods, demonstrating effectiveness in capturing stances in long-form articles and aiding viewpoint diversity.

Conclusion: The work advances stance detection for long-form content and supports applications like diverse news recommendations and media bias analysis.

Abstract: As online news consumption grows, personalized recommendation systems have
become integral to digital journalism. However, these systems risk reinforcing
filter bubbles and political polarization by failing to incorporate diverse
perspectives. Stance detection -- identifying a text's position on a target --
can help mitigate this by enabling viewpoint-aware recommendations and
data-driven analyses of media bias. Yet, existing stance detection research
remains largely limited to short texts and high-resource languages. To address
these gaps, we introduce \textsc{K-News-Stance}, the first Korean dataset for
article-level stance detection, comprising 2,000 news articles with
article-level and 19,650 segment-level stance annotations across 47 societal
issues. We also propose \textsc{JoA-ICL}, a \textbf{Jo}urnalism-guided
\textbf{A}gentic \textbf{I}n-\textbf{C}ontext \textbf{L}earning framework that
employs a language model agent to predict the stances of key structural
segments (e.g., leads, quotes), which are then aggregated to infer the overall
article stance. Experiments show that \textsc{JoA-ICL} outperforms existing
stance detection methods, highlighting the benefits of segment-level agency in
capturing the overall position of long-form news articles. Two case studies
further demonstrate its broader utility in promoting viewpoint diversity in
news recommendations and uncovering patterns of media bias.

</details>


### [206] [LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP](https://arxiv.org/abs/2507.11052)
*Haowei Yang,Ziyu Shen,Junli Shao,Luyao Men,Xinyue Han,Jing Dong*

Main category: cs.CL

TL;DR: A novel LLM-augmented clinical NLP pipeline improves CVD risk prediction by extracting and analyzing unstructured clinical notes, outperforming existing models in precision, recall, and clinical relevance.


<details>
  <summary>Details</summary>
Motivation: Timely identification and accurate risk stratification of CVD are crucial for reducing global mortality, and unstructured clinical notes contain valuable early indicators not captured by structured data.

Method: The study uses domain-adapted large language models (LLMs) for symptom extraction, contextual reasoning, and correlation from free-text reports, integrating cardiovascular-specific fine-tuning, prompt-based inference, and entity-aware reasoning.

Result: Evaluations on MIMIC-III and CARDIO-NLP datasets show improved precision, recall, F1-score, and AUROC, with high clinical relevance (kappa = 0.82). Challenges like contextual hallucination and temporal ambiguity are addressed with prompt engineering and hybrid rule-based verification.

Conclusion: The work highlights LLMs' potential in clinical decision support systems, enhancing early warning systems and translating patient narratives into actionable risk assessments.

Abstract: Timely identification and accurate risk stratification of cardiovascular
disease (CVD) remain essential for reducing global mortality. While existing
prediction models primarily leverage structured data, unstructured clinical
notes contain valuable early indicators. This study introduces a novel
LLM-augmented clinical NLP pipeline that employs domain-adapted large language
models for symptom extraction, contextual reasoning, and correlation from
free-text reports. Our approach integrates cardiovascular-specific fine-tuning,
prompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III
and CARDIO-NLP datasets demonstrate improved performance in precision, recall,
F1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by
cardiologists. Challenges such as contextual hallucination, which occurs when
plausible information contracts with provided source, and temporal ambiguity,
which is related with models struggling with chronological ordering of events
are addressed using prompt engineering and hybrid rule-based verification. This
work underscores the potential of LLMs in clinical decision support systems
(CDSS), advancing early warning systems and enhancing the translation of
patient narratives into actionable risk assessments.

</details>


### [207] [Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach](https://arxiv.org/abs/2507.11084)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha*

Main category: cs.CL

TL;DR: A hybrid transformer-based sentiment analysis framework was developed to analyze Bangla social media comments during the July Revolution in Bangladesh, achieving 83.7% accuracy with a hybrid XMB-BERT and voting classifier.


<details>
  <summary>Details</summary>
Motivation: To decode public opinion expressed in Bangla social media comments during the July Revolution, leveraging machine learning for low-resource languages.

Method: Used a dataset of 4,200 Bangla comments, employed transformer-based models (BanglaBERT, mBERT, XLM-RoBERTa, hybrid XMB-BERT), PCA for dimensionality reduction, and tested eleven classifiers.

Result: The hybrid XMB-BERT with voting classifier achieved 83.7% accuracy, outperforming other models.

Conclusion: Demonstrates the effectiveness of machine learning for sentiment analysis in low-resource languages like Bangla.

Abstract: The July Revolution in Bangladesh marked a significant student-led mass
uprising, uniting people across the nation to demand justice, accountability,
and systemic reform. Social media platforms played a pivotal role in amplifying
public sentiment and shaping discourse during this historic mass uprising. In
this study, we present a hybrid transformer-based sentiment analysis framework
to decode public opinion expressed in social media comments during and after
the revolution. We used a brand new dataset of 4,200 Bangla comments collected
from social media. The framework employs advanced transformer-based feature
extraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the
proposed hybrid XMB-BERT, to capture nuanced patterns in textual data.
Principle Component Analysis (PCA) were utilized for dimensionality reduction
to enhance computational efficiency. We explored eleven traditional and
advanced machine learning classifiers for identifying sentiments. The proposed
hybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of
83.7% and outperform other model classifier combinations. This study
underscores the potential of machine learning techniques to analyze social
sentiment in low-resource languages like Bangla.

</details>


### [208] [Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification](https://arxiv.org/abs/2507.11086)
*Andres Azqueta-Gavaldón,Joaquin Ramos Cosgrove*

Main category: cs.CL

TL;DR: The paper explores using Large Language Models (LLMs) to improve foreign entity classification in financial systems, outperforming traditional methods with higher accuracy and lower false positives.


<details>
  <summary>Details</summary>
Motivation: The need for accurate foreign entity identification in financial systems due to challenges like linguistic variations and outdated names, which traditional methods struggle with.

Method: Comparison of traditional algorithms (Jaccard, cosine, Levenshtein) with LLMs (Hugging Face-based and interface-based like Microsoft Copilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese companies.

Result: Traditional methods achieved >92% accuracy but had high false positives (20-40%). Interface-based LLMs outperformed with >93% accuracy, >96% F1 scores, and lower false positives (40-80%).

Conclusion: LLMs, especially interface-based ones, offer a superior solution for entity-matching tasks in financial systems due to their contextual understanding and adaptability.

Abstract: The growing prevalence of cross-border financial activities in global markets
has underscored the necessity of accurately identifying and classifying foreign
entities. This practice is essential within the Spanish financial system for
ensuring robust risk management, regulatory adherence, and the prevention of
financial misconduct. This process involves a labor-intensive entity-matching
task, where entities need to be validated against available reference sources.
Challenges arise from linguistic variations, special characters, outdated
names, and changes in legal forms, complicating traditional matching algorithms
like Jaccard, cosine, and Levenshtein distances. These methods struggle with
contextual nuances and semantic relationships, leading to mismatches. To
address these limitations, we explore Large Language Models (LLMs) as a
flexible alternative. LLMs leverage extensive training to interpret context,
handle abbreviations, and adapt to legal transitions. We evaluate traditional
methods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft
Copilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.
Results show traditional methods achieve accuracies over 92% but suffer high
false positive rates (20-40%). Interface-based LLMs outperform, achieving
accuracies above 93%, F1 scores exceeding 96%, and lower false positives
(40-80%).

</details>


### [209] [The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs](https://arxiv.org/abs/2507.11097)
*Zichen Wen,Jiashu Qu,Dongrui Liu,Zhiyuan Liu,Ruixi Wu,Yicun Yang,Xiangqi Jin,Haoyun Xu,Xuyang Liu,Weijia Li,Chaochao Lu,Jing Shao,Conghui He,Linfeng Zhang*

Main category: cs.CL

TL;DR: DIJA is a jailbreak attack framework targeting diffusion-based LLMs (dLLMs), exploiting their bidirectional modeling and parallel decoding to bypass alignment mechanisms, achieving high success rates in generating harmful content.


<details>
  <summary>Details</summary>
Motivation: Existing alignment mechanisms fail to protect dLLMs against adversarial prompts, exposing safety vulnerabilities.

Method: DIJA constructs adversarial interleaved mask-text prompts to exploit dLLMs' bidirectional modeling and parallel decoding, bypassing alignment safeguards.

Result: DIJA outperforms prior methods, achieving up to 100% keyword-based ASR and significant improvements in evaluator-based metrics.

Conclusion: The study highlights critical safety gaps in dLLMs, urging a reevaluation of alignment strategies for this model class.

Abstract: Diffusion-based large language models (dLLMs) have recently emerged as a
powerful alternative to autoregressive LLMs, offering faster inference and
greater interactivity via parallel decoding and bidirectional modeling.
However, despite strong performance in code generation and text infilling, we
identify a fundamental safety concern: existing alignment mechanisms fail to
safeguard dLLMs against context-aware, masked-input adversarial prompts,
exposing novel vulnerabilities. To this end, we present DIJA, the first
systematic study and jailbreak attack framework that exploits unique safety
weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial
interleaved mask-text prompts that exploit the text generation mechanisms of
dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional
modeling drives the model to produce contextually consistent outputs for masked
spans, even when harmful, while parallel decoding limits model dynamic
filtering and rejection sampling of unsafe content. This causes standard
alignment mechanisms to fail, enabling harmful completions in alignment-tuned
dLLMs, even when harmful behaviors or unsafe instructions are directly exposed
in the prompt. Through comprehensive experiments, we demonstrate that DIJA
significantly outperforms existing jailbreak methods, exposing a previously
overlooked threat surface in dLLM architectures. Notably, our method achieves
up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior
baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and
by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of
harmful content in the jailbreak prompt. Our findings underscore the urgent
need for rethinking safety alignment in this emerging class of language models.
Code is available at https://github.com/ZichenWen1/DIJA.

</details>


### [210] [Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](https://arxiv.org/abs/2507.11112)
*Sanhanat Sivapiromrat,Caiqi Zhang,Marco Basaldella,Nigel Collier*

Main category: cs.CL

TL;DR: The paper explores the vulnerability of Large Language Models (LLMs) to multi-trigger data poisoning attacks, demonstrating how multiple triggers can coexist and remain robust. It proposes a layer-wise retraining method for mitigation.


<details>
  <summary>Details</summary>
Motivation: Existing research lacks understanding of trigger mechanisms and their interactions in LLMs, leaving a gap in addressing multi-trigger poisoning.

Method: A framework for studying poisoning in LLMs is introduced, showing how multiple triggers coexist. A post hoc recovery method using layer-wise weight difference analysis is proposed.

Result: Multiple triggers can coexist without interference, remaining robust even with token substitutions or spans. The proposed mitigation method effectively removes triggers with minimal updates.

Conclusion: The study reveals a persistent vulnerability in LLMs and offers a practical defence against multi-trigger poisoning through selective retraining.

Abstract: Recent studies have shown that Large Language Models (LLMs) are vulnerable to
data poisoning attacks, where malicious training examples embed hidden
behaviours triggered by specific input patterns. However, most existing works
assume a phrase and focus on the attack's effectiveness, offering limited
understanding of trigger mechanisms and how multiple triggers interact within
the model. In this paper, we present a framework for studying poisoning in
LLMs. We show that multiple distinct backdoor triggers can coexist within a
single model without interfering with each other, enabling adversaries to embed
several triggers concurrently. Using multiple triggers with high embedding
similarity, we demonstrate that poisoned triggers can achieve robust activation
even when tokens are substituted or separated by long token spans. Our findings
expose a broader and more persistent vulnerability surface in LLMs. To mitigate
this threat, we propose a post hoc recovery method that selectively retrains
specific model components based on a layer-wise weight difference analysis. Our
method effectively removes the trigger behaviour with minimal parameter
updates, presenting a practical and efficient defence against multi-trigger
poisoning.

</details>


### [211] [MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models](https://arxiv.org/abs/2507.11114)
*Seif Ahmed,Mohamed T. Younes,Abdelrahman Moustafa,Abdelrahman Allam,Hamza Moustafa*

Main category: cs.CL

TL;DR: A robust ensemble system for multilingual multimodal reasoning, integrating Gemini models, achieved top performance in the ImageCLEF 2025 EXAMS V challenge through precise prompt engineering and cross-lingual augmentation.


<details>
  <summary>Details</summary>
Motivation: To develop a lightweight yet effective system for multilingual multimodal reasoning in educational settings, outperforming heavier end-to-end models.

Method: Integration of Gemini models (2.5 Flash, 1.5 Pro, 2.5 Pro) with few-shot and zero-shot prompts, ablation study on large language models, and evaluation of prompt design impact.

Result: Achieved first place overall (81.4% accuracy) in the multilingual track, leading 11 out of 13 language tracks (e.g., 95.07% for Croatian). Prompt optimization boosted accuracy from 55.9% to 61.7%.

Conclusion: Lightweight OCR-VLM ensembles with precise prompts and cross-lingual augmentation outperform heavier models in multilingual educational tasks.

Abstract: We present a robust ensemble-based system for multilingual multimodal
reasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach
integrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption
refinement and consistency checks, and Gemini 2.5 Pro as a reasoner which
handles final answer selection, all coordinated through carefully engineered
few-shot and zero-shot prompts. We conducted an extensive ablation study,
training several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,
Mistral) on an English dataset and its multilingual augmented version.
Additionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for
comparison and found it to substantially outperform the trained models. Prompt
design also proved critical: enforcing concise, language-normalized formats and
prohibiting explanatory text boosted model accuracy on the English validation
set from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)
achieved first place overall in the multilingual track with 81.4% accuracy, and
led 11 out of 13 individual language tracks, with top results such as 95.07%
for Croatian and 92.12% for Italian. These findings highlight that lightweight
OCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual
augmentation, can outperform heavier end-to-end models in high-stakes,
multilingual educational settings.

</details>


### [212] [What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests](https://arxiv.org/abs/2507.11128)
*Dimitri Staufer*

Main category: cs.CL

TL;DR: The paper addresses privacy concerns in LLMs regarding GDPR's Right to Be Forgotten (RTBF) by introducing WikiMem, a dataset and metric to identify memorized personal data at the individual level.


<details>
  <summary>Details</summary>
Motivation: LLMs can memorize personal data, posing GDPR compliance issues, especially for RTBF. Current methods lack ways to identify individual-fact associations stored in models.

Method: Introduces WikiMem, a dataset of natural language canaries, and a model-agnostic metric to quantify human-fact associations in LLMs using calibrated negative log-likelihood.

Result: Evaluation on 15 LLMs shows memorization correlates with subject web presence and model scale. Provides a foundation for identifying memorized data for unlearning.

Conclusion: The work enables dynamic construction of forget sets for RTBF requests, addressing individual-level data inquiries in LLMs.

Abstract: Large Language Models (LLMs) can memorize and reveal personal information,
raising concerns regarding compliance with the EU's GDPR, particularly the
Right to Be Forgotten (RTBF). Existing machine unlearning methods assume the
data to forget is already known but do not address how to identify which
individual-fact associations are stored in the model. Privacy auditing
techniques typically operate at the population level or target a small set of
identifiers, limiting applicability to individual-level data inquiries. We
introduce WikiMem, a dataset of over 5,000 natural language canaries covering
243 human-related properties from Wikidata, and a model-agnostic metric to
quantify human-fact associations in LLMs. Our approach ranks ground-truth
values against counterfactuals using calibrated negative log-likelihood across
paraphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B
parameters), showing that memorization correlates with subject web presence and
model scale. We provide a foundation for identifying memorized personal data in
LLMs at the individual level, enabling the dynamic construction of forget sets
for machine unlearning and RTBF requests.

</details>


### [213] [Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding](https://arxiv.org/abs/2507.11198)
*Conrad Borchers,Bahar Shahrokhian,Francesco Balzan,Elham Tajik,Sreecharan Sankaranarayanan,Sebastian Simon*

Main category: cs.CL

TL;DR: The study explores how multi-agent systems (MAS) with varied personas and temperature settings affect consensus-building and coding accuracy in LLMs, finding limited benefits over single-agent coding.


<details>
  <summary>Details</summary>
Motivation: To understand the benefits of MAS over single-agent coding in qualitative research using LLMs, particularly how agent persona and temperature influence outcomes.

Method: Experimental study with six open-source LLMs (3B to 32B parameters) and 18 configurations, analyzing 77,000 coding decisions against human-annotated transcripts.

Result: Temperature and multiple personas delayed consensus but did not improve coding accuracy. Single agents often matched or outperformed MAS. Only one model (OpenHermesV2:7B) showed gains under specific conditions.

Conclusion: MAS with diverse personas does not consistently improve coding accuracy, challenging assumptions about their superiority. However, MAS may help refine ambiguous code applications.

Abstract: Large Language Models (LLMs) enable new possibilities for qualitative
research at scale, including coding and data annotation. While multi-agent
systems (MAS) can emulate human coding workflows, their benefits over
single-agent coding remain poorly understood. We conducted an experimental
study of how agent persona and temperature shape consensus-building and coding
accuracy of dialog segments based on a codebook with 8 codes. Our open-source
MAS mirrors deductive human coding through structured agent discussion and
consensus arbitration. Using six open-source LLMs (with 3 to 32 billion
parameters) and 18 experimental configurations, we analyze over 77,000 coding
decisions against a gold-standard dataset of human-annotated transcripts from
online math tutoring sessions. Temperature significantly impacted whether and
when consensus was reached across all six LLMs. MAS with multiple personas
(including neutral, assertive, or empathetic), significantly delayed consensus
in four out of six LLMs compared to uniform personas. In three of those LLMs,
higher temperatures significantly diminished the effects of multiple personas
on consensus. However, neither temperature nor persona pairing lead to robust
improvements in coding accuracy. Single agents matched or outperformed MAS
consensus in most conditions. Only one model (OpenHermesV2:7B) and code
category showed above-chance gains from MAS deliberation when temperature was
0.5 or lower and especially when the agents included at least one assertive
persona. Qualitative analysis of MAS collaboration for these configurations
suggests that MAS may nonetheless aid in narrowing ambiguous code applications
that could improve codebooks and human-AI coding. We contribute new insight
into the limits of LLM-based qualitative methods, challenging the notion that
diverse MAS personas lead to better outcomes. We open-source our MAS and
experimentation code.

</details>


### [214] [EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering](https://arxiv.org/abs/2507.11216)
*Valle Ruiz-Fernández,Mario Mina,Júlia Falcão,Luis Vasquez-Reina,Anna Sallés,Aitor Gonzalez-Agirre,Olatz Perez-de-Viñaspre*

Main category: cs.CL

TL;DR: The paper introduces Spanish and Catalan bias benchmarks (EsBBQ and CaBBQ) to evaluate social biases in LLMs for non-English languages and non-US contexts, showing models often rely on biases in ambiguous scenarios.


<details>
  <summary>Details</summary>
Motivation: Address the lack of resources for evaluating social biases in languages other than English and contexts outside the US.

Method: Develop parallel datasets (EsBBQ and CaBBQ) based on BBQ, assessing bias in 10 categories via multiple-choice QA, adapted to Spanish/Catalan and Spain's social context.

Result: LLMs often fail in ambiguous scenarios, with high QA accuracy linked to greater reliance on social biases.

Conclusion: The benchmarks highlight the need for bias evaluation in diverse languages and contexts, revealing LLMs' tendency to perpetuate biases.

Abstract: Previous literature has largely shown that Large Language Models (LLMs)
perpetuate social biases learnt from their pre-training data. Given the notable
lack of resources for social bias evaluation in languages other than English,
and for social contexts outside of the United States, this paper introduces the
Spanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and
CaBBQ). Based on the original BBQ, these two parallel datasets are designed to
assess social bias across 10 categories using a multiple-choice QA setting, now
adapted to the Spanish and Catalan languages and to the social context of
Spain. We report evaluation results on different LLMs, factoring in model
family, size and variant. Our results show that models tend to fail to choose
the correct answer in ambiguous scenarios, and that high QA accuracy often
correlates with greater reliance on social biases.

</details>


### [215] [An Agentic Flow for Finite State Machine Extraction using Prompt Chaining](https://arxiv.org/abs/2507.11222)
*Fares Wael,Youssef Maklad,Ali Hamdi,Wael Elsersy*

Main category: cs.CL

TL;DR: FlowFSM uses LLMs with prompt chaining to extract accurate FSMs from RFC documents, outperforming existing methods in precision and scalability.


<details>
  <summary>Details</summary>
Motivation: Existing FSM extraction techniques are limited by scalability, incomplete coverage, and ambiguity in natural language specifications.

Method: FlowFSM combines LLMs, prompt chaining, and chain-of-thought reasoning to systematically process RFCs and construct structured FSMs.

Result: FlowFSM achieves high precision in FSM extraction for FTP and RTSP protocols, minimizing hallucinated transitions.

Conclusion: Agent-based LLM systems like FlowFSM show promise for advancing protocol analysis and cybersecurity applications.

Abstract: Finite-State Machines (FSMs) are critical for modeling the operational logic
of network protocols, enabling verification, analysis, and vulnerability
discovery. However, existing FSM extraction techniques face limitations such as
scalability, incomplete coverage, and ambiguity in natural language
specifications. In this paper, we propose FlowFSM, a novel agentic framework
that leverages Large Language Models (LLMs) combined with prompt chaining and
chain-of-thought reasoning to extract accurate FSMs from raw RFC documents.
FlowFSM systematically processes protocol specifications, identifies state
transitions, and constructs structured rule-books by chaining agent outputs.
Experimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM
achieves high extraction precision while minimizing hallucinated transitions,
showing promising results. Our findings highlight the potential of agent-based
LLM systems in the advancement of protocol analysis and FSM inference for
cybersecurity and reverse engineering applications.

</details>


### [216] [Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages](https://arxiv.org/abs/2507.11230)
*Lyzander Marciano Andrylie,Inaya Rahmanisa,Mahardika Krisna Ihsani,Alfan Farizki Wicaksono,Haryo Akbarianto Wibowo,Alham Fikri Aji*

Main category: cs.CL

TL;DR: The paper explores language-specific features in multilingual LLMs using sparse autoencoders (SAEs) and introduces SAE-LAPE to identify these features, revealing their impact on model performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Understanding how LLMs process multiple languages is challenging due to polysemantic neurons, and existing studies lack focus on language-specific features.

Method: The authors use sparse autoencoders (SAEs) and introduce SAE-LAPE, a method based on feature activation probability, to identify language-specific features in LLMs.

Result: Language-specific features are found in middle to final layers, are interpretable, and influence multilingual performance. They also enable language identification comparable to fastText but with more interpretability.

Conclusion: SAE-LAPE effectively identifies language-specific features in LLMs, enhancing understanding of multilingual mechanisms and offering interpretable insights.

Abstract: Understanding the multilingual mechanisms of large language models (LLMs)
provides insight into how they process different languages, yet this remains
challenging. Existing studies often focus on individual neurons, but their
polysemantic nature makes it difficult to isolate language-specific units from
cross-lingual representations. To address this, we explore sparse autoencoders
(SAEs) for their ability to learn monosemantic features that represent concrete
and abstract concepts across languages in LLMs. While some of these features
are language-independent, the presence of language-specific features remains
underexplored. In this work, we introduce SAE-LAPE, a method based on feature
activation probability, to identify language-specific features within the
feed-forward network. We find that many such features predominantly appear in
the middle to final layers of the model and are interpretable. These features
influence the model's multilingual performance and language output and can be
used for language identification with performance comparable to fastText along
with more interpretability. Our code is available at
https://github.com/LyzanderAndrylie/language-specific-features .

</details>


### [217] [KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding](https://arxiv.org/abs/2507.11273)
*Luohe Shi,Zuchao Li,Lefei Zhang,Guoming Liu,Baoyuan Qi,Hai Zhao*

Main category: cs.CL

TL;DR: KV-Latent reduces Key-Value cache footprint in LLMs by down-sampling dimensions into latent space, improving efficiency with minimal extra training.


<details>
  <summary>Details</summary>
Motivation: The increasing Key-Value cache during inference in Transformer Decoders causes memory and bandwidth inefficiencies.

Method: Proposes KV-Latent, down-sampling KV vectors into latent space, and modifies Rotary Positional Embedding for stability.

Result: Significantly reduces KV cache footprint and improves inference speed with less than 1% extra training.

Conclusion: KV-Latent enables more efficient LLMs, offering new possibilities for KV cache optimization.

Abstract: Large language models (LLMs) based on Transformer Decoders have become the
preferred choice for conversational generative AI. Despite the overall
superiority of the Decoder architecture, the gradually increasing Key-Value
(KV) cache during inference has emerged as a primary efficiency bottleneck,
both in aspects of memory consumption and data transfer bandwidth limitations.
To address these challenges, we propose a paradigm called KV-Latent. By
down-sampling the Key-Value vector dimensions into a latent space, we can
significantly reduce the KV Cache footprint and improve inference speed, only
with a small amount of extra training, less than 1\% of pre-training takes.
Besides, we enhanced the stability of Rotary Positional Embedding applied on
lower-dimensional vectors by modifying its frequency sampling mechanism,
avoiding noise introduced by higher frequencies while retaining position
attenuation. Our experiments, including both models with Grouped Query
Attention and those without, have yielded satisfactory results. Finally, we
conducted comparative experiments to study the impact of separately reducing
Key and Value components on model's performance. Our approach allows for the
construction of more efficient language model systems, and opens the new
possibility on KV Cache saving and efficient LLMs. Our code is available at
https://github.com/ShiLuohe/KV-Latent.

</details>


### [218] [FMC: Formalization of Natural Language Mathematical Competition Problems](https://arxiv.org/abs/2507.11275)
*Jiaxuan Xie,Chengwu Liu,Ye Yuan,Siqi Li,Zhiping Xiao,Ming Zhang*

Main category: cs.CL

TL;DR: The paper introduces an autoformalization pipeline using large language models with error feedback to create a high-quality Olympiad-level dataset of formalized mathematical problems. It demonstrates the effectiveness of few-shot learning and error feedback, and validates the dataset as a challenging benchmark for automated theorem provers.


<details>
  <summary>Details</summary>
Motivation: Advancing formal mathematical reasoning by developing efficient and accurate autoformalization methods, leveraging large-scale datasets of natural language mathematical problems.

Method: Proposes an autoformalization pipeline based on large language models with error feedback, creating a dataset of natural language problems and Lean formalizations. Investigates LLM capabilities and enhances the process with few-shot learning and error feedback.

Result: Curated a dataset of 3,922 natural language and 9,787 Lean problems, with 64.46% assessed as above-average quality. Demonstrated improved autoformalization with few-shot learning and error feedback.

Conclusion: The dataset serves as a valuable benchmark for automated theorem provers, and the proposed pipeline enhances autoformalization, contributing to formal reasoning advancements.

Abstract: Efficient and accurate autoformalization methods, which leverage large-scale
datasets of extensive natural language mathematical problems to construct
formal language datasets, are key to advancing formal mathematical reasoning.
In this paper, we propose an autoformalization pipeline based on large language
models with error feedback, achieving a fully automatic and training-free
formalization approach. Using this pipeline, we curate an Olympiad-level
dataset aligning natural language problems with Lean formalizations. The
dataset comprises $3,922$ mathematical problems in natural language and $9,787$
in Lean, of which $64.46\%$ were assessed as at least above-average quality,
making it suitable as a benchmark for automated theorem provers. Additionally,
we investigate the formalization and reasoning capabilities of various LLMs and
empirically demonstrate that few-shot learning, error feedback, and increasing
sampling numbers enhance the autoformalization process. Experiments of three
automated theorem provers on the \dataset\ dataset also highlight its
challenging nature and its value as a benchmark for formal reasoning tasks.

</details>


### [219] [Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources, Coded Term Lexicon, and Enhanced Detection Frameworks](https://arxiv.org/abs/2507.11292)
*Zewen Bai,Liang Yang,Shengdi Yin,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: The paper addresses gaps in Chinese hate speech detection by introducing a span-level dataset (STATE ToxiCN), studying coded hate terms, and proposing a lexicon-integration method to improve detection and interpretability.


<details>
  <summary>Details</summary>
Motivation: The proliferation of hate speech and the lack of research on Chinese hate speech detection, especially regarding interpretability and coded hate terms, motivated this study.

Method: The authors introduced the STATE ToxiCN dataset, studied coded hate terms and LLMs' interpretability, and proposed a lexicon-integration method for models.

Result: The work provided a valuable dataset, insights into coded hate terms, and a method to enhance hate speech detection performance.

Conclusion: The study advances the interpretability of Chinese hate speech detection by addressing dataset scarcity and coded hate term challenges.

Abstract: The proliferation of hate speech has inflicted significant societal harm,
with its intensity and directionality closely tied to specific targets and
arguments. In recent years, numerous machine learning-based methods have been
developed to detect hateful comments on online platforms automatically.
However, research on Chinese hate speech detection lags behind, and
interpretability studies face two major challenges: first, the scarcity of
span-level fine-grained annotated datasets limits models' deep semantic
understanding of hate speech; second, insufficient research on identifying and
interpreting coded hate speech restricts model explainability in complex
real-world scenarios. To address these, we make the following contributions:
(1) We introduce the Span-level Target-Aware Toxicity Extraction dataset (STATE
ToxiCN), the first span-level Chinese hate speech dataset, and evaluate the
hate semantic understanding of existing models using it. (2) We conduct the
first comprehensive study on Chinese coded hate terms, LLMs' ability to
interpret hate semantics. (3) We propose a method to integrate an annotated
lexicon into models, significantly enhancing hate speech detection performance.
Our work provides valuable resources and insights to advance the
interpretability of Chinese hate speech detection research.

</details>


### [220] [Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian](https://arxiv.org/abs/2507.11299)
*Andrei Niculae,Adrian Cosma,Cosmin Dumitrache,Emilian Rǎdoi*

Main category: cs.CL

TL;DR: Dr.Copilot is a multi-agent LLM system for Romanian-speaking doctors, improving the presentation quality of their telemedicine responses via feedback on 17 axes, not clinical accuracy.


<details>
  <summary>Details</summary>
Motivation: Text-based telemedicine often prioritizes communication style over clinical accuracy, necessitating tools to enhance response quality.

Method: Uses three LLM agents with DSPy-optimized prompts, designed for low-resource Romanian data and open-weight models.

Result: Empirical evaluations and live deployment with 41 doctors show improved user reviews and response quality.

Conclusion: Dr.Copilot successfully enhances telemedicine interactions, marking an early real-world LLM deployment in Romanian healthcare.

Abstract: Text-based telemedicine has become increasingly common, yet the quality of
medical advice in doctor-patient interactions is often judged more on how
advice is communicated rather than its clinical accuracy. To address this, we
introduce Dr.Copilot , a multi-agent large language model (LLM) system that
supports Romanian-speaking doctors by evaluating and enhancing the presentation
quality of their written responses. Rather than assessing medical correctness,
Dr.Copilot provides feedback along 17 interpretable axes. The system comprises
of three LLM agents with prompts automatically optimized via DSPy. Designed
with low-resource Romanian data and deployed using open-weight models, it
delivers real-time specific feedback to doctors within a telemedicine platform.
Empirical evaluations and live deployment with 41 doctors show measurable
improvements in user reviews and response quality, marking one of the first
real-world deployments of LLMs in Romanian medical settings.

</details>


### [221] [Internal Value Alignment in Large Language Models through Controlled Value Vector Activation](https://arxiv.org/abs/2507.11316)
*Haoran Jin,Meng Li,Xiting Wang,Zhihao Xu,Minlie Huang,Yantao Jia,Defu Lian*

Main category: cs.CL

TL;DR: The paper introduces ConVA, a method to align LLMs with human values by controlling latent value vectors, ensuring consistency without harming performance.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs with human values is crucial for clarity, transparency, and adaptability in evolving scenarios.

Method: Proposes Controlled Value Vector Activation (ConVA) with context-controlled value vector identification and gated activation for minimal performance impact.

Result: Achieves highest control success rate across 10 values, maintains LLM performance, and resists malicious inputs.

Conclusion: ConVA effectively aligns LLMs with human values while preserving model functionality and robustness.

Abstract: Aligning Large Language Models (LLMs) with human values has attracted
increasing attention since it provides clarity, transparency, and the ability
to adapt to evolving scenarios. In this paper, we introduce a Controlled Value
Vector Activation (ConVA) method that directly aligns the internal values of
LLMs by interpreting how a value is encoded in their latent representations and
modifies relevant activations to ensure consistent values in LLMs. To ensure an
accurate and unbiased interpretation, we propose a context-controlled value
vector identification method. To consistently control values without
sacrificing model performance, we introduce a gated value vector activation
method for effective and minimum degree of value control. Experiments show that
our method achieves the highest control success rate across 10 basic values
without hurting LLM performance and fluency, and ensures target values even
with opposite and potentially malicious input prompts. Source code and data are
available at~ https://github.com/hr-jin/ConVA.

</details>


### [222] [Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge](https://arxiv.org/abs/2507.11330)
*Wenqing Wu,Chengzhi Zhang,Yi Zhao*

Main category: cs.CL

TL;DR: The paper proposes a hybrid approach combining human expertise and LLM knowledge to assess novelty in academic papers, focusing on method novelty. It fine-tunes PLMs using peer review reports and LLM summaries, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: Current novelty assessment methods (expert judgment or reference combinations) are limited. Experts lack comprehensive knowledge, and reference-based methods are uncertain. The paper aims to integrate LLM's knowledge and human judgment for better novelty evaluation.

Method: Extracts novelty-related sentences from peer reviews and uses LLM to summarize methodology sections. These are used to fine-tune PLMs. A text-guided fusion module with Sparse-Attention integrates human and LLM knowledge.

Result: The proposed method outperforms baselines in predicting method novelty, demonstrating superior performance in experiments.

Conclusion: Combining human expertise and LLM knowledge effectively addresses limitations in novelty assessment, improving accuracy in evaluating academic papers.

Abstract: Novelty is a crucial criterion in the peer review process for evaluating
academic papers. Traditionally, it's judged by experts or measure by unique
reference combinations. Both methods have limitations: experts have limited
knowledge, and the effectiveness of the combination method is uncertain.
Moreover, it's unclear if unique citations truly measure novelty. The large
language model (LLM) possesses a wealth of knowledge, while human experts
possess judgment abilities that the LLM does not possess. Therefore, our
research integrates the knowledge and abilities of LLM and human experts to
address the limitations of novelty assessment. The most common novelty in
academic papers is the introduction of new methods. In this paper, we propose
leveraging human knowledge and LLM to assist pretrained language models (PLMs,
e.g. BERT etc.) in predicting the method novelty of papers. Specifically, we
extract sentences related to the novelty of the academic paper from peer review
reports and use LLM to summarize the methodology section of the academic paper,
which are then used to fine-tune PLMs. In addition, we have designed a
text-guided fusion module with novel Sparse-Attention to better integrate human
and LLM knowledge. We compared the method we proposed with a large number of
baselines. Extensive experiments demonstrate that our method achieves superior
performance.

</details>


### [223] [What is the Best Process Model Representation? A Comparative Analysis for Process Modeling with Large Language Models](https://arxiv.org/abs/2507.11356)
*Alexis Brissard,Frédéric Cuppens,Amal Zouaq*

Main category: cs.CL

TL;DR: This paper evaluates various Process Model Representations (PMRs) for Large Language Model (LLM)-based Process Modeling (PMo), introducing a dataset and comparing PMRs on suitability and performance.


<details>
  <summary>Details</summary>
Motivation: The lack of systematic comparison among PMRs and inconsistent evaluation strategies in Process Model Generation (PMG) motivated this study.

Method: The study introduces the PMo Dataset (55 process descriptions paired with models in nine PMRs) and evaluates PMRs on suitability for LLM-based PMo and PMG performance.

Result: Mermaid scored highest overall for PMo, while BPMN text performed best in PMG for process element similarity.

Conclusion: The study provides empirical insights into PMR performance, aiding future LLM-based PMo and PMG research.

Abstract: Large Language Models (LLMs) are increasingly applied for Process Modeling
(PMo) tasks such as Process Model Generation (PMG). To support these tasks,
researchers have introduced a variety of Process Model Representations (PMRs)
that serve as model abstractions or generation targets. However, these PMRs
differ widely in structure, complexity, and usability, and have never been
systematically compared. Moreover, recent PMG approaches rely on distinct
evaluation strategies and generation techniques, making comparison difficult.
This paper presents the first empirical study that evaluates multiple PMRs in
the context of PMo with LLMs. We introduce the PMo Dataset, a new dataset
containing 55 process descriptions paired with models in nine different PMRs.
We evaluate PMRs along two dimensions: suitability for LLM-based PMo and
performance on PMG. \textit{Mermaid} achieves the highest overall score across
six PMo criteria, whereas \textit{BPMN text} delivers the best PMG results in
terms of process element similarity.

</details>


### [224] [Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection with Weighted Loss](https://arxiv.org/abs/2507.11384)
*Xia Cui*

Main category: cs.CL

TL;DR: A weighted loss function improves Transformer models for multi-label emotion detection, enhancing high-frequency classes but struggling with minority classes.


<details>
  <summary>Details</summary>
Motivation: Address data imbalance in multi-label emotion detection without the computational cost of traditional resampling methods.

Method: Apply a weighted loss function to BERT, RoBERTa, and BART models, evaluated on the BRIGHTER dataset using metrics like Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity.

Result: Improved performance on high-frequency emotion classes, but limited impact on minority classes.

Conclusion: The weighted loss function is effective but faces challenges with minority classes in imbalanced multi-label emotion detection.

Abstract: This paper explores the application of a simple weighted loss function to
Transformer-based models for multi-label emotion detection in SemEval-2025
Shared Task 11. Our approach addresses data imbalance by dynamically adjusting
class weights, thereby enhancing performance on minority emotion classes
without the computational burden of traditional resampling methods. We evaluate
BERT, RoBERTa, and BART on the BRIGHTER dataset, using evaluation metrics such
as Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity coefficients.
The results demonstrate that the weighted loss function improves performance on
high-frequency emotion classes but shows limited impact on minority classes.
These findings underscore both the effectiveness and the challenges of applying
this approach to imbalanced multi-label emotion detection.

</details>


### [225] [DCR: Quantifying Data Contamination in LLMs Evaluation](https://arxiv.org/abs/2507.11405)
*Cheng Xu,Nan Yan,Shuhao Guan,Changhong Jin,Yuke Mei,Yibing Guo,M-Tahar Kechadi*

Main category: cs.CL

TL;DR: The paper introduces the Data Contamination Risk (DCR) framework to detect and quantify benchmark data contamination in LLMs, adjusting performance metrics for fairer comparisons.


<details>
  <summary>Details</summary>
Motivation: Concerns about benchmark data contamination (BDC) in LLMs inflating performance metrics and undermining genuine generalization assessment.

Method: DCR framework detects BDC at four granular levels (semantic, informational, data, label) and synthesizes contamination scores via a fuzzy inference system to produce a unified DCR Factor.

Result: Validated on 9 LLMs, DCR reliably diagnoses contamination severity and adjusts accuracy to within 4% average error compared to uncontaminated baselines.

Conclusion: DCR offers a lightweight, interpretable tool for routine contamination assessment, improving LLM benchmarking credibility.

Abstract: The rapid advancement of large language models (LLMs) has heightened concerns
about benchmark data contamination (BDC), where models inadvertently memorize
evaluation data, inflating performance metrics and undermining genuine
generalization assessment. This paper introduces the Data Contamination Risk
(DCR) framework, a lightweight, interpretable pipeline designed to detect and
quantify BDC across four granular levels: semantic, informational, data, and
label. By synthesizing contamination scores via a fuzzy inference system, DCR
produces a unified DCR Factor that adjusts raw accuracy to reflect
contamination-aware performance. Validated on 9 LLMs (0.5B-72B) across
sentiment analysis, fake news detection, and arithmetic reasoning tasks, the
DCR framework reliably diagnoses contamination severity and with accuracy
adjusted using the DCR Factor to within 4% average error across the three
benchmarks compared to the uncontaminated baseline. Emphasizing computational
efficiency and transparency, DCR provides a practical tool for integrating
contamination assessment into routine evaluations, fostering fairer comparisons
and enhancing the credibility of LLM benchmarking practices.

</details>


### [226] [EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes](https://arxiv.org/abs/2507.11407)
*LG AI Research,:,Kyunghoon Bae,Eunbi Choi,Kibong Choi,Stanley Jungkyu Choi,Yemuk Choi,Kyubeen Han,Seokhee Hong,Junwon Hwang,Taewan Hwang,Joonwon Jang,Hyojin Jeon,Kijeong Jeon,Gerrard Jeongwon Jo,Hyunjik Jo,Jiyeon Jung,Euisoon Kim,Hyosang Kim,Jihoon Kim,Joonkee Kim,Seonghwan Kim,Soyeon Kim,Sunkyoung Kim,Yireun Kim,Yongil Kim,Youchul Kim,Edward Hwayoung Lee,Gwangho Lee,Haeju Lee,Honglak Lee,Jinsik Lee,Kyungmin Lee,Sangha Park,Young Min Paik,Yongmin Park,Youngyong Park,Sanghyun Seo,Sihoon Yang,Heuiyeen Yeen,Sihyuk Yi,Hyeongu Yun*

Main category: cs.CL

TL;DR: EXAONE 4.0 integrates Non-reasoning and Reasoning modes, enhances multilingual support (English, Korean, Spanish), and offers two model sizes (32B and 1.2B) for high performance and on-device use. It outperforms open-weight models and competes with frontier-class models.


<details>
  <summary>Details</summary>
Motivation: To combine usability (EXAONE 3.5) and advanced reasoning (EXAONE Deep) while preparing for the agentic AI era with features like tool use and multilingual support.

Method: Incorporates Non-reasoning and Reasoning modes, extends multilingual capabilities, and introduces two model sizes (32B for performance, 1.2B for on-device).

Result: Superior performance compared to open-weight models and competitive against frontier-class models.

Conclusion: EXAONE 4.0 is a versatile, high-performing model series publicly available for research.

Abstract: This technical report introduces EXAONE 4.0, which integrates a Non-reasoning
mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5
and the advanced reasoning abilities of EXAONE Deep. To pave the way for the
agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool
use, and its multilingual capabilities are extended to support Spanish in
addition to English and Korean. The EXAONE 4.0 model series consists of two
sizes: a mid-size 32B model optimized for high performance, and a small-size
1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates
superior performance compared to open-weight models in its class and remains
competitive even against frontier-class models. The models are publicly
available for research purposes and can be easily downloaded via
https://huggingface.co/LGAI-EXAONE.

</details>


### [227] [KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?](https://arxiv.org/abs/2507.11408)
*Soumadeep Saha,Akshay Chaturvedi,Saptarshi Saha,Utpal Garain,Nicholas Asher*

Main category: cs.CL

TL;DR: The paper introduces Causal CoT Graphs (CCGs) to analyze chain-of-thought reasoning in LLMs, showing they mediate answers and align with model reasoning paths.


<details>
  <summary>Details</summary>
Motivation: To understand how chain-of-thought traces improve LLM performance in reasoning tasks.

Method: Introduces CCGs, directed acyclic graphs extracted from reasoning traces, and analyzes them using the KisMATH dataset (1671 problems).

Result: CCG nodes mediate final answers, and LLMs emphasize CCG-aligned reasoning paths.

Conclusion: KisMATH enables controlled interventions and further study of chain-of-thought in LLM reasoning.

Abstract: Chain-of-thought traces have been shown to improve performance of large
language models in a plethora of reasoning tasks, yet there is no consensus on
the mechanism through which this performance boost is achieved. To shed more
light on this, we introduce Causal CoT Graphs (CCGs), which are directed
acyclic graphs automatically extracted from reasoning traces that model
fine-grained causal dependencies in the language model output. A collection of
$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their
associated CCGs are compiled into our dataset -- \textbf{KisMATH}. Our detailed
empirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in
the CCG are mediators for the final answer, a condition necessary for
reasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating
that models internally realise structures akin to our graphs. KisMATH enables
controlled, graph-aligned interventions and opens up avenues for further
investigation into the role of chain-of-thought in LLM reasoning.

</details>


### [228] [Seq vs Seq: An Open Suite of Paired Encoders and Decoders](https://arxiv.org/abs/2507.11412)
*Orion Weller,Kathryn Ricci,Marc Marone,Antoine Chaffin,Dawn Lawrie,Benjamin Van Durme*

Main category: cs.CL

TL;DR: The paper introduces the Ettin suite of models, comparing encoder-only and decoder-only architectures fairly by using identical training setups. It shows their respective strengths in classification/retrieval and generative tasks, and highlights the limitations of adapting one for the other.


<details>
  <summary>Details</summary>
Motivation: To address the lack of fair comparisons between encoder-only and decoder-only models due to varying parameters, training, and datasets.

Method: Developed the Ettin suite with paired encoder-only and decoder-only models (17M to 1B parameters, trained on up to 2T tokens) using identical training recipes.

Result: Encoder-only models outperform in classification/retrieval, while decoder-only models excel in generative tasks. Adapting one for the other is less effective.

Conclusion: Specialized architectures (encoder or decoder) are superior for their respective tasks, and adapting them is suboptimal. All artifacts are open-sourced for further research.

Abstract: The large language model (LLM) community focuses almost exclusively on
decoder-only language models, since they are easier to use for text generation.
However, a large subset of the community still uses encoder-only models for
tasks such as classification or retrieval. Previous work has attempted to
compare these architectures, but is forced to make comparisons with models that
have different numbers of parameters, training techniques, and datasets. We
introduce the SOTA open-data Ettin suite of models: paired encoder-only and
decoder-only models ranging from 17 million parameters to 1 billion, trained on
up to 2 trillion tokens. Using the same recipe for both encoder-only and
decoder-only models produces SOTA recipes in both categories for their
respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as
decoders. Like previous work, we find that encoder-only models excel at
classification and retrieval tasks while decoders excel at generative tasks.
However, we show that adapting a decoder model to encoder tasks (and vice
versa) through continued training is subpar compared to using only the reverse
objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa
for generative tasks). We open-source all artifacts of this study including
training data, training order segmented by checkpoint, and 200+ checkpoints to
allow future work to analyze or extend all aspects of training.

</details>


### [229] [Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?](https://arxiv.org/abs/2507.11423)
*Yanjian Zhang,Guillaume Wisniewski,Nadi Tomeh,Thierry Charnois*

Main category: cs.CL

TL;DR: Prompting can influence LLMs' reasoning strategies, but no single strategy consistently improves accuracy. Adaptive strategy selection may enhance performance.


<details>
  <summary>Details</summary>
Motivation: To explore if prompting can control LLMs' reasoning strategies and improve their problem-solving effectiveness.

Method: Investigating prompting effects on LLMs' reasoning strategies and proposing adaptive strategy selection methods.

Result: No single strategy consistently improves accuracy, but adaptive selection shows potential.

Conclusion: Guiding LLMs in strategy selection can refine their reasoning abilities, suggesting new research directions.

Abstract: Human reasoning involves different strategies, each suited to specific
problems. Prior work shows that large language model (LLMs) tend to favor a
single reasoning strategy, potentially limiting their effectiveness in diverse
reasoning challenges. In this work, we investigate whether prompting can
control LLMs reasoning strategies and assess its impact on logical
problem-solving. While our experiments show that no single strategy
consistently improves accuracy, performance could be enhanced if models could
adaptively choose the optimal strategy. We propose methods to guide LLMs in
strategy selection, highlighting new ways to refine their reasoning abilities.

</details>


### [230] [HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong](https://arxiv.org/abs/2507.11502)
*Sirui Han,Junqi Zhu,Ruiyuan Zhang,Yike Guo*

Main category: cs.CL

TL;DR: Development of HKGAI-V1, a sovereign LLM for Hong Kong, tailored to its multilingual, socio-legal, and cultural context, outperforming general models in local queries and embedding governance for digital sovereignty.


<details>
  <summary>Details</summary>
Motivation: To create a value-aligned AI infrastructure for Hong Kong, addressing its unique multilingual, socio-legal, and cultural needs under the 'one country, two systems' framework.

Method: Built on DeepSeek architecture, fine-tuned for regional norms, and integrated with a RAG system for factual accuracy. Includes a proprietary benchmark for ethical alignment.

Result: HKGAI-V1 outperforms general models in culturally sensitive queries and provides a governance-embedded approach for critical sectors.

Conclusion: The paper offers a replicable blueprint for regionally focused AI systems, combining technological innovation with local identity alignment.

Abstract: This paper presents the development of HKGAI-V1, a foundational sovereign
large language model (LLM), developed as part of an initiative to establish
value-aligned AI infrastructure specifically tailored for Hong Kong. Addressing
the region's unique multilingual environment (Cantonese, Mandarin, and
English), its distinct socio-legal context under the "one country, two systems"
framework, and specific local cultural and value considerations, the model is
built upon the DeepSeek architecture and systematically aligned with regional
norms through a multifaceted full parameter fine-tuning process. It is further
integrated with a retrieval-augmented generation (RAG) system to ensure timely
and factually grounded information access. The core contribution lies in the
design and implementation of a comprehensive, region-specific AI alignment and
safety framework, demonstrated through two key achievements: 1) The successful
development of HKGAI-V1 itself - which outper-forms general-purpose models in
handling Hong Kong-specific culturally sensitive queries, and embodies a
"governance-embedded" approach to digital sovereignty - empowers Hong Kong to
exercise control over AI applications in critical sectors including public
services, legal systems, and edu-cation. 2) The development of the proprietary
Adversarial HK Value Benchmark, a rigorous tool for evaluating model alignment
with local ethical and legal stand-ards under challenging conditions. By
documenting these achievements, the paper provides not only a technological
artifact but also a replicable blueprint for developing advanced, regionally
focused AI systems deeply rooted in their local identities.

</details>


### [231] [Real-World Summarization: When Evaluation Reaches Its Limits](https://arxiv.org/abs/2507.11508)
*Patrícia Schmidtová,Ondřej Dušek,Saad Mahamood*

Main category: cs.CL

TL;DR: Simpler metrics like word overlap correlate well with human judgments for evaluating faithfulness in LLM-generated hotel highlights, while LLMs prove unreliable for evaluation.


<details>
  <summary>Details</summary>
Motivation: To assess faithfulness of LLM-generated hotel highlights to input data and compare evaluation methods.

Method: Human evaluation campaigns with categorical error assessment and span-level annotation, comparing traditional metrics, trainable methods, and LLM-as-a-judge approaches.

Result: Word overlap metrics correlate well with human judgments (Spearman 0.63), outperforming complex methods on out-of-domain data. LLMs are unreliable evaluators, often under- or over-annotating.

Conclusion: Simpler metrics are effective for faithfulness evaluation, while LLMs are unreliable for this task. Incorrect or non-checkable information poses the highest risks in real-world applications.

Abstract: We examine evaluation of faithfulness to input data in the context of hotel
highlights: brief LLM-generated summaries that capture unique features of
accommodations. Through human evaluation campaigns involving categorical error
assessment and span-level annotation, we compare traditional metrics, trainable
methods, and LLM-as-a-judge approaches. Our findings reveal that simpler
metrics like word overlap correlate surprisingly well with human judgments
(Spearman correlation rank of 0.63), often outperforming more complex methods
when applied to out-of-domain data. We further demonstrate that while LLMs can
generate high-quality highlights, they prove unreliable for evaluation as they
tend to severely under- or over-annotate. Our analysis of real-world business
impacts shows incorrect and non-checkable information pose the greatest risks.
We also highlight challenges in crowdsourced evaluations.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [232] [Latent Space Consistency for Sparse-View CT Reconstruction](https://arxiv.org/abs/2507.11152)
*Duoyou Chen,Yunqing Chen,Can Zhang,Zhou Wang,Cheng Chen,Ruoxiu Xiao*

Main category: eess.IV

TL;DR: The paper introduces CLS-DM, a model for 3D CT reconstruction from sparse 2D X-ray images using cross-modal feature contrastive learning to align latent spaces, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency and high radiation exposure of traditional CT by leveraging sparse-view X-ray images and improving latent space alignment for better reconstruction.

Method: Proposes CLS-DM, incorporating cross-modal feature contrastive learning to align 2D X-ray and 3D CT latent spaces.

Result: CLS-DM outperforms classical and state-of-the-art models on LIDC-IDRI and CTSpine1K datasets in PSNR and SSIM metrics.

Conclusion: CLS-DM enhances sparse X-ray CT reconstruction and can generalize to other cross-modal tasks like text-to-image synthesis.

Abstract: Computed Tomography (CT) is a widely utilized imaging modality in clinical
settings. Using densely acquired rotational X-ray arrays, CT can capture 3D
spatial features. However, it is confronted with challenged such as significant
time consumption and high radiation exposure. CT reconstruction methods based
on sparse-view X-ray images have garnered substantial attention from
researchers as they present a means to mitigate costs and risks. In recent
years, diffusion models, particularly the Latent Diffusion Model (LDM), have
demonstrated promising potential in the domain of 3D CT reconstruction.
Nonetheless, due to the substantial differences between the 2D latent
representation of X-ray modalities and the 3D latent representation of CT
modalities, the vanilla LDM is incapable of achieving effective alignment
within the latent space. To address this issue, we propose the Consistent
Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature
contrastive learning to efficiently extract latent 3D information from 2D X-ray
images and achieve latent space alignment between modalities. Experimental
results indicate that CLS-DM outperforms classical and state-of-the-art
generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the
LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing
the effectiveness and economic viability of sparse X-ray reconstructed CT but
can also be generalized to other cross-modal transformation tasks, such as
text-to-image synthesis. We have made our code publicly available at
https://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research
and applications in other domains.

</details>


### [233] [3D Magnetic Inverse Routine for Single-Segment Magnetic Field Images](https://arxiv.org/abs/2507.11293)
*J. Senthilnath,Chen Hao,F. C. Wellstood*

Main category: eess.IV

TL;DR: A novel 3D Magnetic Inverse Routine (3D MIR) combines deep learning, physics constraints, and optimization to accurately recover 3D current flow parameters in semiconductor packaging.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D information recovery is essential for non-destructive testing to localize circuit defects in semiconductor packaging.

Method: The 3D MIR uses a CNN for initial predictions, spatial-physics constraints for parameter estimates, and optimization to refine parameters.

Result: The method achieves high precision in 3D information recovery, setting a new benchmark for magnetic image reconstruction.

Conclusion: Combining deep learning and physics-driven optimization shows great potential for practical applications in semiconductor testing.

Abstract: In semiconductor packaging, accurately recovering 3D information is crucial
for non-destructive testing (NDT) to localize circuit defects. This paper
presents a novel approach called the 3D Magnetic Inverse Routine (3D MIR),
which leverages Magnetic Field Images (MFI) to retrieve the parameters for the
3D current flow of a single-segment. The 3D MIR integrates a deep learning
(DL)-based Convolutional Neural Network (CNN), spatial-physics-based
constraints, and optimization techniques. The method operates in three stages:
i) The CNN model processes the MFI data to predict ($\ell/z_o$), where $\ell$
is the wire length and $z_o$ is the wire's vertical depth beneath the magnetic
sensors and classify segment type ($c$). ii) By leveraging
spatial-physics-based constraints, the routine provides initial estimates for
the position ($x_o$, $y_o$, $z_o$), length ($\ell$), current ($I$), and current
flow direction (positive or negative) of the current segment. iii) An optimizer
then adjusts these five parameters ($x_o$, $y_o$, $z_o$, $\ell$, $I$) to
minimize the difference between the reconstructed MFI and the actual MFI. The
results demonstrate that the 3D MIR method accurately recovers 3D information
with high precision, setting a new benchmark for magnetic image reconstruction
in semiconductor packaging. This method highlights the potential of combining
DL and physics-driven optimization in practical applications.

</details>


### [234] [HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging](https://arxiv.org/abs/2507.11325)
*Arefin Ittesafun Abian,Ripon Kumar Debnath,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Md Rafiqul Islam,Asif Karim,Reem E. Mohamed,Sami Azam*

Main category: eess.IV

TL;DR: HANS-Net, a novel segmentation framework, combines hyperbolic convolutions, wavelet-inspired decomposition, synaptic plasticity, and neural representation for accurate liver and tumor segmentation on CT images, achieving high performance and generalization.


<details>
  <summary>Details</summary>
Motivation: Accurate liver and tumor segmentation is crucial for diagnosis and treatment but is challenging due to complex anatomy, tumor variability, and limited annotated data.

Method: HANS-Net integrates hyperbolic convolutions, wavelet decomposition, synaptic plasticity, neural representation, uncertainty-aware dropout, and temporal attention for robust segmentation.

Result: Achieves Dice scores of 93.26% (LiTS) and 87.45% (3D-IRCADb-01), with strong generalization and anatomical consistency.

Conclusion: HANS-Net is effective and robust for liver and tumor segmentation, providing accurate and confident results.

Abstract: Accurate liver and tumor segmentation on abdominal CT images is critical for
reliable diagnosis and treatment planning, but remains challenging due to
complex anatomical structures, variability in tumor appearance, and limited
annotated data. To address these issues, we introduce Hyperbolic-convolutions
Adaptive-temporal-attention with Neural-representation and Synaptic-plasticity
Network (HANS-Net), a novel segmentation framework that synergistically
combines hyperbolic convolutions for hierarchical geometric representation, a
wavelet-inspired decomposition module for multi-scale texture learning, a
biologically motivated synaptic plasticity mechanism for adaptive feature
enhancement, and an implicit neural representation branch to model fine-grained
and continuous anatomical boundaries. Additionally, we incorporate
uncertainty-aware Monte Carlo dropout to quantify prediction confidence and
lightweight temporal attention to improve inter-slice consistency without
sacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate
that HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an
average symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap
error (VOE) of 11.91%. Furthermore, cross-dataset validation on the
3D-IRCADb-01 dataset obtains an average Dice of 87.45%, IoU of 80.30%, ASSD of
1.525 mm, and VOE of 19.71%, indicating strong generalization across different
datasets. These results confirm the effectiveness and robustness of HANS-Net in
providing anatomically consistent, accurate, and confident liver and tumor
segmentation.

</details>
